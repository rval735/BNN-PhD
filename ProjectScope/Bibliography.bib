%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for RH VT at 2021-01-11 21:16:36 +1300 


%% Saved with string encoding Unicode (UTF-8) 



@article{Feldmann:2021aa,
	abstract = {With the proliferation of ultrahigh-speed mobile networks and internet-connected devices, along with the rise of artificial intelligence (AI)1, the world is generating exponentially increasing amounts of data that need to be processed in a fast and efficient way. Highly parallelized, fast and scalable hardware is therefore becoming progressively more important2. Here we demonstrate a computationally specific integrated photonic hardware accelerator (tensor core) that is capable of operating at speeds of trillions of multiply-accumulate operations per second (1012 MAC operations per second or tera-MACs per second). The tensor core can be considered as the optical analogue of an application-specific integrated circuit (ASIC). It achieves parallelized photonic in-memory computing using phase-change-material memory arrays and photonic chip-based optical frequency combs (soliton microcombs3). The computation is reduced to measuring the optical transmission of reconfigurable and non-resonant passive components and can operate at a bandwidth exceeding 14 gigahertz, limited only by the speed of the modulators and photodetectors. Given recent advances in hybrid integration of soliton microcombs at microwave line rates3--5, ultralow-loss silicon nitride waveguides6,7, and high-speed on-chip detectors and modulators, our approach provides a path towards full complementary metal--oxide--semiconductor (CMOS) wafer-scale integration of the photonic tensor core. Although we focus on convolutional processing, more generally our results indicate the potential of integrated photonics for parallel, fast, and efficient computational hardware in data-heavy AI applications such as autonomous driving, live video processing, and next-generation cloud computing services.},
	author = {Feldmann, J. and Youngblood, N. and Karpov, M. and Gehring, H. and Li, X. and Stappers, M. and Le Gallo, M. and Fu, X. and Lukashchuk, A. and Raja, A. S. and Liu, J. and Wright, C. D. and Sebastian, A. and Kippenberg, T. J. and Pernice, W. H. P. and Bhaskaran, H.},
	da = {2021/01/01},
	date-added = {2021-01-11 21:16:02 +1300},
	date-modified = {2021-01-11 21:16:35 +1300},
	doi = {10.1038/s41586-020-03070-1},
	id = {Feldmann2021},
	isbn = {1476-4687},
	journal = {Nature},
	keywords = {DNN, Nature, Photonics, Silicon, CNN},
	number = {7840},
	pages = {52--58},
	title = {Parallel convolutional processing using an integrated photonic tensor core},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41586-020-03070-1},
	volume = {589},
	year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-020-03070-1}}

@article{Geirhos:2020aa,
	abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning's failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
	author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
	da = {2020/11/01},
	date-added = {2021-01-11 21:14:20 +1300},
	date-modified = {2021-01-11 21:14:40 +1300},
	doi = {10.1038/s42256-020-00257-z},
	id = {Geirhos2020},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	keywords = {DNN, Nature, Supervised learning},
	number = {11},
	pages = {665--673},
	title = {Shortcut learning in deep neural networks},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s42256-020-00257-z},
	volume = {2},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s42256-020-00257-z}}

@article{Schrittwieser:2020aa,
	abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
	author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
	da = {2020/12/01},
	date-added = {2021-01-11 21:07:11 +1300},
	date-modified = {2021-01-11 21:08:58 +1300},
	doi = {10.1038/s41586-020-03051-4},
	id = {Schrittwieser2020},
	isbn = {1476-4687},
	journal = {Nature},
	keywords = {DNN, Nature, RL, Atari},
	number = {7839},
	pages = {604--609},
	title = {Mastering Atari, Go, chess and shogi by planning with a learned model},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41586-020-03051-4},
	volume = {588},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41586-020-03051-4}}

@article{Weng:2020aa,
	abstract = {Analyzing scattered wave to recognize object is of fundamental significance in wave physics. Recently-emerged deep learning technique achieved great success in interpreting wave field such as in ultrasound non-destructive testing and disease diagnosis, but conventionally need time-consuming computer postprocessing or bulky-sized diffractive elements. Here we theoretically propose and experimentally demonstrate a purely-passive and small-footprint meta-neural-network for real-time recognizing complicated objects by analyzing acoustic scattering. We prove meta-neural-network mimics a standard neural network despite its compactness, thanks to unique capability of its metamaterial unit-cells (dubbed meta-neurons) to produce deep-subwavelength phase shift as training parameters. The resulting device exhibits the ``intelligence''to perform desired tasks with potential to overcome the current limitations, showcased by two distinctive examples of handwritten digit recognition and discerning misaligned orbital-angular-momentum vortices. Our mechanism opens the route to new metamaterial-based deep-learning paradigms and enable conceptual devices automatically analyzing signals, with far-reaching implications for acoustics and related fields.},
	author = {Weng, Jingkai and Ding, Yujiang and Hu, Chengbo and Zhu, Xue-Feng and Liang, Bin and Yang, Jing and Cheng, Jianchun},
	da = {2020/12/09},
	date-added = {2021-01-11 18:14:49 +1300},
	date-modified = {2021-01-11 18:15:17 +1300},
	doi = {10.1038/s41467-020-19693-x},
	id = {Weng2020},
	isbn = {2041-1723},
	journal = {Nature Communications},
	keywords = {DNN, Nature, Passive learning, Object recognition},
	number = {1},
	pages = {6309},
	title = {Meta-neural-network for real-time and passive deep-learning-based object recognition},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41467-020-19693-x},
	volume = {11},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-020-19693-x}}

@article{Babanezhad:2020aa,
	abstract = {Bubbly flow behavior simulation in two-phase chemical reactors such bubble column type reactors is widely employed for chemical industry purposes. The computational fluid dynamics (CFD) approach has been employed by engineers and researchers for modeling these types of chemical reactors. In spite of the CFD robustness for simulating transport phenomena and chemical reactions in these reactors, this approach has been known as expensive for modeling such turbulent complex flows. Artificial intelligence (AI) algorithm of the adaptive network-based fuzzy inference system (ANFIS) are largely understood and utilized for the CFD approach optimization. In this hybrid approach, the CFD findings are learned by AI algorithms like ANFIS to save computational time and expenses. Once the pattern of the CFD results have been captured by the AI model, this hybrid model can be then used for process simulation and optimization. As such, there is no need for further simulations of new conditions. The objective of this paper is to obviate the need for expensive CFD computations for two-phase flows in chemical reactors via coupling CFD data to an AI algorithm, i.e., differential evolution based fuzzy inference system (DEFIS). To do so, air velocity as the output and the values of the x, and y coordinates, water velocity, and time step as the inputs are inputted the AI model for learning the flow pattern. The effects of cross over as the DE parameter and also the number of inputs on the best intelligence are investigated. Indeed, DEFIS correlates the air velocity to the nodes coordinates, time, and liquid velocity and then after the CFD modeling could be replaced with the simple correlation. For the first time, a comparison is made between the ANFIS and the DEFIS performances in terms of the prediction capability of the gas (air) velocity. The results released that both ANFIS and DEFIS could accurately predict the CFD pattern. The prediction times of both methods were obtained to be equal. However, the learning time of the DEFIS was fourfold of ANFIS.},
	author = {Babanezhad, Meisam and Behroyan, Iman and Nakhjiri, Ali Taghvaie and Marjani, Azam and Rezakazemi, Mashallah and Shirazian, Saeed},
	da = {2020/12/04},
	date-added = {2021-01-11 18:09:24 +1300},
	date-modified = {2021-01-11 18:11:15 +1300},
	doi = {10.1038/s41598-020-78277-3},
	id = {Babanezhad2020},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	keywords = {Other, Nature, Differential Evolution, Fuzzy inference },
	number = {1},
	pages = {21304},
	title = {High-performance hybrid modeling chemical reactors using differential evolution based fuzzy inference system},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41598-020-78277-3},
	volume = {10},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41598-020-78277-3}}

@book{OpenCL-In-Action,
	author = {Matthew Scarpino},
	date-added = {2020-12-25 23:00:49 +1300},
	date-modified = {2020-12-25 23:04:13 +1300},
	keywords = {OpenCL, Book},
	month = {November},
	publisher = {Manning},
	title = {OpenCL In Action},
	url = {https://www.manning.com/books/opencl-in-action},
	year = {2011},
	Bdsk-Url-1 = {https://www.manning.com/books/opencl-in-action}}

@webpage{Xilinx-Dev-Accel,
	author = {Rob Armstrong},
	date-added = {2020-12-25 21:54:27 +1300},
	date-modified = {2020-12-25 21:55:42 +1300},
	keywords = {URL, Xilinx, Tutorial, Developer, FPGA},
	month = {Nov},
	title = {Get Moving with Alveo},
	url = {https://developer.xilinx.com/en/articles/acceleration-basics.html},
	year = {2019},
	Bdsk-Url-1 = {https://developer.xilinx.com/en/articles/acceleration-basics.html}}

@inproceedings{10.1145/3377929.3398164,
	abstract = {Spectrum-Diverse Neuroevolution with Unified Neural Models (SUNA) has been shown to be a successful alternative to the algorithm NeuroEvolution of Augmenting Topologies (NEAT). Requiring less parameters than NEAT yet possessing a more unified representation power and effective spectrum-based diversity preservation, SUNA outperformed NEAT on most of the problems to be experimented. However, we think a simple improvement approach can be made to improve SUNA's efficiency in the strategic decision-making problem tested by the model itself, i.e. the multiplexer problem. In the proposed method, we try to incorporate the idea of logical gates to the hidden neurons in the model, suggesting it the solutions that solve the problem in the real world in the form of neurons. It is shown that with the simple logic gates neuron variations, SUNA can be slightly enhanced to resolve the multiplexer problem.},
	address = {New York, NY, USA},
	author = {Ta, Anh Due and Vargas, Danilo Vasconcellos},
	booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
	date-added = {2020-12-21 17:11:46 +1300},
	date-modified = {2020-12-21 17:12:07 +1300},
	doi = {10.1145/3377929.3398164},
	isbn = {9781450371278},
	keywords = {Evolutionary, SUNA, Logic Gates, Multuplexer},
	location = {Canc\'{u}n, Mexico},
	numpages = {2},
	pages = {53--54},
	publisher = {Association for Computing Machinery},
	series = {GECCO '20},
	title = {Towards Improvement of SUNA in Multiplexers with Preliminary Results of Simple Logic Gate Neuron Variation},
	url = {https://doi.org/10.1145/3377929.3398164},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1145/3377929.3398164}}

@article{6772729,
	abstract = {The author was led to the study given in this paper from a consideration of large scale computing machines in which a large number of operations must be performed without a single error in the end result. This problem of ``doing things right'' on a large scale is not essentially new; in a telephone central office, for example, a very large number of operations are performed while the errors leading to wrong numbers are kept well under control, though they have not been completely eliminated. This has been achieved, in part, through the use of self-checking circuits. The occasional failure that escapes routine checking is still detected by the customer and will, if it persists, result in customer complaint, while if it is transient it will produce only occasional wrong numbers. At the same time the rest of the central office functions satisfactorily. In a digital computer, on the other hand, a single failure usually means the complete failure, in the sense that if it is detected no more computing can be done until the failure is located and corrected, while if it escapes detection then it invalidates all subsequent operations of the machine. Put in other words, in a telephone central office there are a number of parallel paths which are more or less independent of each other; in a digital machine there is usually a single long path which passes through the same piece of equipment many, many times before the answer is obtained.},
	author = {R. W. {Hamming}},
	date-added = {2020-12-20 22:21:09 +1300},
	date-modified = {2020-12-20 22:21:48 +1300},
	doi = {10.1002/j.1538-7305.1950.tb00463.x},
	issn = {0005-8580},
	journal = {The Bell System Technical Journal},
	keywords = {Other, Hamming Distance, Error detection},
	month = {April},
	number = {2},
	pages = {147-160},
	title = {Error detecting and error correcting codes},
	volume = {29},
	year = {1950},
	Bdsk-Url-1 = {https://doi.org/10.1002/j.1538-7305.1950.tb00463.x}}

@inproceedings{Kassahun07commongenetic,
	author = {Yohannes Kassahun and Gerald Sommer and Mark Edgington and Jan Hendrik Metzen and Frank Kirchner},
	booktitle = {In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO 2007},
	date-added = {2020-12-17 13:15:14 +1300},
	date-modified = {2020-12-17 13:17:27 +1300},
	keywords = {Evolutionary, Genetic Encoding, Genotype Phenotype Mapping},
	pages = {1029--1036},
	publisher = {ACM Press},
	title = {Common genetic encoding for both direct and indirect encodings of networks},
	year = {2007}}

@inproceedings{103390-app9204460,
	author = {Francesco Rundo},
	booktitle = {Journal of Applied Sciences},
	date-added = {2020-12-11 19:09:27 +1300},
	date-modified = {2020-12-11 19:12:56 +1300},
	keywords = {NN-Fin;STM32; financial; deep learning; LSTM; reinforcement learning},
	month = {October},
	title = {Deep LSTM with Reinforcement Learning Layer for Financial Trend Prediction in FX High Frequency Trading Systems},
	url = {https://www.mdpi.com/2076-3417/9/20/4460},
	volume = {9},
	year = {2019},
	Bdsk-Url-1 = {https://www.mdpi.com/2076-3417/9/20/4460}}

@url{Schneier-Crypto,
	author = {Bruce Schneier},
	date-added = {2020-12-09 21:56:38 +1300},
	date-modified = {2020-12-09 21:57:41 +1300},
	keywords = {URL, Cryptoanalisys, Cryptography},
	title = {Memo to the Amateur Cipher Designer},
	url = {https://www.schneier.com/crypto-gram/archives/1998/1015.html#cipherdesign},
	year = {1998},
	Bdsk-Url-1 = {https://www.schneier.com/crypto-gram/archives/1998/1015.html#cipherdesign}}

@electronic{Fast-STS,
	author = {Zdenek {\v R}{\'\i}ha, Marek S{\'y}s},
	date-added = {2020-12-09 21:53:59 +1300},
	date-modified = {2020-12-09 21:55:29 +1300},
	keywords = {URL, STS, NIST, Randomness},
	title = {Faster randomness testing},
	url = {https://randomness-tests.fi.muni.cz/},
	year = {2014},
	Bdsk-Url-1 = {https://randomness-tests.fi.muni.cz/}}

@webpage{gpipe,
	author = {Yanping Huang},
	date-added = {2020-12-09 21:50:35 +1300},
	date-modified = {2020-12-09 21:52:00 +1300},
	keywords = {URL, NN, Training, DNN},
	title = {Introducing GPipe, an Open Source Library for Efficiently Training Large-scale Neural Network Models},
	url = {https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html},
	year = {2019},
	Bdsk-Url-1 = {https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html}}

@article{10.2307/169420,
	abstract = {This paper considers the problem of minimizing a function F(x1,⋯,xn) over a closed, bounded region S in n-dimensional space under the assumption that there exists a unique minimizing point (z1,⋯,zn)∈ S. In a previous paper I represented the coordinates of the minimizing point as the limit of a ratio of integrals. The same type of ratio appears, in a different context, in statistical mechanics where a Monte Carlo method has been developed, by Metropolis et al., for its numerical evaluation. The purpose of this paper is to point out the connection of Metropolis's method with the above type of minimization problem. The idea of the method is to associate with the minimization problem a Markov chain whose sample averages converge with probability one to (approximately) the minimizing point (z1,⋯,zn). The Markov chain should be easily realizable on a computer. An estimate of the error from sampling over a finite time period is given.},
	author = {Martin Pincus},
	date-added = {2020-12-08 15:15:34 +1300},
	date-modified = {2020-12-08 15:16:22 +1300},
	issn = {0030364X, 15265463},
	journal = {Operations Research},
	keywords = {Other, Simulated annealing, Monte Carlo, Probability},
	number = {6},
	pages = {1225--1228},
	publisher = {INFORMS},
	title = {A Monte Carlo Method for the Approximate Solution of Certain Types of Constrained Optimization Problems},
	url = {http://www.jstor.org/stable/169420},
	volume = {18},
	year = {1970},
	Bdsk-Url-1 = {http://www.jstor.org/stable/169420}}

@article{Bianchi:2009aa,
	abstract = {Metaheuristics are general algorithmic frameworks, often nature-inspired, designed to solve complex optimization problems, and they are a growing research area since a few decades. In recent years, metaheuristics are emerging as successful alternatives to more classical approaches also for solving optimization problems that include in their mathematical formulation uncertain, stochastic, and dynamic information. In this paper metaheuristics such as Ant Colony Optimization, Evolutionary Computation, Simulated Annealing, Tabu Search and others are introduced, and their applications to the class of Stochastic Combinatorial Optimization Problems (SCOPs) is thoroughly reviewed. Issues common to all metaheuristics, open problems, and possible directions of research are proposed and discussed. In this survey, the reader familiar to metaheuristics finds also pointers to classical algorithmic approaches to optimization under uncertainty, and useful informations to start working on this problem domain, while the reader new to metaheuristics should find a good tutorial in those metaheuristics that are currently being applied to optimization under uncertainty, and motivations for interest in this field.},
	author = {Bianchi, Leonora and Dorigo, Marco and Gambardella, Luca Maria and Gutjahr, Walter J.},
	da = {2009/06/01},
	date-added = {2020-12-07 15:53:14 +1300},
	date-modified = {2020-12-07 15:53:48 +1300},
	doi = {10.1007/s11047-008-9098-4},
	id = {Bianchi2009},
	isbn = {1572-9796},
	journal = {Natural Computing},
	keywords = {Other, Metaheuristics, Evolutionary computation, Survey},
	number = {2},
	pages = {239--287},
	title = {A survey on metaheuristics for stochastic combinatorial optimization},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s11047-008-9098-4},
	volume = {8},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11047-008-9098-4}}

@article{ILSVRC15,
	author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	date-added = {2020-12-07 15:07:25 +1300},
	date-modified = {2020-12-07 15:07:37 +1300},
	doi = {10.1007/s11263-015-0816-y},
	journal = {International Journal of Computer Vision (IJCV)},
	keywords = {Other, ImageNet, ILSVRC},
	number = {3},
	pages = {211-252},
	title = {{ImageNet Large Scale Visual Recognition Challenge}},
	volume = {115},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11263-015-0816-y}}

@inproceedings{5206848,
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	author = {J. {Deng} and W. {Dong} and R. {Socher} and L. {Li} and {Kai Li} and {Li Fei-Fei}},
	booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
	date-added = {2020-12-07 15:03:21 +1300},
	date-modified = {2020-12-07 15:03:35 +1300},
	doi = {10.1109/CVPR.2009.5206848},
	issn = {1063-6919},
	keywords = {NN, ImageNet, computer vision;image resolution;image retrieval;Internet;multimedia computing;ontologies (artificial intelligence);trees (mathematics);very large databases;visual databases;ImageNet database;large-scale hierarchical image database;Internet;image retrieval;multimedia data;large-scale ontology;wordNet structure;image resolution;subtree;computer vision;Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
	month = {June},
	pages = {248-255},
	title = {ImageNet: A large-scale hierarchical image database},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2009.5206848}}

@article{8766229,
	abstract = {This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.},
	author = {IEEE},
	date-added = {2020-12-06 18:10:11 +1300},
	date-modified = {2020-12-06 18:10:24 +1300},
	doi = {10.1109/IEEESTD.2019.8766229},
	journal = {IEEE Std 754-2019 (Revision of IEEE 754-2008)},
	keywords = {Other, IEEE Standards;Floating-point arithmetic;arithmetic;binary;computer;decimal;exponent;floating-point;format;IEEE 754;interchange;NaN;number;rounding;significand;subnormal.},
	month = {July},
	pages = {1-84},
	title = {IEEE Standard for Floating-Point Arithmetic},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/IEEESTD.2019.8766229}}

@article{935097,
	abstract = {We present methods for optimizing portfolios, asset allocations, and trading systems based on direct reinforcement (DR). In this approach, investment decision-making is viewed as a stochastic control problem, and strategies are discovered directly. We present an adaptive algorithm called recurrent reinforcement learning (RRL) for discovering investment policies. The need to build forecasting models is eliminated, and better trading performance is obtained. The direct reinforcement approach differs from dynamic programming and reinforcement algorithms such as TD-learning and Q-learning, which attempt to estimate a value function for the control problem. We find that the RRL direct reinforcement framework enables a simpler problem representation, avoids Bellman's curse of dimensionality and offers compelling advantages in efficiency. We demonstrate how direct reinforcement can be used to optimize risk-adjusted investment returns (including the differential Sharpe ratio), while accounting for the effects of transaction costs. In extensive simulation work using real financial data, we find that our approach based on RRL produces better trading strategies than systems utilizing Q-learning (a value function method). Real-world applications include an intra-daily currency trader and a monthly asset allocation system for the S&P 500 Stock Index and T-Bills.},
	author = {J. {Moody} and M. {Saffell}},
	date-added = {2020-12-05 18:27:07 +1300},
	date-modified = {2020-12-05 18:27:23 +1300},
	doi = {10.1109/72.935097},
	issn = {1941-0093},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {NN-Fin, RL, DNN, stock markets;investment;decision theory;learning (artificial intelligence);stochastic systems;optimisation;direct reinforcement learning;portfolio optimization;asset allocations;trading systems;DR;investment decision-making;stochastic control problem;recurrent reinforcement learning;RRL;investment policies;forecasting models;risk-adjusted investment return optimization;differential Sharpe ratio;transaction costs;financial data;intra-daily currency trader;monthly asset allocation system;S&P 500 Stock Index;T-Bills;Investments;Asset management;Optimization methods;Portfolios;Decision making;Stochastic processes;Adaptive algorithm;Learning;Predictive models;Dynamic programming},
	month = {July},
	number = {4},
	pages = {875-889},
	title = {Learning to trade via direct reinforcement},
	volume = {12},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1109/72.935097}}

@article{10.1145/2949662,
	abstract = {The power of deep neural networks has sparked renewed interest in reinforcement learning, with applications to games, robotics, and beyond.},
	address = {New York, NY, USA},
	author = {Krakovsky, Marina},
	date-added = {2020-12-05 18:17:29 +1300},
	date-modified = {2020-12-05 18:17:49 +1300},
	doi = {10.1145/2949662},
	issn = {0001-0782},
	issue_date = {August 2016},
	journal = {Commun. ACM},
	keywords = {DNN, RL, Robotics, Article},
	month = jul,
	number = {8},
	numpages = {3},
	pages = {12--14},
	publisher = {Association for Computing Machinery},
	title = {Reinforcement Renaissance},
	url = {https://doi-org.ezproxy.auckland.ac.nz/10.1145/2949662},
	volume = {59},
	year = {2016},
	Bdsk-Url-1 = {https://doi-org.ezproxy.auckland.ac.nz/10.1145/2949662},
	Bdsk-Url-2 = {https://doi.org/10.1145/2949662}}

@book{sra2012optimization,
	author = {Sra, S. and Nowozin, S. and Wright, S.J.},
	date-added = {2020-12-04 21:46:25 +1300},
	date-modified = {2020-12-05 13:34:27 +1300},
	isbn = {9780262016469},
	keywords = {Books, NN, SGD, Gradient Descent, ML, DNN},
	lccn = {2011002059},
	publisher = {MIT Press},
	series = {Neural information processing series},
	title = {Optimization for Machine Learning},
	url = {https://books.google.ca/books?id=JPQx7s2L1A8C},
	year = {2012},
	Bdsk-Url-1 = {https://books.google.ca/books?id=JPQx7s2L1A8C}}

@article{Hassan:2020aa,
	abstract = {Deep Convolutional Neural Networks (CNNs) are the state-of-the-art systems for image classification due to their high accuracy but on the other hand their high computational complexity is very costly. The acceleration is the target in this field nowadays for using these systems in real time applications. The Graphics Processing Units is the solution but its high-power consumption prevents its utilization in daily-used equipment moreover the Field Programmable Gate Array (FPGA) has low power consumption and flexible architecture which fits more for CNN implementations. This work discusses this problem and provides a solution that compromises between the speed of the CNN and the power consumption of the FPGA. This solution depends on two main techniques for speeding up: parallelism of layers resources and pipelining inside some layers. On the other hand, we added a new methodology to compromise the area requirements with the speed and design time by implementing CNN using Xilinx SDSOC tool (including processor and FPGA on the same board). Implementing design using HW/SW partitioning will enhance time design based on high level language(C or C++) in Vivado HLS (High Level Synthesis). It also fits for more large designs than using FPGA only and faster in design time.},
	author = {Hassan, Rania O. and Mostafa, Hassan},
	da = {2020/03/24},
	date-added = {2020-12-02 13:45:26 +1300},
	date-modified = {2020-12-02 13:45:48 +1300},
	doi = {10.1007/s10470-020-01638-5},
	id = {Hassan2020},
	isbn = {1573-1979},
	journal = {Analog Integrated Circuits and Signal Processing},
	keywords = {FPGA-NN, Xilinx, SoC, DNN},
	title = {Implementation of deep neural networks on FPGA-CPU platform using Xilinx SDSOC},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s10470-020-01638-5},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10470-020-01638-5}}

@techreport{zynqDPU,
	author = {Xilinx},
	date-added = {2020-12-01 23:47:16 +1300},
	date-modified = {2020-12-01 23:55:12 +1300},
	keywords = {Other, Zynq, Xilinx, DPU},
	month = {July},
	number = {PG338},
	publisher = {Xilinx},
	title = {Zynq DPU v3.2},
	type = {Product Guide},
	url = {https://www.xilinx.com/support/documentation/ip_documentation/dpu/v3_2/pg338-dpu.pdf},
	year = {2020},
	Bdsk-Url-1 = {https://www.xilinx.com/support/documentation/ip_documentation/dpu/v3_2/pg338-dpu.pdf}}

@inproceedings{DBLP:conf/eccv/HuLWZC18,
	author = {Qinghao Hu and Gang Li and Peisong Wang and Yifan Zhang and Jian Cheng},
	booktitle = {ECCV (13)},
	cdate = {1514764800000},
	crossref = {conf/eccv/2018-13},
	date-added = {2020-12-01 18:09:15 +1300},
	date-modified = {2020-12-01 18:10:58 +1300},
	keywords = {BNN, FPGA, DNN},
	pages = {657-673},
	title = {Training Binary Weight Networks via Semi-Binary Decomposition},
	url = {https://doi.org/10.1007/978-3-030-01261-8_39},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-030-01261-8_39}}

@article{Qin_2020,
	author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
	date-added = {2020-12-01 17:09:05 +1300},
	date-modified = {2020-12-01 17:09:39 +1300},
	doi = {10.1016/j.patcog.2020.107281},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {BNN, Survey, gradient descent},
	month = {Sep},
	pages = {107281},
	publisher = {Elsevier BV},
	title = {Binary neural networks: A survey},
	url = {http://dx.doi.org/10.1016/j.patcog.2020.107281},
	volume = {105},
	year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.patcog.2020.107281}}

@inproceedings{csde-ANE,
	abstract = {Adversarial neuro encoding could provide new insights for ciphering information with different perspectives. Nevertheless, it is still underexplored with a handful of publications on the subject. This work proposes the implementation of neuroevolved binary neural networks based on boolean logic functions only (BiSUNA) that apply payload ciphering between two agents to disperse information from an observer. The BiSUNA framework provides three distinctive attributions: it uses an adversarial neural encoding environment to improve the system data transmission; one execution yields a diversity of results given its population heuristics; lastly, it is an unconventional proposal to employ binary neural networks for the solution of symmetric ciphered problems.},
	author = {R. {Valencia} and C. {Sham}},
	booktitle = {2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)},
	date-added = {2020-11-25 11:42:34 +1300},
	date-modified = {2020-11-29 16:43:03 +1300},
	keywords = {Binary Neural Network, BiSUNA, CPA, Adversarial Neurocryptography, Neuroevolution},
	month = {Dec},
	title = {Adversarial Neuro Encoding with Binary Neural Networks},
	year = {2020}}

@inproceedings{csde-FPGA,
	abstract = {Deep Learning has reached a prominent area of research thanks to advances in semiconductor technologies, with Deep Neural Network (DNN) as the pivot of change. It is capable of solving complex multi-dimensional problems. This paper focuses on one particular example, the Binary Neural Network (BNN): it uses fixed-length bits in its connections and logic functions to perform excitation operations, reducing memory requirements. The conventional use of Field Programmable Gate Arrays (FPGAs) dictates inference only of deep learning networks. This publication demonstrates how the algorithm Binary Spectrum-diverse Unified Neuroevolution Architecture (BiSUNA) can perform training and inference on FPGA by dismissing gradient descent, solving reinforcement learning and reaching maximum parallelism and energy efficiency, up to 16\% faster compared to a CPU. Source code can be found in github.com/rval735/bisunaU50.},
	author = {R. {Valencia} and C. {Sham}},
	booktitle = {2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE)},
	date-added = {2020-11-25 11:40:23 +1300},
	date-modified = {2020-11-29 16:42:58 +1300},
	keywords = {BiSUNA, FPGA, U50, Xilinx},
	month = {Dec},
	title = {FPGA deployment of neuroevolved Binary Neural Networks},
	year = {2020}}

@article{PASQUALINI20201122,
	abstract = {Pseudo-Random Numbers Generators (PRNGs) are algorithms produced to generate long sequences of statistically uncorrelated numbers, i.e. Pseudo-Random Numbers (PRNs). These numbers are widely employed in mid-level cryptography and in software applications. Test suites are used to evaluate PRNGs quality by checking statistical properties of the generated sequences. Machine learning techniques are often used to break these generators, i.e. approximating a certain generator or a certain sequence using a neural network. But what about using machine learning to generate PRNs generators? This paper proposes a Reinforcement Learning (RL) approach to the task of generating PRNGs from scratch by learning a policy to solve an N-dimensional navigation problem. In this context, N is the length of the period of the sequence to generate and the policy is iteratively improved using the average score of an appropriate test suite run over that period. Aim of this work is to demonstrate the feasibility of the proposed approach, to compare it with classical methods, and to lay the foundation of a research path which combines RL and PRNGs.},
	author = {Luca Pasqualini and Maurizio Parton},
	date-added = {2020-11-19 17:48:56 +1300},
	date-modified = {2020-11-19 17:49:16 +1300},
	doi = {https://doi.org/10.1016/j.procs.2020.03.057},
	issn = {1877-0509},
	journal = {Procedia Computer Science},
	keywords = {Crypto, NIST, STS, RL, PRNG, Pseudo-Random Number, Machine Learning, Reinforcement Learning, Deep Learning, Neural Networks},
	note = {The 11th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 3rd International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
	pages = {1122 - 1127},
	title = {Pseudo Random Number Generation: a Reinforcement Learning approach},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050920304944},
	volume = {170},
	year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050920304944},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2020.03.057}}

@inproceedings{pmlr-v80-kleinberg18a,
	abstract = {Stochastic gradient descent (SGD) is widely used in machine learning. Although being commonly viewed as a fast but not accurate version of gradient descent (GD), it always finds better solutions than GD for modern neural networks. In order to understand this phenomenon, we take an alternative view that SGD is working on the convolved (thus smoothed) version of the loss function. We show that, even if the function $f$ has many bad local minima or saddle points, as long as for every point $x$, the weighted average of the gradients of its neighborhoods is one point convex with respect to the desired solution $x^*$, SGD will get close to, and then stay around $x^*$ with constant probability. Our result identifies a set of functions that SGD provably works, which is much larger than the set of convex functions. Empirically, we observe that the loss surface of neural networks enjoys nice one point convexity properties locally, therefore our theorem helps explain why SGD works so well for neural networks.},
	address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	author = {Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
	date-added = {2020-11-06 18:39:43 +1300},
	date-modified = {2020-11-06 18:41:24 +1300},
	editor = {Jennifer Dy and Andreas Krause},
	keywords = {Other, SDG, Stochastic Gradient Descent, Local Minima},
	month = {10--15 Jul},
	pages = {2698--2707},
	pdf = {http://proceedings.mlr.press/v80/kleinberg18a/kleinberg18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {An Alternative View: When Does {SGD} Escape Local Minima?},
	url = {http://proceedings.mlr.press/v80/kleinberg18a.html},
	volume = {80},
	year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v80/kleinberg18a.html}}

@article{robbins1951,
	author = {Robbins, Herbert and Monro, Sutton},
	date-added = {2020-11-05 20:36:23 +1300},
	date-modified = {2020-11-05 20:36:53 +1300},
	doi = {10.1214/aoms/1177729586},
	fjournal = {Annals of Mathematical Statistics},
	journal = {Ann. Math. Statist.},
	keywords = {Other, Stochastic Gradient Descent, SGD, Derivates},
	month = {09},
	number = {3},
	pages = {400--407},
	publisher = {The Institute of Mathematical Statistics},
	title = {A Stochastic Approximation Method},
	url = {https://doi.org/10.1214/aoms/1177729586},
	volume = {22},
	year = {1951},
	Bdsk-Url-1 = {https://doi.org/10.1214/aoms/1177729586}}

@article{gd-Haskell,
	author = {Haskell B. Curry},
	date-added = {2020-11-03 23:05:54 +1300},
	date-modified = {2020-11-03 23:08:40 +1300},
	journal = {Quarterly of Applied Mathematics},
	keywords = {Other, gradient descent,},
	pages = {258-261},
	title = {The method of steepest descent for non-linearminimization problems},
	volume = {2},
	year = {1944}}

@book{aiNorvig,
	author = {Stuart Russell and Peter Norvig},
	date-added = {2020-11-03 22:17:17 +1300},
	date-modified = {2020-11-03 22:19:29 +1300},
	edition = {4},
	keywords = {Book, AI, ML, Unsupervised learning},
	publisher = {Pearson},
	title = {Artificial Intelligence: A Modern Approach},
	volume = {1},
	year = {2020}}

@inproceedings{44873,
	author = {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
	booktitle = {NIPS Deep Learning and Representation Learning Workshop},
	date-added = {2020-11-03 17:38:11 +1300},
	date-modified = {2020-11-03 17:39:30 +1300},
	keywords = {NN, DNN, MNIST, Supervised learning},
	title = {Distilling the Knowledge in a Neural Network},
	url = {http://arxiv.org/abs/1503.02531},
	year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.02531}}

@inproceedings{10.1007/11844297_68,
	abstract = {The evolution of artificial neural networks (ANNs) is often used to tackle difficult control problems. There are different approaches to the encoding of neural networks in artificial genomes. Analog Genetic Encoding (AGE) is a new implicit method derived from the observation of biological genetic regulatory networks. This paper shows how AGE can be used to simultaneously evolve the topology and the weights of ANNs for complex control systems. AGE is applied to a standard benchmark problem and we show that its performance is equivalent or superior to some of the most powerful algorithms for neuroevolution in the literature.},
	address = {Berlin, Heidelberg},
	author = {D{\"u}rr, Peter and Mattiussi, Claudio and Floreano, Dario},
	booktitle = {Parallel Problem Solving from Nature - PPSN IX},
	date-added = {2020-10-28 20:17:07 +1300},
	date-modified = {2020-10-29 12:29:59 +1300},
	editor = {Runarsson, Thomas Philip and Beyer, Hans-Georg and Burke, Edmund and Merelo-Guerv{\'o}s, Juan J. and Whitley, L. Darrell and Yao, Xin},
	isbn = {978-3-540-38991-0},
	keywords = {Book, Evolution, Neuroevolution, Genetic Encoding},
	pages = {671--680},
	publisher = {Springer Berlin Heidelberg},
	title = {Neuroevolution with Analog Genetic Encoding},
	year = {2006}}

@webpage{initializationIntervalXilinx,
	author = {Xilinx},
	date-added = {2020-10-27 20:01:23 +1300},
	date-modified = {2020-10-27 20:02:27 +1300},
	keywords = {Webpage, Xilinx, FPGA,},
	title = {Loop Pipelining and Loop Unrolling},
	url = {https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html},
	year = {2020},
	Bdsk-Url-1 = {https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_unrolling.html}}

@electronic{awsF1,
	author = {Amazon Web Services},
	date-added = {2020-10-27 13:27:49 +1300},
	date-modified = {2020-10-27 13:28:35 +1300},
	keywords = {Webpage, AWS F1, FPGA, Xilinx},
	title = {AWS F1 instance}}

@webpage{nimbixAlveo,
	author = {Nimbix},
	date-added = {2020-10-27 13:27:05 +1300},
	date-modified = {2020-10-27 13:27:45 +1300},
	keywords = {Webpage, Cloud, Numbix, FPGA},
	title = {Alveo FPGA},
	url = {https://www.nimbix.net/alveo},
	year = {2020},
	Bdsk-Url-1 = {https://www.nimbix.net/alveo}}

@webpage{bisunaU50Git,
	author = {R. {Valencia}},
	date-added = {2020-10-27 12:50:38 +1300},
	date-modified = {2020-10-27 12:53:43 +1300},
	keywords = {WebPage, BiSUNA},
	title = {BiSUNA - Alveo U50},
	url = {https://github.com/rval735/bisunaU50},
	year = {2020},
	Bdsk-Url-1 = {https://github.com/rval735/bisunaU50}}

@electronic{gpgpu-ai-dominance,
	author = {James Kobielus},
	date-added = {2020-10-26 23:07:36 +1300},
	date-modified = {2020-10-26 23:09:10 +1300},
	keywords = {WebPage, GPGPU, AI},
	title = {GPUs Continue to Dominate the AI Accelerator Market for Now},
	url = {https://www.informationweek.com/big-data/ai-machine-learning/gpus-continue-to-dominate-the-ai-accelerator-market-for-now/a/d-id/1336475},
	year = {2019},
	Bdsk-Url-1 = {https://www.informationweek.com/big-data/ai-machine-learning/gpus-continue-to-dominate-the-ai-accelerator-market-for-now/a/d-id/1336475}}

@webpage{alveoU50,
	author = {Xilinx},
	date-added = {2020-10-26 23:04:04 +1300},
	date-modified = {2020-10-26 23:05:51 +1300},
	keywords = {WebPage, Xilinx, FPGA, Alveo U50},
	title = {Alveo U50},
	url = {https://www.xilinx.com/products/boards-and-kits/alveo/u50.html},
	year = {2020},
	Bdsk-Url-1 = {https://www.xilinx.com/products/boards-and-kits/alveo/u50.html}}

@webpage{altera-ai,
	author = {Intel},
	date-added = {2020-10-26 22:32:55 +1300},
	date-modified = {2020-10-26 22:34:37 +1300},
	keywords = {WebPage, Intel, FPGA, Altera, AI},
	lastchecked = {2020},
	title = {FPGAs for Artificial Intelligence (AI)},
	url = {https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/overview.html},
	Bdsk-Url-1 = {https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/overview.html}}

@webpage{vitis-ai,
	author = {Xilinx},
	date-added = {2020-10-26 22:25:35 +1300},
	date-modified = {2020-10-26 22:31:16 +1300},
	keywords = {Webpage, Xilinx, FPGA, Vitis, AI,},
	title = {Vitis AI},
	url = {https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html},
	year = {2020},
	Bdsk-Url-1 = {https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html}}

@misc{clary2019lets,
	archiveprefix = {arXiv},
	author = {Kaleigh Clary and Emma Tosch and John Foley and David Jensen},
	date-added = {2020-10-01 11:43:48 +1300},
	date-modified = {2020-10-01 11:44:12 +1300},
	eprint = {1904.06312},
	keywords = {DNN, OpenAI Gym, Baselines, Atari},
	primaryclass = {cs.LG},
	title = {Let's Play Again: Variability of Deep Reinforcement Learning Agents in Atari Environments},
	year = {2019}}

@inproceedings{9102924,
	abstract = {Recent years have witnessed the great success of deep reinforcement learning (DRL) on a variety of vision games. Although DNN has demonstrated strong power in representation learning, such capacity is under-explored in most DRL works whose focus is usually on optimization solvers. In fact, we discover that the state feature learning is the main obstacle for further improvement of DRL algorithms. To address this issue, we propose a new state representation learning scheme with our Adjacent State Consistency Loss (ASC Loss). The loss is defined based on the hypothesis that there are fewer changes between adjacent states than that of far apart ones, since scenes in videos generally evolve smoothly. In this paper, we exploit ASC loss as an assistant of RL loss in the training phase to boost the state feature learning. We conduct evaluation on Atari games and MuJoCo continuous control tasks, which demonstrates that our method is superior to OpenAI baselines.},
	author = {J. {Zhao} and W. {Zhou} and T. {Zhao} and Y. {Zhou} and H. {Li}},
	booktitle = {2020 IEEE International Conference on Multimedia and Expo (ICME)},
	date-added = {2020-10-01 11:32:51 +1300},
	date-modified = {2020-10-01 11:33:16 +1300},
	doi = {10.1109/ICME46284.2020.9102924},
	issn = {1945-788X},
	keywords = {DNN, Baselines, OpenAI Gym, computer games;computer vision;learning (artificial intelligence);neural nets;OpenAI baselines;MuJoCo continuous control tasks;Atari games;adjacent state consistency loss;optimization solvers;DNN;deep reinforcement learning;state representation learning;vision games;RL loss;ASC loss;DRL algorithms;state feature learning;Representation learning;reinforcement learning},
	month = {July},
	pages = {1-6},
	title = {State Representation Learning For Effective Deep Reinforcement Learning},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICME46284.2020.9102924}}

@url{oaiBaseline-Github,
	author = {OpenAI},
	date-added = {2020-10-01 11:24:56 +1300},
	date-modified = {2020-10-01 11:26:52 +1300},
	keywords = {OpenAI Gym, Baselines, Github},
	title = {Baselines},
	url = {https://github.com/openai/baselines},
	Bdsk-Url-1 = {https://github.com/openai/baselines}}

@url{oaiBaselines-Page,
	author = {OpenAI},
	date-added = {2020-10-01 11:23:29 +1300},
	date-modified = {2020-10-01 11:24:30 +1300},
	keywords = {OpenAI Gym, Baselines, WebPage},
	title = {OpenAI Baselines: DQN},
	url = {https://openai.com/blog/openai-baselines-dqn/},
	Bdsk-Url-1 = {https://openai.com/blog/openai-baselines-dqn/}}

@inproceedings{ijcai2019-0452,
	author = {Petroski Such, Felipe and Madhavan, Vashisht and Liu, Rosanne and Wang, Rui and Castro, Pablo Samuel and Li, Yulun and Zhi, Jiale and Schubert, Ludwig and Bellemare, Marc G. and Clune, Jeff and Lehman, Joel},
	booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	date-added = {2020-10-01 11:21:34 +1300},
	date-modified = {2020-10-01 11:22:00 +1300},
	doi = {10.24963/ijcai.2019/452},
	keywords = {DNN, RL, Atari, OpenAI Gym, Baselines},
	month = {7},
	pages = {3260--3267},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents},
	url = {https://doi.org/10.24963/ijcai.2019/452},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2019/452}}

@inproceedings{FastNIST-STS-Muni,
	author = {S{\'y}s, M.; Z. {\v R}{\'\i}ha, V. Maty{\'a}{\v s}, K.M{\'a}rton, A. Suciu},
	booktitle = {Romanian Journal of Information Science and Technology},
	date-added = {2020-09-17 13:39:21 +1200},
	date-modified = {2020-09-17 13:43:18 +1200},
	editor = {Publishing House of the Romanian Academy},
	keywords = {Crypto, NIST, Muni, Fast-STS, STS},
	pages = {18-32},
	title = {On the Interpretation of Results from the NIST Statistical Test Suite},
	volume = {18},
	year = {2015}}

@article{PETRICA2018251,
	abstract = {Pseudo-random number generators (PRNGs) are important to applications ranging from cryptography to Monte-Carlo methods. Consequently, many PRNG architectures have been proposed, including some optimized for FPGA, e.g the LUT-SR family of PRNGs which utilize embedded FPGA shift registers, and self-programmable cellular automaton (SPCA) PRNGs. However, LUT-SR and other PRNGs do not utilize key features of modern Xilinx FPGAs: embedded carry chains and splittable Look-Up Tables (LUTs), i.e., 6-input LUTs which can operate as two 5-input LUTs which share inputs. In this paper we explore the SPCA structure and derive a set of parameter constraints which allow a SPCA PRNG to produce 2 random bits per LUT in every clock cycle on modern Xilinx FPGAs. We determine this to be the maximum logic density achievable for SPCA, and propose an architectural improvement of SPCA to enable further density increase by making use of FPGA embedded carry chains as a method to compute an additional random bit per LUT in each clock cycle. The resulting Split-LUT-Carry SPCA (SLC-SPCA) PRNG achieves 6x improvement in logic density compared to LUT-SR, and a 1.5x density increase compared to SPCA. We evaluate the randomness of SLC-SPCA utilizing the NIST Statistical Test Suite, and we provide a power and energy comparison of LUT-SR and SLC-SPCA on a Xilinx Zynq 7020 FPGA device. Our results indicate that SLC-SPCA generates 3x more bits per clock at approximately the same power dissipation as LUT-SR, and consequently 3x less energy to generate 1 gigabit of random data. SLC-SPCA is also 1.5x more energy-efficient than a SPCA PRNG.},
	author = {Lucian Petrica},
	date-added = {2020-09-17 13:08:37 +1200},
	date-modified = {2020-09-17 13:08:59 +1200},
	doi = {https://doi.org/10.1016/j.jpdc.2017.05.022},
	issn = {0743-7315},
	journal = {Journal of Parallel and Distributed Computing},
	keywords = {Crypto, NIST, STS, PRNG, FPGA, Cellular automaton, Random number generator},
	pages = {251 - 259},
	title = {FPGA optimized cellular automaton random number generator},
	url = {http://www.sciencedirect.com/science/article/pii/S0743731517301892},
	volume = {111},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0743731517301892},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jpdc.2017.05.022}}

@article{R2020103041,
	abstract = {With the advent of computers, networks, distributed systems and wireless data transfer technologies, data communication is subjected to potential threats and hence needs to be protected. Advanced Encryption Standard (AES) is an efficient encryption algorithm used for secure wireless communication, but its security is under threat due to different forms of theoretical and practical attacks such as linear cryptanalysis, differential cryptanalysis, boomerang attack, truncated differentials, hybrid attacks, related-key distinguishing attack, and side channel attacks. This work is focused on reducing the attacks by increasing the complexity of cryptanalysis through small scale variations in the shift row transformation and key expansion unit of the AES algorithm. The proposed small scale variations in the linear layer of the cipher unit and in the word column of key expansion unit enhances confusion during encryption. The inherent diffusion property of the algorithm along with the proposed confusion characteristics allows us to increase the overall security of the algorithm. The confusion and diffusion characteristics of the modified algorithm are found to be higher in terms of balance, Strict Avalanche Criterion and bit independence criteria. The AES variants are also tested using NIST-800-22 statistical test suite for random number generators, the small scale variations made in the architecture provides a higher degree of randomness in the ciphertext thus increasing cryptanalysis complexity. This paper also proposes an obfuscation control unit that allows on-demand selection of the three AES variants resulting in a variable encrypting pattern for a plaintext-key pair thus adding more bottlenecks to the adversaries.},
	author = {Lavanya R and Karpagam M},
	date-added = {2020-09-17 13:04:44 +1200},
	date-modified = {2020-09-17 13:04:59 +1200},
	doi = {https://doi.org/10.1016/j.micpro.2020.103041},
	issn = {0141-9331},
	journal = {Microprocessors and Microsystems},
	keywords = {Crypto, NIST, AES, STS, Advanced Encryption Standard, Security, FPGA, Avalanche effect, Randomness, NIST randomness testing, Obfuscation},
	pages = {103041},
	title = {Enhancing the security of AES through small scale confusion operations for data communication},
	url = {http://www.sciencedirect.com/science/article/pii/S0141933119306568},
	volume = {75},
	year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0141933119306568},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.micpro.2020.103041}}

@inproceedings{8632138,
	abstract = {In recent years, numerous studies of stream symmetric ciphers in Ukraine are continuing, the main purpose of which is to argue the principles of creating a new cryptographic algorithm, which can be based on the national standard. One of the essential aspects in choosing from many alternatives is the statistical properties of the output pseudorandom sequence (key stream). In this paper, the results of comparative studies of statistical properties of output sequences, which are formed by various stream ciphers, in particular, by world-known algorithms Enocoro, Decim, Grain, HC, MUGI, Mickey, Rabbit, RC-4, Salsa20, SNOW2.0, Sosemanuk, Trivium and the Ukrainian cryptographic algorithm Strumok, that was developed in recent years, are presented. For comparative studies, the NIST STS method was used, according to which experimental studies are performed in 15 statistical tests, the purpose of which is to determine the randomness of the output binary sequences. Each of the tests is aimed at studying certain vulnerabilities of the generator, that is, points to the potential usage of different methods of cryptographic analysis. Although each of the considered streaming encryption algorithms has been studied, we have carried out a statistical test of the generated pseudorandom sequences under equal conditions and with identical initial parameters, that is, our results allow us to perform a comparative analysis of ciphers and to justify the best of statistical properties. The estimates presented in the article, as expected, confirmed the high statistical security indexes of modern ciphers. In addition, according to the results of experimental research, it was found that the new Ukrainian development - the stream cipher Strumok does not yield to the best world algorithms in the statistical properties of the initial sequences.},
	author = {O. {Nariezhnii} and E. {Eremin} and V. {Frolenko} and K. {Chernov} and T. {Kuznetsova} and I. {Chepurko}},
	booktitle = {2018 International Scientific-Practical Conference Problems of Infocommunications. Science and Technology (PIC S T)},
	date-added = {2020-09-17 12:55:40 +1200},
	date-modified = {2020-09-17 12:56:01 +1200},
	doi = {10.1109/INFOCOMMST.2018.8632138},
	keywords = {Crypto, NIST, STS, AES, binary sequences;cryptography;random number generation;random sequences;statistical testing;statistical tests;streaming encryption algorithms;experimental studies;Ukrainian cryptographic algorithm Strumok;stream ciphers;output sequences;output pseudorandom sequence;stream symmetric ciphers;stream cipher Strumok;high statistical security indexes;statistical properties;generated pseudorandom sequences;statistical test;output binary sequences;Ciphers;Rabbits;Random sequences;IEC Standards;ISO Standards;Cryptanalysis;gamma;statistical tests;stream cryptographic algorithms;symmetric cryptography},
	month = {Oct},
	pages = {696-700},
	title = {Research of Statistical Properties of Stream Symmetric Ciphers},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/INFOCOMMST.2018.8632138}}

@techreport{NIST-SP800,
	author = {Elaine Barker},
	date-added = {2020-09-17 12:25:43 +1200},
	date-modified = {2020-09-17 12:26:46 +1200},
	institution = {NIST},
	keywords = {Crypto, NIST, SP800, key management},
	number = {800-57-1},
	title = {Recommendation For Key Management - Part 1 General},
	year = {2020}}

@article{8691576,
	abstract = {As the size and source of network traffic increase, so does the challenge of monitoring and analyzing network traffic. Therefore, sampling algorithms are often used to alleviate these scalability issues. However, the use of high entropy data streams, through the use of either encryption or compression, further compounds the challenge as current state-of-the-art algorithms cannot accurately and efficiently differentiate between encrypted and compressed packets. In this paper, we propose a novel traffic classification method named High Entropy DistinGuishEr (HEDGE) to distinguish between compressed and encrypted traffic. HEDGE is based on the evaluation of the randomness of the data streams and can be applied to individual packets without the need to have access to the entire stream. The findings from the evaluation show that our approach outperforms current state of the art. We also make available our statistically sound dataset, based on known benchmarks, to the wider research community.},
	author = {F. {Casino} and K. R. {Choo} and C. {Patsakis}},
	date-added = {2020-09-12 10:47:04 +1200},
	date-modified = {2020-09-12 10:47:37 +1200},
	doi = {10.1109/TIFS.2019.2911156},
	issn = {1556-6021},
	journal = {IEEE Transactions on Information Forensics and Security},
	keywords = {Crypto, Cryptanalysis, NIST, cryptography;data compression;entropy;pattern classification;encryption;HEDGE;compressed traffic;encrypted traffic;encrypted packets;sampling algorithms;high entropy data streams;compressed packets;traffic classification method;high entropy distinguisher;Entropy;Encryption;Payloads;Benchmark testing;Real-time systems;Encryption;high entropy sources;compression;classification;traffic analysis},
	month = {Nov},
	number = {11},
	pages = {2916-2926},
	title = {HEDGE: Efficient Traffic Classification of Encrypted and Compressed Packets},
	volume = {14},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/TIFS.2019.2911156}}

@inproceedings{7016926,
	abstract = {NIST SP800-22 is a statistical test suite for determining whether given sequences are random or not for each statistical test. The statistical test suite has been used widely. However, it was not mentioned in the statistical test suite how many the ratio of passing all the 15 kinds of tests should be for the target generator to be regarded as the ideally true random number generator. In this paper, the ratio of passing all the 15 kinds of tests for the ideally true random number sequences is derived by the theoretical analysis. To verify the theoretical deduction, the statistical tests have been performed on several well known random number generators, such as AES, 3DES, SHA1, stream ciphers wined in eStream project, and some perfect pseudo-random number generator recommended by NIST. The result of the numerical simulations is accord with the theoretical analysis.},
	author = {D. {Lihua} and Z. {Yong} and J. {Ligang} and H. {Xucang}},
	booktitle = {2014 Tenth International Conference on Computational Intelligence and Security},
	date-added = {2020-09-12 10:32:07 +1200},
	date-modified = {2020-09-12 10:32:39 +1200},
	doi = {10.1109/CIS.2014.120},
	keywords = {Crypto, NIST, random number generation;statistical analysis;NIST SP800-22 statistical test suite;statistical test;target generator;random number sequences;stream ciphers;eStream project;pseudo random number generator;numerical simulations;NIST;Generators;Correlation;Numerical simulation;Educational institutions;Probability;Algorithm design and analysis;NIST SP800-22;statistical test;random number;randomnes},
	month = {Nov},
	pages = {402-404},
	title = {Study on the Pass Rate of NIST SP800-22 Statistical Test Suite},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/CIS.2014.120}}

@article{10.1145/2988228,
	abstract = {The NIST Statistical Test Suite (NIST STS) is one of the most popular tools for the analysis of randomness. This test battery is widely used, but its implementation is quite inefficient. A complete randomness analysis using the NIST STS can take hours on a standard computer when the tested data volume is on the order of GB. We improved the most time-consuming test (Linear Complexity) from the previous most efficient implementation of the NIST STS. We also optimized other tests and achieved an overall speedup of 50.6 \texttimes{} compared with the reference implementation. This means that 20MB of data can be tested within a minute using our new optimized version of the NIST STS. To speed up the Linear Complexity test, we proposed a new version of the Berlekamp-Massey algorithm that computes only the linear complexity of a sequence. This new variant does not construct a linear feedback shift register and is approximately 187 \texttimes{} faster than the original NIST implementation of the Berlekamp-Massey algorithm.},
	address = {New York, NY, USA},
	articleno = {27},
	author = {S\'{y}s, Marek and \v{R}\'{\i}ha, Zden\v{e}k and Maty\'{a}\v{s}, Vashek},
	date-added = {2020-09-12 10:15:03 +1200},
	date-modified = {2020-09-12 10:15:23 +1200},
	doi = {10.1145/2988228},
	issn = {0098-3500},
	issue_date = {January 2017},
	journal = {ACM Trans. Math. Softw.},
	keywords = {Crypto, NIST, statistical randomness testing, Berlekamp-Massey algorithm, STS},
	month = dec,
	number = {3},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	title = {Algorithm 970: Optimizing the NIST Statistical Test Suite and the Berlekamp-Massey Algorithm},
	url = {https://doi.org/10.1145/2988228},
	volume = {43},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1145/2988228}}

@proceedings{SP800-22Rev1a,
	abstract = {This paper discusses some aspects of selecting and testing random and pseudorandom number generators. The outputs of such generators may be used in many cryptographic applications, such as the generation of key material. Generators suitable for use in cryptographic applications may need to meet stronger requirements than for other applications. In particular, their outputs must be unpredictable in the absence of knowledge of the inputs. Some criteria for characterizing and selecting appropriate generators are discussed in this document. The subject of statistical testing and its relation to cryptanalysis is also discussed, and some recommended statistical tests are provided. These tests may be useful as a first step in determining whether or not a generator is suitable for a particular cryptographic application. However, no set of statistical tests can absolutely certify a generator as appropriate for usage in a particular application, i.e., statistical testing cannot serve as a substitute for cryptanalysis. The design and cryptanalysis of generators is outside the scope of this paper.

https://csrc.nist.gov/publications/detail/sp/800-22/rev-1a/final},
	annote = {https://csrc.nist.gov/publications/detail/sp/800-22/rev-1a/final},
	author = {Lawrence Bassham, Andrew Rukhin, Juan Soto, James Nechvatal, Miles Smid, Elaine Barker, Stefan Leigh, Mark Levenson, Mark Vangel, David Banks, N. Heckert, James Dray},
	date-added = {2020-09-11 23:50:49 +1200},
	date-modified = {2020-09-11 23:55:10 +1200},
	editor = {NIST},
	keywords = {Crypto, hypothesis test; P-value; random number generator; statistical tests},
	month = {April},
	number = {800},
	organization = {NIST},
	publisher = {NIST},
	title = {A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications},
	volume = {22},
	year = {2010}}

@incollection{NIPS2018_8025,
	author = {Ibarz, Borja and Leike, Jan and Pohlen, Tobias and Irving, Geoffrey and Legg, Shane and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems 31},
	date-added = {2020-09-10 11:19:14 +1200},
	date-modified = {2020-09-10 11:19:57 +1200},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	keywords = {DNN, Atari,},
	pages = {8011--8023},
	publisher = {Curran Associates, Inc.},
	title = {Reward learning from human preferences and demonstrations in Atari},
	url = {http://papers.nips.cc/paper/8025-reward-learning-from-human-preferences-and-demonstrations-in-atari.pdf},
	year = {2018},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/8025-reward-learning-from-human-preferences-and-demonstrations-in-atari.pdf}}

@inproceedings{8490422,
	abstract = {The General Video Game AI (GVGAI) competition and its associated software framework provides a way of benchmarking AI algorithms on a large number of games written in a domain-specific description language. While the competition has seen plenty of interest, it has so far focused on online planning, providing a forward model that allows the use of algorithms such as Monte Carlo Tree Search. In this paper, we describe how we interface GVGAI to the OpenAI Gym environment, a widely used way of connecting agents to reinforcement learning problems. Using this interface, we characterize how widely used implementations of several deep reinforcement learning algorithms fare on a number of GVGAI games. We further analyze the results to provide a first indication of the relative difficulty of these games relative to each other, and relative to those in the Arcade Learning Environment under similar conditions.},
	author = {R. R. {Torrado} and P. {Bontrager} and J. {Togelius} and J. {Liu} and D. {Perez-Liebana}},
	booktitle = {2018 IEEE Conference on Computational Intelligence and Games (CIG)},
	date-added = {2020-09-10 10:47:46 +1200},
	date-modified = {2020-09-10 10:48:41 +1200},
	doi = {10.1109/CIG.2018.8490422},
	issn = {2325-4289},
	keywords = {DNN, RL, DQN, A2C, computer games;learning (artificial intelligence);Monte Carlo methods;planning (artificial intelligence);tree searching;AI algorithms;domain-specific description language;Monte Carlo Tree Search;reinforcement learning problems;deep reinforcement learning algorithms;GVGAI games;Arcade Learning Environment;General Video Game AI;associated software framework;online planning;GVGAI interface;OpenAI Gym environment;Games;Benchmark testing;Machine learning;Planning;Learning (artificial intelligence);deep reinforcement learning;general video game AI;video game description language;OpenAI Gym;advantage actor critic;deep Q-learning},
	month = {Aug},
	pages = {1-8},
	title = {Deep Reinforcement Learning for General Video Game AI},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CIG.2018.8490422}}

@inproceedings{aaai-180305391W,
	author = {Yanzhi Wang; Zheng Zhan; Jiayu Li; Jian Tang; Bo Yuan; Liang Zhao; Wujie Wen; Siyue Wang; Xue Lin},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	date-added = {2020-09-09 17:21:50 +1200},
	date-modified = {2020-09-09 17:26:50 +1200},
	editor = {AAAI},
	keywords = {BNN, AAAI, stochastic computing},
	month = jul,
	number = {01},
	organization = {AAAI},
	publisher = {AAAI},
	title = {Universal Approximation Property and Equivalence of Stochastic Computing-Based Neural Networks and Binary Neural Networks},
	volume = {33},
	year = 2019}

@inproceedings{alizadeh2018a,
	author = {Milad Alizadeh and Javier Fern{\'a}ndez-Marqu{\'e}s and Nicholas D. Lane and Yarin Gal},
	booktitle = {International Conference on Learning Representations},
	date-added = {2020-09-09 15:44:05 +1200},
	date-modified = {2020-09-09 15:44:50 +1200},
	keywords = {BNN, Survey, Binary Neural Network, Straight-Through-Estimator, STE},
	title = {An Empirical study of Binary Neural Networks Optimisation},
	url = {https://openreview.net/forum?id=rJfUCoR5KX},
	year = {2019},
	Bdsk-Url-1 = {https://openreview.net/forum?id=rJfUCoR5KX}}

@article{QUINLAN1987221,
	abstract = {Many systems have been developed for constructing decision trees from collections of examples. Although the decision trees generated by these methods are accurate and efficient, they often suffer the disadvantage of excessive complexity and are therefore incomprehensible to experts. It is questionable whether opaque structures of this kind can be described as knowledge, no matter how well they function. This paper discusses techniques for simplifying decision trees while retaining their accuracy. Four methods are described, illustrated, and compared on a test-bed of decision trees from a variety of domains.},
	author = {J.R. Quinlan},
	date-added = {2020-09-08 17:41:58 +1200},
	date-modified = {2020-09-08 17:42:12 +1200},
	doi = {https://doi.org/10.1016/S0020-7373(87)80053-6},
	issn = {0020-7373},
	journal = {International Journal of Man-Machine Studies},
	keywords = {Other, Decision Trees, Neuromodulation},
	number = {3},
	pages = {221 - 234},
	title = {Simplifying decision trees},
	url = {http://www.sciencedirect.com/science/article/pii/S0020737387800536},
	volume = {27},
	year = {1987},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0020737387800536},
	Bdsk-Url-2 = {https://doi.org/10.1016/S0020-7373(87)80053-6}}

@article{anchieh2018instanas,
	author = {Cheng, An-Chieh and Lin, Chieh Hubert and Juan, Da-Cheng and Wei, Wei and Sun, Min},
	booktitle = {Thirty-Fourth AAAI Conference on Artificial Intelligence},
	date-added = {2020-09-08 10:37:19 +1200},
	date-modified = {2020-09-08 10:37:57 +1200},
	keywords = {DNN, NAS, Neural Architecture Search, ImageNet},
	title = {InstaNAS: Instance-aware Neural Architecture Search},
	year = {2020}}

@electronic{TensorFlowWebPage,
	author = {TensorFlow team},
	date-added = {2020-09-07 13:11:39 +1200},
	date-modified = {2020-09-07 13:16:58 +1200},
	keywords = {URL, Tensorflow, Framework, open source},
	month = {08},
	title = {TensorFlow},
	url = {https://www.tensorflow.org},
	year = {2020},
	Bdsk-Url-1 = {https://www.tensorflow.org}}

@inproceedings{OUAPES-AAAI,
	author = {Yanzhi Wang and Zheng Zhan and Jiayu Li and Jian Tang and Bo Yuan and Liang Zhao and Wujie Wen and Siyue Wang and Xue Lin},
	biburl = {https://dblp.org/rec/journals/corr/abs-1803-05391.bib},
	booktitle = {AAAI Technical Track: Machine Learning},
	date-added = {2020-09-06 14:12:47 +1200},
	date-modified = {2020-09-06 14:16:12 +1200},
	editor = {AAAI},
	eprint = {1803.05391},
	keywords = {BNN, stochastic weights binary neural networks, Binary Neural Network},
	month = {Jul},
	number = {4475},
	organization = {AAAI},
	pages = {7},
	publisher = {AAAI},
	timestamp = {Wed, 24 Jul 2019 13:03:45 +0200},
	title = {On the Universal Approximation Property and Equivalence of Stochastic Computing-based Neural Networks and Binary Neural Networks},
	url = {http://arxiv.org/abs/1803.05391},
	volume = {33},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1803.05391}}

@inproceedings{li2019rtn,
	annote = {DOI: https://doi.org/10.1609/aaai.v34i04.5912},
	archiveprefix = {arXiv},
	author = {Yuhang Li and Xin Dong and Sai Qian Zhang and Haoli Bai and Yuanpeng Chen and Wei Wang},
	booktitle = {AAAI Technical Track: Machine Learning},
	date-added = {2020-09-06 14:04:22 +1200},
	date-modified = {2020-09-06 14:10:48 +1200},
	editor = {AAAI},
	eprint = {1912.02057},
	keywords = {BNN, Binary Neural Network, Quantization},
	organization = {AAAI},
	pages = {7},
	title = {RTN: Reparameterized Ternary Network},
	volume = {34},
	year = {2019}}

@article{0328e31afe5f481bace5457f700c35ea,
	abstract = {Due to the high computational complexity and memory storage requirement, it is hard to directly deploy a full-precision convolutional neural network (CNN) on embedded devices. The hardware-friendly designs are needed for resource-limited and energy-constrained embedded devices. Emerging solutions are adopted for the neural network compression, e.g., binary/ternary weight network, pruned network and quantized network. Among them, binary neural network (BNN) is believed to be the most hardware-friendly framework due to its small network size and low computational complexity. No existing work has further shrunk the size of BNN. In this work, we explore the redundancy in BNN and build a compact BNN (CBNN) based on the bit-level sensitivity analysis and bit-level data pruning. The input data is converted to a high dimensional bit-sliced format. In the post-training stage, we analyze the impact of different bit slices to the accuracy. By pruning the redundant input bit slices and shrinking the network size, we are able to build a more compact BNN. Our result shows that we can further scale down the network size of the BNN up to 3.9x with no more than 1% accuracy drop. The actual runtime can be reduced up to 2x and 9.9x compared with the baseline BNN and its full-precision counterpart, respectively.},
	author = {Yixing Li and Shuai Zhang and Xichuan Zhou and Fengbo Ren},
	date-added = {2020-09-06 13:46:54 +1200},
	date-modified = {2020-09-06 13:47:05 +1200},
	day = {20},
	doi = {10.1016/j.neucom.2020.02.012},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {BNN, Binary neural networks, Deep learning, Deep neural networks, Neural network compression},
	language = {English (US)},
	month = jul,
	pages = {45--54},
	publisher = {Elsevier},
	title = {Build a compact binary neural network through bit-level sensitivity and data pruning},
	volume = {398},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neucom.2020.02.012}}

@webpage{needle-python,
	date-added = {2020-09-03 10:19:21 +1200},
	date-modified = {2020-09-03 10:20:53 +1200},
	keywords = {SourceCode, Python, DQN, TRPO},
	month = {Dec},
	title = {Needle Deep RL Examples},
	url = {https://github.com/roosephu/needle},
	year = {2016},
	Bdsk-Url-1 = {https://github.com/roosephu/needle}}

@webpage{garage-python,
	author = {The garage contributors},
	commit = {711d273b8633fda6c6641b7b536e21b5104557a9},
	date-added = {2020-09-02 15:58:47 +1200},
	date-modified = {2020-09-02 15:59:34 +1200},
	journal = {GitHub repository},
	keywords = {Other, Garare, SourceCode, Python Reinforcement Learning, RL},
	publisher = {GitHub},
	title = {Garage: A toolkit for reproducible reinforcement learning research},
	url = {https://github.com/rlworkgroup/garage},
	year = {2019},
	Bdsk-Url-1 = {https://github.com/rlworkgroup/garage}}

@webpage{neat-python,
	author = {Alan McIntyre and Matt Kallada and Cesar G. Miguel and Carolina Feher da Silva},
	date-added = {2020-09-02 15:51:19 +1200},
	date-modified = {2020-09-02 15:53:28 +1200},
	keywords = {Other, SourceCode, NEAT, Python},
	title = {NEAT Python Github},
	url = {https://github.com/CodeReclaimers/neat-python},
	year = {2014},
	Bdsk-Url-1 = {https://github.com/CodeReclaimers/neat-python}}

@inproceedings{pmlr-v37-schulman15,
	abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
	address = {Lille, France},
	author = {John Schulman and Sergey Levine and Pieter Abbeel and Michael Jordan and Philipp Moritz},
	date-added = {2020-09-02 10:15:44 +1200},
	date-modified = {2020-09-02 10:16:09 +1200},
	editor = {Francis Bach and David Blei},
	keywords = {DNN, RL, TRPO, Reinforcement Learning, Atari},
	month = {07--09 Jul},
	pages = {1889--1897},
	pdf = {http://proceedings.mlr.press/v37/schulman15.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Trust Region Policy Optimization},
	url = {http://proceedings.mlr.press/v37/schulman15.html},
	volume = {37},
	year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v37/schulman15.html}}

@inproceedings{pmlr-v48-wangf16,
	abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
	address = {New York, New York, USA},
	author = {Ziyu Wang and Tom Schaul and Matteo Hessel and Hado Hasselt and Marc Lanctot and Nando Freitas},
	date-added = {2020-08-31 17:33:29 +1200},
	date-modified = {2020-08-31 17:34:04 +1200},
	editor = {Maria Florina Balcan and Kilian Q. Weinberger},
	keywords = {DNN, DQN, DDQN, Dueling DQN, RL, Atari},
	month = {20--22 Jun},
	pages = {1995--2003},
	pdf = {http://proceedings.mlr.press/v48/wangf16.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Dueling Network Architectures for Deep Reinforcement Learning},
	url = {http://proceedings.mlr.press/v48/wangf16.html},
	volume = {48},
	year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/wangf16.html}}

@inbook{Biryukov2011,
	address = {Boston, MA},
	author = {Biryukov, Alex},
	booktitle = {Encyclopedia of Cryptography and Security},
	date-added = {2020-08-29 14:35:38 +1200},
	date-modified = {2020-08-29 14:36:13 +1200},
	doi = {10.1007/978-1-4419-5906-5_557},
	editor = {van Tilborg, Henk C. A. and Jajodia, Sushil},
	isbn = {978-1-4419-5906-5},
	keywords = {Crypto, CPA, Definition},
	pages = {205--206},
	publisher = {Springer US},
	title = {Chosen Plaintext Attack},
	url = {https://doi.org/10.1007/978-1-4419-5906-5_557},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4419-5906-5_557}}

@article{6979219,
	abstract = {Body area networks (BANs) consisting of small sensing and computing devices can help to improve medical care and well-being of humans. Obviously, the data recorded in BANs such as vital parameters are personal and should be private. To protect this privacy, such data are usually encrypted when transmitting it over a wireless link. In the past, many cryptographic algorithms and methods for the encryption and decryption of data have been proposed-and most of them have become obsolete. One-time pads (OTPs) were mathematically proven to be secure and impossible to crack. But, for most purposes, OTPs are complicated to handle, because if applied correctly, for each bit of plain text data, another bit of OTP must be available. Sensors in BANs usually do not generate an enormous amount of data. Hence, also the length of an OTP is not huge. Sustained progress in memory technology makes large amounts of nonvolatile memory available at cheap prices and small sizes. Thus, numerous OTPs can be preinstalled on BAN devices and ensure a long lasting and secure data transmission. In this paper, we present a concept for securing data transmission in BANs by utilizing OTPs. We delineate a system for generation, distribution, and utilization of OTPs in wireless sensor network (WSN) and BAN scenarios, and we show the implementation and evaluation of such a system.},
	author = {F. {B{\"u}sching} and L. {Wolf}},
	date-added = {2020-08-29 12:23:30 +1200},
	date-modified = {2020-08-29 12:24:22 +1200},
	doi = {10.1109/JIOT.2014.2378783},
	issn = {2327-4662},
	journal = {IEEE Internet of Things Journal},
	keywords = {Crypto; OTP;body area networks;health care;wireless sensor networks;body area networks;BAN;secure data transmission;medical care;wireless link;cryptographic algorithms;one-time pads;memory technology;wireless sensor network;WSN;Encryption;Wireless sensor networks;Intelligent sensors;Biomedical monitoring;Wireless sensors networks;Medical devices;Medical services;Computer security;Privacy;Body area networks;Security;Privacy;Encryption;Wireless Sensor Networks;Body Area Networks;One-Time Pad;Body area networks (BANs);encryption;one-time pad;privacy;security;wireless sensor networks (WSNs)},
	month = {Feb},
	number = {1},
	pages = {63-71},
	title = {The Rebirth of One-Time Pads---Secure Data Transmission from BAN to Sink},
	volume = {2},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/JIOT.2014.2378783}}

@incollection{mnih-atari-2013,
	author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
	booktitle = {NIPS Deep Learning Workshop},
	date-added = {2020-08-28 17:56:37 +1200},
	date-modified = {2020-08-28 18:01:37 +1200},
	keywords = {DNN, DeepQNetwork, Atari, RL, Reinforcement Learning},
	publisher = {NIPS},
	title = {Playing Atari With Deep Reinforcement Learning},
	year = {2013}}

@url{OpenVINOStarterKit,
	author = {Terasic},
	date-added = {2020-08-20 21:50:18 +1200},
	date-modified = {2020-08-20 21:51:29 +1200},
	keywords = {Terasic, OpenVINO, FPGA},
	lastchecked = {2020},
	title = {Starter Platform for OpenVINO{\texttrademark} Toolkit},
	url = {https://www.terasic.com.tw/cgi-bin/page/archive.pl?Language=English&CategoryNo=167&No=1159},
	urldate = {2013},
	Bdsk-Url-1 = {https://www.terasic.com.tw/cgi-bin/page/archive.pl?Language=English&CategoryNo=167&No=1159}}

@inproceedings{SurveyCrypto,
	author = {Omar G. Abood, Shawkat K. Guirguis},
	booktitle = {International Journal of Scientific and Research Publications (IJSRP)},
	date-added = {2020-08-19 12:25:17 +1200},
	date-modified = {2020-08-19 12:36:45 +1200},
	keywords = {Crypto, Survey, AES},
	month = {July},
	organization = {IJSRP},
	pages = {21},
	publisher = {IJSRP},
	title = {A Survey on Cryptography Algorithms},
	volume = {8},
	year = {2018}}

@inproceedings{5210990,
	abstract = {We present a new idea on chosen plaintext cryptanalysis, where we can bypass some of the cipher's encryption rounds at its beginning. To illustrate this idea, we developed the pushdown attack. This attack can increase the strength of some chosen plaintext attacks. We applied the pushdown attack on AES and were able to achieve a 6-round attack that requires only 211 chosen plaintext, this reduces the chosen plaintext needed by the square attack with a factor of 221.},
	author = {M. A. {El-Fotouh} and K. {Diepold}},
	booktitle = {2009 Third International Conference on Emerging Security Information, Systems and Technologies},
	date-added = {2020-08-17 18:43:54 +1200},
	date-modified = {2020-08-17 18:43:54 +1200},
	doi = {10.1109/SECURWARE.2009.50},
	issn = {2162-2116},
	keywords = {cryptography;AES;advanced encryption standard;pushdown attack;plaintext cryptanalysis;cipher encryption round;6-round attack;square attack;Cryptography;NIST;Data security;Information security;Data processing;Cryptanalysis;Pushdown Attacks;AES},
	month = {June},
	pages = {280-285},
	title = {The Pushdown Attack on AES},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/SECURWARE.2009.50}}

@book{AIFoundations,
	annote = {https://www.cambridge.org/ca/academic/subjects/computer-science/artificial-intelligence-and-natural-language-processing/artificial-intelligence-foundations-computational-agents-2nd-edition?format=HB&isbn=9781107195394},
	author = {David L Poole; Alan K Mackworth},
	date-added = {2020-07-30 23:19:11 +1200},
	date-modified = {2020-07-30 23:21:29 +1200},
	edition = {2},
	keywords = {AI,},
	month = {Sep},
	number = {9781107195394},
	publisher = {Cambridge University Press},
	title = {Artificial Intelligence, Foundations of Computational Agents},
	volume = {1},
	year = {2017}}

@inproceedings{8852036,
	abstract = {Neural networks with both high accuracy and small network size are urgently required for mobile phone applications. However, previous network search methods do not take network size into account. In this paper, we use the reinforcement learning method to search for networks offline, with both high accuracy and small network size. Gaussian policy is used to explore the number of convolutional channels in a finer manner. Parameter reward is included in our reward function to punish large networks. We also use binary networks to further reduce network size. Without skip connections or branches, the network generated by our method is competitive with other methods on Cifar-10. Our network is much smaller than networks generated by other network search methods. Besides, our accuracy is higher than original binary network reported in BinaryConnect and is competitive with other real-valued networks.},
	author = {J. {Du} and Y. {Qin} and H. {Lu}},
	booktitle = {2019 International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2020-07-30 14:01:34 +1200},
	date-modified = {2020-07-30 14:01:56 +1200},
	doi = {10.1109/IJCNN.2019.8852036},
	issn = {2161-4407},
	keywords = {BNN, Network Search, RL, learning (artificial intelligence);neural nets;binary networks;neural networks;network size;networks offline;original binary network;network search methods;BinaryConnect;real-valued networks;Gaussian policy;Network architecture;Computer architecture;Neural networks;Reinforcement learning;Standards;Mobile handsets;Microprocessors},
	month = {July},
	pages = {1-8},
	title = {Network Search for Binary Networks},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2019.8852036}}

@incollection{NIPS2006_3048,
	author = {Bengio, Yoshua and Lamblin, Pascal and Dan Popovici and Larochelle, Hugo},
	booktitle = {Advances in Neural Information Processing Systems 19},
	date-added = {2020-07-28 23:16:48 +1200},
	date-modified = {2020-07-28 23:17:13 +1200},
	editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
	keywords = {NN, Topology},
	pages = {153--160},
	publisher = {MIT Press},
	title = {Greedy Layer-Wise Training of Deep Networks},
	url = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf},
	year = {2007},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf}}

@inproceedings{10.1007/978-3-319-02621-3_15,
	abstract = {NeuroEvolution (NE) is the application of evolutionary algorithms to Artificial Neural Networks (ANN). This paper reports on an investigation into the relative importance of weight evolution and topology evolution when training ANN using NE. This investigation used the NE technique Cartesian Genetic Programming of Artificial Neural Networks (CGPANN). The results presented show that the choice of topology has a dramatic impact on the effectiveness of NE when only evolving weights; an issue not faced when manipulating both weights and topology. This paper also presents the surprising result that topology evolution alone is far more effective when training ANN than weight evolution alone. This is a significant result as many methods which train ANN manipulate only weights.},
	address = {Cham},
	author = {Turner, Andrew James and Miller, Julian Francis},
	booktitle = {Research and Development in Intelligent Systems XXX},
	date-added = {2020-07-28 16:18:33 +1200},
	date-modified = {2020-07-28 16:20:27 +1200},
	editor = {Bramer, Max and Petridis, Miltos},
	isbn = {978-3-319-02621-3},
	keywords = {Book, Evolutionary, CGP, TWEANN,},
	pages = {213--226},
	publisher = {Springer International Publishing},
	title = {The Importance of Topology Evolution in NeuroEvolution: A Case Study Using Cartesian Genetic Programming of Artificial Neural Networks},
	year = {2013}}

@phdthesis{Rempis2012,
	author = {Christian Wilhelm Rempis},
	date-added = {2020-07-28 15:21:02 +1200},
	date-modified = {2020-07-28 15:23:04 +1200},
	keywords = {Evolutionary, TWEANN, ICONE, Neuroevolution},
	school = {Universit{\"a}t Osnabr{\"u}ck},
	title = {Evolving Complex Neuro-Controllers with Interactively Constrained Neuro-Evolution},
	year = 2012}

@article{265960,
	abstract = {Standard methods for simultaneously inducing the structure and weights of recurrent neural networks limit every task to an assumed class of architectures. Such a simplification is necessary since the interactions between network structure and function are not well understood. Evolutionary computations, which include genetic algorithms and evolutionary programming, are population-based search methods that have shown promise in many similarly complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. GNARL's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods.<>},
	author = {P. J. {Angeline} and G. M. {Saunders} and J. B. {Pollack}},
	date-added = {2020-07-28 15:04:00 +1200},
	date-modified = {2020-07-28 15:04:22 +1200},
	doi = {10.1109/72.265960},
	issn = {1941-0093},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {Evolutionary, GNARL, TWEANN, recurrent neural nets;optimisation;evolutionary algorithm;recurrent neural networks;genetic algorithms;evolutionary programming;population-based search methods;GNARL;Evolutionary computation;Recurrent neural networks;Network topology;Computer architecture;Genetic algorithms;Genetic programming;Search methods;Induction generators;Ash;Artificial intelligence},
	month = {Jan},
	number = {1},
	pages = {54-65},
	title = {An evolutionary algorithm that constructs recurrent neural networks},
	volume = {5},
	year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1109/72.265960}}

@article{10.1016/j.neucom.2013.04.005,
	address = {NLD},
	author = {Mahsal Khan, Maryam and Masood Ahmad, Arbab and Muhammad Khan, Gul and Miller, Julian F.},
	date-added = {2020-07-28 14:22:58 +1200},
	date-modified = {2020-07-28 14:26:42 +1200},
	doi = {10.1016/j.neucom.2013.04.005},
	issn = {0925-2312},
	issue_date = {December, 2013},
	journal = {Neurocomput.},
	keywords = {Evolutionary, CGP, Breast cancer, Artificial neural network, Neuroevolution, Recurrent networks, Pole balancing},
	month = dec,
	numpages = {16},
	pages = {274--289},
	publisher = {Elsevier Science Publishers B. V.},
	title = {Fast Learning Neural Networks Using Cartesian Genetic Programming},
	url = {https://doi.org/10.1016/j.neucom.2013.04.005},
	volume = {121},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.neucom.2013.04.005}}

@inproceedings{DBLP:conf/esann/CliffHH93,
	author = {Dave Cliff and Inman Harvey and Phil Husbands},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/conf/esann/CliffHH93.bib},
	booktitle = {{ESANN} 1993, 1st European Symposium on Artificial Neural Networks, Brussels, Belgium, April 7-9, 1993, Proceedings},
	date-added = {2020-07-28 00:00:34 +1200},
	date-modified = {2020-07-28 00:00:57 +1200},
	keywords = {Evolutionary, Neuroevolution, SAGA, TWEANN},
	timestamp = {Thu, 12 Mar 2020 11:36:48 +0100},
	title = {Incremental evolution of neural network architectures for adaptive behavior},
	url = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1993-506-S.pdf},
	year = {1993},
	Bdsk-Url-1 = {https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1993-506-S.pdf}}

@inproceedings{10.1007/978-3-540-74913-4_94,
	abstract = {A novel approach to topology and weight evolving artificial neural networks (TWEANNs) is presented. Compared with previous TWEANNs, this method has two major characteristics. First, a set of genetic operations may be designed without recombination because it often generates an offspring whose fitness value is considerably worse than its parents. Instead, two topological mutations whose effect on fitness value is assumed to be nearly neutral are provided in the genetic operations set. Second, a new encoding technique is introduced to define a string as a set of substrings called operons. To examine our approach, computer simulations were conducted using the standard reinforcement learning problem known as the double pole balancing without velocity information. The results obtained were compared with NEAT results, which is recognised as one of the most powerful techniques in TWEANNs. It was found that our proposed approach yields competitive results, especially when the problem is difficult.},
	address = {Berlin, Heidelberg},
	author = {Ohkura, Kazuhiro and Yasuda, Toshiyuki and Kawamatsu, Yuichi and Matsumura, Yoshiyuki and Ueda, Kanji},
	booktitle = {Advances in Artificial Life},
	date-added = {2020-07-27 17:41:38 +1200},
	date-modified = {2020-07-27 17:42:43 +1200},
	editor = {Almeida e Costa, Fernando and Rocha, Luis Mateus and Costa, Ernesto and Harvey, Inman and Coutinho, Ant{\'o}nio},
	isbn = {978-3-540-74913-4},
	keywords = {Book, Neuroevolution, TWEANN, NEAT},
	pages = {936--945},
	publisher = {Springer Berlin Heidelberg},
	title = {MBEANN: Mutation-Based Evolving Artificial Neural Networks},
	year = {2007}}

@inbook{Jorgensen2009,
	abstract = {This chapter describes a novel way of complexifying artificial neural networks through topological reorganisation. The neural networks are reorganised to optimise their neural complexity, which is a measure of the information-theoretic complexity of the network. Complexification of neural networks here happens through rearranging connections, i.e. removing one or more connections and placing them elsewhere. The results verify that a structural reorganisation can help to increase the probability of discovering a neural network capable of adequately solving complex tasks. The networks and the methodology proposed are tested in a simulation of a mobile robot racing around a track.},
	address = {Dordrecht},
	author = {Jorgensen, Thomas D. and Haynes, Barry and Norlund, Charlotte},
	booktitle = {Advances in Electrical Engineering and Computational Science},
	date-added = {2020-07-27 17:37:16 +1200},
	date-modified = {2020-07-27 17:39:10 +1200},
	doi = {10.1007/978-90-481-2311-7_35},
	editor = {Ao, Sio-Iong and Gelman, Len},
	isbn = {978-90-481-2311-7},
	keywords = {Book, Neuroevolution, Robotics},
	pages = {411--421},
	publisher = {Springer Netherlands},
	title = {Reorganising Artificial Neural Network Topologies},
	url = {https://doi.org/10.1007/978-90-481-2311-7_35},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-90-481-2311-7_35}}

@article{Turner:2014aa,
	abstract = {NeuroEvolution is the application of Evolutionary Algorithms to the training of Artificial Neural Networks. Currently the vast majority of NeuroEvolutionary methods create homogeneous networks of user defined transfer functions. This is despite NeuroEvolution being capable of creating heterogeneous networks where each neuron's transfer function is not chosen by the user, but selected or optimised during evolution. This paper demonstrates how NeuroEvolution can be used to select or optimise each neuron's transfer function and empirically shows that doing so significantly aids training. This result is important as the majority of NeuroEvolutionary methods are capable of creating heterogeneous networks using the methods described.},
	author = {Turner, Andrew James and Miller, Julian Francis},
	da = {2014/11/01},
	date-added = {2020-07-27 17:20:14 +1200},
	date-modified = {2020-07-27 17:25:00 +1200},
	doi = {10.1007/s12065-014-0115-5},
	id = {Turner2014},
	isbn = {1864-5917},
	journal = {Evolutionary Intelligence},
	keywords = {Evolutionary, Neuroevolution, ANN training},
	number = {3},
	pages = {135--154},
	title = {NeuroEvolution: Evolving Heterogeneous Artificial Neural Networks},
	ty = {JOUR},
	url = {https://doi.org/10.1007/s12065-014-0115-5},
	volume = {7},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1007/s12065-014-0115-5}}

@article{Pujol:1998aa,
	abstract = {Evolutionary computation is a class of global search techniques based on the learning process of a population of potential solutions to a given problem, that has been successfully applied to a variety of problems. In this paper a new approach to the construction of neural networks based on evolutionary computation is presented. A linear chromosome combined to a graph representation of the network are used by genetic operators, which allow the evolution of the architecture and the weights simultaneously without the need of local weight optimization. This paper describes the approach, the operators and reports results of the application of this technique to several binary classification problems.},
	author = {Pujol, Jo{\~a}o Carlos Figueira and Poli, Riccardo},
	da = {1998/01/01},
	date-added = {2020-07-27 17:09:15 +1200},
	date-modified = {2020-07-27 17:10:55 +1200},
	doi = {10.1023/A:1008272615525},
	id = {Pujol1998},
	isbn = {1573-7497},
	journal = {Applied Intelligence},
	keywords = {Evolutionary, weight optimization, XOR},
	number = {1},
	pages = {73--84},
	title = {Evolving the Topology and the Weights of Neural Networks Using a Dual Representation},
	ty = {JOUR},
	url = {https://doi.org/10.1023/A:1008272615525},
	volume = {8},
	year = {1998},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1008272615525}}

@inproceedings{Martinez2020Training,
	author = {Brais Martinez and Jing Yang and Adrian Bulat and Georgios Tzimiropoulos},
	booktitle = {International Conference on Learning Representations},
	date-added = {2020-07-26 21:48:09 +1200},
	date-modified = {2020-07-26 21:48:52 +1200},
	keywords = {BNN, Training, Binary, Floating-point},
	title = {Training binary neural networks with real-to-binary convolutions},
	url = {https://openreview.net/forum?id=BJg4NgBKvH},
	year = {2020},
	Bdsk-Url-1 = {https://openreview.net/forum?id=BJg4NgBKvH}}

@inproceedings{4211782,
	abstract = {Multi-processor systems on chip (MPSoC) platforms are becoming increasingly more heterogeneous and are shifting towards a more communication-centric methodology. Networks on chip (NoC) have emerged as the design paradigm for scalable on-chip communication architectures. As the system complexity grows, the problem emerges as how to design and instantiate such a NoC-based MPSoC platform in a systematic and automated way. This paper presents an integrated flow to automatically generate a highly configurable NoC-based MPSoC for FPGA instantiation. The system specification is done on a high level of abstraction, relieving the designer of error-prone and time consuming work. The flow uses the state-of-the-art /Ethereal NoC, and silicon hive processing cores, both configurable at design- and run-time. The authors use this flow to generate a range of sample designs whose functionality has been verified on a Celoxica RC300E development board. The board, equipped with a Xilinx Virtex II 6000, also offers a huge number of peripherals, and shows how the insertion is automated in the design for easy debugging and prototyping},
	author = {A. {Kumar} and A. {Hansson} and J. {Huisken} and H. {Corporaal}},
	booktitle = {2007 Design, Automation Test in Europe Conference Exhibition},
	date-added = {2020-07-23 14:25:09 +1200},
	date-modified = {2020-07-23 14:25:18 +1200},
	doi = {10.1109/DATE.2007.364577},
	issn = {1558-1101},
	keywords = {FPGA; field programmable gate arrays;network-on-chip;reconfigurable architectures;FPGA design flow;reconfigurable network;multiprocessor systems on chip;networks on chip;scalable on-chip communication architectures;system specification;Ethereal NoC;silicon hive processing cores;Celoxica RC300E development board;Xilinx Virtex II 6000;Field programmable gate arrays;System-on-a-chip;Network-on-a-chip;Runtime;Silicon;Hardware;Automation;Debugging;Testing;Costs},
	month = {April},
	pages = {1-6},
	title = {An FPGA Design Flow for Reconfigurable Network-Based Multi-Processor Systems on Chip},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/DATE.2007.364577}}

@article{10.1145/3154839,
	address = {New York, NY, USA},
	articleno = {18},
	author = {Li, Yixing and Liu, Zichuan and Xu, Kai and Yu, Hao and Ren, Fengbo},
	date-added = {2020-07-22 23:43:14 +1200},
	date-modified = {2020-07-22 23:44:26 +1200},
	doi = {10.1145/3154839},
	issn = {1550-4832},
	issue_date = {July 2018},
	journal = {J. Emerg. Technol. Comput. Syst.},
	keywords = {CNN-FPGA, BNN, energy efficiency, deep learning, FPGA, high-throughput, convolutional neural network, hardware acceleration, binary neural network, GPU},
	month = jul,
	number = {2},
	numpages = {16},
	publisher = {Association for Computing Machinery},
	title = {A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks},
	url = {https://doi-org.ezproxy.auckland.ac.nz/10.1145/3154839},
	volume = {14},
	year = {2018},
	Bdsk-Url-1 = {https://doi-org.ezproxy.auckland.ac.nz/10.1145/3154839},
	Bdsk-Url-2 = {https://doi.org/10.1145/3154839}}

@inproceedings{7154838,
	abstract = {In Digital Signal Processing (DSP), Multiply-Accumulate Computation (MAC) unit plays a very important role and lies in the critical path. Multiplier is one of the most important block in MAC unit. The overall performance of the MAC unit depends on the resources used by the multiplier. Therefore, this paper describes the design of a Partial Product Reduction Block (PPRB) that is used in the implementation of multiplier having better area, delay and power performances. PPRB reduces the partial products row wise by using different multi-bit adder blocks instead of conventional coloumn wise reduction. MAC unit consisting of the multiplier realized using the proposed partial product reduction technique has a delay reduction of 46%, power consumption is reduced by 39% and area requirement is reduced by 17% when compared to MAC unit realised using conventional multiplier architecture.},
	author = {S. {Ahish} and Y. B. N. {Kumar} and D. {Sharma} and M. H. {Vasantha}},
	booktitle = {2015 IEEE International Advance Computing Conference (IACC)},
	date-added = {2020-07-21 21:15:25 +1200},
	date-modified = {2020-07-21 21:15:45 +1200},
	doi = {10.1109/IADCC.2015.7154838},
	keywords = {NN; MAC; digital signal processing chips;logic design;multiplying circuits;high-performance multiply-accumulate computation unit design;digital signal processing;DSP;MAC unit;partial-product reduction block;PPRB;multiplier implementation;multibit adder blocks;delay reduction;power consumption reduction;area requirement reduction;Adders;Delays;Digital signal processing;Computer architecture;Computers;Consumer electronics;Carry-lookahead adder;brent-kung adder;wallace tree;booth multiplier;multiply-accumulate unit},
	month = {June},
	pages = {915-918},
	title = {Design of high performance Multiply-Accumulate Computation unit},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/IADCC.2015.7154838}}

@inproceedings{9087264,
	abstract = {Inspired by the evolution of biological nervous systems, Neuroevolution (NE) is an approach to Artificial Intelligence (AI) which uses evolutionary algorithms to evolve complex artificial neural networks capable of intelligent behavior. Kenneth O' Stanley et al. proposed an extension of this approach called Neuroevolution of Augmenting Topologies (NEAT) [1], which evolves both topology and parameters. It possesses key features such as complexification, avoiding competing conventions via historical markings, speciation and fitness sharing. Over the years, the performance of NEAT has been improved with better approaches such as HyperNEAT and CoDeepNEAT. Better training methods for NEAT have evolved over the years as well. In this paper, we deduce an analysis of the efficiency and performance of the various algorithms which have been proposed for Topology and Weight Evolving Artificial Neural Networks (TWEANNs). A survey on the existing approaches based on their applications and purpose, along with the various training methods and the challenges incurred has also been discussed. This work will provide learners with a better overview of the past and current research trends in the field of Neuroevolution.},
	author = {M. Y. {Ibrahim} and R. {Sridhar} and T. V. {Geetha} and D. S. {S}},
	booktitle = {2019 11th International Conference on Advanced Computing (ICoAC)},
	date-added = {2020-07-21 16:54:07 +1200},
	date-modified = {2020-07-21 16:54:27 +1200},
	doi = {10.1109/ICoAC48765.2019.246825},
	keywords = {Evolutionary, NEAT,evolutionary computation;neural nets;topology;artificial intelligence;evolutionary algorithms;intelligent behavior;NEAT;neuroevolution of augmenting topologies;historical markings;fitness sharing;biological nervous systems;topology and weight evolving artificial neural networks;NEAT;Neuroevolution;Artificial Intelligence;Deep Learning;TWEANN},
	month = {Dec},
	pages = {111-116},
	title = {Advances in Neuroevolution through Augmenting Topologies -- A Case Study},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICoAC48765.2019.246825}}

@inproceedings{8892195,
	abstract = {Training of convolutional neural networks (CNNs) on embedded platforms to support on-device learning is earning vital importance in recent days. Designing flexible training hardware is much more challenging than inference hardware, due to design complexity and large computation/memory requirement. In this work, we present an automatic compiler based FPGA accelerator with 16-bit fixed-point precision for complete CNN training, including Forward Pass (FP), Backward Pass (BP) and Weight Update (WU). We implemented an optimized RTL library to perform training-specific tasks and developed an RTL compiler to automatically generate FPGA-synthesizable RTL based on user-defined constraints. We present a new cyclic weight storage/access scheme for on-chip BRAM and off-chip DRAM to efficiently implement non-transpose and transpose operations during FP and BP phases, respectively. Representative CNNs for CIFAR-10 dataset are implemented and trained on Intel Stratix 10 GX FPGA using proposed hardware architecture, demonstrating up to 479 GOPS performance.},
	author = {S. {Kolala Venkataramanaiah} and Y. {Ma} and S. {Yin} and E. {Nurvithadhi} and A. {Dasu} and Y. {Cao} and J. {Seo}},
	booktitle = {2019 29th International Conference on Field Programmable Logic and Applications (FPL)},
	date-added = {2020-07-21 15:52:17 +1200},
	date-modified = {2020-07-21 15:52:30 +1200},
	doi = {10.1109/FPL.2019.00034},
	issn = {1946-1488},
	keywords = {FPGA-NN, training, convolutional neural nets;DRAM chips;field programmable gate arrays;learning (artificial intelligence);program compilers;automatic compiler;FPGA accelerator;convolutional neural networks;embedded platforms;on-device learning;designing flexible training hardware;inference hardware;16-bit fixed-point precision;complete CNN training;forward pass;FP;backward pass;weight update;optimized RTL library;training-specific tasks;RTL compiler;FPGA-synthesizable RTL;user-defined constraints;BP phases;representative CNNs;Intel Stratix 10 GX FPGA;hardware architecture;GOPS performance;Training;Field programmable gate arrays;Kernel;Hardware;Libraries;Task analysis;Random access memory;Convolution neural networks, neural network training, back-propagation, hardware accelerator, FPGA},
	month = {Sep.},
	pages = {166-172},
	title = {Automatic Compiler Based FPGA Accelerator for CNN Training},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPL.2019.00034}}

@inproceedings{8533530,
	abstract = {To improve flexibility and energy efficiency of Convolutional Neural Networks, a number of cloud computing service providers-including Microsoft, Amazon, and Alibaba-are using FPGA-based CNN accelerators. However, the growing size and complexity of neural networks, coupled with communication and off-chip memory bottlenecks, make it increasingly difficult for multi-FPGA designs to achieve high resource utilization and performance, especially when training. In this work, we present new results for a scalable framework, FPDeep, which helps users efficiently map CNN training logic to multiple FPGAs and automatically generates the resulting RTL implementation. FPDeep is equipped with two mechanisms to facilitate high-performance and energy-efficient training. First, FPDeep improves DSP slice utilization across FPGAs by balancing workload using dedicated partition and mapping strategies. Second, only on-chip memory is used in the CONV layers: a) FPDeep balances CNN weight allocation among FPGAs to improve BRAM utilization; b) training of CNNs is executed in a fine-grained pipelined manner, minimizing the time features need to be cached while waiting for back-propagation leading to a reduced storage demand. We evaluate our framework by training AlexNet, VGG-16, and VGG-19. Experimental results show FPDeep has good scalability to a large number of FPGAs, with the limiting factor being the inter-FPGA bandwidth. With 6 transceivers per FPGA, FPDeep shows linearity up to 83 FPGAs. FPDeep provides, on average, 6.36x higher energy efficiency than GPU servers.},
	author = {T. {Geng} and T. {Wang} and A. {Sanaullah} and C. {Yang} and R. {Patel} and M. {Herbordt}},
	booktitle = {2018 28th International Conference on Field Programmable Logic and Applications (FPL)},
	date-added = {2020-07-21 15:28:58 +1200},
	date-modified = {2020-07-21 15:29:15 +1200},
	doi = {10.1109/FPL.2018.00074},
	issn = {1946-1488},
	keywords = {FPGA-NN, training, CNN, convolutional neural nets;field programmable gate arrays;logic design;random-access storage;resource allocation;BRAM utilization;fine-grained pipelined manner;inter-FPGA bandwidth;deeply-pipelined FPGA clusters;FPGA-based CNN accelerators;off-chip memory bottlenecks;multiFPGA designs;CNN training logic;energy-efficient training;DSP slice utilization;convolutional neural networks;Microsoft;resource utilization;multiple FPGA;RTL implementation;CNN weight allocation;weight load balancing;cloud computing service providers;Amazon;Alibaba;FPDeep scalable framework;Field programmable gate arrays;Training;Graphics processing units;Parallel processing;Load management;Resource management;System-on-chip;CNN Training;FPGA Cluster;High Performance Computing},
	month = {Aug},
	pages = {394-3944},
	title = {A Framework for Acceleration of CNN Training on Deeply-Pipelined FPGA Clusters with Work and Weight Load Balancing},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPL.2018.00074}}

@inproceedings{8641739,
	abstract = {This paper describes the development of an FPGA implementation of a parallel and reconfigurable architecture for sparse neural networks, capable of on-chip training and inference. The network connectivity uses pre-determined, structured sparsity to significantly reduce complexity by lowering memory and computational requirements. The architecture uses a notion of edge-processing, leading to efficient pipelining and parallelization. Moreover, the device can be reconfigured to trade off resource utilization with training time to fit networks and datasets of varying sizes. The combined effects of complexity reduction and easy reconfigurability enable greater exploration of network hyperparameters and structures on-chip. As proof of concept, we show implementation results on an Artix-7 FPGA.},
	author = {S. {Dey} and D. {Chen} and Z. {Li} and S. {Kundu} and K. {Huang} and K. M. {Chugg} and P. A. {Beerel}},
	booktitle = {2018 International Conference on ReConFigurable Computing and FPGAs (ReConFig)},
	date-added = {2020-07-20 23:57:15 +1200},
	date-modified = {2020-07-20 23:57:32 +1200},
	doi = {10.1109/RECONFIG.2018.8641739},
	issn = {2640-0472},
	keywords = {FPGA-NN, Training, field programmable gate arrays;neural nets;reconfigurable architectures;inference;network connectivity;structured sparsity;computational requirements;edge-processing;parallelization;training time;datasets;complexity reduction;network hyperparameters;structures on-chip;Artix-7 FPGA;sparse neural network training;parallel architecture;reconfigurable architecture;sparse neural networks;on-chip training;pipelining;reconfigurability;parallel FPGA implementation;Junctions;Training;Hardware;Neurons;Field programmable gate arrays;Artificial neural networks;Computer architecture;Machine learning;Neural networks;Sparse neural networks;FPGA Training;Parallelism;Pipelining},
	month = {Dec},
	pages = {1-4},
	title = {A Highly Parallel FPGA Implementation of Sparse Neural Network Training},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/RECONFIG.2018.8641739}}

@url{HauweiAscend,
	author = {Huawei},
	date-added = {2020-07-20 23:18:00 +1200},
	date-modified = {2020-07-20 23:21:17 +1200},
	keywords = {URL, Huawei, NPU},
	lastchecked = {July 2020},
	title = {Huawei launches Ascend 910, the world's most powerful AI processor, and MindSpore, an all-scenario AI computing framework},
	url = {https://www.huawei.com/en/news/2019/8/huawei-ascend-910-most-powerful-ai-processor},
	urldate = {Aug 23, 2019},
	Bdsk-Url-1 = {https://www.huawei.com/en/news/2019/8/huawei-ascend-910-most-powerful-ai-processor}}

@inproceedings{7460666,
	abstract = {Deep learning has revolutionized the way sensor measurements are interpreted and application of deep learning has seen a great leap in inference accuracies in a number of fields. However, the significant requirement for memory and computational power has hindered the wide scale adoption of these novel computational techniques on resource constrained wearable and mobile platforms. In this demonstration we present DeepX, a software accelerator for efficiently running deep neural networks and convolutional neural networks on resource constrained embedded platforms, e.g., Nvidia Tegra K1 and Qualcomm Snapdragon 400.},
	author = {N. D. {Lane} and S. {Bhattacharya} and P. {Georgiev} and C. {Forlivesi} and F. {Kawsar}},
	booktitle = {2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)},
	date-added = {2020-07-20 23:09:49 +1200},
	date-modified = {2020-07-20 23:09:49 +1200},
	doi = {10.1109/IPSN.2016.7460666},
	keywords = {embedded systems;learning (artificial intelligence);mobile computing;neural nets;sensors;Qualcomm Snapdragon 400;Nvidia Tegra K1;resource constrained embedded platforms;convolutional neural networks;deep neural networks;software accelerator;mobile platforms;resource constrained wearable platforms;computational techniques;computational power;inference accuracies;sensor measurements;DeepX;embedded deep learning;Machine learning;Computational modeling;Mobile communication;Image recognition;Neural networks;Computer architecture;Prototypes},
	month = {April},
	pages = {1-2},
	title = {Demonstration Abstract: Accelerating Embedded Deep Learning Using DeepX},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPSN.2016.7460666}}

@inproceedings{8665777,
	abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16-FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4× speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
	author = {A. {Haidar} and S. {Tomov} and J. {Dongarra} and N. J. {Higham}},
	booktitle = {SC18: International Conference for High Performance Computing, Networking, Storage and Analysis},
	date-added = {2020-07-20 19:17:43 +1200},
	date-modified = {2020-07-20 19:17:57 +1200},
	doi = {10.1109/SC.2018.00050},
	keywords = {NN; FP16; coprocessors;floating point arithmetic;iterative methods;linear algebra;mathematics computing;tensors;scientific computing applications;artificial intelligence;high-performance computing applications;general HPC problem;dense matrix;double precision solution;FP16-FP64;architecture-specific algorithms;FP16-TC;classical FP16 arithmetic;FP32 arithmetic;mixed-precision iterative refinement solvers;low-precision floating-point arithmetic;fast FP16 arithmetic;GPU tensor core;half-precision tensor cores;GEMM accumulation;Graphics processing units;Iterative algorithms;Acceleration;Matrices;FP16 Arithmetic;Half Precision;Mixed Precision Solvers;Iterative Refinement Computation;GPU Computing;Linear Algebra},
	month = {Nov},
	pages = {603-613},
	title = {Harnessing GPU Tensor Cores for Fast FP16 Arithmetic to Speed up Mixed-Precision Iterative Refinement Solvers},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/SC.2018.00050}}

@inproceedings{9070311,
	abstract = {One of the key obstacles in the advancement of largescale distributed FPGA platforms is the ability of the accelerator to act autonomously from the CPU, whilst maintaining tight coupling to system memory. This work details our efforts in decoupling the networking capabilities of the FPGA from CPU resources using a custom transport layer and network protocol. We highlight the reasons that previous solutions are insufficient for the requirements of HPC, and we show the performance benefits of offloading our transport into the FPGA fabric. Our results show promising throughput and latency benefits, and show competitive Flops being achievable for network dependent computing in a distributed environment.},
	author = {J. {Lant} and J. {Navaridas} and A. {Attwood} and M. {Lujan} and J. {Goodacre}},
	booktitle = {2019 IEEE Symposium on High-Performance Interconnects (HOTI)},
	date-added = {2020-07-18 23:08:09 +1200},
	date-modified = {2020-07-18 23:08:20 +1200},
	doi = {10.1109/HOTI.2019.00019},
	issn = {2332-5569},
	keywords = {FPGA; field programmable gate arrays;microprocessor chips;protocols;largescale distributed FPGA platforms;system memory;networking capabilities;CPU resources;custom transport layer;network protocol;FPGA fabric;latency benefits;network dependent computing;distributed environment;standalone FPGA computing;Field programmable gate arrays;Throughput;Reliability;Engines;Fabrics;Topology;Acceleration;FPGAs;Interconnects;Transport layer},
	month = {Aug},
	pages = {23-26},
	title = {Enabling Standalone FPGA Computing},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/HOTI.2019.00019}}

@inbook{Grozea2010,
	abstract = {Currently there are several interesting alternatives for low-cost high-performance computing. We report here our experiences with an N-gram extraction and sorting problem, originated in the design of a real-time network intrusion detection system. We have considered FPGAs, multi-core CPUs in symmetric multi-CPU machines and GPUs and have created implementations for each of these platforms. After carefully comparing the advantages and disadvantages of each we have decided to go forward with the implementation written for multi-core CPUs. Arguments for and against each platform are presented -- corresponding to our hands-on experience -- that we intend to be useful in helping with the selection of the hardware acceleration solutions for new projects.},
	address = {Berlin, Heidelberg},
	author = {Grozea, Cristian and Bankovic, Zorana and Laskov, Pavel},
	booktitle = {Facing the Multicore-Challenge: Aspects of New Paradigms and Technologies in Parallel Computing},
	date-added = {2020-07-18 23:03:58 +1200},
	date-modified = {2020-07-18 23:04:22 +1200},
	doi = {10.1007/978-3-642-16233-6_12},
	editor = {Keller, Rainer and Kramer, David and Weiss, Jan-Philipp},
	isbn = {978-3-642-16233-6},
	keywords = {Books, FPGA, GPU, CPU, Sorting Algorithm},
	pages = {105--117},
	publisher = {Springer Berlin Heidelberg},
	title = {FPGA vs. Multi-core CPUs vs. GPUs: Hands-On Experience with a Sorting Application},
	url = {https://doi.org/10.1007/978-3-642-16233-6_12},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-16233-6_12}}

@inproceedings{RT-Proc-FPGA,
	address = {Florence, Italy},
	author = {Stefan Aust; Harald Richter},
	booktitle = {ADVCOMP 2010},
	date-added = {2020-07-18 22:55:55 +1200},
	date-modified = {2020-07-18 22:58:58 +1200},
	editor = {The Fourth International Conference on Advanced Engineering Computing and Applications in Sciences},
	keywords = {Other, Non-blocking Network, network on chip; multistage interconnection network; softcore processor; real-time multiprocessor; FPGA-based multiprocessor},
	month = {October},
	number = {2308-4499},
	pages = {47-52},
	publisher = {978-1-61208-101-4},
	title = {Real-time Processor Interconnection Network for FPGA-based Multiprocessor System-on-Chip (MPSoC)},
	volume = {1},
	year = {2010}}

@inproceedings{10.1007/978-3-642-03138-0_11,
	abstract = {The GCA (Global Cellular Automata) model consists of a collection of cells which change their states synchronously depending on the states of their neighbors like in the classical CA (Cellular Automata) model. In differentiation to the CA model the neighbors are not fixed and local, they are variable and global. The GCA model is applicable to a wide range of parallel algorithms. In this paper a general purpose multiprocessor architecture for the massively parallel GCA model is presented. In contrast to a special purpose implementation of a GCA algorithm the multiprocessor system allows the implementation in a flexible way through programming. The architecture mainly consists of a set of processors (Nios II) and a network. The Nios II features a general-purpose RISC CPU architecture designed to address a wide range of applications. The network is a well-known omega network. Only read-accesses through the network are necessary in the GCA model leading to a simplified structure. A system with up to 32 processors was implemented as a prototype on an FPGA. The analysis and implementation results have shown that the performance of the system scales with the number of processors.},
	address = {Berlin, Heidelberg},
	author = {Sch{\"a}ck, Christian and Heenes, Wolfgang and Hoffmann, Rolf},
	booktitle = {Embedded Computer Systems: Architectures, Modeling, and Simulation},
	date-added = {2020-07-18 21:18:15 +1200},
	date-modified = {2020-07-18 21:18:52 +1200},
	editor = {Bertels, Koen and Dimopoulos, Nikitas and Silvano, Cristina and Wong, Stephan},
	isbn = {978-3-642-03138-0},
	keywords = {Books, Non-blocking Network, Parallel, Multiprocessor, cellular automata},
	pages = {98--107},
	publisher = {Springer Berlin Heidelberg},
	title = {A Multiprocessor Architecture with an Omega Network for the Massively Parallel Model GCA},
	year = {2009}}

@book{keyvani2002vhdl,
	author = {Keyvani, M.},
	date-added = {2020-07-18 21:09:49 +1200},
	date-modified = {2020-07-18 21:10:27 +1200},
	isbn = {9780612818378},
	keywords = {Other, Thesis, Non-blocking Network, FPGA},
	publisher = {Thesis (M.A.Sc.)--Simon Fraser University},
	series = {Canadian theses on microfiche},
	title = {VHDL Implementation of a High-speed Symmetric Crossbar Switch [microform]},
	url = {https://books.google.ca/books?id=fFldXwAACAAJ},
	year = {2002},
	Bdsk-Url-1 = {https://books.google.ca/books?id=fFldXwAACAAJ}}

@article{Prasanna:2012aa,
	abstract = {Soft processors often use data caches to reduce the gap between processor andmain memory speeds. To achieve high efficiency, simple, blocking caches are used. Such caches are not appropriate for processor designs such as Runahead and out-of-order execution that require nonblocking caches to tolerate main memory latencies. Instead, these processors use non-blocking caches to extract memory level parallelismand improve performance. However, conventional non-blocking cache designs are expensive and slow on FPGAs as they use content-addressable memories (CAMs). Thiswork proposes NCOR, an FPGA-friendly non-blocking cache that exploits the keyproperties of Runahead execution. NCOR does not require CAMs and utilizes smartcache controllers. A 4\&{\#}x2009;KB NCOR operates at 329\&{\#}x2009;MHz on Stratix III FPGAs while ituses only 270 logic elements. A 32\&{\#}x2009;KB NCOR operates at 278\&{\#}x2009;Mhz and uses 269 logicelements.},
	author = {Prasanna, Viktor K. and Aasaraai, Kaveh and Moshovos, Andreas},
	da = {2011/11/29},
	date-added = {2020-07-18 20:54:06 +1200},
	date-modified = {2020-07-18 20:54:33 +1200},
	doi = {10.1155/2012/915178},
	isbn = {1687-7195},
	journal = {International Journal of Reconfigurable Computing},
	keywords = {Other, Non-blocking Network, FPGA, Cache, Soft processor},
	pages = {915178},
	publisher = {Hindawi Publishing Corporation},
	title = {NCOR: An FPGA-Friendly Nonblocking Data Cache for Soft Processors with Runahead Execution},
	ty = {JOUR},
	url = {https://doi.org/10.1155/2012/915178},
	volume = {2012},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1155/2012/915178}}

@electronic{BiSUNAGithub,
	author = {Raul Valencia},
	date-added = {2020-07-18 00:03:29 +1200},
	date-modified = {2020-07-18 00:03:29 +1200},
	keywords = {BiSUNA,},
	month = {Sep},
	title = {Binary Spectrum-diverse Unified Neuroevolution Architecture},
	url = {https://github.com/rval735/BiSUNA},
	urldate = {Sep 14, 2019},
	year = {2019},
	Bdsk-Url-1 = {https://github.com/rval735/BiSUNA}}

@electronic{TF-DQNTut,
	author = {TensorFlow},
	date-added = {2020-07-17 19:50:35 +1200},
	date-modified = {2020-07-17 19:52:55 +1200},
	keywords = {url, Tutorial, DQN,},
	title = {Train a Deep Q Network with TF-Agents},
	url = {https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial},
	urldate = {Jul,2020},
	Bdsk-Url-1 = {https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial}}

@inproceedings{8951676,
	abstract = {Generating image and video is a hot topic in Deep Learning. Especially, generating video is a difficult but meaningful work. How to generate video which has diversity and plausibility is still a problem to be solved. In this paper, we propose a novel model of Generative Adversarial Network(GAN) which called FDGAN to generate clear contour lines. Unlike existing GAN that only use frames, our method extends to use inter-frame difference. First introduce two temporal difference methods to process the inter-frame. Then increase a frame difference discriminator to discriminate whether the inter-frame is true or not. Using the model and new structure proposed, we perform video generation experiments on several widely used benchmark datasets such as MOVING MNIST, UCF-101. Consequently, the results achieve state-of-the-art performance for clarifying contour lines. Both quantitative and qualitative evaluations were made to show the effectiveness of our methods.},
	author = {R. {Qiu} and D. V. {Vargas} and K. {Sakurai}},
	booktitle = {2019 Seventh International Symposium on Computing and Networking Workshops (CANDARW)},
	date-added = {2020-07-16 19:21:21 +1200},
	date-modified = {2020-07-16 19:21:30 +1200},
	doi = {10.1109/CANDARW.2019.00037},
	keywords = {DCGAN, image motion analysis;image segmentation;learning (artificial intelligence);object detection;video signal processing;frame difference Generative Adversarial networks;clearer contour video generating;generating image;Deep Learning;generating video;plausibility;clear contour lines;inter-frame difference;temporal difference methods;frame difference discriminator;video generation experiments;Frame Difference;Generative Adversarial Networks;Computer Vision;Artificial Intelligence},
	month = {Nov},
	pages = {169-175},
	title = {Frame Difference Generative Adversarial Networks: Clearer Contour Video Generating},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/CANDARW.2019.00037}}

@inproceedings{10.1145/3377929.3389933,
	address = {New York, NY, USA},
	author = {Tenorio, Raul Horacio Valencia and Sham, Chiu Wing and Vargas, Danilo Vasconcellos},
	booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
	date-added = {2020-07-15 17:32:35 +1200},
	date-modified = {2020-07-21 14:24:28 +1200},
	doi = {10.1145/3377929.3389933},
	isbn = {9781450371278},
	keywords = {Neuroevolution, Evolutionary, CPA, binary neural network, BiSUNA, adversarial neurocryptography, neuroevolution},
	location = {Canc\'{u}n, Mexico},
	numpages = {2},
	pages = {291--292},
	publisher = {Association for Computing Machinery},
	series = {GECCO '20},
	title = {Preliminary Study of Applied Binary Neural Networks for Neural Cryptography},
	url = {https://doi.org/10.1145/3377929.3389933},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1145/3377929.3389933}}

@article{Mnih:2015aa,
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	da = {2015/02/01},
	date-added = {2020-07-14 23:22:15 +1200},
	date-modified = {2020-07-14 23:41:54 +1200},
	doi = {10.1038/nature14236},
	id = {Mnih2015},
	isbn = {1476-4687},
	journal = {Nature},
	keywords = {DNN, DQN, Deepmind, Q-Learning},
	number = {7540},
	pages = {529--533},
	title = {Human-level control through deep reinforcement learning},
	ty = {JOUR},
	url = {https://doi.org/10.1038/nature14236},
	volume = {518},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature14236}}

@electronic{gRPCPage,
	author = {Linux Foundation},
	date-added = {2020-07-09 22:26:06 +1200},
	date-modified = {2020-07-09 22:27:10 +1200},
	keywords = {URL},
	lastchecked = {9/Jul/2020},
	title = {gRPC},
	url = {https://grpc.io},
	year = {2020},
	Bdsk-Url-1 = {https://grpc.io}}

@inproceedings{857854,
	abstract = {In this paper we show how the familiar concept of gradient descent can be extended in presence of binary neurons. The procedure we devised formally operates on generic feedforward networks of logistic-like neurons whose activations are re-scaled by an arbitrarily large gauge. Whereas the gradient decays exponentially with increasing values of the gauge, the sign of each component becomes definitely equal to a constant value. Those values are actually computed by means of a "twin" network of binary neurons. This allows the application of any "Manhattan" training algorithm such as resilient propagation.},
	author = {M. {Costa} and D. {Palmisano} and E. {Pasero}},
	booktitle = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
	date-added = {2020-07-09 21:11:02 +1200},
	date-modified = {2020-07-09 21:11:14 +1200},
	doi = {10.1109/IJCNN.2000.857854},
	issn = {1098-7576},
	keywords = {NN, feedforward neural nets;gradient methods;learning (artificial intelligence);gradient descent;feedforward networks;binary neurons;logistic-like neurons;twin neural network;Manhattan training algorithm;resilient propagation;Intelligent networks;Feedforward systems;Neurons;Computer networks;Logistics;Turning;Cost function;Signal resolution;Computer architecture;Parallel processing},
	month = {July},
	pages = {311-316 vol.1},
	title = {Gradient descent in feed-forward networks with binary neurons},
	volume = {1},
	year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2000.857854}}

@inproceedings{8832578,
	abstract = {The most widely used activation functions in current deep feed-forward neural networks are rectified linear units (ReLU), besides many alternatives. However, none of these alternatives have managed to consistently outperform the rest and there is no unified theory connecting properties of the task and networks with properties of activation functions in the sense of efficient training. In order to understand the related problems fundamentally, it is necessary to figure out possible causes of gradient instability mathematically, and how different activation functions can be adopted to improve system performances. Theoretical analysis about gradient instability is given in the paper, as well as the fundamental explanation for the exploding/vanishing gradient and the possible solutions. The performances of different activation functions in a given example network are investigated. Numerical simulations suggest that the convergence rate of gradient varies with the activation function. There is no activation function that performs well to all structures. The findings in the paper provide a reference for the selection of activation function in the design of deep neural network models.},
	author = {X. {Liu} and J. {Zhou} and H. {Qian}},
	booktitle = {2019 Chinese Control And Decision Conference (CCDC)},
	date-added = {2020-07-08 20:45:40 +1200},
	date-modified = {2020-07-08 20:46:08 +1200},
	doi = {10.1109/CCDC.2019.8832578},
	issn = {1948-9447},
	keywords = {NN; exploding gradient, feedforward neural nets;gradient methods;gradient instability;deep neural network models;deep feed-forward neural networks;activation functions;Training;Neurons;Biological neural networks;Backpropagation;Convergence;Task analysis;Activation Functions;Gradient Instability;Deep Neural Network},
	month = {June},
	pages = {3966-3971},
	title = {Comparison and Evaluation of Activation Functions in Term of Gradient Instability in Deep Neural Networks},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/CCDC.2019.8832578}}

@inproceedings{8296359,
	abstract = {Gradient control plays an important role in feed-forward networks applied to various computer vision tasks. Previous work has shown that Recurrent Highway Networks minimize the problem of vanishing or exploding gradients. They achieve this by setting the eigenvalues of the temporal Jacobian to 1 across the time steps. In this work, batch normalized recurrent highway networks are proposed to control the gradient flow in an improved way for network convergence. Specifically, the introduced model can be formed by batch normalizing the inputs at each recurrence loop. The proposed model is tested on an image captioning task using MSCOCO dataset. Experimental results indicate that the batch normalized recurrent highway networks converge faster and performs better compared with the traditional LSTM and RHN based models.},
	author = {C. {Zhang} and T. {Nguyen} and S. {Sah} and R. {Ptucha} and A. {Loui} and C. {Salvaggio}},
	booktitle = {2017 IEEE International Conference on Image Processing (ICIP)},
	date-added = {2020-07-08 19:11:30 +1200},
	date-modified = {2020-07-08 19:11:52 +1200},
	doi = {10.1109/ICIP.2017.8296359},
	issn = {2381-8549},
	keywords = {NN, batch normalization, computer vision;eigenvalues and eigenfunctions;feedforward neural nets;batch-normalized recurrent highway networks;gradient control;feed-forward networks;computer vision tasks;recurrence loop;eigenvalues;temporal Jacobian;gradient flow;image captioning task;Training;Logic gates;Road transportation;Task analysis;Transforms;Measurement;Computer architecture;Gradient control;recurrent highway network;batch normalization;vanishing gradient;exploding gradient},
	month = {Sep.},
	pages = {640-644},
	title = {Batch-normalized recurrent highway networks},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIP.2017.8296359}}

@book{FundML,
	author = {Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar},
	date-added = {2020-07-07 17:31:11 +1200},
	date-modified = {2020-07-07 17:32:05 +1200},
	edition = {2},
	editor = {MIT Press},
	publisher = {MIT Press},
	title = {Foundations of Machine Learning},
	year = {2018}}

@article{10.1147/rd.33.0210,
	address = {USA},
	author = {Samuel, A. L.},
	date-added = {2020-07-02 14:43:11 +1200},
	date-modified = {2020-07-02 14:43:25 +1200},
	doi = {10.1147/rd.33.0210},
	issn = {0018-8646},
	issue_date = {July 1959},
	journal = {IBM J. Res. Dev.},
	keywords = {NN, Machine Learning},
	month = jul,
	number = {3},
	numpages = {20},
	pages = {210--229},
	publisher = {IBM Corp.},
	title = {Some Studies in Machine Learning Using the Game of Checkers},
	url = {https://doi.org/10.1147/rd.33.0210},
	volume = {3},
	year = {1959},
	Bdsk-Url-1 = {https://doi.org/10.1147/rd.33.0210}}

@article{8114708,
	abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
	author = {V. {Sze} and Y. {Chen} and T. {Yang} and J. S. {Emer}},
	date-added = {2020-07-01 16:02:45 +1200},
	date-modified = {2020-07-01 16:03:36 +1200},
	doi = {10.1109/JPROC.2017.2761740},
	issn = {1558-2256},
	journal = {Proceedings of the IEEE},
	keywords = {NN, artificial intelligence;computational complexity;neural nets;energy efficiency;hardware design changes;DNN hardware designs;deep neural networks;hardware cost;computation cost reduction;artificial intelligence;computational complexity;hardware platforms;hardware architecture;DNN hardware implementations;Neurons;Biological neural networks;Artificial intelligence;Machine learning;Neural networks;Tutorials;Convolutional neural networks;Artificial intelligence;Benchmark testing;Computer architecture;ASIC;computer architecture;convolutional neural networks;dataflow processing;deep learning;deep neural networks;energy-efficient accelerators;low power;machine learning;spatial architectures;VLSI},
	month = {Dec},
	number = {12},
	pages = {2295-2329},
	title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
	volume = {105},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/JPROC.2017.2761740}}

@inproceedings{8573476,
	abstract = {The recent popularity of deep neural networks (DNNs) has generated considerable research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference - i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark suite for DNN training, called TBD1, which comprises a representative set of eight DNN models and covers six major machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) performing an extensive performance analysis of these models on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of performance metrics, and methodologies to analyze the results. We also build a new set of tools for memory profiling in three major frameworks. These tools can shed light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. Using our tools and methodologies, we make several important observations and recommendations on where future DNN training research and optimization should be focused.},
	author = {H. {Zhu} and M. {Akrout} and B. {Zheng} and A. {Pelegris} and A. {Jayarajan} and A. {Phanishayee} and B. {Schroeder} and G. {Pekhimenko}},
	booktitle = {2018 IEEE International Symposium on Workload Characterization (IISWC)},
	date-added = {2020-06-30 22:55:37 +1200},
	date-modified = {2020-06-30 22:56:02 +1200},
	doi = {10.1109/IISWC.2018.8573476},
	keywords = {CNN, Benchmark, data handling;data structures;learning (artificial intelligence);neural nets;deep neural network training;deep neural networks;DNN-related computation;trained models;image classification networks;primary benchmark;myopic view;benchmark suite;DNN training;representative set;DNN models;machine translation;speech recognition;object detection;adversarial networks;reinforcement learning;extensive performance analysis;deep learning frameworks;multiGPU;performance analysis tools;performance metrics;optimization;TBD;machine learning applications;hardware configurations;Training;Benchmark testing;Tools;Graphics processing units;Computational modeling;Speech recognition;Hardware},
	month = {Sep.},
	pages = {88-100},
	title = {Benchmarking and Analyzing Deep Neural Network Training},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IISWC.2018.8573476}}

@inproceedings{7298594,
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	author = {C. {Szegedy} and {Wei Liu} and {Yangqing Jia} and P. {Sermanet} and S. {Reed} and D. {Anguelov} and D. {Erhan} and V. {Vanhoucke} and A. {Rabinovich}},
	booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2020-06-29 23:30:16 +1200},
	date-modified = {2020-06-29 23:30:34 +1200},
	doi = {10.1109/CVPR.2015.7298594},
	issn = {1063-6919},
	keywords = {CNN; GoogleNet; convolution;decision making;feature extraction;Hebbian learning;image classification;neural net architecture;resource allocation;convolutional neural network architecture;resource utilization;architectural decision;Hebbian principle;object classification;object detection;Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision},
	month = {June},
	pages = {1-9},
	title = {Going deeper with convolutions},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2015.7298594}}

@article{7332968,
	abstract = {This paper aims to accelerate the test-time computation of convolutional neural networks (CNNs), especially very deep CNNs [1] that have substantially impacted the computer vision community. Unlike previous methods that are designed for approximating linear filters or linear responses, our method takes the nonlinear units into account. We develop an effective solution to the resulting nonlinear optimization problem without the need of stochastic gradient descent (SGD). More importantly, while previous methods mainly focus on optimizing one or two layers, our nonlinear method enables an asymmetric reconstruction that reduces the rapidly accumulated error when multiple (e.g.,  $\ge$ 10) layers are approximated. For the widely used very deep VGG-16 model [1] , our method achieves a whole-model speedup of 4$\times$  with merely a 0.3 percent increase of top-5 error in ImageNet classification. Our 4$\times$  accelerated VGG-16 model also shows a graceful accuracy degradation for object detection when plugged into the Fast R-CNN detector [2] .},
	author = {X. {Zhang} and J. {Zou} and K. {He} and J. {Sun}},
	date-added = {2020-06-29 22:24:04 +1200},
	date-modified = {2020-06-29 22:24:24 +1200},
	doi = {10.1109/TPAMI.2015.2502579},
	issn = {1939-3539},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {CNN, VGGNet, Acceleration;Image reconstruction;Optimization;Accuracy;Object detection;Covariance matrices;Life estimation;Convolutional neural networks;acceleration;image classification;object detection},
	month = {Oct},
	number = {10},
	pages = {1943-1955},
	title = {Accelerating Very Deep Convolutional Networks for Classification and Detection},
	volume = {38},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/TPAMI.2015.2502579}}

@article{JMLR:v18:16-456,
	author = {Itay Hubara and Matthieu Courbariaux and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
	date-added = {2020-06-29 19:00:51 +1200},
	date-modified = {2020-06-29 19:04:29 +1200},
	journal = {Journal of Machine Learning Research},
	keywords = {NN, Quantization, Neural Network, Binarization},
	number = {187},
	pages = {1-30},
	title = {Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations},
	url = {http://jmlr.org/papers/v18/16-456.html},
	volume = {18},
	year = {2018},
	Bdsk-Url-1 = {http://jmlr.org/papers/v18/16-456.html}}

@inproceedings{10.1145/3205455.3205517,
	address = {New York, NY, USA},
	author = {Volz, Vanessa and Schrum, Jacob and Liu, Jialin and Lucas, Simon M. and Smith, Adam and Risi, Sebastian},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	date-added = {2020-06-29 18:05:40 +1200},
	date-modified = {2020-06-29 18:06:49 +1200},
	doi = {10.1145/3205455.3205517},
	isbn = {9781450356183},
	keywords = {DCGAN, generative adversarial network, CMA-ES, procedural content generation, mario, game},
	location = {Kyoto, Japan},
	numpages = {8},
	pages = {221--228},
	publisher = {Association for Computing Machinery},
	series = {GECCO '18},
	title = {Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network},
	url = {https://doi.org/10.1145/3205455.3205517},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1145/3205455.3205517}}

@inproceedings{MatThCryptoShannon,
	author = {Claude Shannon},
	booktitle = {A Mathematical Theory of Cryptography},
	date-added = {2020-05-25 12:38:16 +1200},
	date-modified = {2020-05-25 12:42:57 +1200},
	editor = {International Association for Cryptologic Research},
	keywords = {Crypto, Shannon, Cryptography, mathematical theory},
	number = {20878},
	organization = {International Association for Cryptologic Research},
	pages = {132},
	publisher = {International Association for Cryptologic Research},
	title = {A Mathematical Theory of Cryptography},
	volume = {1},
	year = {1945}}

@article{AES-NIST,
	author = {Morris J. Dworkin, Elaine B. Barker, James R. Nechvatal, James Foti, Lawrence E. Bassham, E. Roback, James F. Dray Jr.},
	date-added = {2020-05-15 15:26:41 +1200},
	date-modified = {2020-05-15 15:45:01 +1200},
	journal = {Federal Inf. Process. Stds. (NIST FIPS)},
	keywords = {Crypto, Cryptography, AES, NIST},
	month = {Nov},
	pages = {47},
	title = {Advanced Encryption Standard (AES)},
	volume = {197},
	year = {2001}}

@article{10.1371/journal.pone.0203434,
	abstract = {The normalized cross-correlation (NCC), usually its 2D version, is routinely encountered in template matching algorithms, such as in facial recognition, motion-tracking, registration in medical imaging, etc. Its rapid computation becomes critical in time sensitive applications. Here I develop a scheme for the computation of NCC by fast Fourier transform that can favorably compare for speed efficiency with other existing techniques and may outperform some of them given an appropriate search scenario.},
	author = {Kaso, Artan},
	date-added = {2020-05-15 13:27:51 +1200},
	date-modified = {2020-05-15 13:28:08 +1200},
	doi = {10.1371/journal.pone.0203434},
	journal = {PLOS ONE},
	keywords = {Other, NCC, fast Fourier transforms},
	month = {09},
	number = {9},
	pages = {1-16},
	publisher = {Public Library of Science},
	title = {Computation of the normalized cross-correlation by fast Fourier transform},
	url = {https://doi.org/10.1371/journal.pone.0203434},
	volume = {13},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0203434}}

@article{CommTheoryShannon,
	author = {Shannon, Claude},
	date-added = {2020-05-15 12:49:39 +1200},
	date-modified = {2020-05-15 12:52:06 +1200},
	journal = {Bell system Technical Journal},
	keywords = {Cryptography, Cryptanalysis},
	month = {1949},
	number = {4},
	pages = {656-715},
	title = {Communication Theory and Secrecy Systems},
	volume = {28},
	year = {1949}}

@inproceedings{10.1007/11423461_14,
	abstract = {Binary sequences with good autocorrelation properties are widely used in cryptography. If the autocorrelation properties are optimum, then the sequences are called perfect. In the last few years, new constructions for perfect sequences have been found. In this paper we investigate the cross-correlation properties between perfect sequences. We give a lower bound for the maximum cross-correlation coefficient between arbitrary perfect sequences. We conjecture that this bound is not best possible. Furthermore, we determine perfect sequences with provable good correlation properties.},
	address = {Berlin, Heidelberg},
	author = {Hertel, Doreen},
	booktitle = {Sequences and Their Applications - SETA 2004},
	date-added = {2020-05-09 12:43:34 +1200},
	date-modified = {2020-05-09 12:43:52 +1200},
	editor = {Helleseth, Tor and Sarwate, Dilip and Song, Hong-Yeop and Yang, Kyeongcheol},
	isbn = {978-3-540-32048-7},
	keywords = {Other, Binary stream, cross-correlation},
	pages = {208--219},
	publisher = {Springer Berlin Heidelberg},
	title = {Cross-Correlation Properties of Perfect Binary Sequences},
	year = {2005}}

@phdthesis{0fb77cd6fdab4aa19e35fc2c45de5f30,
	abstract = {The focus of this dissertation is to present cryptanalytic results on selected block ciphers. Block ciphers are the mathematical structure that will take a plaintext message and convert it into a ciphertext one block at a time using a secret key. They play an essential role in many cryptographic architectures and frameworks. For a long time they were known as the main building block that will provide confidentiality in an information system. They would also be able to represent a full spectrum of cryptographic services as many block ciphers can be used to construct stream ciphers, hash functions, pseudorandom number generators, and authenticated encryption designs. For this reason a multitude of initiatives over the years has been established to provide a secure and sound designs for block ciphers as in the calls for Data Encryption Standard (DES) and Advanced Encryption Standard (AES), lightweight ciphers initiatives, and the Competition for Authenticated Encryption: Security, Applicability, and Robustness (CAESAR).In this thesis, we first present cryptanalytic results on different ciphers. We propose attack named the Invariant Subspace Attack. It is utilized to break the full block cipher PRINTcipher for a significant fraction of its keys. This new attack also gives us new insights into other, more well-established attacks. In addition, we also show that for weak keys, strongly biased linear approximations exists for any number of rounds. Furthermore, we provide variety of attacks on the family of lightweight block cipher SIMON that was published by the U.S National Security Agency (NSA). The ciphers are developed with optimization towards both hardware and software in mind. While the specification paper discusses design requirements and performance of the presented lightweight ciphers thoroughly, no security assessment is given. We present a series of observations on the presented construction that, in some cases, yield attacks, while in other cases may provide basis of further analysis by the cryptographic community. Specifically, The attacks obtained are using classical- as well as truncated differentials. In addition to that, we also investigate the security of SIMON against different linear cryptanalysis methods, i.e., classic linear,and linear hull attacks. we present a connection between linear characteristic and differential characteristic, multiple linear and differential and linear hull and differential, and employ it to adapt the current known results on differential cryptanalysis of SIMON to linear cryptanalysis results. Finally, we investigate links between different methods of cryptanalysis and how they can be utilized for block cipher cryptanalysis. We consider the known results on the links among integral, impossible differential and zero-correlation linear hulls in order to prove that constructing a zero-correlation linear hull always implies the existence of an integral distinguisher. Moreover, we show that constructing zero-correlation linear hull on a Feistel structure with SP-type round functions, where P is a binary matrix, is equivalent to constructing impossible differential on the same structure except that P is substituted by the transposed matrix PT . We present an integral distinguishers of 5-round Feistel structure with bijective round functions and 3-round Feistel structure with round functions not necessarily being bijective. In addition to an integral distinguishers of Camellia so far, i.e., 7-round integral distinguishers of Camellia with FL/FL−1 layer and 8-round integral distinguishers of Camellia without FL/FL−1 layer.},
	author = {Alkhzaimi, {Hoda A.}},
	date-added = {2020-05-08 16:05:53 +1200},
	date-modified = {2020-05-08 16:06:15 +1200},
	keywords = {Crypto, Cryptoanalisys, Block Cipher},
	language = {English},
	number = {360},
	publisher = {Technical University of Denmark},
	series = {DTU Compute PHD-2015},
	title = {Cryptanalysis of Selected Block Ciphers},
	year = {2016}}

@incollection{NIPS2015_5872,
	author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
	booktitle = {Advances in Neural Information Processing Systems 28},
	date-added = {2020-04-29 19:22:36 +1200},
	date-modified = {2020-04-29 19:23:04 +1200},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	keywords = {DNN, AutoML, NAS},
	pages = {2962--2970},
	publisher = {Curran Associates, Inc.},
	title = {Efficient and Robust Automated Machine Learning},
	url = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf},
	year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf}}

@incollection{NIPS2019_8736,
	author = {Sun, Xiao and Choi, Jungwook and Chen, Chia-Yu and Wang, Naigang and Venkataramani, Swagath and Srinivasan, Vijayalakshmi (Viji) and Cui, Xiaodong and Zhang, Wei and Gopalakrishnan, Kailash},
	booktitle = {Advances in Neural Information Processing Systems 32},
	date-added = {2020-04-22 15:56:48 +1200},
	date-modified = {2020-04-22 15:57:25 +1200},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. Alch\'{e}-Buc and E. Fox and R. Garnett},
	keywords = {NN, floating point arithmetic, reduced bit-width},
	pages = {4900--4909},
	publisher = {Curran Associates, Inc.},
	title = {Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks},
	url = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf},
	year = {2019},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks.pdf}}

@misc{real2020automlzero,
	archiveprefix = {arXiv},
	author = {Esteban Real and Chen Liang and David R. So and Quoc V. Le},
	date-added = {2020-04-15 19:45:49 +1200},
	date-modified = {2020-04-15 19:46:11 +1200},
	eprint = {2003.03384},
	keywords = {DNN, Evolutionary, Google, AutoML},
	primaryclass = {cs.LG},
	title = {AutoML-Zero: Evolving Machine Learning Algorithms From Scratch},
	year = {2020}}

@incollection{NIPS2019_9194,
	author = {Lou, Qian and Jiang, Lei},
	booktitle = {Advances in Neural Information Processing Systems 32},
	date-added = {2020-04-14 16:28:05 +1200},
	date-modified = {2020-04-22 13:28:54 +1200},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. Alch\'{e}-Buc and E. Fox and R. Garnett},
	keywords = {Crypto, FHE, DNN, encrypted data},
	pages = {10035--10043},
	publisher = {Curran Associates, Inc.},
	title = {SHE: A Fast and Accurate Deep Neural Network for Encrypted Data},
	url = {http://papers.nips.cc/paper/9194-she-a-fast-and-accurate-deep-neural-network-for-encrypted-data.pdf},
	year = {2019},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/9194-she-a-fast-and-accurate-deep-neural-network-for-encrypted-data.pdf}}

@misc{ruttor2007neural,
	archiveprefix = {arXiv},
	author = {Andreas Ruttor},
	date-added = {2020-04-14 15:35:52 +1200},
	date-modified = {2020-04-14 15:36:34 +1200},
	eprint = {0711.2411},
	keywords = {Crypto, Neural cryptography, Tree Parity Machine, Thesis},
	primaryclass = {cond-mat.dis-nn},
	title = {Neural Synchronization and Cryptography},
	year = {2007}}

@article{10.1145/190616.190617,
	address = {New York, NY, USA},
	author = {Brassard, Gilles},
	date-added = {2020-04-08 16:05:36 +1200},
	date-modified = {2020-04-08 16:06:11 +1200},
	doi = {10.1145/190616.190617},
	issn = {0163-5700},
	issue_date = {December 1994},
	journal = {SIGACT News},
	keywords = {Crypto, Quantum Computing, Cryptography, Physics},
	month = dec,
	number = {4},
	numpages = {7},
	pages = {15--21},
	publisher = {Association for Computing Machinery},
	title = {Quantum Computing: The End of Classical Cryptography?},
	url = {https://doi-org.ezproxy.auckland.ac.nz/10.1145/190616.190617},
	volume = {25},
	year = {1994},
	Bdsk-Url-1 = {https://doi-org.ezproxy.auckland.ac.nz/10.1145/190616.190617},
	Bdsk-Url-2 = {https://doi.org/10.1145/190616.190617}}

@article{Shi:2020aa,
	abstract = {An efficient cryptography scheme is proposed based on continuous-variable quantum neural network (CV-QNN), in which a specified CV-QNN model is introduced for designing the quantum cryptography algorithm. It indicates an approach to design a quantum neural cryptosystem which contains the processes of key generation, encryption and decryption. Security analysis demonstrates that our scheme is security. Several simulation experiments are performed on the Strawberry Fields platform for processing the classical data ``Quantum Cryptography''with CV-QNN to describe the feasibility of our method. Three sets of representative experiments are presented and the second experimental results confirm that our scheme can correctly and effectively encrypt and decrypt data with the optimal learning rate 8e −2 regardless of classical or quantum data, and better performance can be achieved with the method of learning rate adaption (where increase factor R1 = 2, decrease factor R2 = 0.8). Indeed, the scheme with learning rate adaption can shorten the encryption and decryption time according to the simulation results presented in Figure 12. It can be considered as a valid quantum cryptography scheme and has a potential application on quantum devices.},
	author = {Shi, Jinjing and Chen, Shuhui and Lu, Yuhu and Feng, Yanyan and Shi, Ronghua and Yang, Yuguang and Li, Jian},
	da = {2020/02/07},
	date-added = {2020-04-07 22:47:30 +1200},
	date-modified = {2020-04-07 22:50:40 +1200},
	doi = {10.1038/s41598-020-58928-1},
	id = {Shi2020},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	keywords = {Crypto, Quantum Neural Network, Continous variable, Quantum Cryptography},
	number = {1},
	pages = {2107},
	title = {An Approach to Cryptography Based on Continuous-Variable Quantum Neural Network},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s41598-020-58928-1},
	volume = {10},
	year = {2020},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41598-020-58928-1}}

@incollection{NIPS2019_8304,
	author = {Minderer, Matthias and Sun, Chen and Villegas, Ruben and Cole, Forrester and Murphy, Kevin P and Lee, Honglak},
	booktitle = {Advances in Neural Information Processing Systems 32},
	date-added = {2020-04-07 16:26:52 +1200},
	date-modified = {2020-04-08 13:14:49 +1200},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. Alch\'{e}-Buc and E. Fox and R. Garnett},
	keywords = {NN, Unsupervised learning, Video, Human3.6M},
	pages = {92--102},
	publisher = {Curran Associates, Inc.},
	title = {Unsupervised learning of object structure and dynamics from videos},
	url = {http://papers.nips.cc/paper/8304-unsupervised-learning-of-object-structure-and-dynamics-from-videos.pdf},
	year = {2019},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/8304-unsupervised-learning-of-object-structure-and-dynamics-from-videos.pdf}}

@incollection{NIPS2004_2744,
	author = {Einat Klein and Rachel Mislovaty and Ido Kanter and Ruttor, Andreas and Wolfgang Kinzel},
	booktitle = {Advances in Neural Information Processing Systems 17},
	date-added = {2020-04-07 13:01:02 +1200},
	date-modified = {2020-04-07 13:02:02 +1200},
	editor = {L. K. Saul and Y. Weiss and L. Bottou},
	keywords = {Cryptography, Neural Network, Mutual Learning},
	pages = {689--696},
	publisher = {MIT Press},
	title = {Synchronization of neural networks by mutual learning and its application to cryptography},
	url = {http://papers.nips.cc/paper/2744-synchronization-of-neural-networks-by-mutual-learning-and-its-application-to-cryptography.pdf},
	year = {2005},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/2744-synchronization-of-neural-networks-by-mutual-learning-and-its-application-to-cryptography.pdf}}

@incollection{NIPS2019_8778,
	author = {Freeman, Daniel and Ha, David and Metz, Luke},
	booktitle = {Advances in Neural Information Processing Systems 32},
	date-added = {2020-02-05 18:45:58 +1300},
	date-modified = {2020-02-05 18:48:44 +1300},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	keywords = {DNN, NN, reinforcement learning},
	pages = {5380--5391},
	publisher = {Curran Associates, Inc.},
	title = {Learning to Predict Without Looking Ahead: World Models Without Forward Prediction},
	url = {http://papers.nips.cc/paper/8778-learning-to-predict-without-looking-ahead-world-models-without-forward-prediction.pdf},
	year = {2019},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/8778-learning-to-predict-without-looking-ahead-world-models-without-forward-prediction.pdf}}

@article{Jaderberg859,
	abstract = {Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.Science, this issue p. 859Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.},
	author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
	date-added = {2020-02-05 18:34:34 +1300},
	date-modified = {2020-12-10 14:39:47 +1300},
	doi = {10.1126/science.aau6249},
	issn = {0036-8075},
	journal = {Science},
	keywords = {DNN, DeepMind, 3D mutiplayer, multi-agent systems},
	number = {6443},
	pages = {859--865},
	publisher = {American Association for the Advancement of Science},
	title = {Human-level performance in 3D multiplayer games with population-based reinforcement learning},
	volume = {364},
	year = {2019},
	Bdsk-Url-1 = {https://science.sciencemag.org/content/364/6443/859},
	Bdsk-Url-2 = {https://doi.org/10.1126/science.aau6249}}

@incollection{NIPS1992_714,
	author = {Dayan, Peter and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 5},
	date-added = {2020-02-05 18:25:46 +1300},
	date-modified = {2020-02-05 18:26:05 +1300},
	editor = {S. J. Hanson and J. D. Cowan and C. L. Giles},
	keywords = {DNN, reinforcement learning, Training},
	pages = {271--278},
	publisher = {Morgan-Kaufmann},
	title = {Feudal Reinforcement Learning},
	url = {http://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf},
	year = {1993},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/714-feudal-reinforcement-learning.pdf}}

@inproceedings{baker2020emergent,
	author = {Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
	booktitle = {International Conference on Learning Representations},
	date-added = {2020-02-05 18:11:24 +1300},
	date-modified = {2020-02-05 18:11:42 +1300},
	keywords = {DNN, multi-agent systems, OpenAI},
	title = {Emergent Tool Use From Multi-Agent Autocurricula},
	url = {https://openreview.net/forum?id=SkxpxJBKwS},
	year = {2020},
	Bdsk-Url-1 = {https://openreview.net/forum?id=SkxpxJBKwS}}

@inproceedings{burda2018exploration,
	author = {Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
	booktitle = {International Conference on Learning Representations},
	date-added = {2020-02-05 17:50:49 +1300},
	date-modified = {2020-02-05 17:51:15 +1300},
	keywords = {DNN, random structure, Network distillation, OpenAI},
	title = {Exploration by random network distillation},
	url = {https://openreview.net/forum?id=H1lJJnR5Ym},
	year = {2019},
	Bdsk-Url-1 = {https://openreview.net/forum?id=H1lJJnR5Ym}}

@inproceedings{pathak18largescale,
	annote = {https://github.com/openai/large-scale-curiosity},
	author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
	booktitle = {ICLR},
	date-added = {2020-02-05 17:26:59 +1300},
	date-modified = {2020-02-05 17:51:45 +1300},
	keywords = {DNN, Curiosity NN, OpenAI},
	title = {Large-Scale Study of Curiosity-Driven Learning},
	year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBKLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRE5OL0V4cGxvcmF0aW9uIGJ5IFJhbmRvbSBOZXR3b3JrIERpc3RpbGxhdGlvbi5iaWJPEQHuAAAAAAHuAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRXhwbG9yYXRpb24gYnkgUmFuI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAA0ROTgAAAgBgLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkROTjpFeHBsb3JhdGlvbiBieSBSYW5kb20gTmV0d29yayBEaXN0aWxsYXRpb24uYmliAA4AXgAuAEUAeABwAGwAbwByAGEAdABpAG8AbgAgAGIAeQAgAFIAYQBuAGQAbwBtACAATgBlAHQAdwBvAHIAawAgAEQAaQBzAHQAaQBsAGwAYQB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBeVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9ETk4vRXhwbG9yYXRpb24gYnkgUmFuZG9tIE5ldHdvcmsgRGlzdGlsbGF0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJABxAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAmM=}}

@inproceedings{8977877,
	abstract = {The exponential progress of semiconductor tech-nologies has enabled the proliferation of deep learning as a prominent area of research, where neural networks have demon-strated its effectiveness to solve very hard multi dimensional problems. This paper focuses on one in particular, Binary Neural Networks (BNN), which use fixed length bits in its connections and logic functions to perform excitation operations. Exploiting those characteristics, hardware accelerators that integrate field-programmable gate arrays (FPGAs) have been adopted to hasten inference of deep learning networks, given its proficiency to maximize parallelism and energy efficiency. This work will show how the algorithm Binary Spectrum-diverse Unified Neuroevolution Architecture (BiSUNA) can perform training and inference on FPGA without the need of gradient descent. Source code can be found in github.com/rval735/bisunaocl},
	author = {R. {Valencia} and C. W. {Sham} and O. {Sinnen}},
	booktitle = {2019 International Conference on Field-Programmable Technology (ICFPT)},
	date-added = {2020-02-04 20:40:00 +1300},
	date-modified = {2020-02-04 20:40:39 +1300},
	doi = {10.1109/ICFPT47387.2019.00076},
	issn = {null},
	keywords = {BNN, FPGA;BiSUNA;Binary Neural Network;TWEANN;Evolutionary Algorithm},
	month = {Dec},
	pages = {395-398},
	title = {Evolved Binary Neural Networks Through Harnessing FPGA Capabilities},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICFPT47387.2019.00076}}

@book{katz2008introduction,
	author = {Katz, J. and Lindell, Y.},
	date-added = {2020-01-24 16:06:25 +1300},
	date-modified = {2020-01-24 16:12:56 +1300},
	isbn = {9781584885511},
	keywords = {Books, Cryptographic, CPA,},
	lccn = {2007017861},
	publisher = {Chapman \& Hall/CRC},
	series = {Chapman and Hall/CRC Cryptography and Network Security Series},
	title = {Introduction to modern cryptography},
	url = {https://books.google.ca/books?id=WIc\_AQAAIAAJ},
	year = {2008},
	Bdsk-Url-1 = {https://books.google.ca/books?id=WIc%5C_AQAAIAAJ}}

@incollection{NIPS2014_5423,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	booktitle = {Advances in Neural Information Processing Systems 27},
	date-added = {2020-01-22 18:50:40 +1300},
	date-modified = {2020-01-22 18:51:11 +1300},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	keywords = {DCGAN, GAN, CNN, adversarial training},
	pages = {2672--2680},
	publisher = {Curran Associates, Inc.},
	title = {Generative Adversarial Nets},
	url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
	year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}}

@inproceedings{5669194,
	abstract = {Exclusive-OR (XOR) gate is one of the critical components in many applications such as cryptography. In this paper, we present an efficient multi-input XOR circuit design based on pass-transistor logic (PTL). A synthesis algorithm is developed to efficiently generate the PTL-based multi-input XOR circuits. Both pre-layout and post-layout simulation results show that our proposed multi-input XOR design outperforms static CMOS design. The multi-input XOR circuits are also used to design the transformations in the Advanced Encryption Standard (AES).},
	author = {S. {Hsiao} and C. {Wen} and M. {Tsai} and M. {Chen}},
	booktitle = {2010 International Symposium on Next Generation Electronics},
	date-added = {2020-01-22 16:45:37 +1300},
	date-modified = {2020-01-22 16:45:45 +1300},
	doi = {10.1109/ISNE.2010.5669194},
	issn = {2378-8607},
	keywords = {Crypto, CMOS logic circuits;cryptography;logic design;transistor-transistor logic;automatic generation;high performance multiple-input XOR/XNOR circuits;advanced encryption standard;pass transistor logic;static CMOS design;Data structures;IP networks;Lead;Boolean functions;Layout;CMOS integrated circuits;Encryption},
	month = {Nov},
	pages = {77-80},
	title = {Automatic generation of high-performance multiple-input XOR/XNOR circuits and its application in Advanced Encryption Standard (AES)},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNE.2010.5669194}}

@inproceedings{4459620,
	abstract = {Pseudorandom number generator (PRNG) has been used in a variety of applications like computer simulations, and industrial applications including cryptography. High-quality PRNG can be constructed by employing cellular automata (CA). Advantage of the PRNG that employs CA includes that it is fast and suitable for hardware implementation. In this paper, we propose a two-dimensional (2-D) CA based PRNG. Our scheme uses the structure of programmable CA (PCA) for improving randomness quality. The CA rules used our PRNG are additive rules which has XOR and XNOR logic with a hybrid dependency, only rules 15, 31, 47 and 63. Moreover, for minimising of auto correlations among the produced pseudorandom number blocks, a novel time spacing technique is proposed without a loss of an original cycle length. Finally, we provide experimental results to verify the randomness quality using ENT and DIEHARD test suites.},
	author = {B. {Kang} and D. {Lee} and C. {Hong}},
	booktitle = {4th IEEE International Symposium on Electronic Design, Test and Applications (delta 2008)},
	date-added = {2020-01-22 16:42:09 +1300},
	date-modified = {2020-01-22 16:42:19 +1300},
	doi = {10.1109/DELTA.2008.46},
	issn = {null},
	keywords = {Other; cellular automata;random number generation;high-performance pseudorandom number generator;two-dimensional programmable cellular automata;PRNG;randomness quality;additive rules;XOR logic;XNOR logic;time spacing technique;Application software;Computer simulation;Computer industry;Cryptography;Hardware;Two dimensional displays;Principal component analysis;Logic;Autocorrelation;Testing},
	month = {Jan},
	pages = {597-602},
	title = {High-Performance Pseudorandom Number Generator Using Two-Dimensional Cellular Automata},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/DELTA.2008.46}}

@article{DBLP:journals/corr/abs-1808-03819,
	archiveprefix = {arXiv},
	author = {Thomas Shortell and Ali Shokoufandeh},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1808-03819},
	date-added = {2020-01-22 15:58:13 +1300},
	date-modified = {2020-01-22 15:59:30 +1300},
	eprint = {1808.03819},
	journal = {CoRR},
	keywords = {CNN, Crypto, Fully Homomorphic Encryption},
	timestamp = {Sun, 02 Sep 2018 15:01:53 +0200},
	title = {Secure Convolutional Neural Network using {FHE}},
	url = {http://arxiv.org/abs/1808.03819},
	volume = {abs/1808.03819},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1808.03819}}

@inproceedings{1004508,
	abstract = {Neuroevolution, i.e. evolving artificial neural networks with genetic algorithms, has been highly effective in reinforcement learning tasks, particularly those with hidden state information. An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology methods on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, making it possible to evolve increasingly complex solutions over time, thereby strengthening the analogy with biological evolution.},
	author = {K. O. {Stanley} and R. {Miikkulainen}},
	booktitle = {Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)},
	date-added = {2020-01-21 18:47:21 +1300},
	date-modified = {2020-01-21 18:47:39 +1300},
	doi = {10.1109/CEC.2002.1004508},
	issn = {null},
	keywords = {Evolutionary; AI; neural nets;genetic algorithms;neural network topologies;neuroevolution;evolving artificial neural networks;genetic algorithms;reinforcement learning;hidden state information;neuroevolution of augmenting topologies;fixed-topology methods;ablation studies;Neural networks;Network topology;Evolution (biology);Artificial neural networks;Learning;Genetic algorithms;Benchmark testing;Protection;Technological innovation;System testing},
	month = {May},
	pages = {1757-1762 vol.2},
	title = {Efficient evolution of neural network topologies},
	volume = {2},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2002.1004508}}

@article{scarpiniti_hybridizing_2019,
	abstract = {Neuroevolution is the field of study that uses evolutionary computation in order to optimize certain aspect of the design of neural networks, most often its topology and hyperparameters. The field was introduced in the late-1980s, but only in the latest years the field has become mature enough to enable the optimization of deep learning models, such as convolutional neural networks. In this paper, we rely on previous work to apply neuroevolution in order to optimize the topology of deep neural networks that can be used to solve the problem of handwritten character recognition. Moreover, we take advantage of the fact that evolutionary algorithms optimize a population of candidate solutions, by combining a set of the best evolved models resulting in a committee of convolutional neural networks. This process is enhanced by using specific mechanisms to preserve the diversity of the population. Additionally, in this paper, we address one of the disadvantages of neuroevolution: the process is very expensive in terms of computational time. To lessen this issue, we explore the performance of topology transfer learning: whether the best topology obtained using neuroevolution for a certain domain can be successfully applied to a different domain. By doing so, the expensive process of neuroevolution can be reused to tackle different problems, turning it into a more appealing approach for optimizing the design of neural networks topologies. After evaluating our proposal, results show that both the use of neuroevolved committees and the application of topology transfer learning are successful: committees of convolutional neural networks are able to improve classification results when compared to single models, and topologies learned for one problem can be reused for a different problem and data with a good performance. Additionally, both approaches can be combined by building committees of transferred topologies, and this combination attains results that combine the best of both approaches.},
	author = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
	date-added = {2020-01-21 18:37:46 +1300},
	date-modified = {2020-01-21 18:38:36 +1300},
	doi = {10.1155/2019/2952304},
	editor = {Scarpiniti, Michele},
	issn = {1076-2787},
	journal = {Complexity},
	keywords = {CNN, evolutionary computation, MNIST},
	month = mar,
	pages = {2952304},
	title = {Hybridizing {Evolutionary} {Computation} and {Deep} {Neural} {Networks}: {An} {Approach} to {Handwriting} {Recognition} {Using} {Committees} and {Transfer} {Learning}},
	url = {https://doi.org/10.1155/2019/2952304},
	volume = {2019},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1155/2019/2952304}}

@article{YU2006333,
	abstract = {In this Letter, a novel approach of encryption based on chaotic Hopfield neural networks with time varying delay is proposed. We use the chaotic neural network to generate binary sequences which will be used for masking plaintext. The plaintext is masked by switching of chaotic neural network maps and permutation of generated binary sequences. Simulation results were given to show the feasibility and effectiveness in the proposed scheme of this Letter. As a result, chaotic cryptography becomes more practical in the secure transmission of large multi-media files over public data communication network.},
	author = {Wenwu Yu and Jinde Cao},
	date-added = {2020-01-21 15:41:13 +1300},
	date-modified = {2020-01-21 15:41:23 +1300},
	doi = {https://doi.org/10.1016/j.physleta.2006.03.069},
	issn = {0375-9601},
	journal = {Physics Letters A},
	keywords = {Crypto, Synchronization, Time varying delay, Neural network, Chaos, Encryption, Chaotic cryptosystem},
	number = {4},
	pages = {333 - 338},
	title = {Cryptography based on delayed chaotic neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S037596010600510X},
	volume = {356},
	year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S037596010600510X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.physleta.2006.03.069}}

@inproceedings{10.1007/3-540-36178-2_18,
	abstract = {In this paper we analyse the security of a new key exchange protocol proposed in [3], which is based on mutually learning neural networks. This is a new potential source for public key cryptographic schemes which are not based on number theoretic functions, and have small time and memory complexities. In the first part of the paper we analyse the scheme, explain why the two parties converge to a common key, and why an attacker using a similar neural network is unlikely to converge to the same key. However, in the second part of the paper we show that this key exchange protocol can be broken in three different ways, and thus it is completely insecure.},
	address = {Berlin, Heidelberg},
	author = {Klimov, Alexander and Mityagin, Anton and Shamir, Adi},
	booktitle = {Advances in Cryptology --- ASIACRYPT 2002},
	date-added = {2020-01-21 13:39:49 +1300},
	date-modified = {2020-01-21 13:41:24 +1300},
	editor = {Zheng, Yuliang},
	isbn = {978-3-540-36178-7},
	keywords = {Crypto, Neurocryptography, Public key},
	pages = {288--298},
	publisher = {Springer Berlin Heidelberg},
	title = {Analysis of Neural Cryptography},
	year = {2002}}

@inproceedings{10.1007/978-3-642-34500-5_75,
	abstract = {In this paper, we apply a new cryptanalytic attack on DES and Triple-DES. The implemented attack is a known-plaintext attack based on neural networks. In this attack we trained a neural network to retrieve plaintext from ciphertext without retrieving the key used in encryption.},
	address = {Berlin, Heidelberg},
	author = {Alani, Mohammed M.},
	booktitle = {Neural Information Processing},
	date-added = {2020-01-21 13:18:24 +1300},
	date-modified = {2020-01-21 13:26:13 +1300},
	editor = {Huang, Tingwen and Zeng, Zhigang and Li, Chuandong and Leung, Chi Sing},
	isbn = {978-3-642-34500-5},
	keywords = {Book, Neurocryptography, DES,},
	pages = {637--646},
	publisher = {Springer Berlin Heidelberg},
	title = {Neuro-Cryptanalysis of DES and Triple-DES},
	year = {2012}}

@article{MEURICEDEDORMALE200772,
	abstract = {For the last decade, Elliptic Curve Cryptography (ECC) has gained increasing acceptance in the industry and the academic community and has been the subject of several standards. This interest is mainly due to the high level of security with relatively small keys provided by ECC. To sustain the high throughput required by applications like network servers, high-speed implementations of public-key cryptosystems are needed. For that purpose, hardware-based accelerators are often the only solution reaching an acceptable performance-cost ratio. The fundamental question that arises is how to choose the appropriate efficiency--flexibility tradeoff. In this survey, techniques for implementing Elliptic Curve Cryptography at a high-speed are explored. A classification of the work available in the open literature in function of the level of efficiency and flexibility is also proposed. In particular, the subjects of reconfigurable, dedicated, generator, versatile and general purpose scalar multipliers are addressed. Finally, some words about future work that should be tackled are provided.},
	author = {Guerric Meurice de Dormale and Jean-Jacques Quisquater},
	date-added = {2020-01-20 23:26:10 +1300},
	date-modified = {2020-01-20 23:26:10 +1300},
	doi = {https://doi.org/10.1016/j.sysarc.2006.09.002},
	issn = {1383-7621},
	journal = {Journal of Systems Architecture},
	keywords = {Public-key cryptography, Elliptic Curve Cryptography, High-speed hardware implementation, Efficiency--flexibility tradeoffs, Network applications},
	note = {Embedded Hardware for Cryptosystems},
	number = {2},
	pages = {72 - 84},
	title = {High-speed hardware implementations of Elliptic Curve Cryptography: A survey},
	url = {http://www.sciencedirect.com/science/article/pii/S1383762106001044},
	volume = {53},
	year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106001044},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.09.002}}

@article{ISMAYILOV2020307,
	abstract = {Workflow scheduling is a largely studied research topic in cloud computing, which targets to utilize cloud resources for workflow tasks by considering the objectives specified in QoS. In this paper, we model dynamic workflow scheduling problem as a dynamic multi-objective optimization problem (DMOP) where the source of dynamism is based on both resource failures and the number of objectives which may change over time. Software faults and/or hardware faults may cause the first type of dynamism. On the other hand, confronting real-life scenarios in cloud computing may change number of objectives at runtime during the execution of a workflow. In this study, we propose a prediction-based dynamic multi-objective evolutionary algorithm, called NN-DNSGA-II algorithm, by incorporating artificial neural network with the NSGA-II algorithm. Additionally, five leading non-prediction based dynamic algorithms from the literature are adapted for the dynamic workflow scheduling problem. Scheduling solutions are found by the consideration of six objectives: minimization of makespan, cost, energy and degree of imbalance; and maximization of reliability and utilization. The empirical study based on real-world applications from Pegasus workflow management system reveals that our NN-DNSGA-II algorithm significantly outperforms the other alternatives in most cases with respect to metrics used for DMOPs with unknown true Pareto-optimal front, including the number of non-dominated solutions, Schott's spacing and Hypervolume indicator.},
	author = {Goshgar Ismayilov and Haluk Rahmi Topcuoglu},
	date-added = {2020-01-20 23:15:20 +1300},
	date-modified = {2020-01-20 23:15:28 +1300},
	doi = {https://doi.org/10.1016/j.future.2019.08.012},
	issn = {0167-739X},
	journal = {Future Generation Computer Systems},
	keywords = {NN; Workflow scheduling, Resource failures, Changing number of objectives, Dynamic multi-objective evolutionary algorithms, Neural networks},
	pages = {307 - 322},
	title = {Neural network based multi-objective evolutionary algorithm for dynamic workflow scheduling in cloud computing},
	url = {http://www.sciencedirect.com/science/article/pii/S0167739X19306983},
	volume = {102},
	year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167739X19306983},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.future.2019.08.012}}

@article{NALEPA2020102994,
	abstract = {Hyperspectral image analysis has been gaining research attention thanks to the current advances in sensor design which have made acquiring such imagery much more affordable. Although there exist various approaches for segmenting hyperspectral images, deep learning has become the mainstream. However, such large-capacity learners are characterized by significant memory footprints. This is a serious obstacle in employing deep neural networks on board a satellite for Earth observation. In this paper, we introduce resource-frugal quantized convolutional neural networks, and greatly reduce their size without adversely affecting the classification capability. Our experiments performed over two hyperspectral benchmarks showed that the quantization process can be seamlessly applied during the training, and it leads to much smaller and still well-generalizing deep models.},
	author = {Jakub Nalepa and Marek Antoniak and Michal Myller and Pablo Ribalta Lorenzo and Michal Marcinkiewicz},
	date-added = {2020-01-20 23:08:37 +1300},
	date-modified = {2020-01-20 23:08:46 +1300},
	doi = {https://doi.org/10.1016/j.micpro.2020.102994},
	issn = {0141-9331},
	journal = {Microprocessors and Microsystems},
	keywords = {NN, Hyperspectral imaging, Deep neural network, Convolutional neural network, Quantization, Segmentation, Classification},
	pages = {102994},
	title = {Towards resource-frugal deep convolutional neural networks for hyperspectral image segmentation},
	url = {http://www.sciencedirect.com/science/article/pii/S0141933119302844},
	volume = {73},
	year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0141933119302844},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.micpro.2020.102994}}

@inproceedings{1590168,
	abstract = {The goal of any cryptographic system is the exchange of information among the intended users without any leakage of information to others who may have unauthorized access to it. In 1976, Diffie & Hellmann found that a common secret key could be created over a public channel accessible to any opponent. Since then many public key cryptography have been presented which are based on number theory and they demand large computational power. Moreover the process involved in generating public key is very complex and time consuming. To overcome these disadvantages, the neural networks can be used to generate common secret key. This is the motivation for this present work on interacting neural networks and cryptography[1].In the case of neural cryptography, both the communicating networks receive an identical input vector, generate an output bit and are trained based on the output bit. The dynamics of the two networks and their weight vectors is found to exhibit a novel phenomenon, where the networks synchronize to a state with identical time-dependent weights. This concept of synchronization by mutual learning can be applied to a secret key exchange protocol over a public channel. The generation of secret key over a public channel has been studied and the generated key is used for encrypting and decrypting the given message using DES algorithm which is simulated and synthesized using VHDL},
	author = {T. {Godhavari} and N. R. {Alamelu} and R. {Soundararajan}},
	booktitle = {2005 Annual IEEE India Conference - Indicon},
	date-added = {2020-01-20 19:52:11 +1300},
	date-modified = {2020-01-20 19:52:19 +1300},
	doi = {10.1109/INDCON.2005.1590168},
	issn = {2325-9418},
	keywords = {Crypto; neural cryptography;mutual learning;time-dependent weights;Neural networks;Public key cryptography;Biological neural networks;Public key;Computer networks;Concurrent computing;Artificial neural networks;Cryptographic protocols;Computational modeling;Network synthesis;neural cryptography;mutual learning;time-dependent weights},
	month = {Dec},
	pages = {258-261},
	title = {Cryptography Using Neural Network},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/INDCON.2005.1590168}}

@inproceedings{7583925,
	abstract = {In cryptography secret information is made unreadable for an unauthorized user. There are many cryptographic algorithms are available, but they are more complex techniques and requires more computational power. This paper gives a review of how Neural Networks contributes a help in cryptography and how neural network and cryptography together can be used for security.},
	author = {P. P. {Hadke} and S. G. {Kale}},
	booktitle = {2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (Startup Conclave)},
	date-added = {2020-01-20 19:46:34 +1300},
	date-modified = {2020-01-20 19:46:42 +1300},
	doi = {10.1109/STARTUP.2016.7583925},
	issn = {null},
	keywords = {Crypto; authorisation;cryptography;neural nets;neural networks;cryptography secret information;user unauthorization;complex techniques;computational power;Biological neural networks;Encryption;Watermarking;Neurons;cryptography;cryptosystem;Neural Network;key generation and management},
	month = {Feb},
	pages = {1-4},
	title = {Use of Neural Networks in cryptography: A review},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/STARTUP.2016.7583925}}

@inproceedings{861518,
	abstract = {Visual cryptography finds many applications in the cryptographic field such as key management, message concealment, authorization, authentication, identification, and entertainment. The authors propose a novel approach for visual cryptography using neural networks (NNs). To perform encrypting, the input to the NN is a set of gray level images, and the output is a set of binary images (shares) that fulfils the desirable access scheme. This approach is considerably different from the traditional one, and can be applied to cope with very complex access schemes.},
	author = {{Tai-Wen Yue} and {Suchen Chiang}},
	booktitle = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
	date-added = {2020-01-20 19:45:33 +1300},
	date-modified = {2020-01-20 19:45:41 +1300},
	doi = {10.1109/IJCNN.2000.861518},
	issn = {1098-7576},
	keywords = {Crypto; cryptography;neural nets;image processing;quantum computing;neural network approach;visual cryptography;cryptographic field;key management;message concealment;authorization;authentication;identification;entertainment;encrypting;NN;gray level images;binary images;access scheme;Neural networks;Cryptography;Books;Computer science;Application software;Engineering management;Authorization;Authentication;Image recognition;Target recognition},
	month = {July},
	pages = {494-499 vol.5},
	title = {A neural network approach for visual cryptography},
	volume = {5},
	year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2000.861518}}

@inproceedings{8953134,
	abstract = {With the explosive interest in the utilization of Neural Networks, several approaches have taken place to make them faster, more accurate or power efficient; one technique used to simplify inference models is the utilization of binary representations for weights, activations, inputs and outputs. This paper presents a novel approach to train from scratch Binary Neural Networks using neuroevolution as its base technique (gradient descent free), to then apply such results to standard Reinforcement Learning environments tested in the OpenAI Gym. The results and code can be found in https://github.com/rval735/BiSUNA.},
	author = {R. {Valencia} and C. {Sham} and O. {Sinnen}},
	booktitle = {2019 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
	date-added = {2020-01-20 16:32:29 +1300},
	date-modified = {2020-01-20 16:32:40 +1300},
	doi = {10.1109/APCCAS47518.2019.8953134},
	issn = {null},
	keywords = {BNN; Neuroevolution;binary neural networks;BiSUNA;discrete optimization},
	month = {Nov},
	pages = {301-304},
	title = {Using Neuroevolved Binary Neural Networks to solve reinforcement learning environments},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/APCCAS47518.2019.8953134}}

@article{SEC-019,
	author = {David Evans and Vladimir Kolesnikov and Mike Rosulek},
	date-added = {2019-11-06 23:12:31 +1300},
	date-modified = {2019-11-06 23:13:08 +1300},
	doi = {10.1561/3300000019},
	issn = {2474-1558},
	journal = {Foundations and Trends{\textregistered} in Privacy and Security},
	keywords = {Crypto, Multi-party computations, Books},
	number = {2-3},
	pages = {70-246},
	title = {A Pragmatic Introduction to Secure Multi-Party Computation},
	url = {http://dx.doi.org/10.1561/3300000019},
	volume = {2},
	year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1561/3300000019}}

@inproceedings{ijcai2018-0547,
	author = {Qiao Zhang and Cong Wang and Hongyi Wu and Chunsheng Xin and Tran V. Phuong},
	booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
	date-added = {2019-11-06 19:56:30 +1300},
	date-modified = {2019-11-06 19:57:39 +1300},
	doi = {10.24963/ijcai.2018/547},
	keywords = {Crypto, NN, Stegranography, Homomorphic encryption},
	month = {7},
	pages = {3933--3939},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {GELU-Net: A Globally Encrypted, Locally Unencrypted Deep Neural Network for Privacy-Preserved Learning},
	url = {https://doi.org/10.24963/ijcai.2018/547},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2018/547}}

@inproceedings{10.1007/978-3-030-30048-7_24,
	abstract = {Binarized Neural Networks (BNNs) are an important class of neural network characterized by weights and activations restricted to the set {\$}{\$}{\backslash}{\{}-1,+1{\backslash}{\}}{\$}{\$}. BNNs provide simple compact descriptions and as such have a wide range of applications in low-power devices. In this paper, we investigate a model-based approach to training BNNs using constraint programming (CP), mixed-integer programming (MIP), and CP/MIP hybrids. We formulate the training problem as finding a set of weights that correctly classify the training set instances while optimizing objective functions that have been proposed in the literature as proxies for generalizability. Our experimental results on the MNIST digit recognition dataset suggest that---when training data is limited---the BNNs found by our hybrid approach generalize better than those obtained from a state-of-the-art gradient descent method. More broadly, this work enables the analysis of neural network performance based on the availability of optimal solutions and optimality bounds.},
	address = {Cham},
	author = {Toro Icarte, Rodrigo and Illanes, Le{\'o}n and Castro, Margarita P. and Cire, Andre A. and McIlraith, Sheila A. and Beck, J. Christopher},
	booktitle = {Principles and Practice of Constraint Programming},
	date-added = {2019-10-09 12:46:42 +1300},
	date-modified = {2019-10-09 12:47:13 +1300},
	editor = {Schiex, Thomas and de Givry, Simon},
	isbn = {978-3-030-30048-7},
	keywords = {BNN, Mixed-Integer Programming, Constraint Programming},
	pages = {401--417},
	publisher = {Springer International Publishing},
	title = {Training Binarized Neural Networks Using MIP and CP},
	year = {2019}}

@inproceedings{Ma_2019_CVPR_Workshops,
	author = {Ma, Yinglan and Xiong, Hongyu and Hu, Zhe and Ma, Lizhuang},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	date-added = {2019-10-09 12:38:51 +1300},
	date-modified = {2019-10-09 12:39:21 +1300},
	keywords = {BNN, image processing, CNN},
	month = {June},
	title = {Efficient Super Resolution Using Binarized Neural Network},
	year = {2019}}

@article{Su:2017:NNB:3039902.3039915,
	acmid = {3039915},
	address = {New York, NY, USA},
	author = {Su, Jiang and Liu, Jianxiong and Thomas, David B. and Cheung, Peter Y.K.},
	date-added = {2019-10-08 23:15:02 +1300},
	date-modified = {2019-10-08 23:15:25 +1300},
	doi = {10.1145/3039902.3039915},
	issn = {0163-5964},
	issue_date = {September 2016},
	journal = {SIGARCH Comput. Archit. News},
	keywords = {FPGA-NN, DQN, Reinforcement Learning},
	month = jan,
	number = {4},
	numpages = {6},
	pages = {68--73},
	publisher = {ACM},
	title = {Neural Network Based Reinforcement Learning Acceleration on FPGA Platforms},
	url = {http://doi.acm.org/10.1145/3039902.3039915},
	volume = {44},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3039902.3039915},
	Bdsk-Url-2 = {https://doi.org/10.1145/3039902.3039915}}

@article{electronics8060661,
	abstract = {In this work, we review Binarized Neural Networks (BNNs). BNNs are deep neural networks that use binary values for activations and weights, instead of full precision values. With binary values, BNNs can execute computations using bitwise operations, which reduces execution time. Model sizes of BNNs are much smaller than their full precision counterparts. While the accuracy of a BNN model is generally less than full precision models, BNNs have been closing accuracy gap and are becoming more accurate on larger datasets like ImageNet. BNNs are also good candidates for deep learning implementations on FPGAs and ASICs due to their bitwise efficiency. We give a tutorial of the general BNN methodology and review various contributions, implementations and applications of BNNs.},
	article-number = {661},
	author = {Simons, Taylor and Lee, Dah-Jye},
	date-added = {2019-10-08 15:58:12 +1300},
	date-modified = {2019-10-08 15:58:34 +1300},
	doi = {10.3390/electronics8060661},
	issn = {2079-9292},
	journal = {Electronics},
	keywords = {BNN, Survey, backpropagation},
	number = {6},
	title = {A Review of Binarized Neural Networks},
	url = {https://www.mdpi.com/2079-9292/8/6/661},
	volume = {8},
	year = {2019},
	Bdsk-Url-1 = {https://www.mdpi.com/2079-9292/8/6/661},
	Bdsk-Url-2 = {https://doi.org/10.3390/electronics8060661}}

@electronic{BNN-FPT-Paper-Data,
	author = {Raul Valencia},
	date-added = {2019-09-15 23:25:18 +1200},
	date-modified = {2019-09-15 23:27:03 +1200},
	keywords = {Paper, Github, FPT, Data},
	title = {BNN-FPT Raw Data},
	url = {github.com/rval735/BNN-PhD/tree/master/Presentations/Paper-BNNFPGA},
	urldate = {Sep 15},
	Bdsk-Url-1 = {github.com/rval735/BNN-PhD/tree/master/Presentations/Paper-BNNFPGA}}

@electronic{BiSUNAOCLGithub,
	author = {Raul Valencia},
	date-added = {2019-09-14 21:28:31 +1200},
	date-modified = {2020-07-18 00:04:32 +1200},
	keywords = {BiSUNA, OCL},
	month = {Dec},
	title = {Binary Spectrum-diverse Unified Neuroevolution Architecture},
	url = {github.com/rval735/bisunaocl},
	urldate = {Dec 15, 2019},
	year = {2019},
	Bdsk-Url-1 = {https://github.com/rval735/BiSUNA},
	Bdsk-Url-2 = {github.com/rval735/bisunaocl}}

@article{8594633,
	abstract = {Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.},
	author = {A. {Shawahna} and S. M. {Sait} and A. {El-Maleh}},
	date-added = {2019-09-14 16:22:32 +1200},
	date-modified = {2019-09-14 16:23:27 +1200},
	doi = {10.1109/ACCESS.2018.2890150},
	journal = {IEEE Access},
	keywords = {FPGA-NN, Deep learning; FPGA; Neural networks; Hardware; Acceleration;Convolution; Adaptable architectures; CNN; deep learning; dynamic reconfiguration; energy-efficient architecture; hardware accelerator; machine learning; neural networks; optimization;parallel computer architecture;reconfigurable computing},
	pages = {7823-7859},
	title = {FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review},
	volume = {7},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACCESS.2018.2890150}}

@article{2019arXiv190407852B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190407852B},
	archiveprefix = {arXiv},
	author = {{Bulat}, A. and {Kossaifi}, J. and {Tzimiropoulos}, G. and {Pantic}, M.},
	date-added = {2019-07-28 17:05:45 +1200},
	date-modified = {2019-07-28 17:06:01 +1200},
	eprint = {1904.07852},
	journal = {arXiv e-prints},
	keywords = {BNN, Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning},
	month = apr,
	primaryclass = {cs.CV},
	title = {{Matrix and tensor decompositions for training binary neural networks}},
	year = 2019}

@article{2019arXiv190608637B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190608637B},
	archiveprefix = {arXiv},
	author = {{Bethge}, J. and {Yang}, H. and {Bornstein}, M. and {Meinel}, C.},
	date-added = {2019-07-28 17:03:09 +1200},
	date-modified = {2019-07-28 17:03:31 +1200},
	eprint = {1906.08637},
	journal = {arXiv e-prints},
	keywords = {BNN, Machine Learning, Computer Vision and Pattern Recognition, Machine Learning},
	month = jun,
	title = {{Back to Simplicity: How to Train Accurate BNNs from Scratch?}},
	year = 2019,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBkLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL01hdHJpeCBhbmQgdGVuc29yIGRlY29tcG9zaXRpb25zIGZvciB0cmFpbmluZyBiaW5hcnkgbmV1cmFsIG5ldHdvcmtzLmJpYk8RAlYAAAAAAlYAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9NYXRyaXggYW5kIHRlbnNvciAjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADQk5OAAACAHovOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Qk5OOk1hdHJpeCBhbmQgdGVuc29yIGRlY29tcG9zaXRpb25zIGZvciB0cmFpbmluZyBiaW5hcnkgbmV1cmFsIG5ldHdvcmtzLmJpYgAOAJIASABNAGEAdAByAGkAeAAgAGEAbgBkACAAdABlAG4AcwBvAHIAIABkAGUAYwBvAG0AcABvAHMAaQB0AGkAbwBuAHMAIABmAG8AcgAgAHQAcgBhAGkAbgBpAG4AZwAgAGIAaQBuAGEAcgB5ACAAbgBlAHUAcgBhAGwAIABuAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAHhVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0JOTi9NYXRyaXggYW5kIHRlbnNvciBkZWNvbXBvc2l0aW9ucyBmb3IgdHJhaW5pbmcgYmluYXJ5IG5ldXJhbCBuZXR3b3Jrcy5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAiwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAALl}}

@webpage{NectarCloud,
	author = {National Research Infraestructure for Australia},
	date-added = {2019-07-28 12:42:59 +1200},
	date-modified = {2020-08-20 21:48:19 +1200},
	keywords = {Nectar, Cloud, New Zealand, Research},
	lastchecked = {Jul, 2019},
	month = {Nov},
	title = {New Zealand Nectar Cloud Research},
	url = {https://nectar.org.au/new-zealand-researchers-join-nectar-cloud/},
	urldate = {Nov, 2017},
	year = {2017},
	Bdsk-Url-1 = {https://nectar.org.au/new-zealand-researchers-join-nectar-cloud/}}

@inproceedings{10.1007/978-3-319-78890-6_44,
	abstract = {OpenCL has been proposed as a means of accelerating functional computation using FPGA and GPU accelerators. Although it provides ease of programmability and code portability, questions remain about the performance portability and underlying vendor's compiler capabilities to generate efficient implementations without user-defined, platform specific optimizations. In this work, we systematically evaluate this by formalizing a design space exploration strategy using platform-independent micro-architectural and application-specific optimizations only. The optimizations are then applied across Altera FPGA, NVIDIA GPU and ARM Mali GPU platforms for three computing examples, namely matrix-matrix multiplication, binomial-tree option pricing and 3-dimensional finite difference time domain. Our strategy enables a fair comparison across platforms in terms of throughput and energy efficiency by using the same design effort. Our results indicate that FPGA provides better performance portability in terms of achieved percentage of device's peak performance (68{\%}) compared to NVIDIA GPU (20{\%}) and also achieves better energy efficiency (up to 1.4{\$}{\$}{\backslash}times {\$}{\$}) for some of the considered cases without requiring in-depth hardware design expertise.},
	address = {Cham},
	annote = {Inside book "Applied Reconfigurable Computing. Architectures, Tools, and Applications"},
	author = {Minhas, Umar Ibrahim and Woods, Roger and Karakonstantis, Georgios},
	booktitle = {Applied Reconfigurable Computing. Architectures, Tools, and Applications},
	date-added = {2019-06-14 08:22:58 +1200},
	date-modified = {2019-06-14 08:23:35 +1200},
	editor = {Voros, Nikolaos and Huebner, Michael and Keramidas, Georgios and Goehringer, Diana and Antonopoulos, Christos and Diniz, Pedro C.},
	isbn = {978-3-319-78890-6},
	keywords = {Books, FPGA, OpenCL, Odroid},
	pages = {551--563},
	publisher = {Springer International Publishing},
	title = {Exploring Functional Acceleration of OpenCL on FPGAs and GPUs Through Platform-Independent Optimizations},
	year = {2018}}

@misc{cryptoeprint:2018:1056,
	author = {Ahmad Al Badawi and Jin Chao and Jie Lin and Chan Fook Mun and Jun Jie Sim and Benjamin Hong Meng Tan and Xiao Nan and Khin Mi Mi Aung and Vijay Ramaseshan Chandrasekhar},
	date-added = {2019-06-06 21:06:38 +1200},
	date-modified = {2020-01-21 18:33:45 +1300},
	howpublished = {Cryptology ePrint Archive, Report 2018/1056},
	keywords = {Crypto, Homomorphic encryption, CNN},
	note = {\url{https://eprint.iacr.org/2018/1056}},
	title = {The AlexNet Moment for Homomorphic Encryption: HCNN, the First Homomorphic CNN on Encrypted Data with GPUs},
	year = {2018}}

@inproceedings{10.1007/978-3-642-22792-9_28,
	abstract = {At Eurocrypt 2010 van Dijk et al. described a fully homomorphic encryption scheme over the integers. The main appeal of this scheme (compared to Gentry's) is its conceptual simplicity. This simplicity comes at the expense of a public key size in {\$}{\{}{\backslash}cal {\backslash}tilde O{\}}({\backslash}lambda^{\{}10{\}}){\$}which is too large for any practical system. In this paper we reduce the public key size to {\$}{\{}{\backslash}cal {\backslash}tilde O{\}}({\backslash}lambda^{\{}7{\}}){\$}by encrypting with a quadratic form in the public key elements, instead of a linear form. We prove that the scheme remains semantically secure, based on a stronger variant of the approximate-GCD problem, already considered by van Dijk et al.},
	address = {Berlin, Heidelberg},
	author = {Coron, Jean-S{\'e}bastien and Mandal, Avradip and Naccache, David and Tibouchi, Mehdi},
	booktitle = {Advances in Cryptology -- CRYPTO 2011},
	date-added = {2019-06-06 21:04:34 +1200},
	date-modified = {2019-06-06 21:05:00 +1200},
	editor = {Rogaway, Phillip},
	isbn = {978-3-642-22792-9},
	keywords = {Crypto, Fully Homomorphic Encryption, Public key},
	pages = {487--504},
	publisher = {Springer Berlin Heidelberg},
	title = {Fully Homomorphic Encryption over the Integers with Shorter Public Keys},
	year = {2011}}

@article{10.1371/journal.pone.0122236,
	abstract = {Secure multiparty computation allows for a set of users to evaluate a particular function over their inputs without revealing the information they possess to each other. Theoretically, this can be achieved using fully homomorphic encryption systems, but so far they remain in the realm of computational impracticability. An alternative is to consider secure function evaluation using homomorphic public-key cryptosystems or Garbled Circuits, the latter being a popular trend in recent times due to important breakthroughs. We propose a technique for computing the logsum operation using Garbled Circuits. This technique relies on replacing the logsum operation with an equivalent piecewise linear approximation, taking advantage of recent advances in efficient methods for both designing and implementing Garbled Circuits. We elaborate on how all the required blocks should be assembled in order to obtain small errors regarding the original logsum operation and very fast execution times.},
	author = {Port{\^e}lo, Jos{\'e} AND Raj, Bhiksha AND Trancoso, Isabel},
	date-added = {2019-06-06 20:35:53 +1200},
	date-modified = {2019-06-06 20:37:16 +1200},
	doi = {10.1371/journal.pone.0122236},
	journal = {PLOS ONE},
	keywords = {Crypto, garbled-circuit, Linear approximation},
	month = {03},
	number = {3},
	pages = {1-16},
	publisher = {Public Library of Science},
	title = {Logsum Using Garbled Circuits},
	url = {https://doi.org/10.1371/journal.pone.0122236},
	volume = {10},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0122236}}

@article{Rouhani:2018:RRP:3299999.3242899,
	acmid = {3242899},
	address = {New York, NY, USA},
	articleno = {21},
	author = {Rouhani, Bita Darvish and Hussain, Siam Umar and Lauter, Kristin and Koushanfar, Farinaz},
	date-added = {2019-06-06 14:16:45 +1200},
	date-modified = {2019-06-06 14:16:55 +1200},
	doi = {10.1145/3242899},
	issn = {1936-7406},
	issue_date = {December 2018},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	keywords = {Crypto, Secure machine learning, data mining, deep learning, garbled Circuit, privacy-preserving computation},
	month = dec,
	number = {3},
	numpages = {21},
	pages = {21:1--21:21},
	publisher = {ACM},
	title = {ReDCrypt: Real-Time Privacy-Preserving Deep Learning Inference in Clouds Using FPGAs},
	url = {http://doi.acm.org/10.1145/3242899},
	volume = {11},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3242899},
	Bdsk-Url-2 = {https://doi.org/10.1145/3242899}}

@inproceedings{1133,
	attachments = {http://aceslab.org/sites/default/files/XONN.pdf},
	author = {M. Sadegh Riazi and Mohammad Samragh and Hao Chen and Kim Laine and Kristin Lauter and Farinaz Koushanfar},
	booktitle = {USENIX Security},
	date-added = {2019-06-06 14:06:32 +1200},
	date-modified = {2019-06-06 14:07:02 +1200},
	keywords = {Crypto, Homomorphic encryption, BNN, Cryptography and Security},
	month = {05/2019},
	organization = {USENIX},
	publisher = {USENIX},
	title = {XONN: XNOR-based Oblivious Deep Neural Network Inference},
	url = {https://arxiv.org/pdf/1902.07342.pdf},
	year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1902.07342.pdf}}

@inproceedings{1136,
	abstract = {<p>We present FASE, an FPGA accelerator for Secure Function Evaluation (SFE) by employing the well-known cryptographic protocol named Yao\&rsquo;s Garbled Circuit (GC). SFE allows two parties to jointly compute a function on their private data and learn the output without revealing their inputs to each other. FASE is designed to allow cloud servers to provide secure services to a large number of clients in parallel while preserving the privacy of the data from both sides. Current SFE accelerators either target specific applications, and therefore are not amenable to generic use, or have low throughput due to inefficient management of resources. In this work, we present a pipelined architecture along with an efficient scheduling scheme to ensure optimal usage of the available resources. The scheme is built around a simulator of the hardware design that schedules the workload and assigns the most suitable task to the encryption cores at each cycle. This, coupled with optimal management of the read and write cycles of the Block RAM on FPGA, results in a minimum 2 orders of magnitude improvement in terms of throughput per core for the reported benchmarks compared to the most recent generic GC accelerator. Moreover, our encryption core requires 17\% less resource compared to the most recent secure GC realization.\&nbsp;</p>
},
	address = {San Diego},
	attachments = {http://aceslab.org/sites/default/files/FASE.pdf},
	author = {Siam U. Hussain and Farinaz Koushanfar},
	booktitle = {Field-Programmable Custom Computing Machines (FCCM)},
	date-added = {2019-06-06 14:03:09 +1200},
	date-modified = {2019-06-06 14:03:30 +1200},
	keywords = {Crypto, garbled-circuit, Secure computation, FPGA},
	month = {04/2019},
	title = {FASE: FPGA Acceleration of Secure Function Evaluation},
	year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBVLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRE5OL0RlZXAgUmVpbmZvcmNlbWVudCBMZWFybmluZyBmb3IgR2VuZXJhbCBWaWRlbyBHYW1lIEFJLmJpYk8RAhwAAAAAAhwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9EZWVwIFJlaW5mb3JjZW1lbnQjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADRE5OAAACAGsvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RE5OOkRlZXAgUmVpbmZvcmNlbWVudCBMZWFybmluZyBmb3IgR2VuZXJhbCBWaWRlbyBHYW1lIEFJLmJpYgAADgB0ADkARABlAGUAcAAgAFIAZQBpAG4AZgBvAHIAYwBlAG0AZQBuAHQAIABMAGUAYQByAG4AaQBuAGcAIABmAG8AcgAgAEcAZQBuAGUAcgBhAGwAIABWAGkAZABlAG8AIABHAGEAbQBlACAAQQBJAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBpVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9ETk4vRGVlcCBSZWluZm9yY2VtZW50IExlYXJuaW5nIGZvciBHZW5lcmFsIFZpZGVvIEdhbWUgQUkuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAB8AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAApw=}}

@inproceedings{10.1007/978-3-319-96878-0_17,
	abstract = {The rise of machine learning as a service multiplies scenarios where one faces a privacy dilemma: either sensitive user data must be revealed to the entity that evaluates the cognitive model (e.g., in the Cloud), or the model itself must be revealed to the user so that the evaluation can take place locally. Fully Homomorphic Encryption (FHE) offers an elegant way to reconcile these conflicting interests in the Cloud-based scenario and also preserve non-interactivity. However, due to the inefficiency of existing FHE schemes, most applications prefer to use Somewhat Homomorphic Encryption (SHE), where the complexity of the computation to be performed has to be known in advance, and the efficiency of the scheme depends on this global complexity.},
	address = {Cham},
	annote = {Found in book "Advances in Cryptology -- CRYPTO 2018"},
	author = {Bourse, Florian and Minelli, Michele and Minihold, Matthias and Paillier, Pascal},
	booktitle = {Advances in Cryptology -- CRYPTO 2018},
	date-added = {2019-06-05 22:22:42 +1200},
	date-modified = {2019-06-05 22:32:53 +1200},
	editor = {Shacham, Hovav and Boldyreva, Alexandra},
	isbn = {978-3-319-96878-0},
	keywords = {Books, Homomorphic encryption, MNIST, Discrete NN},
	pages = {483--512},
	publisher = {Springer International Publishing},
	title = {Fast Homomorphic Evaluation of Deep Discretized Neural Networks},
	year = {2018}}

@inproceedings{10.1007/978-3-319-98530-5_66,
	abstract = {Machine learning servers with mass storage and computing power is an ideal platform to store, manage, and analyze data and support decision-making. However, the main issue is providing security and privacy to the data, as the data is stored in a public way. Recently, homomorphic data encryption has been proposed as a solution due to its capabilities in performing computations over encrypted data. In this paper, we proposed an encrypted all convolutional net that transformed traditional all convolutional net into a net based on homomorphic encryption. This scheme allows different data holders to send their encrypted data to cloud service, complete predictions, and return them in encrypted form as the cloud service provider does not have a secret key. Therefore, the cloud service provider and others cannot get unencrypted raw data. When applied to the MNIST database, privacy-preserving all convolutional based on homomorphic encryption predict efficiently, accurately and with privacy protection.},
	address = {Cham},
	annote = {Inside book Advances in Network-Based Information Systems},
	author = {Liu, Wenchao and Pan, Feng and Wang, Xu An and Cao, Yunfei and Tang, Dianhua},
	booktitle = {Advances in Network-Based Information Systems},
	date-added = {2019-06-05 21:55:08 +1200},
	date-modified = {2019-06-05 22:16:49 +1200},
	editor = {Barolli, Leonard and Kryvinska, Natalia and Enokido, Tomoya and Takizawa, Makoto},
	isbn = {978-3-319-98530-5},
	keywords = {Books, Homomorphic encryption, CNN, MNIST},
	pages = {752--762},
	publisher = {Springer International Publishing},
	title = {Privacy-Preserving All Convolutional Net Based on Homomorphic Encryption},
	year = {2019}}

@inproceedings{10.1007/978-3-319-61982-8_9,
	abstract = {In this paper, the suitability of implementing parallel homomorphic word searching on Intel Xeon Phi coprocessors is evaluated for the first time. Homomorphic encryption allows to produce a cryptogram that encrypts the result of applying some values to any function, even when the input values are encrypted and without access to the private-key. For example, it is possible to search if any word of a set of encrypted words matches a plaintext reference word and generate a new cryptogram that encrypts the amount of matches. In this paper it is shown that this operation is about 834 times faster by using a system with 4 Intel Xeon Phi coprocessors 5110P attached to an Intel Xeon CPU E5-2630 v2, when compared with an implementation on a single core of the Xeon CPU.},
	address = {Cham},
	author = {Martins, Paulo and Sousa, Leonel},
	booktitle = {High Performance Computing for Computational Science -- VECPAR 2016},
	date-added = {2019-06-05 21:38:48 +1200},
	date-modified = {2019-06-05 21:39:17 +1200},
	editor = {Dutra, In{\^e}s and Camacho, Rui and Barbosa, Jorge and Marques, Osni},
	isbn = {978-3-319-61982-8},
	keywords = {Books, Crypto, homomorphic encryption, Word search, Xeon Phi},
	pages = {75--88},
	publisher = {Springer International Publishing},
	title = {HPC on the Intel Xeon Phi: Homomorphic Word Searching},
	year = {2017}}

@inproceedings{10.1007/978-3-662-48324-4_8,
	abstract = {Homomorphic encryption allows computation on encrypted data and makes it possible to securely outsource computational tasks to untrusted environments. However, all proposed schemes are quite inefficient and homomorphic evaluation of ciphertexts usually takes several seconds on high-end CPUs, even for evaluating simple functions. In this work we investigate the potential of FPGAs for speeding up those evaluation operations. We propose an architecture to accelerate schemes based on the ring learning with errors (RLWE) problem and specifically implemented the somewhat homomorphic encryption scheme YASHE, which was proposed by Bos, Lauter, Loftus, and Naehrig in 2013. Due to the large size of ciphertexts and evaluation keys, on-chip storage of all data is not possible and external memory is required. For efficient utilization of the external memory we propose an efficient double-buffered memory access scheme and a polynomial multiplier based on the number theoretic transform (NTT). For the parameter set ({\$}{\$}n=16384,{\backslash}lceil {\backslash}log {\_}2 q {\backslash}rceil ={\{}512{\}}{\$}{\$}n=16384,⌈log2q⌉=512) capable of evaluating 9 levels of multiplications, we can perform a homomorphic addition in 0.94 ms and a homomorphic multiplication in 48.67 ms.},
	address = {Berlin, Heidelberg},
	author = {P{\"o}ppelmann, Thomas and Naehrig, Michael and Putnam, Andrew and Macias, Adrian},
	booktitle = {Cryptographic Hardware and Embedded Systems -- CHES 2015},
	date-added = {2019-06-05 21:18:08 +1200},
	date-modified = {2019-06-05 21:19:33 +1200},
	editor = {G{\"u}neysu, Tim and Handschuh, Helena},
	isbn = {978-3-662-48324-4},
	keywords = {Books, Crypto, Homomorphic encryption, FPGA, YASHE, Stratix},
	pages = {143--163},
	publisher = {Springer Berlin Heidelberg},
	title = {Accelerating Homomorphic Evaluation on Reconfigurable Hardware},
	year = {2015}}

@inproceedings{7561676,
	abstract = {Neural networks (NNs) have been widely used in microwave device modeling. One of the greatest challenges is how to speed up the model training process and reduce the development cost. To address the issue, this paper exploits FPGAs to accelerate NN training. Experimental results demonstrate that the model training time can be reduced by up to 99.1%, compared to the traditional software implementation.},
	author = {R. {Sang} and Q. {Liu} and Q. {Zhang}},
	booktitle = {2016 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization (NEMO)},
	date-added = {2019-05-30 17:33:25 +1200},
	date-modified = {2019-05-30 17:33:34 +1200},
	doi = {10.1109/NEMO.2016.7561676},
	keywords = {FPGA-NN, cost reduction;field programmable gate arrays;learning (artificial intelligence);microwave devices;FPGA-based acceleration;neural network training;microwave device modeling;model training process;development cost reduction;Training;Field programmable gate arrays;Artificial neural networks;Software;Random access memory;Hardware;neural network;quasi-Newton method;FPGA;hardware acceleration},
	month = {July},
	pages = {1-2},
	title = {FPGA-based acceleration of neural network training},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/NEMO.2016.7561676}}

@inproceedings{Conti:2018:IEE:3327345.3327410,
	acmid = {3327410},
	address = {USA},
	author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	booktitle = {Proceedings of the 32Nd International Conference on Neural Information Processing Systems},
	date-added = {2019-05-30 13:56:09 +1200},
	date-modified = {2020-12-10 14:35:04 +1300},
	keywords = {Evolutionary, evolution strategies, Uber, Mujoco, Atari},
	location = {Montreal, Canada},
	numpages = {12},
	pages = {5032--5043},
	publisher = {Curran Associates Inc.},
	series = {NIPS'18},
	title = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-seeking Agents},
	url = {http://dl.acm.org/citation.cfm?id=3327345.3327410},
	year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3327345.3327410}}

@inproceedings{Cong:2018:UPD:3174243.3174970,
	acmid = {3174970},
	address = {New York, NY, USA},
	author = {Cong, Jason and Fang, Zhenman and Lo, Michael and Wang, Hanrui and Xu, Jingxian and Zhang, Shaochong},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2019-05-30 13:29:20 +1200},
	date-modified = {2019-05-30 13:30:40 +1200},
	doi = {10.1145/3174243.3174970},
	isbn = {978-1-4503-5614-5},
	keywords = {FPGA, accelerator, gpu, rodinia},
	location = {Monterey, CALIFORNIA, USA},
	numpages = {1},
	pages = {288--288},
	publisher = {ACM},
	series = {FPGA '18},
	title = {Understanding Performance Differences of FPGAs and GPUs: (Abtract Only)},
	url = {http://doi.acm.org/10.1145/3174243.3174970},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3174243.3174970},
	Bdsk-Url-2 = {https://doi.org/10.1145/3174243.3174970}}

@inproceedings{10.1007/3-540-44614-1_27,
	abstract = {In this paper, we show that a co-processor system with a Virtex FPGA can achieve high performance in evolutionary computations by utilizing the two features of the FPGA. First, agents in evolutionary computation models which are usually expressed using short bit-strings can be stored in distributed select RAMs of Virtex FPGAs very efficiently. Second, the partial reconfiguration and readback functions of the FPGAs make it possible to exploit more parallelism without thinking about circuits for data I/O. The preliminary results of a model base on Iterated Prisoner's Dilemma showed that the system can achieve high performance because of the two features.},
	address = {Berlin, Heidelberg},
	author = {Yamaguchi, Yoshiki and Miyashita, Akira and Maruyama, Tsutomu and Hoshino, Tsutomu},
	booktitle = {Field-Programmable Logic and Applications: The Roadmap to Reconfigurable Computing},
	date-added = {2019-05-29 11:22:10 +1200},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {Hartenstein, Reiner W. and Gr{\"u}nbacher, Herbert},
	isbn = {978-3-540-44614-9},
	keywords = {Books, Evolutionary, FPGA, Co-design},
	pages = {240--249},
	publisher = {Springer Berlin Heidelberg},
	title = {A Co-processor System with a Virtex FPGA for Evolutionary Computation},
	year = {2000}}

@article{Gomez-Pulido2011,
	abstract = {Many large combinatorial optimization problems tackled with evolutionary algorithms often require very high computational times, usually due to the fitness evaluation. This fact forces programmers to use clusters of computers, a computational solution very useful for running applications of intensive calculus but having a high acquisition price and operation cost, mainly due to the Central Processing Unit (CPU) power consumption and refrigeration devices. A low-cost and high-performance alternative comes from reconfigurable computing, a hardware technology based on Field Programmable Gate Array devices (FPGAs). The main objective of the work presented in this paper is to compare implementations on FPGAs and CPUs of different fitness functions in evolutionary algorithms in order to study the performance of the floating-point arithmetic in FPGAs and CPUs that is often present in the optimization problems tackled by these algorithms. We have taken advantage of the parallelism at chip-level of FPGAs pursuing the acceleration of the fitness functions (and consequently, of the evolutionary algorithms) and showing the parallel scalability to reach low cost, low power and high performance computational solutions based on FPGA. Finally, the recent popularity of GPUs as computational units has moved us to introduce these devices in our performance comparisons. We analyze performance in terms of computation times and economic cost.},
	author = {Gomez-Pulido, Juan A. and Vega-Rodriguez, Miguel A. and Sanchez-Perez, Juan M. and Priem-Mendes, Silvio and Carreira, Vitor},
	date-added = {2019-05-29 11:20:45 +1200},
	date-modified = {2019-05-29 11:21:02 +1200},
	day = {01},
	doi = {10.1007/s10710-011-9137-2},
	issn = {1573-7632},
	journal = {Genetic Programming and Evolvable Machines},
	keywords = {Evolutionary, FPGA, GPU, floating-point},
	month = {Dec},
	number = {4},
	pages = {403--427},
	title = {Accelerating floating-point fitness functions in evolutionary algorithms: a FPGA-CPU-GPU performance comparison},
	url = {https://doi.org/10.1007/s10710-011-9137-2},
	volume = {12},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-011-9137-2}}

@article{EmpiricalAssetML,
	author = {Gu, Shihao and T. Kelly, Bryan and Xiu, Dacheng},
	date-added = {2019-05-22 13:04:51 +1200},
	date-modified = {2020-01-21 18:33:58 +1300},
	doi = {10.2139/ssrn.3159577},
	journal = {SSRN Electronic Journal},
	keywords = {NN-Fin, Machine Learning, Return Prediction, Cross-Section of Returns, Ridge Regression, Lasso, Elastic Net, Random Forest, Gradient Boosting, Neural Networks, Fintech},
	month = {01},
	title = {Empirical Asset Pricing Via Machine Learning},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.2139/ssrn.3159577}}

@inproceedings{10.1007/978-3-642-04843-2_4,
	abstract = {Genetic Algorithm (GA) is a powerful tool for science computing, while Parallel Genetic Algorithm (PGA) further promotes the performance of computing. However, the traditional parallel computing environment is very difficult to set up, much less the price. This gives rise to the appearance of moving dense computing to graphics hardware, which is inexpensive and more powerful. The paper presents a hierarchical parallel genetic algorithm, implemented by NVIDIA's Compute Unified Device Architecture (CUDA). Mixed with master-slave parallelization method and multiple-demes parallelization method, this algorithm has contributed to better utilization of threads and high-speed shared memory in CUDA.},
	address = {Berlin, Heidelberg},
	annote = {Inside book Advances In Computation And Intelligence 2009},
	author = {Zhang, Sifa and He, Zhenming},
	booktitle = {Advances in Computation and Intelligence},
	date-added = {2019-05-14 18:33:01 +1200},
	date-modified = {2019-05-14 18:35:20 +1200},
	editor = {Cai, Zhihua and Li, Zhenhua and Kang, Zhuo and Liu, Yong},
	isbn = {978-3-642-04843-2},
	keywords = {Books, Evolutionary, Advances In Computation And Intelligence 2009, Genetic Algorithm, CUDA},
	pages = {24--30},
	publisher = {Springer Berlin Heidelberg},
	title = {Implementation of Parallel Genetic Algorithm Based on CUDA},
	year = {2009}}

@article{Tsang:2004aa,
	abstract = {Evolutionary Dynamic Data Investment Evaluator (EDDIE) is a genetic programming (GP)-based decision support tool for financial forecasting. EDDIE itself does not replace forecasting experts. It serves to improve the productivity of experts in searching the space of decision trees, with the aim to improve the odds in its user's favour. The efficacy of EDDIE has been reported in the literature. However, discovering patterns in historical data is only the first step towards building a practical financial forecasting tool. Data preparation, rules organization and application are all important issues. This paper describes an architecture that embeds EDDIE for learning from and monitoring the stock market.},
	author = {Tsang, Edward and Yung, Paul and Li, Jin},
	booktitle = {Data mining for financial decision making},
	da = {2004/09/01/},
	date-added = {2019-05-13 14:05:58 +1200},
	date-modified = {2019-05-13 14:06:27 +1200},
	doi = {https://doi.org/10.1016/S0167-9236(03)00087-3},
	isbn = {0167-9236},
	journal = {Decision Support Systems},
	keywords = {NN-Fin, Financial forecasting tools; Genetic programming},
	number = {4},
	pages = {559--565},
	title = {EDDIE-Automation, a decision support tool for financial forecasting},
	ty = {JOUR},
	url = {http://www.sciencedirect.com/science/article/pii/S0167923603000873},
	volume = {37},
	year = {2004},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167923603000873},
	Bdsk-Url-2 = {https://doi.org/10.1016/S0167-9236(03)00087-3}}

@inproceedings{Brookhouse:2014:WOS:2598394.2605689,
	acmid = {2605689},
	address = {New York, NY, USA},
	author = {Brookhouse, James and Otero, Fernando E.B. and Kampouridis, Michael},
	booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
	date-added = {2019-05-13 13:51:50 +1200},
	date-modified = {2019-05-13 13:53:15 +1200},
	doi = {10.1145/2598394.2605689},
	isbn = {978-1-4503-2881-4},
	keywords = {NN-Fin, financial forecasting, genetic programming, gpu, opencl},
	location = {Vancouver, BC, Canada},
	numpages = {8},
	pages = {1117--1124},
	publisher = {ACM},
	series = {GECCO Comp '14},
	title = {Working with OpenCL to Speed Up a Genetic Programming Financial Forecasting Algorithm: Initial Results},
	url = {http://doi.acm.org/10.1145/2598394.2605689},
	year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2598394.2605689},
	Bdsk-Url-2 = {https://doi.org/10.1145/2598394.2605689}}

@inproceedings{GECCO06-trading,
	author = {Harish Subramanian and Subramanian Ramamoorthy and Peter Stone and Benjamin Kuipers},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	date-added = {2019-05-13 10:10:07 +1200},
	date-modified = {2019-05-13 10:10:32 +1200},
	keywords = {NN-Fin, Evolutionary Algorithms, Finance, Trading},
	month = {July},
	title = {Designing Safe, Profitable Automated Stock Trading Agents Using Evolutionary Algorithms},
	url = {http://www.cs.utexas.edu/users/ai-lab/?GECCO06-trading},
	year = {2006},
	Bdsk-Url-1 = {http://www.cs.utexas.edu/users/ai-lab/?GECCO06-trading}}

@inproceedings{Camargo-Bareno:2011:GECCOcomp,
	abstract = {This paper presents a novel a parallel genetic programming (PGP) Boolean synthesis implementation on a low cost cluster of an embedded open platform called SIE. Some tasks of the PGP have been accelerated through a hardware coprocessor called FCU, that allows to evaluate individuals onchip as intrinsic evolution. Results have been compared with GPU and HPC implementations, resulting in speedup values up to approximately 2 and 180 respectively.},
	address = {Dublin, Ireland},
	author = {Carlos Ivan {Camargo Bareno} and Cesar Augusto {Pedraza Bonilla} and Luis Fernado Nino and Jose Ignacio {Martinez Torre}},
	booktitle = {GECCO '11: Proceedings of the 13th annual conference companion on Genetic and evolutionary computation},
	date-added = {2019-05-13 09:53:07 +1200},
	date-modified = {2019-05-13 09:53:30 +1200},
	doi = {doi:10.1145/2001858.2001964},
	editor = {Natalio Krasnogor and Pier Luca Lanzi and Andries Engelbrecht and David Pelta and Carlos Gershenson and Giovanni Squillero and Alex Freitas and Marylyn Ritchie and Mike Preuss and Christian Gagne and Yew Soon Ong and Guenther Raidl and Marcus Gallager and Jose Lozano and Carlos Coello-Coello and Dario Landa Silva and Nikolaus Hansen and Silja Meyer-Nieberg and Jim Smith and Gus Eiben and Ester Bernado-Mansilla and Will Browne and Lee Spector and Tina Yu and Jeff Clune and Greg Hornby and Man-Leung Wong and Pierre Collet and Steve Gustafson and Jean-Paul Watson and Moshe Sipper and Simon Poulding and Gabriela Ochoa and Marc Schoenauer and Carsten Witt and Anne Auger},
	isbn13 = {978-1-4503-0690-4},
	keywords = {Evolutionary, FPGA, genetic algorithms, genetic programming, GPU: Poster},
	month = {12-16 } # jul,
	notes = {Also known as \cite{2001964} Distributed on CD-ROM at GECCO-2011. ACM Order Number 910112.},
	organisation = {SIGEVO},
	pages = {189--190},
	publisher = {ACM},
	publisher_address = {New York, NY, USA},
	title = {Intrinsic evolvable hardware for combinatorial synthesis based on SoC+FPGA and GPU platforms},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1145/2001858.2001964}}

@inproceedings{10.1007/978-3-642-24013-3_11,
	abstract = {We discuss the parallel implementation of Genetic Algorithms and Evolution Strategy on General-Purpose Graphical Units, using the OpenCL framework. Multiple evolutionary operators are tested (tournament, roulette wheel selection, uniform and Gaussian mutation, crossover, recombination), as well as different approaches for parallelism, for small and large problem sizes. We use the Island Model of Parallel GA, with random migration. Performance is measured using two graphic cards: NVidia GeForce GTX 560Ti and AMD Radeon 6950. Tests are performed in a distributed grid, using the Java Parallel Processing Framework.},
	address = {Berlin, Heidelberg},
	annote = {Inside book "Intelligent Distributed Computing V"},
	author = {L{\H{o}}rentz, Istv{\'a}n and Andonie, R{\u{a}}zvan and Mali{\c{T}}a, Mihaela},
	booktitle = {Intelligent Distributed Computing V},
	date-added = {2019-05-12 22:07:17 +1200},
	date-modified = {2019-05-12 22:08:18 +1200},
	editor = {Brazier, F. M. T. and Nieuwenhuis, Kees and Pavlin, Gregor and Warnier, Martijn and Badica, Costin},
	isbn = {978-3-642-24013-3},
	keywords = {Books, Evolutionary, Intelligent Distributed Computing V, OpenCL, Evolutionary algorithms, Genetic Algorithm},
	pages = {103--113},
	publisher = {Springer Berlin Heidelberg},
	title = {An Implementation of Evolutionary Computation Operators in OpenCL},
	year = {2012}}

@article{7604133,
	abstract = {In this paper we report on our advances designing and implementing an FPGA-based computation accelerator as part of a Homomorphic Encryption Processing Unit (HEPU) co-processor. This hardware accelerator technology improves the practicality of computing on encrypted data by reducing the computational bottlenecks of lattice encryption primitives that support homomorphic encryption schemes. We focus on accelerating the Chinese Remainder Transform (CRT) and inverse Chinese Remainder Transform (iCRT) for power-of-2 cyclotomic rings, but also accelerate other basic ring arithmetic such as Ring Addition, Ring Subtraction and Ring Multiplication. We instantiate this capability in a Xilinx Virtex-7 FPGA that can attach to a host computer through either a PCI-Express port or Ethernet. We focus our experimental performance analysis on the NTRU-based LTV Homomorphic Encryption scheme. This is a leveled homomorphic encryption scheme, but our accelerator is compatible with other lattice-based schemes and recent improved bootstrapping designs to support arbitrary depth computation. We experimentally compare performance with a reference software implementations of the CRT and iCRT bottlenecks and when used in a practical application of encrypted string comparison.},
	author = {D. B. {Cousins} and K. {Rohloff} and D. {Sumorok}},
	date-added = {2019-05-12 00:57:09 +1200},
	date-modified = {2019-05-12 00:57:19 +1200},
	doi = {10.1109/TETC.2016.2619669},
	issn = {2168-6750},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	keywords = {Crypto; coprocessors;cryptography;field programmable gate arrays;local area networks;logic design;peripheral interfaces;transforms;FPGA designing;homomorphic encryption processing unit coprocessor;HEPU coprocessor;hardware accelerator;inverse Chinese remainder transform;iCRT;ring arithmetic;Xilinx Virtex-7 FPGA;PCI-Express port;Ethernet;Encryption;Cathode ray tubes;Acceleration;Hardware;Field programmable gate arrays;Applied cryptography;hardware acceleration;homomorphic encryption},
	month = {April},
	number = {2},
	pages = {193-206},
	title = {Designing an FPGA-Accelerated Homomorphic Encryption Co-Processor},
	volume = {5},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TETC.2016.2619669}}

@article{8318681,
	abstract = {In this paper, we present an FPGA based hardware accelerator `$\mathsf{HEPCloud}$' for homomorphic evaluations of medium depth functions which has applications in cloud computing. Our$\mathsf{HEPCloud}$architecture supports the polynomial ring based homomorphic encryption scheme FV for a ring-LWE parameter set of dimension$2^{15}$, modulus size 1,228-bit, and a standard deviation 50. This parameter-set offers a multiplicative depth 36 and at least 85 bit security. The processor of$\mathsf{HEPCloud}$is composed of multiple parallel cores. To achieve fast computation time for such a large parameter-set, various optimizations in both algorithm and architecture levels are performed. For fast polynomial multiplications, we use CRT with NTT and achieve two dimensional parallelism in$\mathsf{HEPCloud}$. We optimize the BRAM access, use a fast Barrett like polynomial reduction method, optimize the cost of CRT, and design a fast divide-and-round unit. Beside parallel processing, we apply pipelining strategy in several of the sequential building blocks to reduce the impact of sequential computations. Finally, we implement$\mathsf{HEPCloud}$on a medium-size Xilinx Virtex 6 FPGA board ML605 board and measure its on-board performance. To store the ciphertexts during a homomorphic function evaluation, we use the large DDR3 memory of the ML605 board. Our FPGA-based implementation of$\mathsf{HEPCloud}$computes a homomorphic multiplication in 26.67 s, of which the actual computation takes only 3.36 s and the rest is spent for off-chip memory access. It requires about 37,551 s to evaluate the SIMON-64/128 block cipher, but the per-block timing is only about 18 s because$\mathsf{HEPCloud}$processes 2,048 blocks simultaneously. The results show that FPGA-based acceleration of homomorphic function evaluations is feasible, but fast memory interface is crucial for the performance.},
	author = {S. {Sinha Roy} and K. {J{\"a}rvinen} and J. {Vliegen} and F. {Vercauteren} and I. {Verbauwhede}},
	date-added = {2019-05-12 00:55:19 +1200},
	date-modified = {2019-05-12 00:55:33 +1200},
	doi = {10.1109/TC.2018.2816640},
	issn = {0018-9340},
	journal = {IEEE Transactions on Computers},
	keywords = {Crypto; Hardware;Encryption;Computer architecture;Cloud computing;Acceleration;Homomorphic encryption;FV;lattice-based cryptography;ring-LWE;polynomial multiplication;number theoretic transform;hardware implementation},
	month = {Nov},
	number = {11},
	pages = {1637-1650},
	title = {HEPCloud: An FPGA-Based Multicore Processor for FV Somewhat Homomorphic Function Evaluation},
	volume = {67},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TC.2018.2816640}}

@misc{TFHE,
	author = {Ilaria Chillotti and Nicolas Gama and Mariya Georgieva and Malika Izabach{\`e}ne},
	date-added = {2019-05-12 00:39:10 +1200},
	date-modified = {2019-05-12 00:39:22 +1200},
	keywords = {Crypto, Library, Homomorphic encryption},
	note = {https://tfhe.github.io/tfhe/},
	title = {{TFHE}: Fast Fully Homomorphic Encryption Library},
	year = {August 2016}}

@article{Roy2019FPGABasedHP,
	author = {Sujoy Sinha Roy and Furkan Turan and Kimmo J{\"a}rvinen and Frederik Vercauteren and Ingrid Verbauwhede},
	date-added = {2019-05-12 00:31:44 +1200},
	date-modified = {2019-05-12 00:31:58 +1200},
	journal = {2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
	keywords = {Crypto, Homomorphic encryption, FPGA},
	pages = {387-398},
	title = {FPGA-Based High-Performance Parallel Architecture for Homomorphic Computing on Encrypted Data},
	year = {2019}}

@article{Barcau2018BoundedFH,
	author = {Mugurel Barcau and Vicentiu Pasol},
	date-added = {2019-05-12 00:14:22 +1200},
	date-modified = {2019-05-12 00:14:36 +1200},
	journal = {IACR Cryptology ePrint Archive},
	keywords = {Crypto, Patented, Monoid},
	pages = {584},
	title = {Bounded Fully Homomorphic Encryption from Monoid Algebras},
	volume = {2018},
	year = {2018}}

@article{journals/iacr/BouraGG18,
	added-at = {2018-09-12T00:00:00.000+0200},
	author = {Boura, Christina and Gama, Nicolas and Georgieva, Mariya},
	biburl = {https://www.bibsonomy.org/bibtex/23a26ad6c2b19b760421d9d209b35db7b/dblp},
	date-added = {2019-05-12 00:02:23 +1200},
	date-modified = {2019-05-12 00:13:27 +1200},
	ee = {https://eprint.iacr.org/2018/758},
	interhash = {6fdde7eaad29a2d2e98bf0a44d60cea2},
	intrahash = {3a26ad6c2b19b760421d9d209b35db7b},
	journal = {IACR Cryptology ePrint Archive},
	keywords = {Crypto, Homomorphic encryption},
	pages = 758,
	timestamp = {2018-09-13T11:37:07.000+0200},
	title = {Chimera: a unified framework for B/FV, TFHE and HEAAN fully homomorphic encryption and predictions for deep learning.},
	url = {http://dblp.uni-trier.de/db/journals/iacr/iacr2018.html#BouraGG18},
	volume = 2018,
	year = 2018,
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/journals/iacr/iacr2018.html#BouraGG18}}

@inproceedings{pmlr-v80-sanyal18a,
	abstract = {Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the nature of computation and amount of communication required between client and server. Fully homomorphic encryption offers a way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The one drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine several ideas from the machine learning literature, particularly work on quantization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.},
	address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	author = {Sanyal, Amartya and Kusner, Matt and Gascon, Adria and Kanade, Varun},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	date-added = {2019-05-11 23:51:50 +1200},
	date-modified = {2019-05-11 23:52:20 +1200},
	editor = {Dy, Jennifer and Krause, Andreas},
	keywords = {Crypto, Homomorphic encryption, PaaS},
	month = {10--15 Jul},
	pages = {4490--4499},
	pdf = {http://proceedings.mlr.press/v80/sanyal18a/sanyal18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{TAPAS}: Tricks to Accelerate (encrypted) Prediction As a Service},
	url = {http://proceedings.mlr.press/v80/sanyal18a.html},
	volume = {80},
	year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v80/sanyal18a.html}}

@article{2018arXiv181000845D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181000845D},
	archiveprefix = {arXiv},
	author = {{Dathathri}, R. and {Saarikivi}, O. and {Chen}, H. and {Laine}, K. and {Lauter}, K. and {Maleki}, S. and {Musuvathi}, M. and {Mytkowicz}, T.},
	date-added = {2019-05-11 23:38:53 +1200},
	date-modified = {2019-05-11 23:40:32 +1200},
	eprint = {1810.00845},
	journal = {Privacy Preserving Machine Learning},
	keywords = {Crypto, Machine Learning, Computer Science - Cryptography and Security, Computer Science - Programming Languages, Statistics - Machine Learning},
	month = Dec,
	title = {{CHET: Compiler and Runtime for Homomorphic Evaluation of Tensor Programs}},
	year = 2018}

@inproceedings{crypto-2018-28796,
	author = {Florian Bourse and Michele Minelli and Matthias Minihold and Pascal Paillier},
	booktitle = {Advances in Cryptology -- CRYPTO 2018},
	date-added = {2019-05-11 22:29:00 +1200},
	date-modified = {2019-05-11 23:16:46 +1200},
	doi = {10.1007/978-3-319-96878-0_17},
	keywords = {Crypto, Homomorphic encryption, Git, MNIST},
	pages = {483-512},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {Fast Homomorphic Evaluation of Deep Discretized Neural Networks},
	volume = {10993},
	year = 2018,
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-319-96878-0_17}}

@inproceedings{10.1007/978-3-540-68083-3_25,
	abstract = {Scientific computing applications with highly demanding data capacity and computation power drive a computing platform migration from shared memory machines to multi-core/multiprocessor computer clusters. However, overheads in coordinating operations across computing nodes could counteract the benefit of having extra machines. Furthermore, the hidden dependency in applications slows down the simulation over non-shared memory machines. This paper proposed a framework to utilize multi-core/multiprocessor clusters for distributed simulation. Among several coordination schemes, decentralized control approach has demonstrated its effectiveness in reducing the communication overheads. A speculative execution strategy is applied to exploit parallelism thoroughly and overcome strong data dependency. Performance analysis and experiments are provided to demonstrate the performance gains.},
	address = {Berlin, Heidelberg},
	annote = {Inside book "Advances in Grid and Pervasive Computing"},
	author = {Li, Ruipeng and Jiang, Hai and Su, Hung-Chi and Zhang, Bin and Jenness, Jeff},
	booktitle = {Advances in Grid and Pervasive Computing},
	date-added = {2019-05-10 19:51:09 +1200},
	date-modified = {2019-05-10 19:51:40 +1200},
	editor = {Wu, Song and Yang, Laurence T. and Xu, Tony Li},
	isbn = {978-3-540-68083-3},
	keywords = {Books, Decentralized, Advances in Grid and Pervasive Computing},
	pages = {244--255},
	publisher = {Springer Berlin Heidelberg},
	title = {Parallel and Distributed Particle Collision Simulation with Decentralized Control},
	year = {2008}}

@inproceedings{10.1007/978-3-319-19282-6_4,
	abstract = {Comingle is a logic programming framework aimed at simplifying the development of applications distributed over multiple mobile devices. Applications are written as a single declarative program (in a system-centric way) rather than in the traditional node-centric manner, where separate communicating code is written for each participating node. Comingle is based on committed-choice multiset rewriting and is founded on linear logic. We describe a prototype targeting the Android operating system and illustrate how Comingle is used to program distributed mobile applications. As a proof of concept, we discuss several such applications orchestrated using Comingle.},
	address = {Cham},
	annote = {Inside book "Coordination Models and Languages"},
	author = {Lam, Edmund Soon Lee and Cervesato, Iliano and Fatima, Nabeeha},
	booktitle = {Coordination Models and Languages},
	date-added = {2019-05-10 19:48:30 +1200},
	date-modified = {2019-05-10 19:48:55 +1200},
	editor = {Holvoet, Tom and Viroli, Mirko},
	isbn = {978-3-319-19282-6},
	keywords = {Books, Decentralized, Coordination Models and Languages},
	pages = {51--66},
	publisher = {Springer International Publishing},
	title = {Comingle: Distributed Logic Programming for Decentralized Mobile Ensembles},
	year = {2015}}

@inbook{Tantitharanukul2015,
	abstract = {Decentralized distributed systems, such as grids, clouds or networks of sensors, have been widely investigated recently. An important nature of such systems is the heterogeneity of their resources; in order to archive the availability, scalability and flexibility. As a consequence, managing the systems to meet requirements is obviously a nontrivial work. The issue is even more challenging in term of job scheduling when the task dependency within each job exists. In this paper, we address such problem of job scheduling, so called workflow-based job scheduling, in the decentralized distributed systems with heterogeneous resources. As such problem is proven to be an NP-complete problem, an efficient heuristic algorithm to address this problem is proposed. The algorithm is based on an observation that the heterogeneity of the resources can affect the execution time of the scheduling. We compare the effectiveness and efficiency of the proposed algorithm with a baseline algorithm. The result shows that our algorithm is highly effective and efficient for the scheduling problem in the decentralized distributed system with heterogeneous resources environment both in terms of the solution quality and the execution time respectively.},
	address = {Cham},
	author = {Tantitharanukul, Nasi and Natwichai, Juggapong and Boonma, Pruet},
	booktitle = {Computer and Information Science},
	date-added = {2019-05-10 19:37:11 +1200},
	date-modified = {2019-05-10 19:37:31 +1200},
	doi = {10.1007/978-3-319-10509-3_8},
	editor = {Lee, Roger},
	isbn = {978-3-319-10509-3},
	keywords = {Books, Decentralized, Computer and Information Science},
	pages = {101--114},
	publisher = {Springer International Publishing},
	title = {A Heuristic Algorithm for Workflow-Based Job Scheduling in Decentralized Distributed Systems with Heterogeneous Resources},
	url = {https://doi.org/10.1007/978-3-319-10509-3_8},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-319-10509-3_8}}

@inproceedings{10.1007/978-3-642-22110-1_41,
	abstract = {In distributed systems, local controllers often need to impose global guarantees. A solution that will not impose additional synchronization may not be feasible due to the lack of ability of one process to know the current situation at another. On the other hand, a completely centralized solution will eliminate all concurrency. A good solution is usually a compromise between these extremes, where synchronization is allowed for in principle, but avoided whenever possible. In a quest for practicable solutions to the distributed control problem, one can constrain the executions of a system based on the pre-calculation of knowledge properties and allow for temporary interprocess synchronization in order to combine the knowledge needed to control the system. This type of control, however, may incur a heavy communication overhead. We introduce the use of simple supervisor processes that accumulate information about processes until sufficient knowledge is collected to allow for safe progression. We combine the knowledge approach with a game theoretic search that prevents progressing to states from which there is no way to guarantee the imposed constraints.},
	address = {Berlin, Heidelberg},
	annote = {Inside book "Computer Aided Verification"},
	author = {Katz, Gal and Peled, Doron and Schewe, Sven},
	booktitle = {Computer Aided Verification},
	date-added = {2019-05-10 19:25:17 +1200},
	date-modified = {2019-05-10 19:25:47 +1200},
	editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
	isbn = {978-3-642-22110-1},
	keywords = {Books, Decentralized, Computer Aided Verification},
	pages = {510--525},
	publisher = {Springer Berlin Heidelberg},
	title = {Synthesis of Distributed Control through Knowledge Accumulation},
	year = {2011}}

@inbook{Kushner1997,
	abstract = {This chapter is concerned with decentralized and asynchronous forms of the stochastic approximation algorithms, a relatively new area of research. Compared with the rapid progress and extensive literature in stochastic approximation methods, the study of parallel stochastic approximations is still in its infancy. Perhaps the first work on the subject was [14], which dealt with a very particular class of algorithms, where similar computations were done by several processors and convex combinations were taken. The general ideas of weak convergence theory were applied to a fairly broad class of realistic algorithms in [111, 112]. The general ideas presented there and in [105, 171] form the basis of this chapter. Analogously to the problems in Chapter 8, those methods can handle correlated and state dependent noise, delays in communication, and asynchronous and distributed network forms. Various examples are given in Section 1. In the basic model, there are several processors; each one is responsible for the updating of only a part of the parameter vector. There might be overlaps in that several processors contribute to the updating of the same component of the parameter. Such models were treated in [105, 171, 172]. For a similar model, the problem of finding zeros of a nonlinear function with noisy observations via parallel processing methods and with random truncation bounds was treated in [201]. An attempt to get real-time implementable procedures via pipelining (see Section 1.2) of communication and computation for algorithms with ``delayed'' observations was in [203].},
	address = {New York, NY},
	annote = {Part of book "Stochastic Approximation Algorithms and Applications"},
	author = {Kushner, Harold J. and Yin, G. George},
	booktitle = {Stochastic Approximation Algorithms and Applications},
	date-added = {2019-05-10 19:14:26 +1200},
	date-modified = {2019-05-10 19:15:05 +1200},
	doi = {10.1007/978-1-4899-2696-8_12},
	isbn = {978-1-4899-2696-8},
	keywords = {Books, Decentralized, Stochastic Approximation Algorithms and Applications},
	pages = {347--391},
	publisher = {Springer New York},
	title = {Distributed/Decentralized and Asynchronous Algorithms},
	url = {https://doi.org/10.1007/978-1-4899-2696-8_12},
	year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4899-2696-8_12}}

@inproceedings{10.1007/978-3-319-22572-2_11,
	abstract = {In this paper we present BitWorker, a platform for community distributed computing based on BitTorrent. Any splittable task can be easily specified by a user in a meta-information task file, such that it can be downloaded and performed by other volunteers. Peers find each other using Distributed Hash Tables, download existing results, and compute missing ones. Unlike existing distributed computing schemes relying on centralized coordination point(s), our scheme is totally distributed, therefore, highly robust. We evaluate the performance of BitWorker using mathematical models and real tests, showing processing and robustness gains. BitWorker is available for download [1] and use by the community.},
	address = {Cham},
	annote = {Part of Book "Wired/Wireless Internet Communications"},
	author = {Durand, Arnaud and Gasparian, Mikael and Rouvinez, Thomas and Aad, Imad and Braun, Torsten and Trinh, Tuan Anh},
	booktitle = {Wired/Wireless Internet Communications},
	date-added = {2019-05-10 19:10:21 +1200},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {Aguayo-Torres, Mari Carmen and G{\'o}mez, Gerardo and Poncela, Javier},
	isbn = {978-3-319-22572-2},
	keywords = {Books, Decentralized},
	pages = {151--164},
	publisher = {Springer International Publishing},
	title = {BitWorker, a Decentralized Distributed Computing System Based on BitTorrent},
	year = {2015}}

@article{article,
	author = {Alesawy, Othman and Muniyandi, Ravie},
	date-added = {2019-05-09 11:30:09 +1200},
	date-modified = {2019-05-09 11:31:27 +1200},
	doi = {10.3923/itj.2016.77.83},
	journal = {Information Technology Journal},
	keywords = {Evolutionary; Cryptography; Diffie-Hellman; Neural Network, Genetic Algorithm},
	month = {06},
	pages = {77-83},
	title = {Elliptic Curve Diffie-Hellman Random Keys Using Artificial Neural Network and Genetic Algorithm for Secure Data over Private Cloud},
	volume = {15},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.3923/itj.2016.77.83}}

@article{CryptoNNSurvey,
	author = {A El-Zoghabi, Adel and Yassin, Amr and Hamdy, Hany},
	date-added = {2019-05-08 21:34:28 +1200},
	date-modified = {2019-05-08 21:36:49 +1200},
	journal = {International Journal of Emerging Technology and Advanced Engineering},
	keywords = {NN; Cryptography, Survey},
	month = {12},
	pages = {449-455},
	title = {Survey Report on Cryptography Based on Neural Network},
	volume = {9001},
	year = {2013}}

@inproceedings{6612177,
	abstract = {By making use of Artificial Intelligence (AI), Human Intelligence can be simulated by a machine, Neural Networks is one such sub field of AI. Artificial Neural Networks (ANN) consists of neurons and weights assigned to inter neuron connections helps in storing the acquired knowledge. This paper makes use of Hebbian learning rule to train the ANN of both sender and receiver machines. In the field of Public Key Cryptography (PKC), Pseudo Random Number Generator (PRNG) are widely used to generate unique keys and random numbers used in ANN which are found to possess many types of possible attacks. It is essential for a key to possess randomness for key strength and security. This paper proposes key generation for PKC by application of ANN using Genetic Algorithm (GA). It was noticed that use of ANN along with GA has not as yet been explored. GA approach is often applied for obtaining optimization and solutions in search problems. GA correlates to the nature to a large extent producing population of numbers where number possessing higher fitness value is replicated more. Thus, making GA a very good contender for PRNGs. Good Fitness function helps in exploring search space of random numbers in more efficient manner. GA PRNGs result samples satisfies frequency test and gap test. Thus the numbers generated after each iteration by GA PRNG are statistically verified to be random and nonrepeating, having no prior relation of next number from the previous ones, acting as an essential initialization parameter for neural algorithm overcomes the problem of acknowledging the random number generated by traditional PRNG. For generating public and private keys, different number of rounds of mixing is used. This ensures that the private key generated cannot be derived from public key. Our algorithm was observed to give fast and improved performance results having practical and feasible implementation.},
	author = {S. {Jhajharia} and S. {Mishra} and S. {Bali}},
	booktitle = {2013 Sixth International Conference on Contemporary Computing (IC3)},
	date-added = {2019-05-08 21:12:10 +1200},
	date-modified = {2019-05-08 21:12:18 +1200},
	doi = {10.1109/IC3.2013.6612177},
	keywords = {NN; genetic algorithms;neural nets;public key cryptography;search problems;public key cryptography;genetic algorithm;artificial intelligence;human intelligence;artificial neural networks;neurons;interneuron connection;acquired knowledge storage;Hebbian learning rule;ANN training;sender machine;receiver machine;PKC;pseudo random number generator;PRNG;unique key generation;key strength;security;optimization;search problem;fitness value;search space exploration;random numbers;frequency test;gap test;initialization parameter;neural algorithm;public key generation;private key generation;Genetic algorithms;Artificial neural networks;Biological cells;Sociology;Statistics;Neurons;Biological neural networks;artificial neural networks;neural cryptography;hebbian theory;genetic algorithm;public key cryptography;random number},
	month = {Aug},
	pages = {137-142},
	title = {Public key cryptography using neural networks and genetic algorithms},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/IC3.2013.6612177}}

@article{2018arXiv181108162G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181108162G},
	archiveprefix = {arXiv},
	author = {{Goyal}, M. and {Tatwawadi}, K. and {Chandak}, S. and {Ochoa}, I.},
	date-added = {2019-05-08 16:38:43 +1200},
	date-modified = {2019-05-08 16:38:51 +1200},
	eprint = {1811.08162},
	journal = {arXiv e-prints},
	keywords = {NN, Computation and Language, Electrical Engineering and Systems Science - Signal Processing, Quantitative Biology - Genomics},
	month = nov,
	primaryclass = {cs.CL},
	title = {{DeepZip: Lossless Data Compression using Recurrent Neural Networks}},
	year = 2018}

@article{2017arXiv171111279K,
	archiveprefix = {arXiv},
	author = {{Kim}, B. and Wattenberg M. and {Gilmer}, J. and {Cai} C. and {Wexler} J. and and {Viegas}, F. and {Sayres}, R.},
	date-added = {2019-05-08 13:01:02 +1200},
	date-modified = {2019-05-08 13:02:11 +1200},
	eprint = {1711.11279},
	journal = {ICML},
	keywords = {NN, Explainable AI, Concept Activation Vectors},
	title = {{ Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) }},
	year = 2018}

@inproceedings{pmlr-v80-srouji18a,
	abstract = {In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.},
	address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	author = {Srouji, Mario and Zhang, Jian and Salakhutdinov, Ruslan},
	booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	date-added = {2019-05-07 22:07:24 +1200},
	date-modified = {2019-05-07 22:07:49 +1200},
	editor = {Dy, Jennifer and Krause, Andreas},
	keywords = {Other, Reinforcement Learning, OpenAI Gym},
	month = {10--15 Jul},
	pages = {4742--4751},
	pdf = {http://proceedings.mlr.press/v80/srouji18a/srouji18a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Structured Control Nets for Deep Reinforcement Learning},
	url = {http://proceedings.mlr.press/v80/srouji18a.html},
	volume = {80},
	year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v80/srouji18a.html}}

@inproceedings{Duan:2016:BDR:3045390.3045531,
	acmid = {3045531},
	author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	date-added = {2019-05-07 19:16:39 +1200},
	date-modified = {2019-05-07 19:17:04 +1200},
	keywords = {Other, Reinforcement Learning, benchmark data},
	location = {New York, NY, USA},
	numpages = {10},
	pages = {1329--1338},
	publisher = {JMLR.org},
	series = {ICML'16},
	title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	url = {http://dl.acm.org/citation.cfm?id=3045390.3045531},
	year = {2016},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3045390.3045531}}

@electronic{nchainv0OpenAI,
	author = {blole},
	date-added = {2019-05-02 22:52:37 +1200},
	date-modified = {2019-05-02 22:54:27 +1200},
	keywords = {WebPage, OpenAI Gym, SUNA},
	lastchecked = {2/May/2019},
	title = {Algorithm on NChain-v0},
	url = {https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/},
	year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/}}

@electronic{duplicatedOpenAIGym,
	author = {colinmorris},
	date-added = {2019-04-30 21:35:34 +1200},
	date-modified = {2019-04-30 21:37:44 +1200},
	keywords = {WebPage, OpenAI Gym, Reinforcement Learning, BiSUNA},
	lastchecked = {30 Apr 2019},
	title = {Algorithm on DuplicatedInput-v0},
	url = {https://gym.openai.com/evaluations/eval_5PlrBv8wRNGI2J1lp3otUA/},
	year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_5PlrBv8wRNGI2J1lp3otUA/}}

@electronic{rouletteV0OpenAI,
	author = {tanemaki},
	date-added = {2019-04-30 21:30:55 +1200},
	date-modified = {2019-04-30 21:38:56 +1200},
	keywords = {WebPage, OpenAI Gym, Reinforcement Learning, BiSUNA},
	lastchecked = {30 Apr 2019},
	title = {Algorithm on Roulette-v0},
	url = {https://gym.openai.com/evaluations/eval_DFWdtrdSCikuZWwf8HN8A/},
	year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_DFWdtrdSCikuZWwf8HN8A/}}

@phdthesis{Moore-1991-13223,
	address = {Pittsburgh, PA},
	author = {Andrew Moore},
	date-added = {2019-04-30 21:14:43 +1200},
	date-modified = {2019-04-30 21:15:10 +1200},
	keywords = {NN, SUNA, Reinforcement Learning, Mountain Car},
	month = {March},
	school = {Carnegie Mellon University},
	title = {Efficient Memory-based Learning for Robot Control},
	year = {1991}}

@inproceedings{8398183,
	abstract = {Stock price volatility is a highly complex nonlinear dynamic system. The stock's trading volume affects the stock's self correlation, self correlation and inertial effect, and the adjustment of the stock is not to advance with a homogeneous time process, which has its own independent time to promote the process. LSTM (Term Memory Long-Short) is a kind of time recurrent neural network, which is suitable for processing and predicting the important events of interval and long delay in time series. Based on temporal characteristics of stock and LSTM neural network algorithm, this paper uses the LSTM recurrent neural networks to filter, extract feature value and analyze the stock data, and set up the the prediction model of the corresponding stock transaction.},
	author = {S. {Liu} and G. {Liao} and Y. {Ding}},
	booktitle = {2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA)},
	date-added = {2019-04-29 12:00:47 +1200},
	date-modified = {2019-04-29 12:01:04 +1200},
	doi = {10.1109/ICIEA.2018.8398183},
	issn = {2158-2297},
	keywords = {NN-Fin; SUNA; nonlinear dynamical systems;pricing;recurrent neural nets;stock markets;time series;homogeneous time process;time recurrent neural network;time series;LSTM neural network algorithm;LSTM recurrent neural networks;stock price volatility;nonlinear dynamic system;stock transaction prediction modeling;stock trading volume;stock self correlation;term memory long short;feature value extraction;stock data analysis;Predictive models;Logic gates;Computational modeling;Indexes;Neural networks;Time series analysis;Feature extraction;machine learning;neural network;stock transaction prediction;LSTM},
	month = {May},
	pages = {2787-2790},
	title = {Stock transaction prediction modeling and analysis based on LSTM},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIEA.2018.8398183}}

@inproceedings{6215974,
	abstract = {Flip-flops are critical timing elements in digital circuits which have a large impact on circuit speed and power consumption. The performance of the Flip-Flop is an important element to determine the performance of the whole synchronous circuit. The pulse generator can be shared among many flip-flops to reduce the power dissipation. Firstly, in the Dual edge static pulsed flip-flop suffers from high leakage current leads to more power consumption. Secondly, Dual edge trigger sense amplifier flip-flop having unnecessary transitions which causes power consumption. Thirdly, Dual edge trigger NAND keeper flip-flop keeper technique is used to pull up the voltage to VDD having full swing and this keeper transistor width is high and which consumes more power. The power consumption of the Dual edge nand keeper flip-flop is 347uW. Lastly, Dual edge trigger pulsed flip-flop is introduced by employing a technique called conditional switching for further power reduction. The circuits are designed in a 0.18-um standard CMOS process with a 1.8V power supply voltage.},
	author = {D. {Bhargavaram} and M. G. K. {Pillai}},
	booktitle = {IEEE-International Conference On Advances In Engineering, Science And Management (ICAESM -2012)},
	date-added = {2019-04-29 11:48:45 +1200},
	date-modified = {2019-04-29 11:48:58 +1200},
	keywords = {Other, SUNA, circuit switching;CMOS logic circuits;flip-flops;logic design;low-power electronics;NAND circuits;fow power dual edge triggered flip-flop;digital circuit;power consumption;synchronous circuit;pulse generator;power dissipation reduction;dual edge static pulsed flip-flop;dual edge trigger sense amplifier flip-flop;dual edge trigger NAND keeper flip-flop keeper technique;keeper transistor width;conditional switching;circuit design;CMOS process;power 347 muW;voltage 1.8 V;size 0.18 mum;Switches;Flip-flops;Clocks;MOS devices;Topology;Dual pulse generator;sense amplifier flip-flop;Static pulsed flip-flop;NAND keeper flip-flop;Pulsed flip-flop},
	month = {March},
	pages = {63-67},
	title = {Low power dual edge triggered flip-flop},
	year = {2012}}

@article{6804688,
	abstract = {The ability to carry out signal processing, classification, recognition, and computation in artificial spiking neural networks (SNNs) is mediated by their synapses. In particular, through activity-dependent alteration of their efficacies, synapses play a fundamental role in learning. The mathematical prescriptions under which synapses modify their weights are termed synaptic plasticity rules. These learning rules can be based on abstract computational neuroscience models or on detailed biophysical ones. As these rules are being proposed and developed by experimental and computational neuroscientists, engineers strive to design and implement them in silicon and en masse in order to employ them in complex real-world applications. In this paper, we describe analog very large-scale integration (VLSI) circuit implementations of multiple synaptic plasticity rules, ranging from phenomenological ones (e.g., based on spike timing, mean firing rates, or both) to biophysically realistic ones (e.g., calcium-dependent models). We discuss the application domains, weaknesses, and strengths of various representative approaches proposed in the literature, and provide insight into the challenges that engineers face when designing and implementing synaptic plasticity rules in VLSI technology for utilizing them in real-world applications.},
	author = {M. {Rahimi Azghadi} and N. {Iannella} and S. F. {Al-Sarawi} and G. {Indiveri} and D. {Abbott}},
	date-added = {2019-04-28 22:43:25 +1200},
	date-modified = {2019-04-28 22:43:25 +1200},
	doi = {10.1109/JPROC.2014.2314454},
	issn = {0018-9219},
	journal = {Proceedings of the IEEE},
	keywords = {analogue integrated circuits;learning (artificial intelligence);neural chips;VLSI;spike based synaptic plasticity;artificial spiking neural networks;mathematical prescriptions;learning rules;abstract computational neuroscience models;analog very large scale integration circuit;VLSI;neural chips;Transistors;Logic gates;Silicon;Neuromorphics;Neurons;Neuroscience;Learning systems;Plastics;Analog/digital synapse;Bienenstock--Cooper--Munro (BCM);calcium-based plasticity;learning;local correlation plasticity (LCP);neuromorphic engineering;rate-based plasticity;spike-timing-dependent plasticity (STDP);spike-based plasticity;spiking neural networks;synaptic plasticity;triplet STDP;very large-scale integration (VLSI);voltage-based STDP;Analog/digital synapse;Bienenstock¿Cooper¿Munro (BCM);calcium-based plasticity;learning;local correlation plasticity (LCP);neuromorphic engineering;rate-based plasticity;spike-timing-dependent plasticity (STDP);spike-based plasticity;spiking neural networks;synaptic plasticity;triplet STDP;very large-scale integration (VLSI);voltage-based STDP},
	month = {May},
	number = {5},
	pages = {717-737},
	title = {Spike-Based Synaptic Plasticity in Silicon: Design, Implementation, Application, and Challenges},
	volume = {102},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/JPROC.2014.2314454}}

@article{5678632,
	abstract = {Neurological disorders are becoming increasingly common in developed countries as a result of the aging population. In spite of medications, these disorders can result in progressive loss of function as well as chronic physical, cognitive, and emotional disability that ultimately places enormous emotional and economic on the patient, caretakers, and the society in general. Neuromodulation is emerging as a therapeutic option in these patients. Neuromodulation is a field, which involves implantable devices that allow for the reversible adjustable application of electrical, chemical, or biological agents to the central or peripheral nervous system with the objective of altering its functioning with the objective of achieving a therapeutic or clinically beneficial effect. It is a rapidly evolving field that brings together many different specialties in the fields of medicine, materials science, computer science and technology, biomedical, and neural engineering as well as the surgical or interventional specialties. It has multiple current and emerging indications, and an enormous potential for growth. The main challenges before it are in the need for effective collaboration between engineers, basic scientists, and clinicians to develop innovations that address specific problems resulting in new devices and clinical applications.},
	author = {C. O. {Oluigbo} and A. R. {Rezai}},
	date-added = {2019-04-28 20:57:10 +1200},
	date-modified = {2019-04-28 20:57:24 +1200},
	doi = {10.1109/TBME.2010.2102758},
	issn = {0018-9294},
	journal = {IEEE Transactions on Biomedical Engineering},
	keywords = {Other; SUNA; biochemistry;bioelectric phenomena;brain;cognition;electrochemistry;macromolecules;medical disorders;molecular biophysics;neuromuscular stimulation;prosthetics;neurological disorders;neuromodulation;emotional disability;aging population;chronic physical disability;patient therapy;implantable devices;reversible adjustable application;electrical agents;chemical agents;biological agents;central nervous system;peripheral nervous system;clinical applications;electrical stimulation;cognitive disability;Neuromuscular;Aging;Medical services;Neuromodulation;Gerontology;Neuromodulation;neurological disorders;Adult;Biomedical Engineering;Deep Brain Stimulation;Electric Stimulation Therapy;Electric Stimulation Therapy;Electrodes, Implanted;Female;Humans;Infusion Pumps, Implantable;Male;Nerve Net;Nervous System Diseases;Neurosciences;Vagus Nerve Stimulation},
	month = {July},
	number = {7},
	pages = {1907-1917},
	title = {Addressing Neurological Disorders With Neuromodulation},
	volume = {58},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TBME.2010.2102758}}

@phdthesis{stanley:phd04,
	author = {Kenneth O. Stanley},
	date-added = {2019-04-28 16:54:44 +1200},
	date-modified = {2019-04-28 16:54:59 +1200},
	keywords = {Books, NeuroEvolution, NEAT},
	school = {Department of Computer Sciences, The University of Texas at Austin},
	title = {Efficient Evolution of Neural Networks Through Complexification},
	url = {http://nn.cs.utexas.edu/?stanley:phd2004},
	year = {2004},
	Bdsk-Url-1 = {http://nn.cs.utexas.edu/?stanley:phd2004}}

@article{Siebel:2007:ERL:1367012.1367016,
	acmid = {1367016},
	address = {Amsterdam, The Netherlands, The Netherlands},
	author = {Siebel, Nils T. and Sommer, Gerald},
	date-added = {2019-04-28 16:41:08 +1200},
	date-modified = {2019-04-28 16:41:33 +1200},
	issn = {1448-5869},
	issue_date = {August 2007},
	journal = {Int. J. Hybrid Intell. Syst.},
	keywords = {Evolutionary, Reinforcement Learning,},
	month = aug,
	number = {3},
	numpages = {13},
	pages = {171--183},
	publisher = {IOS Press},
	title = {Evolutionary Reinforcement Learning of Artificial Neural Networks},
	url = {http://dl.acm.org/citation.cfm?id=1367012.1367016},
	volume = {4},
	year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1367012.1367016}}

@inproceedings{Gruau:1996:CCE:1595536.1595547,
	acmid = {1595547},
	address = {Cambridge, MA, USA},
	author = {Gruau, Fr{\'e}d{\'e}ric and Whitley, Darrell and Pyeatt, Larry},
	booktitle = {Proceedings of the 1st Annual Conference on Genetic Programming},
	date-added = {2019-04-28 16:27:02 +1200},
	date-modified = {2019-04-28 16:27:30 +1200},
	isbn = {0-262-61127-9},
	keywords = {Evolutionary, Cellular Encoding, Genetic Algorithm},
	location = {Stanford, California},
	numpages = {9},
	pages = {81--89},
	publisher = {MIT Press},
	title = {A Comparison Between Cellular Encoding and Direct Encoding for Genetic Neural Networks},
	url = {http://dl.acm.org/citation.cfm?id=1595536.1595547},
	year = {1996},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1595536.1595547}}

@article{Kung2018,
	abstract = {Memory performance is a key bottleneck for deep learning systems. Binarization of both activations and weights is one promising approach that can best scale to realize the highest energy efficient system using the lowest possible precision. In this paper, we utilize and analyze the binarized neural network in doing human detection on infrared images. Our results show comparable algorithmic performance of binarized versus 32bit floating-point networks, with the added benefit of greatly simplified computation and reduced memory overhead. In addition, we present a system architecture designed specifically for computation using binary representation that achieves at least 4{\texttimes} speedup and the energy is improved by three orders of magnitude over GPU.},
	author = {Kung, Jaeha and Zhang, David and van der Wal, Gooitzen and Chai, Sek and Mukhopadhyay, Saibal},
	date-added = {2019-04-28 14:28:44 +1200},
	date-modified = {2019-04-28 14:32:52 +1200},
	day = {01},
	doi = {10.1007/s11265-017-1255-5},
	issn = {1939-8115},
	journal = {Journal of Signal Processing Systems},
	keywords = {BNN, object detection, GPU, Infrared, FPGA},
	month = {Jun},
	number = {6},
	pages = {877--890},
	title = {Efficient Object Detection Using Embedded Binarized Neural Networks},
	url = {https://doi.org/10.1007/s11265-017-1255-5},
	volume = {90},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11265-017-1255-5}}

@inproceedings{Yaman:2018:LEC:3205455.3205555,
	acmid = {3205555},
	address = {New York, NY, USA},
	author = {Yaman, Anil and Mocanu, Decebal Constantin and Iacca, Giovanni and Fletcher, George and Pechenizkiy, Mykola},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	date-added = {2019-04-27 14:01:29 +1200},
	date-modified = {2019-04-27 14:03:02 +1200},
	doi = {10.1145/3205455.3205555},
	isbn = {978-1-4503-5618-3},
	keywords = {Evolutionary, cooperative co-evolution, differential evolution, direct encoding, neuroevolution, LECCDE},
	location = {Kyoto, Japan},
	numpages = {8},
	pages = {569--576},
	publisher = {ACM},
	series = {GECCO '18},
	title = {Limited Evaluation Cooperative Co-evolutionary Differential Evolution for Large-scale Neuroevolution},
	url = {http://doi.acm.org/10.1145/3205455.3205555},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3205455.3205555},
	Bdsk-Url-2 = {https://doi.org/10.1145/3205455.3205555}}

@inproceedings{10.1007/978-3-030-00111-7_23,
	abstract = {Stochastic gradient descent is the most prevalent algorithm to train neural networks. However, other approaches such as evolutionary algorithms are also applicable to this task. Evolutionary algorithms bring unique trade-offs that are worth exploring, but computational demands have so far restricted exploration to small networks with few parameters. We implement an evolutionary algorithm that executes entirely on the GPU, which allows to efficiently batch-evaluate a whole population of networks. Within this framework, we explore the limited evaluation evolutionary algorithm for neural network training and find that its batch evaluation idea comes with a large accuracy trade-off. In further experiments, we explore crossover operators and find that unprincipled random uniform crossover performs extremely well. Finally, we train a network with 92k parameters on MNIST using an EA and achieve 97.6{\%} test accuracy compared to 98{\%} test accuracy on the same network trained with Adam. Code is available at https://github.com/jprellberg/gpuea.},
	address = {Cham},
	author = {Prellberg, Jonas and Kramer, Oliver},
	booktitle = {KI 2018: Advances in Artificial Intelligence},
	date-added = {2019-04-27 12:26:17 +1200},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {Trollmann, Frank and Turhan, Anni-Yasmin},
	isbn = {978-3-030-00111-7},
	keywords = {Books, LEEA, MNIST, GPU},
	pages = {270--283},
	publisher = {Springer International Publishing},
	title = {Limited Evaluation Evolutionary Optimization of Large Neural Networks},
	year = {2018}}

@article{Dietterich:2000:HRL:1622262.1622268,
	acmid = {1622268},
	address = {USA},
	author = {Dietterich, Thomas G.},
	date-added = {2019-04-18 23:38:36 +1200},
	date-modified = {2019-04-18 23:40:14 +1200},
	issn = {1076-9757},
	issue_date = {August 2000},
	journal = {J. Artif. Int. Res.},
	keywords = {NN, OpenAI Gym, Reinforcement Learning},
	month = nov,
	number = {1},
	numpages = {77},
	pages = {227--303},
	publisher = {AI Access Foundation},
	title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
	volume = {13},
	year = {2000},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1622262.1622268}}

@inproceedings{pmlr-v48-zaremba16,
	abstract = {We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using Q-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by Q-learning.},
	address = {New York, New York, USA},
	author = {Wojciech Zaremba and Tomas Mikolov and Armand Joulin and Rob Fergus},
	booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	date-added = {2019-04-14 21:01:09 +1200},
	date-modified = {2019-04-14 21:01:31 +1200},
	editor = {Maria Florina Balcan and Kilian Q. Weinberger},
	keywords = {NN, Reinforcement Learning, OpenAI Gym, Q-Learning},
	month = {20--22 Jun},
	pages = {421--429},
	pdf = {http://proceedings.mlr.press/v48/zaremba16.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Learning Simple Algorithms from Examples},
	url = {http://proceedings.mlr.press/v48/zaremba16.html},
	volume = {48},
	year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/zaremba16.html}}

@inproceedings{Strens:2000:BFR:645529.658114,
	acmid = {658114},
	annote = {Publisher
Morgan Kaufmann Publishers Inc.

Address
San Francisco, CA, USA},
	author = {Strens, Malcolm J. A.},
	booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	date-added = {2019-04-14 16:42:28 +1200},
	date-modified = {2020-07-21 14:11:13 +1200},
	isbn = {1-55860-707-2},
	keywords = {NN, Reinforcement Learning, OpenAI Gym},
	numpages = {8},
	pages = {943--950},
	series = {ICML '00},
	title = {A Bayesian Framework for Reinforcement Learning},
	url = {http://dl.acm.org/citation.cfm?id=645529.658114},
	year = {2000},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=645529.658114}}

@article{Krotov201820458,
	abstract = {Despite great success of deep learning a question remains to what extent the computational properties of deep neural networks are similar to those of the human brain. The particularly nonbiological aspect of deep learning is the supervised training process with the backpropagation algorithm, which requires massive amounts of labeled data, and a nonlocal learning rule for changing the synapse strengths. This paper describes a learning algorithm that does not suffer from these two problems. It learns the weights of the lower layer of neural networks in a completely unsupervised fashion. The entire algorithm utilizes local learning rules which have conceptual biological plausibility.It is widely believed that end-to-end training with the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility and which is motivated by Hebb{\textquoteright}s idea that change of the synapse strength should be local{\textemdash}i.e., should depend only on the activities of the pre- and postsynaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer and is capable of learning early feature detectors in a completely unsupervised way. These learned lower-layer feature detectors can be used to train higher-layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm on simple tasks.},
	author = {Krotov, Dmitry and J. Hopfield, John},
	date-added = {2019-04-11 22:23:26 +1200},
	date-modified = {2019-04-11 22:24:04 +1200},
	doi = {10.1073/pnas.1820458116},
	elocation-id = {201820458},
	eprint = {https://www.pnas.org/content/early/2019/03/27/1820458116.full.pdf},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {DNN, Biological neural networks, MNIST, CIFAR, Hebb's rule, SGD},
	publisher = {National Academy of Sciences},
	title = {Unsupervised learning by competing hidden units},
	url = {https://www.pnas.org/content/early/2019/03/27/1820458116},
	year = {2019},
	Bdsk-Url-1 = {https://www.pnas.org/content/early/2019/03/27/1820458116},
	Bdsk-Url-2 = {https://doi.org/10.1073/pnas.1820458116}}

@article{2018arXiv181112028K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181112028K},
	archiveprefix = {arXiv},
	author = {{Kitai}, H. and {Cruz}, J.~P. and {Yanai}, N. and {Nishida}, N. and {Oba}, T. and {Unagami}, Y. and {Teruya}, T. and {Attrapadung}, N. and {Matsuda}, T. and {Hanaoka}, G.},
	date-added = {2019-04-05 19:42:55 +1300},
	date-modified = {2019-04-05 19:43:07 +1300},
	eprint = {1811.12028},
	journal = {arXiv e-prints},
	keywords = {BNN; Cryptography and Security, Artificial Intelligence},
	month = nov,
	primaryclass = {cs.CR},
	title = {{MOBIUS: Model-Oblivious Binarized Neural Networks}},
	year = 2018}

@inproceedings{Juvekar:2018:GLL:3277203.3277326,
	acmid = {3277326},
	address = {Berkeley, CA, USA},
	author = {Juvekar, Chiraag and Vaikuntanathan, Vinod and Chandrakasan, Anantha},
	booktitle = {Proceedings of the 27th USENIX Conference on Security Symposium},
	date-added = {2019-04-05 19:00:22 +1300},
	date-modified = {2019-04-05 19:25:39 +1300},
	isbn = {978-1-931971-46-1},
	keywords = {DNN, Homomorphic encryption, packed additive homomorphic encryption, Security, garbled-circuit},
	location = {Baltimore, MD, USA},
	numpages = {18},
	pages = {1651--1668},
	publisher = {USENIX Association},
	series = {SEC'18},
	title = {GAZELLE: A Low Latency Framework for Secure Neural Network Inference},
	url = {http://dl.acm.org/citation.cfm?id=3277203.3277326},
	year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3277203.3277326}}

@inproceedings{10.1007/978-3-319-78890-6_3,
	abstract = {Modern Convolutional Neural Networks (CNNs) are typically based on floating point linear algebra based implementations. Recently, reduced precision Neural Networks (NNs) have been gaining popularity as they require significantly less memory and computational resources compared to floating point. This is particularly important in power constrained compute environments. However, in many cases a reduction in precision comes at a small cost to the accuracy of the resultant network. In this work, we investigate the accuracy-throughput trade-off for various parameter precision applied to different types of NN models. We firstly propose a quantization training strategy that allows reduced precision NN inference with a lower memory footprint and competitive model accuracy. Then, we quantitatively formulate the relationship between data representation and hardware efficiency. Our experiments finally provide insightful observation. For example, one of our tests show 32-bit floating point is more hardware efficient than 1-bit parameters to achieve 99{\%} MNIST accuracy. In general, 2-bit and 4-bit fixed point parameters show better hardware trade-off on small-scale datasets like MNIST and CIFAR-10 while 4-bit provide the best trade-off in large-scale tasks like AlexNet on ImageNet dataset within our tested problem domain.},
	address = {Cham},
	author = {Su, Jiang and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Durelli, Gianluca and Thomas, David B. and Leong, Philip H. W. and Cheung, Peter Y. K.},
	booktitle = {Applied Reconfigurable Computing. Architectures, Tools, and Applications},
	date-added = {2019-04-04 23:58:00 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {Voros, Nikolaos and Huebner, Michael and Keramidas, Georgios and Goehringer, Diana and Antonopoulos, Christos and Diniz, Pedro C.},
	isbn = {978-3-319-78890-6},
	keywords = {Books, BNN, MNIST, CIFAR},
	pages = {29--42},
	publisher = {Springer International Publishing},
	title = {Accuracy to Throughput Trade-Offs for Reduced Precision Neural Networks on Reconfigurable Logic},
	year = {2018}}

@article{2018arXiv181211800D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181211800D},
	archiveprefix = {arXiv},
	author = {{Darabi}, S. and {Belbahri}, M. and {Courbariaux}, M. and {Partovi Nia}, V.},
	date-added = {2019-04-04 23:44:44 +1300},
	date-modified = {2019-04-04 23:44:56 +1300},
	eprint = {1812.11800},
	journal = {arXiv e-prints},
	keywords = {BNN; Machine Learning, Computer Vision and Pattern Recognition},
	month = dec,
	title = {{BNN+: Improved Binary Network Training}},
	year = 2018}

@inproceedings{10.1007/978-3-319-46493-0_32,
	abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}faster convolutional operations (in terms of number of the high precision operations) and 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than {\$}{\$}16{\backslash},{\backslash}{\%}{\$}{\$}16{\%}in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.},
	address = {Cham},
	author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	booktitle = {Computer Vision -- ECCV 2016},
	date-added = {2019-04-04 23:37:48 +1300},
	date-modified = {2019-04-04 23:38:31 +1300},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	isbn = {978-3-319-46493-0},
	keywords = {BNN, XNOR, ImageNet, CNN, AlexNet},
	pages = {525--542},
	publisher = {Springer International Publishing},
	title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
	year = {2016}}

@article{2018arXiv181201965B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181201965B},
	archiveprefix = {arXiv},
	author = {{Bethge}, J. and {Bornstein}, M. and {Loy}, A. and {Yang}, H. and {Meinel}, C.},
	date-added = {2019-04-04 23:09:42 +1300},
	date-modified = {2019-04-04 23:10:00 +1300},
	eprint = {1812.01965},
	journal = {arXiv e-prints},
	keywords = {BNN; Machine Learning, Computer Vision and Pattern Recognition, Machine Learning},
	month = dec,
	title = {{Training Competitive Binary Neural Networks from Scratch}},
	year = 2018}

@article{2019arXiv190309807K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2019arXiv190309807K},
	archiveprefix = {arXiv},
	author = {{Kim}, H. and {Kim}, Y. and {Ryu}, S. and {Kim}, J.-J.},
	date-added = {2019-04-04 22:42:22 +1300},
	date-modified = {2019-04-04 22:42:40 +1300},
	eprint = {1903.09807},
	journal = {arXiv e-prints},
	keywords = {BNN; Neural and Evolutionary Computing, Computer Vision and Pattern Recognition, Machine Learning},
	month = mar,
	title = {{BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation Function}},
	year = 2019}

@inproceedings{Cai_2017_CVPR,
	author = {Cai, Zhaowei and He, Xiaodong and Sun, Jian and Vasconcelos, Nuno},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2019-04-04 22:25:05 +1300},
	date-modified = {2019-04-04 22:25:44 +1300},
	keywords = {BNN, Guassian, HWGQ, VGGNet, GoogLeNet},
	month = {July},
	title = {Deep Learning With Low Precision by Half-Wave Gaussian Quantization},
	year = {2017}}

@inproceedings{10.1007/978-3-030-01237-3_23,
	abstract = {Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets.},
	address = {Cham},
	author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
	booktitle = {Computer Vision -- ECCV 2018},
	date-added = {2019-04-04 19:51:38 +1300},
	date-modified = {2019-04-04 19:52:17 +1300},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	isbn = {978-3-030-01237-3},
	keywords = {BNN, LQ-Net, Quantization, CIFAR, ImageNet},
	pages = {373--390},
	publisher = {Springer International Publishing},
	title = {LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks},
	year = {2018}}

@inproceedings{Liang:2018:EAS:3205455.3205489,
	acmid = {3205489},
	address = {New York, NY, USA},
	author = {Liang, Jason and Meyerson, Elliot and Miikkulainen, Risto},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	date-added = {2019-03-18 22:58:02 +1300},
	date-modified = {2019-03-18 22:58:12 +1300},
	doi = {10.1145/3205455.3205489},
	isbn = {978-1-4503-5618-3},
	keywords = {Evolutionary, artificial intelligence, neural networks/deep learning},
	location = {Kyoto, Japan},
	numpages = {8},
	pages = {466--473},
	publisher = {ACM},
	series = {GECCO '18},
	title = {Evolutionary Architecture Search for Deep Multitask Networks},
	url = {http://doi.acm.org/10.1145/3205455.3205489},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3205455.3205489},
	Bdsk-Url-2 = {https://doi.org/10.1145/3205455.3205489}}

@article{Stanley:2019aa,
	abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field's contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
	author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
	da = {2019/01/01},
	date-added = {2019-03-18 21:31:28 +1300},
	date-modified = {2019-03-18 21:31:49 +1300},
	doi = {10.1038/s42256-018-0006-z},
	id = {Stanley2019},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	keywords = {Evolutionary, NeuroEvolution, Nature, Survey},
	number = {1},
	pages = {24--35},
	title = {Designing neural networks through neuroevolution},
	ty = {JOUR},
	url = {https://doi.org/10.1038/s42256-018-0006-z},
	volume = {1},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s42256-018-0006-z}}

@inproceedings{7257254,
	abstract = {In the real world, the environment is constantly changing with the input variables under the effect of noise. However, few algorithms were shown to be able to work under those circumstances. Here, Novelty-Organizing Team of Classifiers (NOTC) is applied to the continuous action mountain car as well as two variations of it: a noisy mountain car and an unstable weather mountain car. These problems take respectively noise and change of problem dynamics into account. Moreover, NOTC is compared with NeuroEvolution of Augmenting Topologies (NEAT) in these problems, revealing a trade-off between the approaches. While NOTC achieves the best performance in all of the problems, NEAT needs less trials to converge. It is demonstrated that NOTC achieves better performance because of its division of the input space (creating easier problems). Unfortunately, this division of input space also requires a bit of time to bootstrap.},
	author = {D. V. {Vargas} and H. {Takano} and J. {Murata}},
	booktitle = {2015 IEEE Congress on Evolutionary Computation (CEC)},
	date-added = {2019-03-18 20:50:14 +1300},
	date-modified = {2019-03-18 20:50:24 +1300},
	doi = {10.1109/CEC.2015.7257254},
	issn = {1089-778X},
	keywords = {Evolutionary; neural nets;pattern classification;statistical analysis;dynamic environments;noisy environments;novelty-organizing team of classifiers;NOTC;noisy mountain car;continuous action mountain car;unstable weather mountain car;neuroevolution of augmenting topologies;NEAT;bootstrap;Sociology;Statistics;Arrays;Noise;Meteorology;Noise measurement},
	month = {May},
	pages = {2937-2944},
	title = {Novelty-organizing team of classifiers in noisy and dynamic environments},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2015.7257254}}

@article{REHMAN2014239,
	abstract = {Feedback in Neuro-Evolution is explored and evaluated for its application in devising prediction models for foreign currency exchange rates. A novel approach to foreign currency exchange rates forecasting based on Recurrent Neuro-Evolution is introduced. Cartesian Genetic Programming (CGP) is the algorithm deployed for the forecasting model. Recurrent Cartesian Genetic Programming evolved Artificial Neural Network (RCGPANN) is demonstrated to produce computationally efficient and accurate model for forex prediction with an accuracy of as high as 98.872% for a period of 1000 days. The approach utilizes the trends that are being followed in historical data to predict five currency rates against Australian dollar. The model is evaluated using statistical metrics and compared. The computational method outperforms the other methods particularly due to its capability to select the best possible feature in real time and the flexibility that the system provides in feature selection, connectivity pattern and network.},
	author = {Mehreen Rehman and Gul Muhammad Khan and Sahibzada Ali Mahmud},
	date-added = {2019-03-14 15:59:59 +1300},
	date-modified = {2019-03-14 16:03:00 +1300},
	doi = {https://doi.org/10.1016/j.ieri.2014.09.083},
	issn = {2212-6678},
	journal = {IERI Procedia},
	keywords = {Evolutionary, Finance, Foreign exchange rate forecasting, Neural Networks, Cartesian Genetic Programming, Neuro-evolution, Recurrent Networks, Time Series Prediction},
	note = {International Conference on Future Information Engineering (FIE 2014)},
	pages = {239 - 244},
	title = {Foreign Currency Exchange Rates Prediction Using CGP and Recurrent Neural Network},
	url = {http://www.sciencedirect.com/science/article/pii/S2212667814001312},
	volume = {10},
	year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212667814001312},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ieri.2014.09.083}}

@article{2016arXiv160601540B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160601540B},
	archiveprefix = {arXiv},
	author = {{Brockman}, G. and {Cheung}, V. and {Pettersson}, L. and {Schneider}, J. and {Schulman}, J. and {Tang}, J. and {Zaremba}, W.},
	date-added = {2019-03-13 17:26:41 +1300},
	date-modified = {2019-03-13 17:26:58 +1300},
	eprint = {1606.01540},
	journal = {arXiv e-prints},
	keywords = {Other, OpenAI, Machine Learning, Artificial Intelligence},
	month = jun,
	title = {{OpenAI Gym}},
	year = 2016}

@article{Turner2017,
	abstract = {Cartesian Genetic Programming of Artificial Neural Networks is a NeuroEvolutionary method based on Cartesian Genetic Programming. Cartesian Genetic Programming has recently been extended to allow recurrent connections. This work investigates applying the same recurrent extension to Cartesian Genetic Programming of Artificial Neural Networks in order to allow the evolution of recurrent neural networks. The new Recurrent Cartesian Genetic Programming of Artificial Neural Networks method is applied to the domain of series forecasting where it is shown to significantly outperform all standard forecasting techniques used for comparison including autoregressive integrated moving average and multilayer perceptrons. An ablation study is also performed isolating which specific aspects of Recurrent Cartesian Genetic Programming of Artificial Neural Networks contribute to it's effectiveness for series forecasting.},
	author = {Turner, Andrew James and Miller, Julian Francis},
	date-added = {2019-03-13 14:15:39 +1300},
	date-modified = {2019-03-13 14:16:03 +1300},
	day = {01},
	doi = {10.1007/s10710-016-9276-6},
	issn = {1573-7632},
	journal = {Genetic Programming and Evolvable Machines},
	keywords = {Evolutionary, CGP, RCGP, Cartesian Genetic Programming, Time Series},
	month = {Jun},
	number = {2},
	pages = {185--212},
	title = {Recurrent Cartesian Genetic Programming of Artificial Neural Networks},
	url = {https://doi.org/10.1007/s10710-016-9276-6},
	volume = {18},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-016-9276-6}}

@article{2019arXiv190200730L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2019arXiv190200730L},
	archiveprefix = {arXiv},
	author = {{Lahoud}, F. and {Achanta}, R. and {M{\'a}rquez-Neila}, P. and {S{\"u}sstrunk}, S.},
	date-added = {2019-03-08 13:24:44 +1300},
	date-modified = {2019-03-08 13:24:57 +1300},
	eprint = {1902.00730},
	journal = {arXiv e-prints},
	keywords = {BNN, Computer Vision and Pattern Recognition},
	month = feb,
	primaryclass = {cs.CV},
	title = {{Self-Binarizing Networks}},
	year = 2019}

@inproceedings{Shabash:2018:ECE:3205651.3208282,
	acmid = {3208282},
	address = {New York, NY, USA},
	author = {Shabash, Boris and Wiese, Kay C.},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
	date-added = {2019-03-06 19:11:06 +1300},
	date-modified = {2019-03-06 19:11:14 +1300},
	doi = {10.1145/3205651.3208282},
	isbn = {978-1-4503-5764-7},
	keywords = {Evolutionary, artificial neural networks, evolutionary computation, evolving neural network activation functions, fitness functions},
	location = {Kyoto, Japan},
	numpages = {8},
	pages = {1449--1456},
	publisher = {ACM},
	series = {GECCO '18},
	title = {EvoNN: A Customizable Evolutionary Neural Network with Heterogenous Activation Functions},
	url = {http://doi.acm.org/10.1145/3205651.3208282},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3205651.3208282},
	Bdsk-Url-2 = {https://doi.org/10.1145/3205651.3208282}}

@article{8651459,
	abstract = {Privacy in internet of things is a fundamental challenge for Ubiquitous healthcare systems that depends on data aggregated and collaborative deep learning among different parties. This paper proposes MSCryptoNet, a novel framework that enables scalable execution and the conversion of state-of-the-art learned neural network to MSCryptoNet models in the privacy-preservation setting.We also design a method for approximation of the activation function basically used in convolutional neural network (CNN) (i.e. Sigmoid, Rectified linear unit (ReLU) etc.) with low degree polynomials which is vital for computations in homomorphic encryption schemes. Our model seems to target the following scenarios: (1) The practical way to enforce the evaluation of classifier whose inputs are encrypted with possibly different encryption schemes or even different keys whiles securing all operations including intermediate results. (2) The minimization of communication and computational cost of Data Providers. MSCryptoNet is based on multi-scheme fully homomorphic encryption (MS-FHE). We also prove that MSCryptoNet as a privacy-preserving deep learning scheme over the aggregated encrypted data are secured.},
	author = {O. {Kwabena} and Z. {Qin} and T. {Zhuang} and Z. {Qin}},
	date-added = {2019-02-26 18:48:08 +1300},
	date-modified = {2019-02-26 18:48:15 +1300},
	doi = {10.1109/ACCESS.2019.2901219},
	issn = {2169-3536},
	journal = {IEEE Access},
	keywords = {CNN; Cryptography;Neural networks;Training;Cloud computing;Deep learning;Computational modeling;Data models;Internet of Things;Privacy-preserving;Fully Homomorphic Encryption},
	pages = {1-1},
	title = {MSCryptoNet: Multi-Scheme privacy-preserving deep learning in cloud computing},
	year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBFLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgUmV2aWV3IG9mIEJpbmFyaXplZCBOZXVyYWwgTmV0d29ya3MuYmliTxEB3AAAAAAB3AACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgUmV2aWV3IG9mIEJpbmFyaSNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANCTk4AAAIAWy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpCTk46QSBSZXZpZXcgb2YgQmluYXJpemVkIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4AVAApAEEAIABSAGUAdgBpAGUAdwAgAG8AZgAgAEIAaQBuAGEAcgBpAHoAZQBkACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAFlVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0JOTi9BIFJldmlldyBvZiBCaW5hcml6ZWQgTmV1cmFsIE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAbAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAJM},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACCESS.2019.2901219}}

@article{7460958,
	abstract = {Learning algorithms are being increasingly adopted in various applications. However, further expansion will require methods that work more automatically. To enable this level of automation, a more powerful solution representation is needed. However, by increasing the representation complexity, a second problem arises. The search space becomes huge, and therefore, an associated scalable and efficient searching algorithm is also required. To solve both the problems, first a powerful representation is proposed that unifies most of the neural networks features from the literature into one representation. Second, a new diversity preserving method called spectrum diversity is created based on the new concept of chromosome spectrum that creates a spectrum out of the characteristics and frequency of alleles in a chromosome. The combination of spectrum diversity with a unified neuron representation enables the algorithm to either surpass or equal NeuroEvolution of Augmenting Topologies on all of the five classes of problems tested. Ablation tests justify the good results, showing the importance of added new features in the unified neuron representation. Part of the success is attributed to the novelty-focused evolution and good scalability with a chromosome size provided by spectrum diversity. Thus, this paper sheds light on a new representation and diversity preserving mechanism that should impact algorithms and applications to come.},
	author = {Danilo Vargas and Junichi Murata},
	date-added = {2019-02-25 23:58:32 +1300},
	date-modified = {2020-12-17 14:36:44 +1300},
	doi = {10.1109/TNNLS.2016.2551748},
	issn = {2162-237X},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Evolutionary; computational complexity;evolutionary computation;learning (artificial intelligence);neural nets;search problems;spectrum-diverse neuroevolution;unified neural models;learning algorithms;solution representation;representation complexity;search space;efficient searching algorithm;neural networks features;diversity preserving method;chromosome spectrum;NeuroEvolution;augmenting topologies;unified neuron representation;chromosome size;Neurons;Biological neural networks;Topology;Network topology;Biological cells;Encoding;Technological innovation;General artificial intelligence;neuroevolution;neuroEvolution of Augmenting Topology (NEAT);reinforcement learning;spectrum diversity;topology and weight evolving artificial neural network (TWEANN);unified neuron model; SUNA},
	month = {Aug},
	number = {8},
	pages = {1759-1773},
	title = {Spectrum-Diverse Neuroevolution With Unified Neural Models},
	volume = {28},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2016.2551748}}

@inproceedings{8590945,
	abstract = {Modern cryptographic schemes is developed based on the mathematical theory. Recently works show a new direction about cryptography based on the neural networks. Instead of learning a specific algorithm, a cryptographic scheme is generated automatically. While one kind of neural network is used to achieve the scheme, the idea of the neural cryptography can be realized by other neural network architecture is unknown. In this paper, we make use of this property to create neural cryptography scheme on a new topology evolving neural network architecture called Spectrum-diverse unified neuroevolution architecture. First, experiments are conducted to verify that Spectrum-diverse unified neuroevolution architecture is able to achieve automatic encryption and decryption. Subsequently, we do experiments to achieve the neural symmetric cryptosystem by using adversarial training.},
	author = {Y. {Zhu} and D. V. {Vargas} and K. {Sakurai}},
	booktitle = {2018 Sixth International Symposium on Computing and Networking Workshops (CANDARW)},
	date-added = {2019-02-25 23:04:26 +1300},
	date-modified = {2019-04-28 20:13:28 +1200},
	doi = {10.1109/CANDARW.2018.00091},
	keywords = {Evolutionary; SUNA; cryptography;learning (artificial intelligence);neural nets;topology evolving neural networks;neural network architecture;neural cryptography scheme;neural symmetric cryptosystem;mathematical theory;spectrum-diverse unified neuroevolution architecture;automatic encryption;automatic decryption;adversarial training;Neurons;Encryption;Biological neural networks;Network topology;Topology;Neural cryptography, Symmetric cryptosystem, Spectrum-diverse unified neuroevolution architecture, Topology evolving neural networks},
	month = {Nov},
	pages = {472-478},
	title = {Neural Cryptography Based on the Topology Evolving Neural Networks},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CANDARW.2018.00091}}

@inproceedings{8588788,
	abstract = {It has been proven that Recurrent Neural Networks (RNNs) are Turing Complete, i.e. for any given computable function there exists a finite RNN to compute it. Consequently, researchers have trained Recurrent Neural Networks to learn simple functions like sorting, addition, compression and more recently, even classical cryptographic ciphers such as the Enigma. In this paper, we try to identify the characteristics of functions that make them easy or difficult for the RNN to learn. We look at functions from a cryptographic point of view by studying the ways in which the output depends on the input. We use cryptographic parameters (confusion and diffusion) for determining the strength of a cipher and quantify this dependence to show that a strong correlation exists between the learning capability of an RNN and the function's cryptographic parameters.},
	author = {S. {Srivastava} and A. {Bhatia}},
	booktitle = {2018 IEEE International Conference on Big Knowledge (ICBK)},
	date-added = {2019-02-25 22:52:19 +1300},
	date-modified = {2019-02-25 22:52:38 +1300},
	doi = {10.1109/ICBK.2018.00029},
	keywords = {DNN; RNN, cryptography;learning (artificial intelligence);recurrent neural nets;learning capability;recurrent neural networks;cryptographic perspective;simple functions;classical cryptographic ciphers;RNN;computable function;Ciphers;Task analysis;Recurrent neural networks;Computer architecture;Training;Recurrent Neural Networks;Cryptographic Ciphers;Confusion Parameter;Diffusion Parameter},
	month = {Nov},
	pages = {162-167},
	title = {On the Learning Capabilities of Recurrent Neural Networks: A Cryptographic Perspective},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICBK.2018.00029}}

@inproceedings{8622812,
	abstract = {With the rapidly emerging encryption techniques for network traffic, the classification of encrypted traffic has increasingly become significantly important in network management and security. In this paper, we propose a novel deep neural network that combines both the convolutional network and the recurrent network to improve the accuracy of the classification results. The convolutional network is used to extract the packet features for a single packet. The recurrent network is trained to pick out the flow features based on the inputs of the packet features of any three consecutive packets in a flow. The proposed model surpasses the existing studies which ask for the first packets of a flow, and it provides more flexibility in real practice. We compare our model with the existing work under deep learning for encrypted traffic classification, based on the public dataset. The experimental results show that our model outperforms the state-of-the-art work in terms of both higher efficiency and effectiveness.},
	author = {Z. {Zou} and J. {Ge} and H. {Zheng} and Y. {Wu} and C. {Han} and Z. {Yao}},
	booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
	date-added = {2019-02-25 22:35:03 +1300},
	date-modified = {2019-02-25 22:35:32 +1300},
	doi = {10.1109/HPCC/SmartCity/DSS.2018.00074},
	keywords = {CNN; communications, computer network management;computer network security;cryptography;learning (artificial intelligence);recurrent neural nets;telecommunication traffic;encryption techniques;public dataset;consecutive packets;flow features;packet features;recurrent network;deep neural network;security;network management;network traffic;convolutional long short-term memory neural network;encrypted traffic classification;Feature extraction;Encryption;Virtual private networks;Neural networks;Machine learning;Payloads;Encrypted Traffic Classification, Deep Learning},
	month = {June},
	pages = {329-334},
	title = {Encrypted Traffic Classification with a Convolutional Long Short-Term Memory Neural Network},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/HPCC/SmartCity/DSS.2018.00074}}

@article{Alaghi:2013:SSC:2465787.2465794,
	acmid = {2465794},
	address = {New York, NY, USA},
	articleno = {92},
	author = {Alaghi, Armin and Hayes, John P.},
	date-added = {2019-02-25 16:29:35 +1300},
	date-modified = {2019-02-25 16:29:43 +1300},
	doi = {10.1145/2465787.2465794},
	issn = {1539-9087},
	issue_date = {May 2013},
	journal = {ACM Trans. Embed. Comput. Syst.},
	keywords = {Other, Probabilistic computation, stochastic computing, stochastic logic},
	month = may,
	number = {2s},
	numpages = {19},
	pages = {92:1--92:19},
	publisher = {ACM},
	title = {Survey of Stochastic Computing},
	url = {http://doi.acm.org/10.1145/2465787.2465794},
	volume = {12},
	year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2465787.2465794},
	Bdsk-Url-2 = {https://doi.org/10.1145/2465787.2465794}}

@inproceedings{7100476,
	abstract = {In conventional security mechanism, cryptography is a process of information and data hiding from unauthorized access. It offers the unique possibility of certifiably secure data transmission among users at different remote locations. Cryptography is used to achieve availability, privacy and integrity over different networks. Usually, there are two categories of cryptography i.e. symmetric and asymmetric. In this paper, we have proposed a new symmetric key algorithm based on genetic algorithm (GA) and error back propagation neural network (EBP-NN). Genetic algorithm has been used for encryption and neural network has been used for decryption process. Consequently, this paper proposes an easy cryptographic secure algorithm for communication over the public computer networks.},
	author = {V. {Sagar} and K. {Kumar}},
	booktitle = {2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)},
	date-added = {2019-02-25 16:07:03 +1300},
	date-modified = {2019-02-25 16:07:27 +1300},
	keywords = {Evolutionary; backpropagation;computer network security;cryptography;genetic algorithms;neural network;symmetric key cryptography;data hiding;information hiding;unauthorized access;certifiably secure data transmission;remote locations;data integrity;data privacy;genetic algorithm;error back propagation neural network;EBP-NN;GA;decryption process;cryptographic secure algorithm;public computer networks;Neurons;Genetic algorithms;Encryption;Artificial neural networks;Receivers;cryptography;error back propagation neural network;genetic algorithm;symmetric key},
	month = {March},
	pages = {1386-1391},
	title = {A symmetric key cryptography using genetic algorithm and error back propagation neural network},
	year = {2015}}

@article{lmcs:1132,
	author = {Hirschowitz, Tom},
	date-added = {2019-02-01 09:35:36 +1300},
	date-modified = {2019-02-01 09:36:13 +1300},
	doi = {10.2168/LMCS-9(3:10)2013},
	journal = {Logical Methods in Computer Science},
	keywords = {Haskell, Logic in Computer Science ; Programming Languages ; Mathematics; Category Theory},
	month = Sep,
	title = {Cartesian closed 2-categories and permutation equivalence in higher-order rewriting},
	url = {https://lmcs.episciences.org/1132},
	volume = {Volume 9, Issue 3},
	year = {2013},
	Bdsk-Url-1 = {https://lmcs.episciences.org/1132},
	Bdsk-Url-2 = {https://doi.org/10.2168/LMCS-9(3:10)2013}}

@inbook{Cotta2002,
	abstract = {Training artificial neural networks is a complex task of great practical importance. Besides classical ad-hoc algorithms such as backpropagation, this task can be approached by using Evolutionary Computation, a highly configurable and effective optimization paradigm. This chapter provides a brief overview of these techniques, and shows how they can be readily applied to the resolution of this problem. Three popular variants of Evolutionary Algorithms ---Genetic Algorithms, Evolution Strategies and Estimation of Distribution Algorithms--- are described and compared. This comparison is done on the basis of a benchmark comprising several standard classification problems of interest for neural networks. The experimental results confirm the general appropriateness of Evolutionary Computation for this problem. Evolution Strategies seem particularly proficient techniques in this optimization domain, and Estimation of Distribution Algorithms are also a competitive approach.},
	address = {Boston, MA},
	author = {Cotta, C. and Alba, E. and Sagarna, R. and Larra{\~{n}}aga, P.},
	booktitle = {Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation},
	date-added = {2019-01-29 20:09:59 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	doi = {10.1007/978-1-4615-1539-5_18},
	editor = {Larra{\~{n}}aga, Pedro and Lozano, Jose A.},
	isbn = {978-1-4615-1539-5},
	keywords = {Books, Evolutionary, NN, GA},
	pages = {361--377},
	publisher = {Springer US},
	title = {Adjusting Weights in Artificial Neural Networks using Evolutionary Algorithms},
	url = {https://doi.org/10.1007/978-1-4615-1539-5_18},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4615-1539-5_18}}

@inproceedings{Modesitt2018NeuralC,
	author = {Dylan Modesitt and Tim D Henry and Jon Coden and Rachel Lathe},
	booktitle = {MIT Coursework},
	date-added = {2019-01-14 19:46:16 +1300},
	date-modified = {2019-01-14 19:49:12 +1300},
	keywords = {NN; Cryptonet, Stegranography, Adversarial Neural Cryptography},
	title = {Neural Cryptography: From Symmetric Encryption to Adversarial Steganography},
	year = {2018}}

@phdthesis{Homomorphic-Gentry,
	author = {Craig Gentry},
	date-added = {2019-01-13 00:28:06 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	keywords = {Books, Homomorphic encryption, Thesis, Encryption},
	month = {September},
	school = {Stanford University},
	title = {A Fully Homomorphic Encryption Scheme},
	type = {Dissertation},
	year = {2009}}

@article{6591252,
	abstract = {Artificial intelligence can be approached through the fast-time evolution of finite-state machines. Random mutation of an arbitrary machine yields an ``offspring.'' Both machines are driven by the available history and evaluated in terms of the given goal, and the machine having the higher score is selected to serve as the new parent. Such fast-time mutation and selection is continued with real-time decisions being based on the logic of the surviving machine. Saving the best few machines increases the security against gross nonstationarity of the environment. The efficiency of the evolutionary program is improved by introducing a cost-for-complexity weighting on each machine. An ability to predict one's environment is prerequisite to purposeful behavior. With this in mind, IBM7094 experiments were conducted to examine evolutionary prediction. As expected, cyclic signals in various degrees of noise were soon characterized by the predictor-machines. The transition probabilities within the sequence of predictions of low-order Markov processes were in close correspondence with those of the environment. The evolutionary program was also required to predict the (4-symbol) output sequence of an arbitrary machine that was driven by random binary noise. After 160 predictions the percent correct reached 51.5. When the evolutionary program was also given, the input binary variable this score reached 80 percent, showing a rapid approach toward the 100 percent asymptote. In contrast, providing an uncorrelated binary variable degraded the performance to 40.5 percent by requiring an attempt to extract nonexistent information. A formal technique was devised which translates a predictor machine into a set of hypotheses concerning the logic of the environment.},
	author = {L. J. Fogel and A. J. Owens and M. J. Walsh},
	date-added = {2019-01-12 18:38:20 +1300},
	date-modified = {2019-01-12 18:38:32 +1300},
	doi = {10.1109/THFE.1965.6591252},
	issn = {0096-249X},
	journal = {IEEE Transactions on Human Factors in Electronics},
	keywords = {Evolutionary; Noise;Automata;Decision making;Markov processes;Prediction algorithms;Shape;Springs},
	month = {Sep.},
	number = {1},
	pages = {13-23},
	title = {Intelligent decision-making through a simulation of evolution},
	volume = {HFE-6},
	year = {1965},
	Bdsk-Url-1 = {https://doi.org/10.1109/THFE.1965.6591252}}

@incollection{NIPS2017_6768,
	author = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	booktitle = {Advances in Neural Information Processing Systems 30},
	date-added = {2019-01-12 14:34:44 +1300},
	date-modified = {2019-01-12 14:56:53 +1300},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	keywords = {NN; SGD; Parallel; communicating structure},
	pages = {1709--1720},
	publisher = {Curran Associates, Inc.},
	title = {QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},
	url = {http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf}}

@phdthesis{Brodsky-CAN,
	author = {Stephen A. Brodsky},
	date-added = {2019-01-11 21:58:01 +1300},
	date-modified = {2019-01-11 22:01:45 +1300},
	keywords = {Thesis, BNN, CAN},
	school = {University of California, San Diego},
	title = {Content Addressable Networks},
	year = {1995}}

@inproceedings{Liu_2018_ECCV,
	author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
	booktitle = {The European Conference on Computer Vision (ECCV)},
	date-added = {2019-01-11 21:15:50 +1300},
	date-modified = {2019-01-11 21:26:47 +1300},
	keywords = {CNN; search problem, Reinforcement learning,},
	month = {September},
	title = {Progressive Neural Architecture Search},
	year = {2018}}

@url{EvolStraOAI,
	author = {Open AI},
	date-added = {2019-01-11 17:13:38 +1300},
	date-modified = {2019-01-11 17:15:07 +1300},
	keywords = {evolution strategies, OpenAI, Atari, Videogames, Reinforcement learning},
	lastchecked = {11},
	month = {Jan},
	title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
	url = {https://blog.openai.com/evolution-strategies/},
	year = {2018},
	Bdsk-Url-1 = {https://blog.openai.com/evolution-strategies/}}

@webpage{DeepNeuroEvol,
	author = {Uber},
	date-added = {2019-01-11 17:11:10 +1300},
	date-modified = {2019-01-11 17:13:02 +1300},
	keywords = {Neuroevolution; Atari, Videogames; evolution strategies; uber},
	lastchecked = {11/Jan},
	month = {January},
	title = {Accelerating Deep Neuroevolution: Train Atari in Hours on a Single Personal Computer},
	url = {https://eng.uber.com/accelerated-neuroevolution/},
	year = {2019},
	Bdsk-Url-1 = {https://eng.uber.com/accelerated-neuroevolution/}}

@inproceedings{4670938,
	abstract = {This paper introduces a chaotic encryption system using a principal component analysis (PCA) neural network. The PCA neural network can produce the chaotic behaviors under certain conditions so that it serves as a pseudo-random number generator to generate random private keys. In this encryption system, the one-time pad encryption method is used, which is regarded as the most secure encryption method. The proposed system can encrypt any kind of data. The security and high performance of encryption are illustrated via some simulations.},
	author = {Xiao Fei and Guisong Liu and Bochuan Zheng},
	booktitle = {2008 IEEE Conference on Cybernetics and Intelligent Systems},
	date-added = {2019-01-11 16:41:09 +1300},
	date-modified = {2019-01-11 16:41:25 +1300},
	doi = {10.1109/ICCIS.2008.4670938},
	issn = {2326-8123},
	keywords = {NN; chaotic communication;neural nets;principal component analysis;private key cryptography;random number generation;chaotic encryption system;PCA neural networks;principal component analysis;pseudo-random number generator;random private keys;one-time pad encryption method;Chaos;Cryptography;Principal component analysis;Neural networks;Information security;Random number generation;Computational intelligence;Laboratories;Computer science;Paper technology;Principal Component Analysis;Neural Networks;Chaotic Encryption;Information Security; Cryptonet},
	month = {Sep.},
	pages = {465-469},
	title = {A chaotic encryption system using PCA neural networks},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCIS.2008.4670938}}

@inproceedings{4582655,
	abstract = {Two constructive generalized synchronization (GS) theorems for a kind of neural network are introduced, which are described by discrete-time array equation systems (DTAEs). Based on the theorems, one can design a GS driven DTAE via a driving chaotic DTAE and an inverse function of H. As an application, a generalized Henon cellular neural network (CNN) with three state variables is introduced. Using the GS theorems and the generalized Henon CNN constructs a coupled GS DTAE with 2646 cells. The hyper chaotic GS phenomena of the GS DTAE have been simulated. The numerical simulation results display complex behaviors of the GS DTAE. Using the DTAE designs a encryption scheme with ldquoone-time padrdquo function. This scheme is able successfully to encrypt and decrypt original information without any loss. The scheme is sensitive to the perturbations of the initial conditions and some system parameters of the DTAE. The key space is huge.},
	author = {Hongyan Zang and Lequan Min},
	booktitle = {2008 3rd IEEE Conference on Industrial Electronics and Applications},
	date-added = {2019-01-11 16:38:55 +1300},
	date-modified = {2019-01-11 16:39:11 +1300},
	doi = {10.1109/ICIEA.2008.4582655},
	issn = {2156-2318},
	keywords = {NN; cellular neural nets;chaotic communication;cryptography;discrete time systems;Henon mapping;synchronisation;generalized synchronization theorems;neural network;data encryption;discrete-time array equation systems;inverse function;generalized Henon cellular neural network;three state variables;hyper chaotic phenomena;one-time pad function;key space; Cryptonet},
	month = {June},
	pages = {948-953},
	title = {Generalized synchronization theorems for a kind of Neural Network with application in data encryption},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIEA.2008.4582655}}

@inproceedings{7965079,
	abstract = {Scaling up Artificial Intelligence (AI) algorithms for massive datasets to improve their performance is becoming crucial. In Machine Translation (MT), one of most important research fields of AI, models based on Recurrent Neural Net- works (RNN) show state-of-the-art performance in recent years, and many researchers keep working on improving RNN-based models to achieve better accuracy in translation tasks. Most implementations of Neural Machine Translation (NMT) models employ a padding strategy when processing a mini-batch to make all sentences in a mini-batch have the same length. This enables an efficient utilization of caches and GPU/SIMD parallelism but leads to a waste of computation time. In this paper, we implement and parallelize batch learning for a Sequence-to- Sequence (Seq2Seq) model, which is the most basic model of NMT, without using a padding strategy. More specifically, our approach forms vectors which represent the input words as well as the neural network's states at different time steps into matrices when it processes one sentence, and as a result, the approach makes a better use of cache and optimizes the process that adjusts weights and biases during the back-propagation phase. Our experimental evaluation shows that our implementation achieves better scalability on multi-core CPUs. We also discuss our approach's potential to be used in other implementations of RNN-based models.},
	author = {Y. Qiao and K. Hashimoto and A. Eriguchi and H. Wang and D. Wang and Y. Tsuruoka and K. Taura},
	booktitle = {2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
	date-added = {2019-01-11 16:36:22 +1300},
	date-modified = {2019-01-11 16:36:34 +1300},
	doi = {10.1109/IPDPSW.2017.165},
	keywords = {NN; backpropagation;cache storage;graphics processing units;language translation;matrix algebra;multiprocessing systems;recurrent neural nets;vectors;cache friendly parallelization;neural encoder-decoder models;multicore architecture;scaling up artificial intelligence algorithms;neural machine translation models;NMT models;recurrent neural networks;RNN-based models;translation tasks;GPU-SIMD parallelism;batch learning parallelization;sequence-to-sequence model;Seq2Seq model;back-propagation phase;multicore CPUs;Computational modeling;Instruction sets;Scalability;Electronic mail;Recurrent neural networks;Training;Libraries;Neural Machine Translation;Cache Optimization;Parallel Programming; Cryptonet},
	month = {May},
	pages = {437-440},
	title = {Cache Friendly Parallelization of Neural Encoder-Decoder Models Without Padding on Multi-core Architecture},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPDPSW.2017.165}}

@inproceedings{5209334,
	abstract = {Based on best square approximation theory, new feed-forward neural networks are introduced where hidden units activation functions employ Laguerre orthogonal polynomials. Use these neural networks as the identifier model of the chaotic time series. Then, by varying the chaotic initial value and inputting to the networks, can produce new chaotic series, which are close to the theoretical values. We extract a subsequence as same length as the plaintext from the chaotic series and sort it. At last, by permuting the plaintext according to the sorted results of the subsequence, we can achieve the ciphertext. In the encryption system, the security of it depends completely on the complexity and unpredictability of the chaos. Especially, by varying the chaotic initial value, we can implement asynchronous "one-time pad cipher" encryption. The theoretical analysis and encryption instances proved that our arithmetic is useful, simple and high security, and it also has many advantages that a synchronous system can never achieve.},
	author = {A. Zou and X. Xiao},
	booktitle = {2009 WRI Global Congress on Intelligent Systems},
	date-added = {2019-01-11 16:33:09 +1300},
	date-modified = {2019-01-11 16:33:58 +1300},
	doi = {10.1109/GCIS.2009.82},
	issn = {2155-6083},
	keywords = {NN; computational complexity;cryptography;feedforward neural nets;time series;asynchronous encryption arithmetic;Laguerre chaotic neural networks;feedforward neural networks;Laguerre orthogonal polynomials;chaotic time series;chaotic initial value;ciphertext;one-time pad cipher;Cryptography;Arithmetic;Chaos;Neural networks;Chaotic communication;Polynomials;Approximation methods;Intelligent systems;Intelligent networks;Educational institutions;Neural Networks;Chaos;Permute;Laguerre Polynomial;Asynchronous Encryption; Cryptonet},
	month = {May},
	pages = {36-39},
	title = {An Asynchronous Encryption Arithmetic Based on Laguerre Chaotic Neural Networks},
	volume = {4},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/GCIS.2009.82}}

@article{2018arXiv180201548R,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180201548R},
	archiveprefix = {arXiv},
	author = {{Real}, E. and {Aggarwal}, A. and {Huang}, Y. and {Le}, Q.~V},
	date-added = {2019-01-11 15:55:33 +1300},
	date-modified = {2019-01-11 15:56:13 +1300},
	eprint = {1802.01548},
	journal = {arXiv e-prints},
	keywords = {Evolutionarym Neural Computing, Artificial Intelligence, Computer Vision and Pattern Recognition, Distributed, Parallel, and Cluster Computing, Google},
	month = feb,
	title = {{Regularized Evolution for Image Classifier Architecture Search}},
	year = 2018}

@inproceedings{pmlr-v70-real17a,
	abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
	address = {International Convention Centre, Sydney, Australia},
	author = {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	date-added = {2019-01-11 13:48:56 +1300},
	date-modified = {2019-01-11 15:28:49 +1300},
	editor = {Doina Precup and Yee Whye Teh},
	keywords = {Evolutionary; Neural and Evolutionary Computing; Artificial Intelligence; Computer Vision; Pattern Recognition; Distributed, Parallel, Cluster Computing, image classification},
	month = {06--11 Aug},
	pages = {2902--2911},
	pdf = {http://proceedings.mlr.press/v70/real17a/real17a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Large-Scale Evolution of Image Classifiers},
	url = {http://proceedings.mlr.press/v70/real17a.html},
	volume = {70},
	year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/real17a.html}}

@article{s18051306,
	abstract = {Researches in Artificial Intelligence (AI) have achieved many important breakthroughs, especially in recent years. In some cases, AI learns alone from scratch and performs human tasks faster and better than humans. With the recent advances in AI, it is natural to wonder whether Artificial Neural Networks will be used to successfully create or break cryptographic algorithms. Bibliographic review shows the main approach to this problem have been addressed throughout complex Neural Networks, but without understanding or proving the security of the generated model. This paper presents an analysis of the security of cryptographic algorithms generated by a new technique called Adversarial Neural Cryptography (ANC). Using the proposed network, we show limitations and directions to improve the current approach of ANC. Training the proposed Artificial Neural Network with the improved model of ANC, we show that artificially intelligent agents can learn the unbreakable One-Time Pad (OTP) algorithm, without human knowledge, to communicate securely through an insecure communication channel. This paper shows in which conditions an AI agent can learn a secure encryption scheme. However, it also shows that, without a stronger adversary, it is more likely to obtain an insecure one.},
	article-number = {1306},
	author = {Coutinho, Murilo and de Oliveira Albuquerque, Robson and Borges, F{\'a}bio and Garc{\'\i}a Villalba, Luis Javier and Kim, Tai-Hoon},
	date-added = {2019-01-10 20:35:57 +1300},
	date-modified = {2019-01-11 15:27:46 +1300},
	doi = {10.3390/s18051306},
	issn = {1424-8220},
	journal = {Sensors},
	keywords = {DCGAN, Cryptography, Adversarial Neural Cryptography, Cryptonet},
	number = {5},
	title = {Learning Perfectly Secure Cryptography to Protect Communications with Adversarial Neural Cryptography},
	url = {http://www.mdpi.com/1424-8220/18/5/1306},
	volume = {18},
	year = {2018},
	Bdsk-Url-1 = {http://www.mdpi.com/1424-8220/18/5/1306},
	Bdsk-Url-2 = {https://doi.org/10.3390/s18051306}}

@article{2018arXiv181004714D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181004714D},
	archiveprefix = {arXiv},
	author = {{Dong}, H.-W. and {Yang}, Y.-H.},
	date-added = {2019-01-10 16:41:50 +1300},
	date-modified = {2019-01-10 16:42:08 +1300},
	eprint = {1810.04714},
	journal = {arXiv e-prints},
	keywords = {BNN; Machine Learning, Statistics, Machine Learning},
	month = oct,
	title = {{Training Generative Adversarial Networks with Binary Neurons by End-to-end Backpropagation}},
	year = 2018}

@conference{AAAI1817150,
	abstract = {The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107% in terms of mAP (See Table 2).},
	author = {Jingkuan Song and Tao He and Lianli Gao and Xing Xu and Alan Hanjalic and Heng Tao Shen},
	booktitle = {AAAI Publications, Thirty-Second AAAI Conference on Artificial Intelligence},
	conference = {AAAI Conference on Artificial Intelligence},
	date-added = {2019-01-10 16:21:03 +1300},
	date-modified = {2020-12-10 14:30:58 +1300},
	keywords = {DCGAN; hashing; GAN; image retrieval},
	publisher = {AAAI},
	title = {Binary Generative Adversarial Networks for Image Retrieval},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17150},
	year = {2018},
	Bdsk-Url-1 = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17150}}

@electronic{titanRTXSpecs,
	author = {Tech Power Up},
	date-added = {2019-01-09 17:13:52 +1300},
	date-modified = {2019-01-09 17:16:02 +1300},
	keywords = {NVidia, GPU, URL},
	lastchecked = {9/Jan/2018},
	title = {NVidia Titan RTX},
	url = {https://www.techpowerup.com/gpu-specs/titan-rtx.c3311},
	Bdsk-Url-1 = {https://www.techpowerup.com/gpu-specs/titan-rtx.c3311}}

@inbook{Tettamanzi2001,
	abstract = {WE saw in Chapter 2 that artificial neural networks are biologically-inspired computational models that have the capability of somehow ``learning'' or ``self-organizing'' to accomplish a given task. They are particularly efficient when the nature of the task is ill-defined and the input/output mapping largely unknown. However, many aspects may affect the performance of an ANN on a given problem. Among them, the most important is the structure of the neuron connections i.e., the topology of the net, the connection weights, the details of the learning rules and of the neural activation function, and the data sets to be used for learning. There are guidelines for picking or finding reasonable values for all of these network parameters but most are rules of thumb with little theoretical background and without any relationship with each other.},
	address = {Berlin, Heidelberg},
	author = {Tettamanzi, Andrea and Tomassini, Marco},
	booktitle = {Soft Computing: Integrating Evolutionary, Neural, and Fuzzy Systems},
	date-added = {2018-12-18 15:56:40 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	doi = {10.1007/978-3-662-04335-6_4},
	isbn = {978-3-662-04335-6},
	keywords = {Evolutionary, Neural Network, Books},
	pages = {123--159},
	publisher = {Springer Berlin Heidelberg},
	title = {Evolutionary Design of Artificial Neural Networks},
	url = {https://doi.org/10.1007/978-3-662-04335-6_4},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-662-04335-6_4}}

@article{Jha2018,
	abstract = {This paper proposes a novel approach for the evolution of artificial creatures which moves in a 3D virtual environment based on the neuroevolution of augmenting topologies (NEAT) algorithm. The NEAT algorithm is used to evolve neural networks that observe the virtual environment and respond to it, by controlling the muscle force of the creature. The genetic algorithm is used to emerge the architecture of creature based on the distance metrics for fitness evaluation. The damaged morphologies of creature are elaborated, and a crossover algorithm is used to control it. Creatures with similar morphological traits are grouped into the same species to limit the complexity of the search space. The motion of virtual creature having 2--3 limbs is recorded at three different angles to check their performance in different types of viscous mediums. The qualitative demonstration of motion of virtual creature represents that improved swimming of virtual creatures is achieved in simulating mediums with viscous drag 1--10 arbitrary unit.},
	author = {Jha, Sunil Kr. and Josheski, Filip},
	date-added = {2018-12-17 20:49:27 +1300},
	date-modified = {2018-12-17 20:49:47 +1300},
	day = {01},
	doi = {10.1007/s00521-016-2664-2},
	issn = {1433-3058},
	journal = {Neural Computing and Applications},
	keywords = {Evolutionary, NEAT, Neuroevolution,},
	month = {Jun},
	number = {12},
	pages = {1337--1347},
	title = {Artificial evolution using neuroevolution of augmenting topologies (NEAT) for kinetics study in diverse viscous mediums},
	url = {https://doi.org/10.1007/s00521-016-2664-2},
	volume = {29},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00521-016-2664-2}}

@inproceedings{10.1007/978-3-642-01129-0_27,
	abstract = {Neuro-evolution of augmenting topologies (NEAT) is a recently developed neuro-evolutionary algorithm. This study uses NEAT to evolve dynamic trading agents for the German Bond Futures Market. High frequency data for three German Bond Futures is used to train and test the agents. Four fitness functions are tested and their out of sample performance is presented. The results suggest the methodology can outperform a random agent. However, while some structure was found in the data, the agents fail to yield positive returns when realistic transaction costs are included. A number of avenues of future work are indicated.},
	address = {Berlin, Heidelberg},
	author = {Bradley, Robert and Brabazon, Anthony and O'Neill, Michael},
	booktitle = {Applications of Evolutionary Computing},
	date-added = {2018-12-17 19:33:30 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {Giacobini, Mario and Brabazon, Anthony and Cagnoni, Stefano and Di Caro, Gianni A. and Ek{\'a}rt, Anik{\'o} and Esparcia-Alc{\'a}zar, Anna Isabel and Farooq, Muddassar and Fink, Andreas and Machado, Penousal},
	isbn = {978-3-642-01129-0},
	keywords = {Evolutionary, Neuroevolution, Finance, HFT, Books},
	pages = {233--242},
	publisher = {Springer Berlin Heidelberg},
	title = {Dynamic High Frequency Trading: A Neuro-Evolutionary Approach},
	year = {2009}}

@inbook{Castellano2007,
	abstract = {In recent years, the use of hybrid Soft Computing methods has shown that in various applications the synergism of several techniques is superior to a single technique. For example, the use of a neural fuzzy system and an evolutionary fuzzy system hybridises the approximate reasoning mechanism of fuzzy systems with the learning capabilities of neural networks and evolutionary algorithms. Evolutionary neural systems hybridise the neurocomputing approach with the solution-searching ability of evolutionary computing. Such hybrid methodologies retain limitations that can be overcome with full integration of the three basic Soft Computing paradigms, and this leads to evolutionary neural fuzzy systems. The objective of this chapter is to provide an account of hybrid Soft Computing systems, with special attention to the combined use of evolutionary algorithms and neural networks in order to endow fuzzy systems with learning and adaptive capabilities. After an introduction to basic Soft Computing paradigms, the various forms of hybridisation are considered, which results in evolutionary neural fuzzy systems. The chapter also describes a particular approach that jointly uses neural learning and genetic optimisation to learn a fuzzy model from the given data and to optimise it for accuracy and interpretability.},
	address = {Berlin, Heidelberg},
	author = {Castellano, G. and Castiello, C. and Fanelli, A. M. and Jain, L.},
	booktitle = {Advances in Evolutionary Computing for System Design},
	date-added = {2018-12-17 18:43:55 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	doi = {10.1007/978-3-540-72377-6_2},
	editor = {Jain, Lakhmi C. and Palade, Vasile and Srinivasan, Dipti},
	isbn = {978-3-540-72377-6},
	keywords = {Evolutionary, Neuroevolution, Fuzzy systems, Books},
	pages = {11--45},
	publisher = {Springer Berlin Heidelberg},
	title = {Evolutionary Neuro-Fuzzy Systems and Applications},
	url = {https://doi.org/10.1007/978-3-540-72377-6_2},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-540-72377-6_2}}

@article{Yalcin2018,
	abstract = {This study introduces a derivative of the well-known optimization algorithm, Big Bang--Big Crunch (BB--BC), named Nuclear Fission--Nuclear Fusion-based BB--BC, simply referred to as N2F. Broadly preferred in the engineering optimization community, BB--BC provides accurate solutions with reasonably fast convergence rates for many engineering problems. Regardless, the algorithm often suffers from stagnation issues. More specifically, for some problems, BB--BC either converges prematurely or exploits the promising regions inefficiently, both of which prevent obtaining the optimal solution. To overcome such problems, N2F algorithm is proposed, inspired by two major phenomena of nuclear physics: fission and fusion reactions. In N2F, two concepts named ``Nuclear Fission'' and ``Nuclear Fusion'' are introduced, replacing the ``Big Bang'' and ``Big Crunch'' phases of BB--BC, respectively. With the ``Nuclear Fission'' phase represented through a parameter named amplification factor, premature convergence issues are eliminated to a great extent. Meanwhile, convergence rate and exploitation capability of the algorithm are enhanced largely through a precision control parameter named magnification factor, in the ``Nuclear Fusion'' phase. The performance of N2F algorithm is investigated through unconstrained test functions and compared with the conventional BB--BC and other metaheuristics including genetic algorithm, Particle Swarm Optimization (PSO), Artificial Bee Colony Optimization (ABC), Drone Squadron Optimization (DSO) and Salp Swarm Algorithm (SSA). Then, further analyses are performed with constrained design benchmarks, validating the applicability of N2F to engineering problems. With superior statistical performance compared to BB--BC, GA, PSO, ABC, DSO and SSA in unconstrained problems and improved results with respect to the literature studies, N2F is proven to be an efficient and robust optimization algorithm.},
	author = {Yalcin, Yagizer and Pekcan, Onur},
	date-added = {2018-12-17 11:38:17 +1300},
	date-modified = {2018-12-17 11:38:34 +1300},
	day = {15},
	doi = {10.1007/s00521-018-3907-1},
	issn = {1433-3058},
	journal = {Neural Computing and Applications},
	keywords = {Other, N2F, global optimization},
	month = {Dec},
	title = {Nuclear Fission--Nuclear Fusion algorithm for global optimization: a modified Big Bang--Big Crunch algorithm},
	url = {https://doi.org/10.1007/s00521-018-3907-1},
	year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBPLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRE5OL1Vuc3VwZXJ2aXNlZCBsZWFybmluZyBieSBjb21wZXRpbmcgaGlkZGVuIHVuaXRzLmJpYk8RAgQAAAAAAgQAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9VbnN1cGVydmlzZWQgbGVhcm4jRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADRE5OAAACAGUvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RE5OOlVuc3VwZXJ2aXNlZCBsZWFybmluZyBieSBjb21wZXRpbmcgaGlkZGVuIHVuaXRzLmJpYgAADgBoADMAVQBuAHMAdQBwAGUAcgB2AGkAcwBlAGQAIABsAGUAYQByAG4AaQBuAGcAIABiAHkAIABjAG8AbQBwAGUAdABpAG4AZwAgAGgAaQBkAGQAZQBuACAAdQBuAGkAdABzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBjVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9ETk4vVW5zdXBlcnZpc2VkIGxlYXJuaW5nIGJ5IGNvbXBldGluZyBoaWRkZW4gdW5pdHMuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAB2AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAn4=},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00521-018-3907-1}}

@article{Mittal2018,
	abstract = {Deep convolutional neural networks (CNNs) have recently shown very high accuracy in a wide range of cognitive tasks, and due to this, they have received significant interest from the researchers. Given the high computational demands of CNNs, custom hardware accelerators are vital for boosting their performance. The high energy efficiency, computing capabilities and reconfigurability of FPGA make it a promising platform for hardware acceleration of CNNs. In this paper, we present a survey of techniques for implementing and optimizing CNN algorithms on FPGA. We organize the works in several categories to bring out their similarities and differences. This paper is expected to be useful for researchers in the area of artificial intelligence, hardware architecture and system design.},
	author = {Mittal, Sparsh},
	date-added = {2018-12-17 00:07:23 +1300},
	date-modified = {2018-12-17 00:07:52 +1300},
	day = {06},
	doi = {10.1007/s00521-018-3761-1},
	issn = {1433-3058},
	journal = {Neural Computing and Applications},
	keywords = {FPGA-NN, Binary neural network, Batch processing},
	month = {Oct},
	title = {A survey of FPGA-based accelerators for convolutional neural networks},
	url = {https://doi.org/10.1007/s00521-018-3761-1},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00521-018-3761-1}}

@inproceedings{8056820,
	abstract = {Convolutional Neural Networks (CNNs) can achieve high classification accuracy while they require complex computation. Binarized Neural Networks (BNNs) with binarized weights and activations can simplify computation but suffer from obvious accuracy loss. In this paper, low bit-width CNNs, BNNs and standard CNNs are compared to show that low bit-width CNNs is better suited for embedded systems. An architecture based on the two-stage arithmetic unit (TSAU) as the basic processing element is proposed to process each layer iteratively for low bit-width CNN accelerators. Then the DoReFa-Net which is trained with weights and activations represented in 1 bit and 2 bits respectively is implemented on Zynq XC7Z020 FPGA with a 410.2 GOPS performance. The accelerator can meet the real-time requirement of embedded applications with a 106 FPS throughput and a 73.1% top-5 accuracy on the ImageNet dataset. The accelerator outperforms existing FPGA-based CNN accelerators in the tradeoff among accuracy, energy and resource efficiency.},
	author = {L. Jiao and C. Luo and W. Cao and X. Zhou and L. Wang},
	booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
	date-added = {2018-12-16 23:41:56 +1300},
	date-modified = {2018-12-16 23:42:05 +1300},
	doi = {10.23919/FPL.2017.8056820},
	issn = {1946-1488},
	keywords = {BNN; digital arithmetic;embedded systems;field programmable gate arrays;neural nets;low bit-width CNN accelerators;Zynq XC7Z020 FPGA;DoReFa-Net neural nets;two-stage arithmetic unit;embedded systems;embedded systems;binarized activations;binarized weights;Binarized Neural Networks;complex computation;high classification accuracy;embedded FPGA;low bit-width convolutional neural networks;Kernel;Field programmable gate arrays;Table lookup;Neural networks;Computational modeling;Quantization (signal);Convolution},
	month = {Sept},
	pages = {1-4},
	title = {Accelerating low bit-width convolutional neural networks with embedded FPGA},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/FPL.2017.8056820}}

@inproceedings{8297303,
	abstract = {Deep convolutional neural networks (CNNs) are widely used in many computer vision tasks. Since CNNs involve billions of computations, it is critical to reduce the resource /power consumption and improve parallelism. Compared with extensive researches on fixed point conversion for cost reduction, floating point customization has not been paid enough attention due to its higher cost than fixed point. This paper explores the customized floating point for both the training and inference of CNNs. 9-bit customized floating point is found sufficient for the training of ResNet-20 on CIFAR-10 dataset with less than 1% accuracy loss, which can also be applied to the inference of CNNs. With reduced bit-width, a computational unit (CU) based on Quad-Multiplier Packing is proposed to improve the resource efficiency of CNNs on FPGA. This design can save 87.5% DSP slices and 62.5% LUTs on Xilinx Kintex-7 platform compared to CU using 32-bit floating point. More CUs can be arranged on FPGA and higher throughput can be expected accordingly.},
	author = {Z. Zhang and D. Zhou and S. Wang and S. Kimura},
	booktitle = {2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)},
	date-added = {2018-12-16 22:57:17 +1300},
	date-modified = {2018-12-16 22:57:28 +1300},
	doi = {10.1109/ASPDAC.2018.8297303},
	issn = {2153-697X},
	keywords = {CNN-FPGA, computer vision;field programmable gate arrays;neural nets;fixed point conversion;point customization;reduced bit-width;computational unit;FPGA;32-bit floating point;Quad-multiplier packing;computer vision tasks;resource /power consumption;deep convolutional neural networks;CNNs;9-bit customized floating point;ResNet-20;Xilinx Kintex-7 platform;Training;Field programmable gate arrays;Kernel;Hardware;Convolutional neural networks;Computer vision},
	month = {Jan},
	pages = {184-189},
	title = {Quad-multiplier packing based on customized floating point for convolutional neural networks on FPGA},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASPDAC.2018.8297303}}

@inproceedings{Zhao:2018:HRA:3220162.3220178,
	acmid = {3220178},
	address = {New York, NY, USA},
	author = {Zhao, Boya and Li, Jingqun and Pan, Hongli and Wang, Mingjiang},
	booktitle = {Proceedings of the 3rd International Conference on Multimedia Systems and Signal Processing},
	date-added = {2018-12-16 22:51:49 +1300},
	date-modified = {2018-12-16 22:51:59 +1300},
	doi = {10.1145/3220162.3220178},
	isbn = {978-1-4503-6457-7},
	keywords = {FPGA-NN; ASIC, AlexNet, CNN, GoogleNet, ResNet, accelerator, convolutional neural network},
	location = {Shenzhen, China},
	numpages = {6},
	pages = {150--155},
	publisher = {ACM},
	series = {ICMSSP '18},
	title = {A High-Performance Reconfigurable Accelerator for Convolutional Neural Networks},
	url = {http://doi.acm.org/10.1145/3220162.3220178},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3220162.3220178},
	Bdsk-Url-2 = {https://doi.org/10.1145/3220162.3220178}}

@article{Blott:2018:FRE:3299999.3242897,
	acmid = {3242897},
	address = {New York, NY, USA},
	articleno = {16},
	author = {Blott, Michaela and Preusser, Thomas B. and Fraser, Nicholas J. and Gambardella, Giulio and O'brien, Kenneth and Umuroglu, Yaman and Leeser, Miriam and Vissers, Kees},
	date-added = {2018-12-16 22:41:43 +1300},
	date-modified = {2019-04-28 15:11:33 +1200},
	doi = {10.1145/3242897},
	issn = {1936-7406},
	issue_date = {December 2018},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	keywords = {BNN; FINN, FPGA, Neural network, artificial intelligence, convolutional neural networks, hardware accellerator, inference, quantized neural networks},
	month = dec,
	number = {3},
	numpages = {23},
	pages = {16:1--16:23},
	publisher = {ACM},
	title = {FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks},
	url = {http://doi.acm.org/10.1145/3242897},
	volume = {11},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3242897},
	Bdsk-Url-2 = {https://doi.org/10.1145/3242897}}

@article{MapFPGADSP2015,
	author = {Bajaj, Ronak and Fahmy, Suhaib},
	date-added = {2018-12-16 15:13:50 +1300},
	date-modified = {2018-12-16 15:14:20 +1300},
	doi = {10.1109/TCAD.2015.2474363},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {FPGA, DSP},
	month = {01},
	pages = {1-1},
	title = {Mapping for maximum performance on FPGA DSP blocks},
	volume = {35},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCAD.2015.2474363}}

@article{Esser11441,
	abstract = {Brain-inspired computing seeks to develop new technologies that solve real-world problems while remaining grounded in the physical requirements of energy, speed, and size. Meeting these challenges requires high-performing algorithms that are capable of running on efficient hardware. Here, we adapt deep convolutional neural networks, which are today{\textquoteright}s state-of-the-art approach for machine perception in many domains, to perform classification tasks on neuromorphic hardware, which is today{\textquoteright}s most efficient platform for running neural networks. Using our approach, we demonstrate near state-of-the-art accuracy on eight datasets, while running at between 1,200 and 2,600 frames/s and using between 25 and 275 mW.Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that (i) approach state-of-the-art classification accuracy across eight standard datasets encompassing vision and speech, (ii) perform inference while preserving the hardware{\textquoteright}s underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1,200 and 2,600 frames/s and using between 25 and 275 mW (effectively \&gt;6,000 frames/s per Watt), and (iii) can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. This approach allows the algorithmic power of deep learning to be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
	author = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Andreopoulos, Alexander and Berg, David J. and McKinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and di Nolfo, Carmelo and Datta, Pallab and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
	date-added = {2018-12-16 12:32:34 +1300},
	date-modified = {2018-12-16 12:32:55 +1300},
	doi = {10.1073/pnas.1604850113},
	eprint = {https://www.pnas.org/content/113/41/11441.full.pdf},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences},
	keywords = {CNN; ASIC, TrueNorth, Neuromorphic},
	number = {41},
	pages = {11441--11446},
	publisher = {National Academy of Sciences},
	title = {Convolutional networks for fast, energy-efficient neuromorphic computing},
	url = {https://www.pnas.org/content/113/41/11441},
	volume = {113},
	year = {2016},
	Bdsk-Url-1 = {https://www.pnas.org/content/113/41/11441},
	Bdsk-Url-2 = {https://doi.org/10.1073/pnas.1604850113}}

@inproceedings{8436912,
	abstract = {Scalability, distributivity, interoperability and modularity introduced in cloud computing have deeply changed the legacy data center's architecture, implementation and processing capabilities. The atomic network services offered by cloud architectures are called microservices. Unlike virtual machines, microservices can be implemented in the form of low resources footprint applications as containers (Docker, LXC etc.) or even smaller as unikernels (IncludeOS, ClickOS, Rumprun, HermitOS etc.). The need to efficiently offload the processing of computation-intensive applications has motivated the introduction of Field Programmable Gate Arrays (FPGA) boards in servers. FPGAs can nowadays be considered as cloud-standard processing resources. However, in today's cloud data centers, FPGAs cannot be accessed to run concurrent microservices. This severely limits the efficient deployment of microservices. This paper aims at introducing an FPGA-based system for the concurrent acceleration of cloud-native microservices onto FPGAs.},
	author = {J. Lallet and A. Enrici and A. Saffar},
	booktitle = {2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
	date-added = {2018-12-16 11:58:55 +1300},
	date-modified = {2018-12-16 11:59:07 +1300},
	doi = {10.1109/BMSB.2018.8436912},
	issn = {2155-5052},
	keywords = {FPGA, cloud computing;computer centres;field programmable gate arrays;open systems;FPGA-based system;concurrent acceleration;cloud-native microservices;cloud microservices;interoperability;modularity;cloud computing;legacy data center;atomic network services;cloud architectures;virtual machines;low resources footprint applications;computation-intensive applications;Field Programmable Gate Arrays boards;cloud-standard processing resources;cloud data centers;concurrent microservices;Field programmable gate arrays;Acceleration;Cloud computing;Servers;Containers;Computer architecture;FPGA;Cloud;Microservice;Acceleration},
	month = {June},
	pages = {1-5},
	title = {FPGA-Based System for the Acceleration of Cloud Microservices},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/BMSB.2018.8436912}}

@inproceedings{8443248,
	abstract = {Network Functions Virtualization paradigm has emerged as a new concept in networking which aims at cost reduction and ease of network scalability by leveraging on virtualization technologies and commercial-off-the-shelf hardware to disintegrate the software implementation of network functions from the underlying hardware. Recently, lightweight virtualization techniques have emerged as efficient alternatives to traditional Virtual Network Functions (VNFs) developed as VMs. At the same time ARMv8 servers are gaining traction in the server world, mostly because of their interesting performance per watt characteristics. In this paper, the CPU, memory and Input/Output (I/O) performance of such lightweight techniques are compared with that of classic virtual machines on both x86 and ARMv8 platforms. More in particular, we selected KVM as hypervisor solution, Docker and rkt as container engines and finally Rumprun and OSv as unikernels. On x86, our results for CPU and memory related workloads highlight a slightly better performance for containers and unikernels whereas both of them perform almost twice as better as KVM for network I/O operations. This highlights performance issues of the Linux tap bridge with KVM but that can easily be overcome by using a user space virtual switch such as VOSYSwitch and OVS/DPDK. On ARM, both KVM and containers produce similar results for CPU and memory workloads, but have an exception for network I/O operations where KVM proves to be the fastest. We also showcase the several shortcomings of unikernels on ARM which account for their lack of stable support for this architecture.},
	author = {A. Acharya and J. Fangu{\`e}de and M. Paolino and D. Raho},
	booktitle = {2018 European Conference on Networks and Communications (EuCNC)},
	date-added = {2018-12-16 11:52:27 +1300},
	date-modified = {2018-12-16 11:52:36 +1300},
	doi = {10.1109/EuCNC.2018.8443248},
	issn = {2575-4912},
	keywords = {Other; Containers;Virtualization;Virtual machine monitors;Benchmark testing;Hardware;Kernel;Linux},
	month = {June},
	pages = {282-9},
	title = {A Performance Benchmarking Analysis of Hypervisors Containers and Unikernels on ARMv8 and x86 CPUs},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/EuCNC.2018.8443248}}

@inproceedings{8567366,
	abstract = {Unikernels are a relatively recent way to create and quickly deploy extremely small virtual machines that do not require as much functional and operational software overhead as containers or virtual machines by leaving out unnecessary parts. This paradigm aims to replace bulky virtual machines on one hand, and to open up new classes of hardware for virtualization and networking applications on the other. In recent years, the tool chains used to create unikernels have grown from proof of concept to platforms that can run both new and existing software written in various programming languages. This paper studies the performance (both execution time and memory footprint) of unikernels versus Docker containers in the context of REST services and heavy processing workloads, written in Java, Go, and Python. With the results of the performance evaluations, predictions can be made about which cases could benefit from the use of unikernels over containers.},
	author = {T. Goethals and M. Sebrechts and A. Atrey and B. Volckaert and F. De Turck},
	booktitle = {2018 IEEE 8th International Symposium on Cloud and Service Computing (SC2)},
	date-added = {2018-12-16 11:51:43 +1300},
	date-modified = {2018-12-16 11:52:05 +1300},
	doi = {10.1109/SC2.2018.00008},
	keywords = {Other, Containers;Virtual machining;Unikernel;Java;Python;Virtual machine monitors;containers;microservices;virtualization;IoT},
	month = {Nov},
	pages = {1-8},
	title = {Unikernels vs Containers: An In-Depth Benchmarking Study in the Context of Microservice Applications},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/SC2.2018.00008}}

@inproceedings{Ioffe:2015:BNA:3045118.3045167,
	acmid = {3045167},
	author = {Ioffe, Sergey and Szegedy, Christian},
	booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
	date-added = {2018-12-16 10:36:12 +1300},
	date-modified = {2018-12-16 19:09:08 +1300},
	keywords = {NN, batch normalization operation, Google},
	location = {Lille, France},
	numpages = {9},
	pages = {448--456},
	publisher = {JMLR.org},
	series = {ICML'15},
	title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
	year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3045118.3045167}}

@inproceedings{He:2015:DDR:2919332.2919814,
	acmid = {2919814},
	address = {Washington, DC, USA},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
	date-added = {2018-12-16 10:32:16 +1300},
	date-modified = {2018-12-16 10:32:45 +1300},
	doi = {10.1109/ICCV.2015.123},
	isbn = {978-1-4673-8391-2},
	keywords = {NN; ReLu, ImageNet, Microsoft},
	numpages = {9},
	pages = {1026--1034},
	publisher = {IEEE Computer Society},
	series = {ICCV '15},
	title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
	url = {http://dx.doi.org/10.1109/ICCV.2015.123},
	year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCV.2015.123}}

@inproceedings{7528127,
	abstract = {The Fully Homomorphic Encryption (FHE) is an encryption technique for processing encrypted data without the need of decrypting them. This method is suitable for use in untrusted environments, such as cloud computing platforms. Various methods have been proposed to implement this technique. But the greatest problem of these methods is that for its operation, there is the need to generate public keys with large sizes, whose immediate consequence is to cause such encryption schemes not reach the desired runtime performance. This article aims to optimize public keys compression techniques using Genetic Algorithms (GA) for parameter calibration of Coron test variants primitives in order to speed up the execution time of each of these primitives.},
	author = {J. Gavinho Filho and G. P. Silva and C. Miceli},
	booktitle = {2016 19th International Conference on Information Fusion (FUSION)},
	date-added = {2018-12-16 10:20:53 +1300},
	date-modified = {2018-12-16 10:21:04 +1300},
	keywords = {Evolutionary, genetic algorithms;public key cryptography;fully homomorphic encryption;genetic algorithms;FHE;encrypted data;cloud computing platforms;public keys compression;GA;parameter calibration;Coron test variants primitives;Encryption;Public key;Genetic algorithms;Proposals;Sociology;Fully Homomorphic Encryption;Compression;Security;Genetic Algorithm},
	month = {July},
	pages = {1991-1998},
	title = {A public key compression method for Fully Homomorphic Encryption using Genetic Algorithms},
	year = {2016}}

@article{10.3389/fnins.2017.00496,
	abstract = {Artificial neural networks (ANNs) trained using backpropagation are powerful learning architectures that have achieved state-of-the-art performance in various benchmarks. Significant effort has been devoted to developing custom silicon devices to accelerate inference in ANNs. Accelerating the training phase, however, has attracted relatively little attention. In this paper, we describe a hardware-efficient on-line learning technique for feedforward multi-layer ANNs that is based on pipelined backpropagation. Learning is performed in parallel with inference in the forward pass, removing the need for an explicit backward pass and requiring no extra weight lookup. By using binary state variables in the feedforward network and ternary errors in truncated-error backpropagation, the need for any multiplications in the forward and backward passes is removed, and memory requirements for the pipelining are drastically reduced.  Further reduction in addition operations owing to the sparsity in the forward neural and backpropagating error signal paths contributes to highly efficient hardware implementation.  For proof-of-concept validation, we demonstrate on-line learning of MNIST handwritten digit classification on a Spartan 6 FPGA interfacing with an external 1Gb DDR2 DRAM, that shows small degradation in test error performance compared to an equivalently sized binary ANN trained off-line using standard back-propagation and exact errors. Our results highlight an attractive synergy between pipelined backpropagation and binary-state networks in substantially reducing computation and memory requirements, making pipelined on-line learning practical in deep networks.},
	author = {Mostafa, Hesham and Pedroni, Bruno and Sheik, Sadique and Cauwenberghs, Gert},
	date-added = {2018-12-16 00:39:15 +1300},
	date-modified = {2018-12-16 00:39:33 +1300},
	doi = {10.3389/fnins.2017.00496},
	issn = {1662-453X},
	journal = {Frontiers in Neuroscience},
	keywords = {BNN, FPGA, Back propagation, Quantization},
	pages = {496},
	title = {Hardware-Efficient On-line Learning through Pipelined Truncated-Error Backpropagation in Binary-State Networks},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2017.00496},
	volume = {11},
	year = {2017},
	Bdsk-Url-1 = {https://www.frontiersin.org/article/10.3389/fnins.2017.00496},
	Bdsk-Url-2 = {https://doi.org/10.3389/fnins.2017.00496}}

@article{Shackleford2001,
	abstract = {Accelerating a genetic algorithm (GA) by implementing it in a reconfigurable field programmable gate array (FPGA) is described. The implemented GA features: random parent selection, which conserves selection circuitry; a steady-state memory model, which conserves chip area; survival of fitter child chromosomes over their less-fit parent chromosomes, which promotes evolution. A net child chromosome generation rate of one per clock cycle is obtained by pipelining the parent selection, crossover, mutation, and fitness evaluation functions. Complex fitness functions can be further pipelined to maintain a high-speed clock cycle. Fitness functions with a pipeline initiation interval of greater than one can be plurally implemented to maintain a net evaluated-chromosome throughput of one per clock cycle. Two prototypes are described: The first prototype (c. 1996 technology) is a multiple-FPGA chip implementation, running at a 1 MHz clock rate, that solves a 94-row {\texttimes} 520-column set covering problem 2,200{\texttimes} faster than a 100 MHz workstation running the same algorithm in C. The second prototype (Xilinx XVC300) is a single-FPGA chip implementation, running at a 66 MHZ clock rate, that solves a 36-residue protein folding problem in a 2-d lattice 320{\texttimes} faster than a 366 MHz Pentium II. The current largest FPGA (Xilinx XCV3200E) has circuitry available for the implementation of 30 fitness function units which would yield an acceleration of 9,600{\texttimes} for the 36-residue protein folding problem.},
	author = {Shackleford, Barry and Snider, Greg and Carter, Richard J. and Okushi, Etsuko and Yasuda, Mitsuhiro and Seo, Katsuhiko and Yasuura, Hiroto},
	date-added = {2018-12-16 00:23:25 +1300},
	date-modified = {2018-12-16 00:23:40 +1300},
	day = {01},
	doi = {10.1023/A:1010018632078},
	issn = {1573-7632},
	journal = {Genetic Programming and Evolvable Machines},
	keywords = {FPGA, GA, Pipeline, HPC},
	month = {Mar},
	number = {1},
	pages = {33--60},
	title = {A High-Performance, Pipelined, FPGA-Based Genetic Algorithm Machine},
	url = {https://doi.org/10.1023/A:1010018632078},
	volume = {2},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1010018632078}}

@article{Guo:2016:PGA:2927964.2927980,
	acmid = {2927980},
	address = {New York, NY, USA},
	author = {Guo, Liucheng and Funie, Andreea Ingrid and Thomas, David B. and Fu, Haohuan and Luk, Wayne},
	date-added = {2018-12-16 00:15:15 +1300},
	date-modified = {2018-12-16 00:15:36 +1300},
	doi = {10.1145/2927964.2927980},
	issn = {0163-5964},
	issue_date = {September 2015},
	journal = {SIGARCH Comput. Archit. News},
	keywords = {FPGA, Parallel, Traveling Salesman Problem},
	month = apr,
	number = {4},
	numpages = {8},
	pages = {86--93},
	publisher = {ACM},
	title = {Parallel Genetic Algorithms on Multiple FPGAs},
	url = {http://doi.acm.org/10.1145/2927964.2927980},
	volume = {43},
	year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2927964.2927980},
	Bdsk-Url-2 = {https://doi.org/10.1145/2927964.2927980}}

@inproceedings{1321812,
	abstract = {This paper presents the research work directed regards the synthesis and implementation of a parallel-pipelined hardware genetic algorithm (PPHGA) utilizing very high speed integrated circuit hardware description language (VHDL) for programming field programmable gate arrays (FPGAs). The main design is divided into several modules. The modules are autonomous in operation once the system starts to run. They communicate with each other using a handshaking protocol. Three applications are then experimented using the PPHGA to test its optimization power. These are linear interpolation, thermistor data processing, and vehicle acceleration computation.},
	author = {H. E. Mostafa and A. I. Khadragi and Y. Y. Hanafi},
	booktitle = {Proceedings of the Twenty-First National Radio Science Conference, 2004. NRSC 2004.},
	date-added = {2018-12-16 00:13:44 +1300},
	date-modified = {2018-12-16 00:13:58 +1300},
	doi = {10.1109/NRSC.2004.240504},
	keywords = {FPGA; parallel programming;pipeline processing;program testing;hardware description languages;logic testing;genetic algorithms;logic programming;logic design;interpolation;thermistors;protocols;parallel-pipelined hardware genetic algorithm;very high speed integrated circuit hardware description language;VHDL;field programmable gate array;gate arrays programming;FPGA design;handshaking protocol;program testing;optimization power;linear interpolation;thermistor data processing;vehicle acceleration computation;lateral acceleration;Genetic algorithms;Field programmable gate arrays;Integrated circuit synthesis;Very high speed integrated circuits;Hardware design languages;Parallel programming;Protocols;Circuit testing;Interpolation;Thermistors},
	month = {March},
	pages = {C9-1},
	title = {Hardware implementation of genetic algorithm on FPGA},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/NRSC.2004.240504}}

@inproceedings{inproceedings,
	author = {Deliparaschos, Kyriakos and Tzafestas, Spyros},
	booktitle = {Panhellenic Conference on Electronics and Telecommunications},
	date-added = {2018-12-15 23:47:47 +1300},
	date-modified = {2018-12-15 23:53:06 +1300},
	doi = {10.13140/2.1.1543.1689},
	keywords = {FPGA, GA, Fuzzy logic, ASIC, Traveling Salesman Problem},
	month = {September},
	title = {Design Paradigms of Intelligent Control Systems on a Chip},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.13140/2.1.1543.1689}}

@article{Torquato2018HighPerformancePI,
	author = {Matheus F. Torquato and Marcelo A. C. Fernandes},
	date-added = {2018-12-15 23:21:11 +1300},
	date-modified = {2018-12-15 23:21:32 +1300},
	journal = {CoRR},
	keywords = {FPGA, GA, parallel computing, HPC},
	title = {High-Performance Parallel Implementation of Genetic Algorithm on FPGA},
	volume = {abs/1806.11555},
	year = {2018}}

@article{PhysRevLett.119.208301,
	author = {Farkhooi, Farzad and Stannat, Wilhelm},
	date-added = {2018-12-15 22:47:16 +1300},
	date-modified = {2018-12-15 22:47:38 +1300},
	doi = {10.1103/PhysRevLett.119.208301},
	issue = {20},
	journal = {Phys. Rev. Lett.},
	keywords = {BNN, Mean field theory, Recurrent NN},
	month = {Nov},
	numpages = {5},
	pages = {208301},
	publisher = {American Physical Society},
	title = {Complete Mean-Field Theory for Dynamics of Binary Recurrent Networks},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.119.208301},
	volume = {119},
	year = {2017},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.119.208301},
	Bdsk-Url-2 = {https://doi.org/10.1103/PhysRevLett.119.208301}}

@article{2015arXiv150602078K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://beta.openreview.net/forum?id=71BmK0m6qfAE8VvKUQWB},
	author = {{Karpathy}, A. and {Johnson}, J. and {Fei-Fei}, L.},
	date-added = {2018-12-15 22:23:49 +1300},
	date-modified = {2018-12-15 22:28:23 +1300},
	eprint = {1506.02078},
	journal = {International Conference on Learning Representations},
	keywords = {CNN; Machine Learning, Computation and Language, Neural and Evolutionary Computing},
	month = May,
	title = {{Visualizing and Understanding Recurrent Networks}},
	year = 2016}

@inproceedings{1809.11086,
	author = {Arash Ardakani, Sean C. Smithson, Warren J. Gross, Zhengyun Ji, Brett H. Meyer},
	booktitle = {International Conference on Learning Representations},
	date-added = {2018-12-15 18:36:35 +1300},
	date-modified = {2018-12-15 18:59:39 +1300},
	keywords = {BNN, Recurrent NN, Text analysis,},
	note = {under review},
	organization = {McGill University},
	title = {Learning Recurrent Binary/Ternary Weights},
	url = {https://openreview.net/forum?id=HkNGYjR9FX},
	year = {2019},
	Bdsk-Url-1 = {https://openreview.net/forum?id=HkNGYjR9FX}}

@article{Hesamifard2017CryptoDLDN,
	author = {Ehsan Hesamifard and Hassan Takabi and Mehdi Ghasemi},
	date-added = {2018-12-15 16:57:47 +1300},
	date-modified = {2018-12-15 16:58:11 +1300},
	journal = {CoRR},
	keywords = {NN, Cryptography, Homomorphic encryption, MNIST},
	title = {CryptoDL: Deep Neural Networks over Encrypted Data},
	volume = {abs/1711.05189},
	year = {2017}}

@inproceedings{pmlr-v48-gilad-bachrach16,
	abstract = {Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.},
	address = {New York, New York, USA},
	author = {Ran Gilad-Bachrach and Nathan Dowlin and Kim Laine and Kristin Lauter and Michael Naehrig and John Wernsing},
	booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	date-added = {2018-12-15 16:14:29 +1300},
	date-modified = {2019-01-11 15:27:42 +1300},
	editor = {Maria Florina Balcan and Kilian Q. Weinberger},
	keywords = {NN; Cryptography, Neural Network, Homomorphic encryption, Cryptonet},
	month = {20--22 Jun},
	pages = {201--210},
	pdf = {http://proceedings.mlr.press/v48/gilad-bachrach16.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy},
	url = {http://proceedings.mlr.press/v48/gilad-bachrach16.html},
	volume = {48},
	year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/gilad-bachrach16.html}}

@article{PractBNN,
	author = {Austin, Jim},
	date-added = {2018-12-14 20:34:29 +1300},
	date-modified = {2018-12-16 15:14:48 +1300},
	journal = {Adaptive computing and Information processing conference},
	keywords = {BNN, ADAM, Associative Memory, Image Segmentation},
	month = {07},
	title = {The Practical Application of Binary Neural Networks.},
	year = {1994}}

@article{CUCS-017-15,
	author = {Townsend, Richard Morse; Kim, Martha Allen; Edwards, Stephen A.},
	date-added = {2018-12-14 12:56:57 +1300},
	date-modified = {2018-12-14 12:58:42 +1300},
	journal = {Columbia University Computer Science Technical Reports},
	keywords = {Haskell-FPGA, Memory, Computer programs},
	title = {Hardware in Haskell: Implementing Memories in a Stream-Based World},
	year = {2015}}

@phdthesis{AronssonHWSWCoDesignThesis,
	abstract = {Developing software for embedded systems presents quite the challenge---not only do these systems demand good knowledge of the hardware they run on, but their limited resources also make it difficult to achieve efficiency. For embedded systems with different kinds of processing elements, the challenge is even greater; the presence of heterogeneous elements both raises all of the issues associated with homogeneous systems, and may also cause non-uniform system development and capability.

In this thesis we explore a functional approach to heterogeneous system development, with a staged hardware software co-design language embedded in Haskell, to address many of the modularity problems typically found in such systems. This staged approach enables designers to build their applications from reusable components and skeletons, while retaining control over much of the generated source code. Design exploration also benefits from the functional approach, since Haskell's type classes can be used to ensure that certain operations will be available. As a result, a developer can not only write for hardware and software in the co-design language, but she can also write generic programs that are suitable for both.

Internally, the co-design language is based on a monadic representation of imperative programs that abstracts away from its underlying statement, expression, and predicate types by establishing an interface to their respective interpreters. Programs are thus loosely coupled to their underlying types, giving a clear separation of concerns. The compilation process is expressed as a series of translations between progressively smaller typed languages, which safeguards against many common errors.

In addition to the hardware software co-design language, this thesis also introduces a language for expressing digital signal processing algorithms, using a model of synchronous data-flow that is embedded in Haskell. The language supports definitions in a functional style, reducing the gap between an algorithm's mathematical specification and its implementation. A vector language is also presented, which builds on a functional representation that guarantees fusion for arrays. Both of these languages are intended to be extensions of the co-design language, but neither one is dependent on it and can thus be used to extend other languages as well.},
	author = {Markus Aronsson},
	date-added = {2018-12-14 12:38:48 +1300},
	date-modified = {2018-12-14 12:41:47 +1300},
	keywords = {Haskell-FPGA, Thesis, Hardware, Software, Co-design},
	month = {October},
	school = {Chalmers, Computer Science and Engineering, Functional Programming},
	title = {A Functional Approach to Hardware Software Co-Design},
	type = {Licentiate thesis},
	year = {2018}}

@inproceedings{10.1007/978-3-540-85373-2_8,
	abstract = {For the memory intensive task of graph reduction, modern PCs are limited not by processor speed, but by the rate that data can travel between processor and memory. This limitation is known as the von Neumann bottleneck. We explore the effect of widening this bottleneck using a special-purpose graph reduction machine with wide, parallel memories. Our prototype machine -- the Reduceron -- is implemented using an FPGA, and is based on a simple template-instantiation evaluator. Running at only 91.5MHz on an FPGA, the Reduceron is faster than mature bytecode implementations of Haskell running on a 2.8GHz PC.},
	address = {Berlin, Heidelberg},
	author = {Naylor, Matthew and Runciman, Colin},
	booktitle = {Implementation and Application of Functional Languages},
	date-added = {2018-12-14 11:56:31 +1300},
	date-modified = {2018-12-14 11:56:55 +1300},
	editor = {Chitil, Olaf and Horv{\'a}th, Zolt{\'a}n and Zs{\'o}k, Vikt{\'o}ria},
	isbn = {978-3-540-85373-2},
	keywords = {Haskel-FPGA, Graph reduction, Parallel memory},
	pages = {129--146},
	publisher = {Springer Berlin Heidelberg},
	title = {The Reduceron: Widening the von Neumann Bottleneck for Graph Reduction Using an FPGA},
	year = {2008}}

@inproceedings{Aronsson:2017:HSC:3122955.3122970,
	acmid = {3122970},
	address = {New York, NY, USA},
	author = {Aronsson, Markus and Sheeran, Mary},
	booktitle = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell},
	date-added = {2018-12-14 11:42:12 +1300},
	date-modified = {2018-12-14 11:42:22 +1300},
	doi = {10.1145/3122955.3122970},
	isbn = {978-1-4503-5182-9},
	keywords = {Haskel-FPGA, domain specific language, hardware software co-design},
	location = {Oxford, UK},
	numpages = {12},
	pages = {162--173},
	publisher = {ACM},
	series = {Haskell 2017},
	title = {Hardware Software Co-design in Haskell},
	url = {http://doi.acm.org/10.1145/3122955.3122970},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3122955.3122970},
	Bdsk-Url-2 = {https://doi.org/10.1145/3122955.3122970}}

@inproceedings{Zhai:2015:HSR:2830840.2830850,
	acmid = {2830850},
	address = {Piscataway, NJ, USA},
	author = {Zhai, Kuangya and Townsend, Richard and Lairmore, Lianne and Kim, Martha A. and Edwards, Stephen A.},
	booktitle = {Proceedings of the 10th International Conference on Hardware/Software Codesign and System Synthesis},
	date-added = {2018-12-14 11:23:43 +1300},
	date-modified = {2018-12-14 11:23:52 +1300},
	isbn = {978-1-4673-8321-9},
	keywords = {Haskell-FPGA, functional hardware, high-level synthesis, recursion},
	location = {Amsterdam, The Netherlands},
	numpages = {11},
	pages = {83--93},
	publisher = {IEEE Press},
	series = {CODES '15},
	title = {Hardware Synthesis from a Recursive Functional Language},
	url = {http://dl.acm.org/citation.cfm?id=2830840.2830850},
	year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2830840.2830850}}

@book{Yu2008,
	abstract = {Deploying Evolutionary Computation (EC) solutions to real-world problems involves a wide spectrum of activities, ranging from framing the business problems and implementing the solutions to the final deployment of the solutions to the field. However, issues related to these activities are not commonly discussed in a typical EC course curriculum. Meanwhile, although the values of applied research are acknowledged by most EC technologists, the perception seems to be very narrow: success stories boost morale and high profile applications can help to secure funding for future research and can help to attract high caliber students.},
	address = {Berlin, Heidelberg},
	author = {Yu, Tina and Davis, Lawrence},
	booktitle = {Evolutionary Computation in Practice},
	date-added = {2018-12-14 11:01:07 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	doi = {10.1007/978-3-540-75771-9_1},
	editor = {Yu, Tina and Davis, Lawrence and Baydar, Cem and Roy, Rajkumar},
	isbn = {978-3-540-75771-9},
	keywords = {Evolutionary, Books},
	pages = {1--8},
	publisher = {Springer Berlin Heidelberg},
	title = {Evolutionary Computation in Practice},
	url = {https://doi.org/10.1007/978-3-540-75771-9_1},
	year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-540-75771-9_1}}

@inproceedings{Brown00agenetic,
	author = {Deryck Brown and A. Beatriz and Beatriz Garmendia-Doval and John A.W. McCall and The Robert},
	booktitle = {In Proceedings of 2nd Asia-Pacific Workshop on Genetic Algorithms and their Applications (APGA 2000},
	date-added = {2018-12-14 10:40:55 +1300},
	date-modified = {2018-12-14 10:42:03 +1300},
	keywords = {Haskell, Genetic Algorithm,},
	pages = {152--163},
	publisher = {Global-Link Publishing Company},
	title = {A genetic algorithm framework using Haskell},
	year = {2000}}

@inproceedings{Wang:2016:AUG:2976002.2976009,
	acmid = {2976009},
	address = {New York, NY, USA},
	author = {Wang, Yisu Remy and Nunez, Diogenes and Fisher, Kathleen},
	booktitle = {Proceedings of the 9th International Symposium on Haskell},
	date-added = {2018-12-14 10:34:30 +1300},
	date-modified = {2018-12-14 10:34:30 +1300},
	doi = {10.1145/2976002.2976009},
	isbn = {978-1-4503-4434-0},
	keywords = {Haskell, genetic algorithms, laziness, strictness annotations},
	location = {Nara, Japan},
	numpages = {13},
	pages = {114--126},
	publisher = {ACM},
	series = {Haskell 2016},
	title = {Autobahn: Using Genetic Algorithms to Infer Strictness Annotations},
	url = {http://doi.acm.org/10.1145/2976002.2976009},
	year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2976002.2976009},
	Bdsk-Url-2 = {https://doi.org/10.1145/2976002.2976009}}

@article{2018arXiv181012894B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181012894B},
	archiveprefix = {arXiv},
	author = {{Burda}, Y. and {Edwards}, H. and {Storkey}, A. and {Klimov}, O.},
	date-added = {2018-12-13 22:12:36 +1300},
	date-modified = {2018-12-13 22:12:46 +1300},
	eprint = {1810.12894},
	journal = {arXiv e-prints},
	keywords = {DCGAN, Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Uber},
	month = oct,
	title = {{Exploration by Random Network Distillation}},
	year = 2018}

@article{2018arXiv180600175K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180600175K},
	archiveprefix = {arXiv},
	author = {{Keramati}, R. and {Whang}, J. and {Cho}, P. and {Brunskill}, E.},
	date-added = {2018-12-13 22:10:44 +1300},
	date-modified = {2018-12-13 22:11:14 +1300},
	eprint = {1806.00175},
	journal = {arXiv e-prints},
	keywords = {DCGAN, Artificial Intelligence, Uber},
	month = may,
	primaryclass = {cs.AI},
	title = {{Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning}},
	year = 2018}

@article{2018arXiv181111357T,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181111357T},
	archiveprefix = {arXiv},
	author = {{Turner}, R. and {Hung}, J. and {Saatci}, Y. and {Yosinski}, J.},
	date-added = {2018-12-13 21:54:57 +1300},
	date-modified = {2018-12-13 21:55:25 +1300},
	eprint = {1811.11357},
	journal = {arXiv e-prints},
	keywords = {DCGAN, Statistics, Machine Learning, Machine Learning, Uber},
	month = nov,
	primaryclass = {stat.ML},
	title = {{Metropolis-Hastings Generative Adversarial Networks}},
	year = 2018}

@book{Salvaris2018Book,
	abstract = {This chapter discusses some of the trends in deep learning and related fields. We cover specifically which trends might be useful for what tasks as well as discuss some of the methods and ideas that could have far-reaching implications but have yet to be applied to many real-world problems. We finish by covering briefly some of the current limitations of deep learning as well as some other areas of AI that seem to hold promise for future AI applications, and discuss briefly some of the ethical and legal implications of deep learning applications.},
	address = {Berkeley, CA},
	author = {Salvaris, Mathew and Dean, Danielle and Tok, Wee Hyong},
	booktitle = {Deep Learning with Azure: Building and Deploying Artificial Intelligence Solutions on the Microsoft AI Platform},
	date-added = {2018-12-12 22:25:37 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	doi = {10.1007/978-1-4842-3679-6_3},
	isbn = {978-1-4842-3679-6},
	keywords = {Evolutionary, Books, Neuroevolution},
	publisher = {Apress},
	title = {Trends in Deep Learning},
	url = {https://doi.org/10.1007/978-1-4842-3679-6_3},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4842-3679-6_3}}

@book{Sher2013NeuroEvol,
	abstract = {This chapter discusses the numerous reasons for why one might wish to study the subject of neuroevolution. I cover a number of different applications of such a system, giving examples and scenarios of a neuroevolutionary system being applied within a variety of different fields. A discussion then follows on where all of this research is heading, and what the next step within this field might be. Finally, a whirlwind introduction of the book is given, with a short summary of what is covered in every chapter.},
	address = {New York, NY},
	author = {Sher, Gene I.},
	booktitle = {Handbook of Neuroevolution Through Erlang},
	date-added = {2018-12-12 19:57:45 +1300},
	date-modified = {2018-12-12 19:59:29 +1300},
	doi = {10.1007/978-1-4614-4463-3_1},
	isbn = {978-1-4614-4463-3},
	keywords = {Evolutionary, Neuroevolution, Erlang, Finance},
	pages = {1--39},
	publisher = {Springer New York},
	title = {Handbook of Neuroevolution Through Erlang},
	url = {https://doi.org/10.1007/978-1-4614-4463-3_1},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4614-4463-3_1}}

@inproceedings{10.1007/978-3-642-33860-1_3,
	abstract = {The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), has now been around for over 20 years. The main appeal of this approach is that, because it does not rely on gradient information (e.g. backpropagation), it can potentially harness the universal function approximation capability of neural networks to solve reinforcement learning (RL) tasks, where there is no ``teacher'' (i.e. no targets or examples of correct behavior). Instead of incrementally adjusting the synaptic weights of a single network, the space of network parameters is searched directly according to principles inspired by natural selection: (1) encode a population of networks as strings, or genomes, (2) transform them into networks, (3) evaluate them on the task, (4) generate new, hopefully better, nets by recombining those that are most ``fit'', (5) goto step 2 until a solution is found. By evolving neural networks, NE can cope naturally with tasks that have continuous inputs and outputs, and, by evolving networks with feedback connections (recurrent networks), it can tackle more general tasks that require memory.},
	address = {Berlin, Heidelberg},
	author = {Gomez, Faustino},
	booktitle = {Theory and Practice of Natural Computing},
	date-added = {2018-12-12 18:32:49 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {Dediu, Adrian-Horia and Mart{\'\i}n-Vide, Carlos and Truthe, Bianca},
	isbn = {978-3-642-33860-1},
	keywords = {Evolutionary, Books, Scalable, Reinforcement Learning},
	pages = {27--29},
	publisher = {Springer Berlin Heidelberg},
	title = {Scalable Neuroevolution for Reinforcement Learning},
	year = {2012}}

@inproceedings{10.1007/11893295_130,
	abstract = {This paper addresses the problem of accelerating large artificial neural networks (ANN), whose topology and weights can evolve via the use of a genetic algorithm. The proposed digital hardware architecture is capable of processing any evolved network topology, whilst at the same time providing a good trade off between throughput, area and power consumption. The latter is vital for a longer battery life on mobile devices. The architecture uses multiple parallel arithmetic units in each processing element (PE). Memory partitioning and data caching are used to minimise the effects of PE pipeline stalling. A first order minimax polynomial approximation scheme, tuned via a genetic algorithm, is used for the activation function generator. Efficient arithmetic circuitry, which leverages modified Booth recoding, column compressors and carry save adders, is adopted throughout the design.},
	address = {Berlin, Heidelberg},
	author = {Larkin, Daniel and Kinane, Andrew and O'Connor, Noel},
	booktitle = {Neural Information Processing},
	date-added = {2018-12-12 18:30:15 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	editor = {King, Irwin and Wang, Jun and Chan, Lai-Wan and Wang, DeLiang},
	isbn = {978-3-540-46485-3},
	keywords = {Evolutionary, Books, Neuroevolution, Processing Element},
	pages = {1178--1188},
	publisher = {Springer Berlin Heidelberg},
	title = {Towards Hardware Acceleration of Neuroevolution for Multimedia Processing Applications on Mobile Devices},
	year = {2006}}

@article{7307180,
	abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyze the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The paper also highlights important open research challenges in the field.},
	author = {S. Risi and J. Togelius},
	date-added = {2018-12-12 16:15:08 +1300},
	date-modified = {2018-12-12 16:15:18 +1300},
	doi = {10.1109/TCIAIG.2015.2494596},
	issn = {1943-068X},
	journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	keywords = {evolutionary; computer games;learning (artificial intelligence);neural nets;neuroevolution;NE;computer game;artificial neural network training;evolutionary algorithm;Games;Artificial intelligence;Evolutionary computation;Genetic algorithms;Biological neural networks;Network topology;Evolutionary algorithms;neural networks;neuroevolution},
	month = {March},
	number = {1},
	pages = {25-41},
	title = {Neuroevolution in Games: State of the Art and Open Challenges},
	volume = {9},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCIAIG.2015.2494596}}

@inproceedings{4250190,
	abstract = {Appropriate topology and connection weight are two very important properties a neural network must have in order to successfully perform data classification. In this paper, we propose a hybrid training scheme Learning-NEAT (L-NEAT) for data classification problem. L-NEAT simplifies evolution by dividing the complete problem domain into sub tasks and learn the sub tasks by incorporating back propagation rule into the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. The new algorithm combines the strength of searching for topology and weights from NEAT and back propagation respectively while overcoming problems associated with direct use of NEAT. We claim that L-NEAT can produce neural network for classification problem effectively and efficiently. Empirical evaluation shows that L-NEAT evolves classifying neural network with good generalization ability. Its accuracy outperforms original NEAT.},
	author = {L. Chen and D. Alahakoon},
	booktitle = {2006 International Conference on Information and Automation},
	date-added = {2018-12-12 16:06:11 +1300},
	date-modified = {2018-12-12 16:06:25 +1300},
	doi = {10.1109/ICINFA.2006.374100},
	issn = {2151-1802},
	keywords = {Evolutionary; backpropagation;neural nets;pattern classification;search problems;topology;neuroevolution of augmenting topologies;augmenting topology;data classification learning;neural network;learning-NEAT training scheme;backpropagation;search problem;Network topology;Artificial neural networks;Neural networks;Supervised learning;Biological cells;Information technology;Evolutionary computation;Unsupervised learning;Technological innovation},
	month = {Dec},
	pages = {367-371},
	title = {NeuroEvolution of Augmenting Topologies with Learning for Data Classification},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICINFA.2006.374100}}

@inproceedings{7991745,
	abstract = {The article describes the problematic issues of neuroevolution, i.e. a promising approach for solving complex problems of machine learning, neural networks, adaptive management and multi-agent systems, evolutionary robotics, gaming strategies, and computer art. The authors have suggested neuro evolutional algorithm and presented experiment results on a standard task: the balancing trolley with two flagpoles of different lengths.},
	author = {S. Rodzin and O. Rodzina and L. Rodzina},
	booktitle = {2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)},
	date-added = {2018-12-12 13:20:17 +1300},
	date-modified = {2018-12-12 13:20:28 +1300},
	doi = {10.1109/ICAICT.2016.7991745},
	issn = {2472-8586},
	keywords = {Evolutionary;learning (artificial intelligence);multi-agent systems;neural nets;machine learning;neural networks;adaptive management;multiagent systems;evolutionary robotics;gaming strategies;computer art;neuro evolutional algorithm;balancing trolley;Biological neural networks;Neurons;Encoding;Sociology;Statistics;Topology;Head;Neuroevolution;reinforcement machine learning;optimization;evolutionary computation;fitness function},
	month = {Oct},
	pages = {1-4},
	title = {Neuroevolution: Problems, algorithms, and experiments},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICAICT.2016.7991745}}

@inproceedings{8489184,
	abstract = {Memetic algorithms have been a promising strategy to enhance neuroevolution in the past. Cooperative coevolution has been combined as memetic cooperative neuroevolution with application to chaotic time series prediction. Although the method has shown promising performance, there are limitations in the balance between global and local search. The previous study used a specific local search strategy for intensification that affected the diversity of solutions. In this study, we address this limitation by information (meme) collection strategies that maintains and refines a pool of memes during global search. We present two strategies where one is sequential and the other is concurrent meme collection implemented at different stages of evolution. In the majority of the given problems, the proposed strategies showed improvement in prediction accuracy over the related methods.},
	author = {G. Wong and A. Sharma and R. Chandra},
	booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2018-12-12 13:05:44 +1300},
	date-modified = {2018-12-12 13:05:58 +1300},
	doi = {10.1109/IJCNN.2018.8489184},
	issn = {2161-4407},
	keywords = {Evolutionary;search problems;social sciences;time series;memetic algorithms;neuroevolution;coevolution;chaotic time series prediction;global search;specific local search strategy;information collection strategies;concurrent meme collection;prediction accuracy;Memetics;Time series analysis;Neurons;Prediction algorithms;Sociology;Cooperative Coevolution;Memetic Algorithms;Time Series Prediction Global Search;Local Search;Neuroevolution},
	month = {July},
	pages = {1-6},
	title = {Information Collection Strategies In Memetic Cooperative Neuroevolution For Time Series Prediction},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2018.8489184}}

@inproceedings{Han:2016:EEI:3001136.3001163,
	acmid = {3001163},
	address = {Piscataway, NJ, USA},
	author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
	booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
	date-added = {2018-12-11 11:22:33 +1300},
	date-modified = {2018-12-11 11:22:42 +1300},
	doi = {10.1109/ISCA.2016.30},
	isbn = {978-1-4673-8947-1},
	keywords = {FPGA-NN, ASIC, algorithm-hardware co-design, deep learning, hardware acceleration, model compression},
	location = {Seoul, Republic of Korea},
	numpages = {12},
	pages = {243--254},
	publisher = {IEEE Press},
	series = {ISCA '16},
	title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
	url = {https://doi.org/10.1109/ISCA.2016.30},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCA.2016.30}}

@inproceedings{shayer2018learning,
	author = {Oran Shayer and Dan Levi and Ethan Fetaya},
	booktitle = {International Conference on Learning Representations},
	date-added = {2018-12-09 16:39:21 +1300},
	date-modified = {2018-12-09 16:39:50 +1300},
	keywords = {BNN, Discrete, MNIST, CIFAR, ImageNet},
	title = {Learning Discrete Weights Using the Local Reparameterization Trick},
	url = {https://openreview.net/forum?id=BySRH6CpW},
	year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=BySRH6CpW}}

@inproceedings{wu2018training,
	author = {Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},
	booktitle = {International Conference on Learning Representations},
	date-added = {2018-12-09 16:29:45 +1300},
	date-modified = {2018-12-09 17:10:02 +1300},
	keywords = {BNN; Binary, MNIST, CIFAR, ImageNet},
	month = {Feb},
	title = {Training and Inference with Integers in Deep Neural Networks},
	url = {https://openreview.net/forum?id=HJGXzmspb},
	year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCNLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0xpbWl0ZWQgZXZhbHVhdGlvbiBjb29wZXJhdGl2ZSBjby1ldm9sdXRpb25hcnkgZGlmZmVyZW50aWFsIGV2b2x1dGlvbiBmb3IgbGFyZ2Utc2NhbGUgbmV1cm9ldm9sdXRpb24uYmliTxEC8gAAAAAC8gACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0xpbWl0ZWQgZXZhbHVhdGlvbiNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAxFdm9sdXRpb25hcnkAAgCjLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkV2b2x1dGlvbmFyeTpMaW1pdGVkIGV2YWx1YXRpb24gY29vcGVyYXRpdmUgY28tZXZvbHV0aW9uYXJ5IGRpZmZlcmVudGlhbCBldm9sdXRpb24gZm9yIGxhcmdlLXNjYWxlIG5ldXJvZXZvbHV0aW9uLmJpYgAADgDSAGgATABpAG0AaQB0AGUAZAAgAGUAdgBhAGwAdQBhAHQAaQBvAG4AIABjAG8AbwBwAGUAcgBhAHQAaQB2AGUAIABjAG8ALQBlAHYAbwBsAHUAdABpAG8AbgBhAHIAeQAgAGQAaQBmAGYAZQByAGUAbgB0AGkAYQBsACAAZQB2AG8AbAB1AHQAaQBvAG4AIABmAG8AcgAgAGwAYQByAGcAZQAtAHMAYwBhAGwAZQAgAG4AZQB1AHIAbwBlAHYAbwBsAHUAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAoVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0xpbWl0ZWQgZXZhbHVhdGlvbiBjb29wZXJhdGl2ZSBjby1ldm9sdXRpb25hcnkgZGlmZmVyZW50aWFsIGV2b2x1dGlvbiBmb3IgbGFyZ2Utc2NhbGUgbmV1cm9ldm9sdXRpb24uYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAC0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA6o=},
	Bdsk-Url-1 = {https://openreview.net/forum?id=HJGXzmspb}}

@inbook{Moussa2006,
	abstract = {Artificial Neural Networks (ANNs) are inherently parallel architectures which represent a natural fit for custom implementation on FPGAs. One important implementation issue is to determine the numerical precision format that allows an optimum tradeoff between precision and implementation areas. Standard single or double precision floating-point representations minimize quantization errors while requiring significant hardware resources. Less precise fixed-point representation may require less hardware resources but add quantization errors that may prevent learning from taking place, especially in regression problems. This chapter examines this issue and reports on a recent experiment where we implemented a Multi-layer perceptron (MLP) on an FPGA using both fixed and floating point precision. Results show that the fixed-point MLP implementation was over 12x greater in speed, over 13x smaller in area, and achieves far greater processing density compared to the floating-point FPGA-based MLP.},
	address = {Boston, MA},
	author = {Moussa, Medhat and Areibi, Shawki and Nichols, Kristian},
	booktitle = {FPGA Implementations of Neural Networks},
	date-added = {2018-12-06 14:20:11 +1300},
	date-modified = {2018-12-06 14:20:23 +1300},
	doi = {10.1007/0-387-28487-7_2},
	editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	isbn = {978-0-387-28487-3},
	keywords = {FPGA-NN, Book Chapter},
	pages = {37--61},
	publisher = {Springer US},
	title = {On the Arithmetic Precision for Implementing Back-Propagation Networks on FPGA: A Case Study},
	url = {https://doi.org/10.1007/0-387-28487-7_2},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_2}}

@inbook{Paul2006,
	abstract = {Back propagation is a well known technique used in the implementation of artificial neural networks. The algorithm can be described essentially as a sequence of matrix vector multiplications and outer product operations interspersed with the application of a point wise non linear function. The algorithm is compute intensive and lends itself to a high degree of parallelism. These features motivate a systolic design of hardware to implement the Back Propagation algorithm. We present in this chapter a new systolic architecture for the complete back propagation algorithm. For a neural network with N input neurons, P hidden layer neurons and M output neurons, the proposed architecture with P processors, has a running time of (2N + 2M + P + max(M,P)) for each training set vector. This is the first such implementation of the back propagation algorithm which completely parallelizes the entire computation of learning phase. The array has been implemented on an Annapolis FPGA based coprocessor and it achieves very favorable performance with range of 5 GOPS. The proposed new design targets Virtex boards.},
	address = {Boston, MA},
	author = {Paul, Kolin and Rajopadhye, Sanjay},
	booktitle = {FPGA Implementations of Neural Networks},
	date-added = {2018-12-06 14:17:29 +1300},
	date-modified = {2018-12-06 14:17:42 +1300},
	doi = {10.1007/0-387-28487-7_5},
	editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	isbn = {978-0-387-28487-3},
	keywords = {FPGA-NN, Book Chapter},
	pages = {137--165},
	publisher = {Springer US},
	title = {Back-Propagation Algorithm Achieving 5 Gops on the Virtex-E},
	url = {https://doi.org/10.1007/0-387-28487-7_5},
	year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBYLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYk8RAiIAAAAAAiIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IUEMgb24gdGhlIEludGVsIFgjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGQ3J5cHRvAAIAbi86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDcnlwdG86SFBDIG9uIHRoZSBJbnRlbCBYZW9uIFBoaS0gSG9tb21vcnBoaWMgV29yZCBTZWFyY2hpbmcuYmliAA4AdAA5AEgAUABDACAAbwBuACAAdABoAGUAIABJAG4AdABlAGwAIABYAGUAbwBuACAAUABoAGkALQAgAEgAbwBtAG8AbQBvAHIAcABoAGkAYwAgAFcAbwByAGQAIABTAGUAYQByAGMAaABpAG4AZwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAbFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAB/AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAqU=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBYLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYk8RAiIAAAAAAiIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IUEMgb24gdGhlIEludGVsIFgjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGQ3J5cHRvAAIAbi86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDcnlwdG86SFBDIG9uIHRoZSBJbnRlbCBYZW9uIFBoaS0gSG9tb21vcnBoaWMgV29yZCBTZWFyY2hpbmcuYmliAA4AdAA5AEgAUABDACAAbwBuACAAdABoAGUAIABJAG4AdABlAGwAIABYAGUAbwBuACAAUABoAGkALQAgAEgAbwBtAG8AbQBvAHIAcABoAGkAYwAgAFcAbwByAGQAIABTAGUAYQByAGMAaABpAG4AZwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAbFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAB/AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAqU=},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_5}}

@inbook{Omondi2006,
	abstract = {This introductory chapter reviews the basics of artificial-neural-network theory, discusses various aspects of the hardware implementation of neural networks (in both ASIC and FPGA technologies, with a focus on special features of artificial neural networks), and concludes with a brief note on performance-evaluation. Special points are the exploitation of the parallelism inherent in neural networks and the appropriate implementation of arithmetic functions, especially the sigmoid function. With respect to the sigmoid function, the chapter includes a significant contribution.},
	address = {Boston, MA},
	author = {Omondi, Amos R. and Rajapakse, Jagath C. and Bajger, Mariusz},
	booktitle = {FPGA Implementations of Neural Networks},
	date-added = {2018-12-06 14:08:39 +1300},
	date-modified = {2018-12-06 14:17:53 +1300},
	doi = {10.1007/0-387-28487-7_1},
	editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	isbn = {978-0-387-28487-3},
	keywords = {FPGA-NN, NN, Neurocomputers, Book Chapter},
	pages = {1--36},
	publisher = {Springer US},
	title = {FPGA Neurocomputers},
	url = {https://doi.org/10.1007/0-387-28487-7_1},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_1}}

@inproceedings{758889,
	abstract = {A comparison between a bit-level and a conventional VLSI implementation of a binary neural network is presented. This network is based on Correlation Matrix Memory (CMM) that stores relationships between pairs of binary vectors. The bit-level architecture consists of an n/spl times/m array of bit-level processors holding the storage and computation elements. The conventional CMM architecture consists of a RAM memory holding the CMM storage and an array of counters. Since we are interested in the VLSI implementation of such networks, hardware complexities and speeds of both bit-level and conventional architecture were compared by using VLSI tools. It is shown that a significant speedup is achieved by using the bit-level architecture since the speed of this last configuration is not limited by the memory addressing delay. Moreover, the bit-level architecture is very simple and reduces the bus/routing, making the architecture suitable for VLSI implementation. The main drawback of such an approach compared to the conventional one is the demand for a high number of adders for dealing with a large number of inputs.},
	author = {A. Bermak and J. Austin},
	booktitle = {Proceedings of the Seventh International Conference on Microelectronics for Neural, Fuzzy and Bio-Inspired Systems},
	date-added = {2018-12-06 10:54:22 +1300},
	date-modified = {2018-12-06 10:54:32 +1300},
	doi = {10.1109/MN.1999.758889},
	keywords = {BNN; VLSI;neural chips;neural net architecture;CMOS digital integrated circuits;VLSI implementation;binary neural network;bit-level architecture;conventional architecture;correlation matrix memory;bit-level processors;RAM memory;counter array;hardware complexities;speed comparison;memory addressing delay;adders;VLSI digital design;Very large scale integration;Neural networks;Coordinate measuring machines;Computer architecture;Random access memory;Read-write memory;Counting circuits;Hardware;Added delay;Routing},
	month = {April},
	pages = {374-379},
	title = {VLSI implementation of a binary neural network-two case studies},
	year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1109/MN.1999.758889}}

@electronic{xnor.ai-webpage,
	author = {Ali Farhadi; Mohammad Rastegari},
	date-added = {2018-12-06 10:26:37 +1300},
	date-modified = {2018-12-06 11:06:21 +1300},
	keywords = {BNN, XNOR, Yolo},
	lastchecked = {6/Dec/2018},
	month = {Dec},
	title = {xnor.ai Company},
	url = {https://www.xnor.ai/},
	urldate = {2018},
	year = {2016},
	Bdsk-Url-1 = {https://www.xnor.ai/}}

@inproceedings{8416941,
	author = {H. Yonekawa and S. Sato and H. Nakahara},
	booktitle = {2018 IEEE 48th International Symposium on Multiple-Valued Logic (ISMVL)},
	date-added = {2018-12-05 21:41:24 +1300},
	date-modified = {2018-12-05 22:14:52 +1300},
	doi = {10.1109/ISMVL.2018.00038},
	issn = {2378-2226},
	keywords = {BNN; Neurons;Training;Two dimensional displays;Embedded systems;Character recognition;Computational modeling;Convolutional neural networks},
	month = {May},
	pages = {174-179},
	title = {A Ternary Weight Binary Input Convolutional Neural Network: Realization on the Embedded Processor},
	url = {doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038},
	volume = {00},
	year = {2018},
	Bdsk-Url-1 = {doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038},
	Bdsk-Url-2 = {https://doi.org/10.1109/ISMVL.2018.00038}}

@inproceedings{Faraone_2018_CVPR,
	author = {Faraone, Julian and Fraser, Nicholas and Blott, Michaela and Leong, Philip H.W.},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-12-04 23:43:14 +1300},
	date-modified = {2018-12-05 16:06:33 +1300},
	keywords = {BNN; Quantization, FPGA, LUT},
	month = {June},
	title = {SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks},
	year = {2018}}

@inproceedings{Zhou_2018_CVPR,
	author = {Zhou, Aojun and Yao, Anbang and Wang, Kuan and Chen, Yurong},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-12-04 23:14:06 +1300},
	date-modified = {2018-12-04 23:15:40 +1300},
	keywords = {BNN; Quantization, Regularization},
	month = {June},
	title = {Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks},
	year = {2018}}

@inproceedings{Wang_2018_CVPR,
	author = {Wang, Peisong and Hu, Qinghao and Zhang, Yifan and Zhang, Chunjie and Liu, Yang and Cheng, Jian},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-12-03 22:57:17 +1300},
	date-modified = {2018-12-03 22:57:41 +1300},
	keywords = {BNN; Quantization, Low-bit},
	month = {June},
	title = {Two-Step Quantization for Low-Bit Neural Networks},
	year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB2Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRE5OL09uIHRoZSBMZWFybmluZyBDYXBhYmlsaXRpZXMgb2YgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3Jrcy0gQSBDcnlwdG9ncmFwaGljIFBlcnNwZWN0aXZlLmJpYk8RAp4AAAAAAp4AAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9PbiB0aGUgTGVhcm5pbmcgQ2EjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADRE5OAAACAIwvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RE5OOk9uIHRoZSBMZWFybmluZyBDYXBhYmlsaXRpZXMgb2YgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3Jrcy0gQSBDcnlwdG9ncmFwaGljIFBlcnNwZWN0aXZlLmJpYgAOALYAWgBPAG4AIAB0AGgAZQAgAEwAZQBhAHIAbgBpAG4AZwAgAEMAYQBwAGEAYgBpAGwAaQB0AGkAZQBzACAAbwBmACAAUgBlAGMAdQByAHIAZQBuAHQAIABOAGUAdQByAGEAbAAgAE4AZQB0AHcAbwByAGsAcwAtACAAQQAgAEMAcgB5AHAAdABvAGcAcgBhAHAAaABpAGMAIABQAGUAcgBzAHAAZQBjAHQAaQB2AGUALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAIpVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ROTi9PbiB0aGUgTGVhcm5pbmcgQ2FwYWJpbGl0aWVzIG9mIFJlY3VycmVudCBOZXVyYWwgTmV0d29ya3MtIEEgQ3J5cHRvZ3JhcGhpYyBQZXJzcGVjdGl2ZS5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAnQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAM/}}

@inproceedings{Xie_2017_ICCV,
	author = {Xie, Lingxi and Yuille, Alan},
	booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	date-added = {2018-12-03 22:12:21 +1300},
	date-modified = {2018-12-03 22:12:58 +1300},
	keywords = {CNN, Genetic Algorithm, VGGNet, ResNet, DenseNet},
	month = {Oct},
	title = {Genetic CNN},
	year = {2017}}

@inproceedings{Sholomon_2013_ICCV_Workshops,
	author = {Sholomon, Dror and David, Omid and Netanyahu, Nathan S.},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-12-03 22:02:22 +1300},
	date-modified = {2018-12-03 22:03:08 +1300},
	keywords = {Evolutionary, Genetic Algorithm, Jigzaw, Puzzle},
	month = {June},
	title = {A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles},
	year = {2013},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB7Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvT3RoZXIvTkNPUi0gQW4gRlBHQS1GcmllbmRseSBOb25ibG9ja2luZyBEYXRhIENhY2hlIGZvciBTb2Z0IFByb2Nlc3NvcnMgd2l0aCBSdW5haGVhZCBFeGVjdXRpb24ucmlzTxECsgAAAAACsgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H05DT1ItIEFuIEZQR0EtRnJpZSNGRkZGRkZGRi5yaXMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAVPdGhlcgAAAgCRLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk90aGVyOk5DT1ItIEFuIEZQR0EtRnJpZW5kbHkgTm9uYmxvY2tpbmcgRGF0YSBDYWNoZSBmb3IgU29mdCBQcm9jZXNzb3JzIHdpdGggUnVuYWhlYWQgRXhlY3V0aW9uLnJpcwAADgC8AF0ATgBDAE8AUgAtACAAQQBuACAARgBQAEcAQQAtAEYAcgBpAGUAbgBkAGwAeQAgAE4AbwBuAGIAbABvAGMAawBpAG4AZwAgAEQAYQB0AGEAIABDAGEAYwBoAGUAIABmAG8AcgAgAFMAbwBmAHQAIABQAHIAbwBjAGUAcwBzAG8AcgBzACAAdwBpAHQAaAAgAFIAdQBuAGEAaABlAGEAZAAgAEUAeABlAGMAdQB0AGkAbwBuAC4AcgBpAHMADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCPVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9PdGhlci9OQ09SLSBBbiBGUEdBLUZyaWVuZGx5IE5vbmJsb2NraW5nIERhdGEgQ2FjaGUgZm9yIFNvZnQgUHJvY2Vzc29ycyB3aXRoIFJ1bmFoZWFkIEV4ZWN1dGlvbi5yaXMAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAKIAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADWA==}}

@article{2017arXiv171100205Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100205Z},
	archiveprefix = {arXiv},
	author = {{Zhuang}, B. and {Shen}, C. and {Tan}, M. and {Liu}, L. and {Reid}, I.},
	date-added = {2018-12-03 21:17:35 +1300},
	date-modified = {2018-12-03 21:21:48 +1300},
	eprint = {1711.00205},
	journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	keywords = {BNN; Computer Vision; Pattern Recognition},
	month = Jun,
	primaryclass = {cs.CV},
	title = {{Towards Effective Low-bitwidth Convolutional Neural Networks}},
	year = 2017}

@article{6134678,
	abstract = {This paper proposes a set of adaptive learning rules for binary feedforward neural networks (BFNNs) by means of the sensitivity measure that is established to investigate the effect of a BFNN's weight variation on its output. The rules are based on three basic adaptive learning principles: the benefit principle, the minimal disturbance principle, and the burden-sharing principle. In order to follow the benefit principle and the minimal disturbance principle, a neuron selection rule and a weight adaptation rule are developed. Besides, a learning control rule is developed to follow the burden-sharing principle. The advantage of the rules is that they can effectively guide the BFNN's learning to conduct constructive adaptations and avoid destructive ones. With these rules, a sensitivity-based adaptive learning (SBALR) algorithm for BFNNs is presented. Experimental results on a number of benchmark data demonstrate that the SBALR algorithm has better learning performance than the Madaline rule II and backpropagation algorithms.},
	author = {S. Zhong and X. Zeng and S. Wu and L. Han},
	date-added = {2018-12-02 16:32:30 +1300},
	date-modified = {2018-12-02 16:32:45 +1300},
	doi = {10.1109/TNNLS.2011.2177860},
	issn = {2162-237X},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {BNN; feedforward neural nets;learning (artificial intelligence);sensitivity analysis;sensitivity-based adaptive learning rules;binary feedforward neural network;sensitivity measurement;BFNN weight variation;adaptive learning principles;benefit principle;minimal disturbance principle;burden-sharing principle;neuron selection rule;weight adaptation rule;learning control rule;BFNN learning;benchmark data;SBALR algorithm;Madaline rule II;backpropagation algorithm;Neurons;Sensitivity;Training;Learning systems;Weight measurement;Biological neural networks;Feedforward neural networks;Adaptive learning algorithm;binary feedforward neural networks;learning rule;sensitivity;Artificial Intelligence;Databases, Factual;Neural Networks (Computer)},
	month = {March},
	number = {3},
	pages = {480-491},
	title = {Sensitivity-Based Adaptive Learning Rules for Binary Feedforward Neural Networks},
	volume = {23},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2011.2177860}}

@article{7878541,
	abstract = {Convolutional neural networks (CNNs) have revolutionized the world of computer vision over the last few years, pushing image classification beyond human accuracy. The computational effort of today's CNNs requires power-hungry parallel processors or GP-GPUs. Recent developments in CNN accelerators for system-on-chip integration have reduced energy consumption significantly. Unfortunately, even these highly optimized devices are above the power envelope imposed by mobile and deeply embedded applications and face hard limitations caused by CNN weight I/O and storage. This prevents the adoption of CNNs in future ultralow power Internet of Things end-nodes for near-sensor analytics. Recent algorithmic and theoretical advancements enable competitive classification accuracy even when limiting CNNs to binary (+1/-1) weights during training. These new findings bring major optimization opportunities in the arithmetic core by removing the need for expensive multiplications, as well as reducing I/O bandwidth and storage. In this paper, we present an accelerator optimized for binary-weight CNNs that achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate equivalent (MGE) or 1.9 mm<sup>2</sup>and with a power dissipation of 895 μW in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms the state-of-the-art in terms of energy and area efficiency achieving 61.2 TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.},
	author = {R. Andri and L. Cavigelli and D. Rossi and L. Benini},
	date-added = {2018-12-02 15:33:17 +1300},
	date-modified = {2018-12-02 15:33:26 +1300},
	doi = {10.1109/TCAD.2017.2682138},
	issn = {0278-0070},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {BNN; CMOS logic circuits;computer vision;convolution;coprocessors;embedded systems;image classification;integrated circuit design;low-power electronics;neural nets;optimisation;power aware computing;system-on-chip;I/O storage;I/O bandwidth;algorithmic advancements;binary weights;competitive classification accuracy;hard limitations;deeply embedded applications;mobile embedded applications;power envelope;energy consumption;system-on-chip integration;CNN accelerators;GP-GPUs;power-hungry parallel processors;computational effort;human accuracy;image classification;computer vision;convolutional neural networks;ultralow power binary-weight CNN acceleration;power dissipation;binary-weight CNNs;accelerator;optimization opportunities;ASIC;binary weights;convolutional neural networks (CNNs);hardware accelerator;Internet of Things (IoT)},
	month = {Jan},
	number = {1},
	pages = {48-60},
	title = {YodaNN: An Architecture for Ultralow Power Binary-Weight CNN Acceleration},
	volume = {37},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCAD.2017.2682138}}

@article{POSEWSKY2018151,
	abstract = {Deep neural networks are an extremely successful and widely used technique for various pattern recognition and machine learning tasks. Due to power and resource constraints, these computationally intensive networks are difficult to implement in embedded systems. Yet, the number of applications that can benefit from the mentioned possibilities is rapidly rising. In this paper, we propose novel architectures for the inference of previously learned and arbitrary deep neural networks on FPGA-based SoCs that are able to overcome these limitations. Our key contributions include the reuse of previously transferred weight matrices across multiple input samples, which we refer to as batch processing, and the usage of compressed weight matrices, also known as pruning. An extensive evaluation of these optimizations is presented. Both techniques allow a significant mitigation of data transfers and speed-up the network inference by one order of magnitude. At the same time, we surpass the data throughput of fully-featured x86-based systems while only using a fraction of their energy consumption.},
	author = {Thorbj{\"o}rn Posewsky and Daniel Ziener},
	date-added = {2018-12-02 13:07:55 +1300},
	date-modified = {2018-12-02 13:08:06 +1300},
	doi = {https://doi.org/10.1016/j.micpro.2018.04.004},
	issn = {0141-9331},
	journal = {Microprocessors and Microsystems},
	keywords = {FPGA-NN; Deep neural networks, Batch processing, Pruning, Compression, FPGA, Inference, Throughput optimizations, Fully-connected},
	pages = {151 - 161},
	title = {Throughput optimizations for FPGA-based deep neural network inference},
	url = {http://www.sciencedirect.com/science/article/pii/S014193311730296X},
	volume = {60},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S014193311730296X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.micpro.2018.04.004}}

@article{HAJDUK2018227,
	abstract = {This brief paper presents two implementations of feed-forward artificial neural networks in FPGAs. The implementations differ in the FPGA resources requirement and calculations speed. Both implementations exercise floating point arithmetic, apply very high accuracy activation function realization, and enable easy alteration of the neural network's structure without the need of a re-implementation of the entire FPGA project.},
	author = {Zbigniew Hajduk},
	date-added = {2018-12-02 12:40:03 +1300},
	date-modified = {2018-12-02 12:40:15 +1300},
	doi = {https://doi.org/10.1016/j.neucom.2018.04.077},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {FPGA-NN, FPGA, Neural networks},
	pages = {227 - 234},
	title = {Reconfigurable FPGA implementation of neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231218305393},
	volume = {308},
	year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBrLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0NIRVQtIENvbXBpbGVyIGFuZCBSdW50aW1lIGZvciBIb21vbW9ycGhpYyBFdmFsdWF0aW9uIG9mIFRlbnNvciBQcm9ncmFtcy5iaWJPEQJwAAAAAAJwAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fQ0hFVC0gQ29tcGlsZXIgYW5kI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABkNyeXB0bwACAIEvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Q3J5cHRvOkNIRVQtIENvbXBpbGVyIGFuZCBSdW50aW1lIGZvciBIb21vbW9ycGhpYyBFdmFsdWF0aW9uIG9mIFRlbnNvciBQcm9ncmFtcy5iaWIAAA4AmgBMAEMASABFAFQALQAgAEMAbwBtAHAAaQBsAGUAcgAgAGEAbgBkACAAUgB1AG4AdABpAG0AZQAgAGYAbwByACAASABvAG0AbwBtAG8AcgBwAGgAaQBjACAARQB2AGEAbAB1AGEAdABpAG8AbgAgAG8AZgAgAFQAZQBuAHMAbwByACAAUAByAG8AZwByAGEAbQBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgB/VXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DcnlwdG8vQ0hFVC0gQ29tcGlsZXIgYW5kIFJ1bnRpbWUgZm9yIEhvbW9tb3JwaGljIEV2YWx1YXRpb24gb2YgVGVuc29yIFByb2dyYW1zLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAkgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMG},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231218305393},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2018.04.077}}

@book{Omondi:2010:FIN:1941654,
	author = {Omondi, Amos R. and Rajapakse, Jagath C.},
	date-added = {2018-12-02 11:27:41 +1300},
	date-modified = {2018-12-02 11:28:42 +1300},
	edition = {1st},
	isbn = {1441939423, 9781441939425},
	keywords = {FPGA-NN, ASIC, Book by Publications},
	publisher = {Springer Publishing Company, Incorporated},
	title = {FPGA Implementations of Neural Networks},
	year = {2010}}

@inproceedings{Raina:2009:LDU:1553374.1553486,
	acmid = {1553486},
	address = {New York, NY, USA},
	author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	date-added = {2018-12-01 22:23:56 +1300},
	date-modified = {2018-12-01 22:24:24 +1300},
	doi = {10.1145/1553374.1553486},
	isbn = {978-1-60558-516-1},
	keywords = {NN; GPU, Neural Network},
	location = {Montreal, Quebec, Canada},
	numpages = {8},
	pages = {873--880},
	publisher = {ACM},
	series = {ICML '09},
	title = {Large-scale Deep Unsupervised Learning Using Graphics Processors},
	url = {http://doi.acm.org/10.1145/1553374.1553486},
	year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1553374.1553486},
	Bdsk-Url-2 = {https://doi.org/10.1145/1553374.1553486}}

@proceedings{2016arXiv160207261S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160207261S},
	archiveprefix = {arXiv},
	author = {{Szegedy}, C. and {Ioffe}, S. and {Vanhoucke}, V. and {Alemi}, A.},
	date-added = {2018-12-01 20:45:24 +1300},
	date-modified = {2018-12-01 20:47:18 +1300},
	eprint = {1602.07261},
	journal = {Prceedings of the 31st AAAI Conference on Artificial Intelligence},
	keywords = {CNN; Computer Vision; Pattern Recognition; Inception},
	month = feb,
	number = {31},
	organization = {AAAI},
	primaryclass = {cs.CV},
	publisher = {AAAI},
	title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
	volume = {31},
	year = 2017}

@article{2016arXiv160507678C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160507678C},
	annote = {https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
https://towardsdatascience.com/neural-network-architectures-156e5bad51ba
},
	archiveprefix = {arXiv},
	author = {{Canziani}, A. and {Paszke}, A. and {Culurciello}, E.},
	date-added = {2018-12-01 20:05:07 +1300},
	date-modified = {2018-12-01 23:10:43 +1300},
	eprint = {1605.07678},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = may,
	primaryclass = {cs.CV},
	title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
	year = 2016}

@inproceedings{8099726,
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
	author = {G. Huang and Z. Liu and L. v. d. Maaten and K. Q. Weinberger},
	booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-12-01 19:33:27 +1300},
	date-modified = {2018-12-01 19:33:50 +1300},
	doi = {10.1109/CVPR.2017.243},
	issn = {1063-6919},
	keywords = {CNN; convolution;feedforward neural nets;learning (artificial intelligence);DenseNet;traditional convolutional networks;feature propagation;feature reuse;object recognition benchmark tasks;dense convolutional network;vanishing-gradient problem;Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation; DenseNet},
	month = {July},
	pages = {2261-2269},
	title = {Densely Connected Convolutional Networks},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2017.243}}

@article{Simonyan14c,
	author = {Simonyan, K. and Zisserman, A.},
	date-added = {2018-12-01 13:32:36 +1300},
	date-modified = {2018-12-01 13:33:51 +1300},
	journal = {ILSVRC - CoRR},
	keywords = {CNN; image classification, VGG network},
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	volume = {abs/1409.1556},
	year = {2014}}

@electronic{AppleFaceDetectionURL,
	author = {Computer Vision Machine Learning Team},
	date-added = {2018-11-29 21:01:15 +1300},
	date-modified = {2020-07-20 18:48:48 +1200},
	keywords = {URL, NPU, Neural engine, Apple, Vision Framework, Face recognition},
	lastchecked = {29/Nov/2018},
	month = {11},
	title = {An On-device Deep Neural Network for Face Detection},
	url = {https://machinelearning.apple.com/2017/11/16/face-detection.html},
	urldate = {November 2017},
	year = {2017},
	Bdsk-Url-1 = {https://machinelearning.apple.com/2017/11/16/face-detection.html}}

@inproceedings{5524599,
	abstract = {Artificial neural networks play an important role in robot programming by demonstration. In this paper we present a method for artificial neural network training. The main idea of this method is to train the artificial neural network with all of the data, before the current training step, and at a certain step the network is already trained a huge number of times. Some features of the quality of neural network training, using this method, were presented in. Because the method uses all of the data before the current training step, in this paper, we are concerned about training time and computing time comportment of the neural network. A software application for obtaining training time based on the number of training steps was designed. This software application implements the training method on an unidirectional multi-layer neural network and prints into a graph the training time and computing time. The results obtained using the software application and important conclusions towards the training and computing time comportment are also presented.},
	author = {M. Stoica and G. A. Calangiu and F. Sisak},
	booktitle = {19th International Workshop on Robotics in Alpe-Adria-Danube Region (RAAD 2010)},
	date-added = {2018-11-29 19:30:41 +1300},
	date-modified = {2018-11-29 19:30:48 +1300},
	doi = {10.1109/RAAD.2010.5524599},
	keywords = {NN; graph theory;learning (artificial intelligence);neural nets;robot programming;neural network training;training steps;artificial neural networks;robot programming;unidirectional multi-layer neural network;graph;Time measurement;Neural networks;Robot kinematics;Service robots;Sliding mode control;Control systems;Biological system modeling;Humans;Biological neural networks;Artificial neural networks},
	month = {June},
	pages = {109-113},
	title = {Measuring the time needed for training a neural network based on the number of training steps},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/RAAD.2010.5524599}}

@inproceedings{ijcai2017-316,
	author = {Tao Lin and Tian Guo and Karl Aberer},
	booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {IJCAI-17}},
	date-added = {2018-11-29 19:19:54 +1300},
	date-modified = {2018-11-29 19:20:41 +1300},
	doi = {10.24963/ijcai.2017/316},
	keywords = {NN; Neural Network, LSTM, Convolutional Neural Network, Hybrid},
	pages = {2273--2279},
	title = {Hybrid Neural Networks for Learning the Trend in Time Series},
	url = {https://doi.org/10.24963/ijcai.2017/316},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2017/316}}

@article{SCHMIDHUBER201585,
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	author = {J{\"u}rgen Schmidhuber},
	date-added = {2018-11-28 23:30:32 +1300},
	date-modified = {2018-11-28 23:31:02 +1300},
	doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {DNN; Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
	pages = {85 - 117},
	title = {Deep learning in neural networks: An overview},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	volume = {61},
	year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2014.09.003}}

@inproceedings{5981829,
	abstract = {In this paper we present a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms - neuFlow - and a dataflow compiler - luaFlow - that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications. We present an application of the system on street scene analysis, segmenting 20 categories on 500 × 375 frames at 12 frames per second on our custom hardware neuFlow.},
	author = {C. Farabet and B. Martini and B. Corda and P. Akselrod and E. Culurciello and Y. LeCun},
	booktitle = {CVPR 2011 WORKSHOPS},
	date-added = {2018-11-28 17:19:47 +1300},
	date-modified = {2018-11-28 17:19:58 +1300},
	doi = {10.1109/CVPRW.2011.5981829},
	issn = {2160-7508},
	keywords = {CNN-FPGA; computer vision;field programmable gate arrays;flow graphs;NeuFlow;runtime reconfigurable dataflow processor;scalable dataflow hardware architecture;dataflow compiler;luaFlow;flow graph representations;machine code;Xilinx Virtex 6 FPGA platform;laptop computer;Tiles;Computer architecture;Runtime;Field programmable gate arrays;Hardware;Convolvers;Feature extraction},
	month = {June},
	pages = {109-116},
	title = {NeuFlow: A runtime reconfigurable dataflow processor for vision},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPRW.2011.5981829}}

@book{Hertz:1991:ITN:104000,
	address = {Boston, MA, USA},
	author = {Hertz, John and Krogh, Anders and Palmer, Richard G.},
	date-added = {2018-11-26 21:51:03 +1300},
	date-modified = {2019-06-05 21:20:16 +1200},
	isbn = {0-201-50395-6},
	keywords = {NN; Books, Neural Computation},
	publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	source = {ISBN 0-201-51560-1},
	title = {Introduction to the Theory of Neural Computation},
	year = {1991}}

@article{Silver:2017aa,
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	date = {2017/10/18/online},
	date-added = {2018-11-26 19:46:00 +1300},
	date-modified = {2018-11-26 19:46:20 +1300},
	day = {18},
	journal = {Nature},
	keywords = {NN, Go, Google, AlphaGo},
	l3 = {10.1038/nature24270; https://www.nature.com/articles/nature24270#supplementary-information},
	m3 = {Article},
	month = {10},
	pages = {354 EP -},
	publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN -},
	title = {Mastering the game of Go without human knowledge},
	ty = {JOUR},
	url = {https://doi.org/10.1038/nature24270},
	volume = {550},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature24270}}

@article{doi:10.1177/1555343417695197,
	abstract = { Autonomous and semiautonomous vehicles are currently being developed by over14 companies. These vehicles may improve driving safety and convenience, or they may create new challenges for drivers, particularly with regard to situation awareness (SA) and autonomy interaction. I conducted a naturalistic driving study on the autonomy features in the Tesla Model S, recording my experiences over a 6-month period, including assessments of SA and problems with the autonomy. This preliminary analysis provides insights into the challenges that drivers may face in dealing with new autonomous automobiles in realistic driving conditions, and it extends previous research on human-autonomy interaction to the driving domain. Issues were found with driver training, mental model development, mode confusion, unexpected mode interactions, SA, and susceptibility to distraction. New insights into challenges with semiautonomous driving systems include increased variability in SA, the replacement of continuous control with serial discrete control, and the need for more complex decisions. Issues that deserve consideration in future research and a set of guidelines for driver interfaces of autonomous systems are presented and used to create recommendations for improving driver SA when interacting with autonomous vehicles. },
	author = {Mica R. Endsley},
	date-added = {2018-11-26 19:43:09 +1300},
	date-modified = {2018-11-26 19:43:24 +1300},
	doi = {10.1177/1555343417695197},
	eprint = {https://doi.org/10.1177/1555343417695197},
	journal = {Journal of Cognitive Engineering and Decision Making},
	keywords = {NN, autopilot, tesla},
	number = {3},
	pages = {225-238},
	title = {Autonomous Driving Systems: A Preliminary Naturalistic Study of the Tesla Model S},
	url = {https://doi.org/10.1177/1555343417695197},
	volume = {11},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1177/1555343417695197}}

@inproceedings{nirkin2018_faceswap,
	author = {Nirkin, Yuval and Masi, Iacopo and Tran, Anh Tuan and Hassner, Tal and Medioni, G\'{e}rard},
	booktitle = {IEEE Conference on Automatic Face and Gesture Recognition},
	date-added = {2018-11-26 19:30:06 +1300},
	date-modified = {2018-11-26 19:35:44 +1300},
	keywords = {CNN, Faceswap, DeepFakes, CNN},
	title = {On Face Segmentation, Face Swapping, and Face Perception},
	year = {2018}}

@article{McCulloch1943,
	abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	author = {McCulloch, Warren S. and Pitts, Walter},
	date-added = {2018-11-25 21:59:10 +1300},
	date-modified = {2018-11-25 21:59:28 +1300},
	day = {01},
	doi = {10.1007/BF02478259},
	issn = {1522-9602},
	journal = {The bulletin of mathematical biophysics},
	keywords = {NN, Neural network, mathematical definition},
	month = {Dec},
	number = {4},
	pages = {115--133},
	title = {A logical calculus of the ideas immanent in nervous activity},
	url = {https://doi.org/10.1007/BF02478259},
	volume = {5},
	year = {1943},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02478259}}

@inproceedings{Qiu:2016:GDE:2847263.2847265,
	acmid = {2847265},
	address = {New York, NY, USA},
	author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
	booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-11-25 19:49:32 +1300},
	date-modified = {2018-11-25 19:49:42 +1300},
	doi = {10.1145/2847263.2847265},
	isbn = {978-1-4503-3856-1},
	keywords = {FPGA-NN; bandwidth utilization, convolutional neural network (cnn), dynamic-precision data quantization, embedded fpga},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {26--35},
	publisher = {ACM},
	series = {FPGA '16},
	title = {Going Deeper with Embedded FPGA Platform for Convolutional Neural Network},
	url = {http://doi.acm.org/10.1145/2847263.2847265},
	year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2847263.2847265},
	Bdsk-Url-2 = {https://doi.org/10.1145/2847263.2847265}}

@inproceedings{Zhang:2017:IPO:3020078.3021698,
	acmid = {3021698},
	address = {New York, NY, USA},
	author = {Zhang, Jialiang and Li, Jing},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-11-25 19:34:22 +1300},
	date-modified = {2018-11-25 19:34:32 +1300},
	doi = {10.1145/3020078.3021698},
	isbn = {978-1-4503-4354-1},
	keywords = {FPGA-NN; convolutional neural networks, fpga, hardware accelerator, opencl},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {25--34},
	publisher = {ACM},
	series = {FPGA '17},
	title = {Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network},
	url = {http://doi.acm.org/10.1145/3020078.3021698},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021698},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021698}}

@inproceedings{Langhammer:2015:FDB:2684746.2689071,
	acmid = {2689071},
	address = {New York, NY, USA},
	author = {Langhammer, Martin and Pasca, Bogdan},
	booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-11-25 16:46:58 +1300},
	date-modified = {2018-11-25 16:47:11 +1300},
	doi = {10.1145/2684746.2689071},
	isbn = {978-1-4503-3315-3},
	keywords = {FPGA, altera, arria10, dsp, floating-point, single-precision},
	location = {Monterey, California, USA},
	numpages = {9},
	pages = {117--125},
	publisher = {ACM},
	series = {FPGA '15},
	title = {Floating-Point DSP Block Architecture for FPGAs},
	url = {http://doi.acm.org/10.1145/2684746.2689071},
	year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689071},
	Bdsk-Url-2 = {https://doi.org/10.1145/2684746.2689071}}

@inproceedings{Aydonat:2017:ODL:3020078.3021738,
	acmid = {3021738},
	address = {New York, NY, USA},
	author = {Aydonat, Utku and O'Connell, Shane and Capalija, Davor and Ling, Andrew C. and Chiu, Gordon R.},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-11-25 15:35:07 +1300},
	date-modified = {2018-11-25 15:35:29 +1300},
	doi = {10.1145/3020078.3021738},
	isbn = {978-1-4503-4354-1},
	keywords = {FPGA-NN, convolutional neural networks, deep neural networks},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {55--64},
	publisher = {ACM},
	series = {FPGA '17},
	title = {An OpenCL Deep Learning Accelerator on Arria 10},
	url = {http://doi.acm.org/10.1145/3020078.3021738},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021738},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021738}}

@inproceedings{10.1007/11550822_51,
	abstract = {In this paper, we analyze characteristics of GA-based learning method of Binary Neural Networks (BNN). First, we consider coding methods to a chromosome in a GA and discuss the necessary chromosome length for a learning of BNN. Then, we compare some selection methods in a GA. We show that the learning results can be obtained in the less number of generations by properly setting selection methods and parameters in a GA. We also show that the quality of the learning results can be almost the same as that of the conventional method. These results can be verified by numerical experiments.},
	address = {Berlin, Heidelberg},
	author = {Hirane, Tatsuya and Toryu, Tetsuya and Nakano, Hidehiro and Miyauchi, Arata},
	booktitle = {Artificial Neural Networks: Biological Inspirations -- ICANN 2005},
	date-added = {2018-11-25 00:34:18 +1300},
	date-modified = {2018-11-25 14:47:15 +1300},
	editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.{z}}ny, S{\l}awomir},
	isbn = {978-3-540-28754-4},
	keywords = {BNN, Genetic Algorithm,},
	pages = {323--329},
	publisher = {Springer Berlin Heidelberg},
	title = {Analysis for Characteristics of GA-Based Learning Method of Binary Neural Networks},
	year = {2005}}

@inproceedings{Umuroglu:2017:FFF:3020078.3021744,
	acmid = {3021744},
	address = {New York, NY, USA},
	author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-11-25 00:31:00 +1300},
	date-modified = {2019-04-28 15:11:49 +1200},
	doi = {10.1145/3020078.3021744},
	isbn = {978-1-4503-4354-1},
	keywords = {BNN; FPGA, binarized neural network, binary neural network, hardware acceleration, neural networks, reconfigurable logic},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {65--74},
	publisher = {ACM},
	series = {FPGA '17},
	title = {FINN: A Framework for Fast, Scalable Binarized Neural Network Inference},
	url = {http://doi.acm.org/10.1145/3020078.3021744},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021744},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021744}}

@article{Yamada1992,
	abstract = {This paper presents an ultra-high-speed sorter based upon a simplified parallel sorting algorithm using a binary neural network which consists both of binary neurons and of AND-OR synaptic connections to solve sorting problems at two and only two clock cycles. Our simplified algorithm is based on the super parallel sorting algorithm proposed by Takefuji and Lee. Nevertheless, our algorithm does not need any adders, while Takefuji's algorithm needs n{\texttimes}(n−1) analog adders of which each has multiple input ports. For an example of the simplified parallel sorter, a hardware design and its implementation will be introduced in this paper, which performs a sorting operation at two clock cycles. Both results of a logic circuit simulation and of an algorithm simulation show the justice of our hardware implementation even if in the practical size of the problem.},
	author = {Yamada, Manabu and Nakagawa, Tohru and Kitagawa, Hajime},
	date-added = {2018-11-25 00:23:46 +1300},
	date-modified = {2018-11-25 00:24:03 +1300},
	day = {01},
	doi = {10.1007/BF00228719},
	issn = {1573-1979},
	journal = {Analog Integrated Circuits and Signal Processing},
	keywords = {BNN; Parallel processing; Sorting Algorithm},
	month = {Nov},
	number = {4},
	pages = {389--393},
	title = {A super parallel sorter using a binary neural network with AND-OR synaptic connections},
	url = {https://doi.org/10.1007/BF00228719},
	volume = {2},
	year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00228719}}

@inproceedings{10.1007/978-3-642-42042-9_86,
	abstract = {This paper studies application of the dynamic binary neural network to control signal of switching circuits. The network is characterized by the signum activation function and ternary weighting parameters. In the application, the teacher signal is one binary periodic orbit corresponding to the controls. The learning algorithm is based on the genetic algorithm. As an application object, we consider a control signal of the basic matrix converter. Performing a basic numerical experiment, we have confirmed that the teacher signal is stored successfully and is stabilized automatically.},
	address = {Berlin, Heidelberg},
	author = {Nakayama, Yuta and Kouzuki, Ryota and Saito, Toshimichi},
	booktitle = {Neural Information Processing},
	date-added = {2018-11-25 00:10:15 +1300},
	date-modified = {2018-11-25 00:20:07 +1300},
	editor = {Lee, Minho and Hirose, Akira and Hou, Zeng-Guang and Kil, Rhee Man},
	isbn = {978-3-642-42042-9},
	keywords = {BNN; Dynamic NN; Control signal, Genetic Algorithm},
	pages = {697--704},
	publisher = {Springer Berlin Heidelberg},
	title = {Application of the Dynamic Binary Neural Network to Switching Circuits},
	year = {2013}}

@inproceedings{10.1007/978-3-642-59041-2_23,
	abstract = {In the very near future large amounts of Remotely Sensed data will become available on a daily basis. Unfortunately, it is not clear if the processing methods are available to deal with this data in a timely fashion. This paper describes research towards an approach which will allow a user to perform a rapid pre-search of large amounts of image data for regions of interest based on texture. The method is based on a novel neural network architecture (ADAM) that is designed primarily for speed of operation by making use of computationally simple pre-processing and only uses Boolean operations in the weights of the network. To facilitate interactive use of the network, it is capable of rapid training. The paper outlines the neural network, its application to RS data in comparison with other methods, and briefly describes a fast hardware implementation of the network.},
	address = {Berlin, Heidelberg},
	author = {Austin, Jim},
	booktitle = {Neurocomputation in Remote Sensing Data Analysis},
	date-added = {2018-11-24 23:37:04 +1300},
	date-modified = {2018-11-24 23:37:33 +1300},
	editor = {Kanellopoulos, Ioannis and Wilkinson, Graeme G. and Roli, Fabio and Austin, James},
	isbn = {978-3-642-59041-2},
	keywords = {BNN, Image Segmentation, Data Analysis},
	pages = {202--213},
	publisher = {Springer Berlin Heidelberg},
	title = {High Speed Image Segmentation Using a Binary Neural Network},
	year = {1997}}

@inproceedings{10.1007/978-81-322-2208-8_43,
	abstract = {In this paper, a quantum based binary neural network learning algorithm is proposed for solving two class problems. The proposed method constructively forms the neural network architecture and weights are decided by quantum computing concept. The use of quantum computing optimizes the network structure and the performance in terms of number of neurons at hidden layer and classification accuracy. This approach is compared with MTiling-real networks algorithm and it is found that there is a significant improvement in terms of number of neurons at the hidden layer, number of iterations, training accuracy and generalization accuracy.},
	address = {New Delhi},
	author = {Patel, Om Prakash and Tiwari, Aruna},
	booktitle = {Computational Intelligence in Data Mining - Volume 2},
	date-added = {2018-11-24 23:17:08 +1300},
	date-modified = {2018-11-24 23:17:53 +1300},
	editor = {Jain, Lakhmi C. and Behera, Himansu Sekhar and Mandal, Jyotsna Kumar and Mohapatra, Durga Prasad},
	isbn = {978-81-322-2208-8},
	keywords = {BNN; Quantum Algorithm; Neural Network},
	pages = {473--482},
	publisher = {Springer India},
	title = {Quantum Based Learning with Binary Neural Network},
	year = {2015}}

@incollection{NIPS2016_6573,
	annote = {Editor:
D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	booktitle = {Advances in Neural Information Processing Systems 29},
	date-added = {2018-11-24 21:41:12 +1300},
	date-modified = {2020-07-21 14:08:11 +1200},
	keywords = {BNN; Neural Network},
	pages = {4107--4115},
	publisher = {Curran Associates, Inc.},
	title = {Binarized Neural Networks},
	url = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf},
	year = {2016},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf}}

@incollection{NIPS2015_5647,
	annote = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf},
	author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	booktitle = {Advances in Neural Information Processing Systems 28},
	date-added = {2018-11-24 21:39:32 +1300},
	date-modified = {2019-07-23 17:14:58 +1200},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	keywords = {BNN; Machine Learning; Computer Vision; Pattern Recognition; Neural and Evolutionary Computing},
	pages = {3123--3131},
	publisher = {Curran Associates, Inc.},
	title = {BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
	year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf}}

@inproceedings{Nurvitadhi:2017:FBG:3020078.3021740,
	acmid = {3021740},
	address = {New York, NY, USA},
	author = {Nurvitadhi, Eriko and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan and Subhaschandra, Suchit and Boudoukh, Guy},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-11-24 20:45:12 +1300},
	date-modified = {2018-11-24 20:45:24 +1300},
	doi = {10.1145/3020078.3021740},
	isbn = {978-1-4503-4354-1},
	keywords = {FPGA-NN; FPGA, GPU, accelerator, deep learning, intel stratix 10},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {5--14},
	publisher = {ACM},
	series = {FPGA '17},
	title = {Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?},
	url = {http://doi.acm.org/10.1145/3020078.3021740},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021740},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021740}}

@inproceedings{Cheng_2014_CVPR,
	author = {Cheng, Ming-Ming and Zhang, Ziming and Lin, Wen-Yan and Torr, Philip},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-11-23 10:09:08 +1300},
	date-modified = {2018-11-23 10:09:18 +1300},
	keywords = {CNN; BING},
	month = {June},
	title = {BING: Binarized Normed Gradients for Objectness Estimation at 300fps},
	year = {2014}}

@article{bhandari2012design,
	author = {Bhandari, Sachin and Tiwari, Aruna},
	date-added = {2018-11-22 18:18:02 +1300},
	date-modified = {2018-11-22 18:18:27 +1300},
	keywords = {BNN; Clustering, ETL, Fuzzy logic},
	title = {Design and implementation of binary neural network learning with fuzzy clustering},
	year = {2012}}

@misc{Muselli99hammingclustering:,
	author = {Marco Muselli and Diego Liberati},
	date-added = {2018-11-22 17:37:41 +1300},
	date-modified = {2018-11-22 17:37:52 +1300},
	keywords = {BNN},
	title = {Hamming Clustering: A New Approach to Rule Extraction},
	year = {1999}}

@inproceedings{817981,
	abstract = {A new constructive learning algorithm, called Hamming clustering (HC), for binary neural networks is proposed. It is able to generate a set of rules in if-then form underlying an unknown classification problem starting from a training set of samples. The performance of HC has been evaluated through a variety of artificial and real-world benchmarks. In particular, its application in the diagnosis of breast cancer has led to the derivation of a reduced set of rules solving the associated classification problem.},
	author = {M. Muselli and D. Liberati},
	booktitle = {1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)},
	date-added = {2018-11-22 16:46:03 +1300},
	date-modified = {2018-11-22 16:46:11 +1300},
	doi = {10.1049/cp:19991161},
	issn = {0537-9989},
	keywords = {BNN; neural nets;learning algorithm;Hamming clustering;binary neural networks;breast cancer;pattern classification;patient diagnosis;Boolean function},
	month = {Sept},
	pages = {515-520 vol.2},
	title = {Rule extraction from binary neural networks},
	volume = {2},
	year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1049/cp:19991161}}

@inproceedings{5178979,
	abstract = {This paper presents a learning algorithm of digital binary neural networks for approximation of desired Boolean functions. In the learning, the genetic algorithms is used with flexible fitness that tolerates error: it is suitable to reduce the number of hidden neurons and to tolerate noise and outliers. We then apply the algorithm to design of cellular automata with rich spatio-temporal patterns and various applications. Performing basic numerical experiment, the algorithm efficiency is confirmed.},
	author = {S. Kabeya and T. Abe and T. Saito},
	booktitle = {2009 International Joint Conference on Neural Networks},
	date-added = {2018-11-22 11:37:15 +1300},
	date-modified = {2018-11-22 11:37:25 +1300},
	doi = {10.1109/IJCNN.2009.5178979},
	issn = {2161-4393},
	keywords = {BNN; Boolean functions;cellular automata;function approximation;genetic algorithms;learning (artificial intelligence);neural nets;flexible learning algorithm;error tolerance;digital binary neural networks;Boolean function approximation;genetic algorithms;cellular automata;spatio-temporal patterns;Neural networks;Signal processing algorithms;Neurons;Boolean functions;Approximation algorithms;Genetic algorithms;Noise reduction;Nonlinear dynamical systems;USA Councils;Algorithm design and analysis},
	month = {June},
	pages = {1476-1480},
	title = {A GA-based flexible learning algorithm with error tolerance for digital binary neural networks},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2009.5178979}}

@inproceedings{714090,
	abstract = {This paper considers the use of binary neural networks for pattern classification. An expand-and-truncate learning (ETL) algorithm is used to determine the required number of neurons as well as the connecting weights in a three-layered feedforward network for classifying input patterns. The ETL algorithm is guaranteed to find a network for any binary-to-binary mappings. The ETL algorithm's performance in pattern classification is tested using a breast cancer database that have been used for benchmarking performance other machine learning methods.},
	author = {C. H. Chu and J. H. Kim},
	booktitle = {Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
	date-added = {2018-11-21 23:44:42 +1300},
	date-modified = {2019-06-05 21:20:32 +1200},
	doi = {10.1109/IJCNN.1993.714090},
	keywords = {BNN; feedforward neural nets;pattern classification;learning (artificial intelligence);medical diagnostic computing;geometrical learning;binary neural networks;pattern classification;expand-and-truncate learning algorithm;connecting weights;three-layered feedforward network;binary-to-binary mappings;breast cancer;Pattern classification;Neural networks;Neurons;Machine learning algorithms;Testing;Breast cancer;Databases;Hamming distance;Computer networks;Joining processes},
	month = {Oct},
	pages = {1039-1042 vol.1},
	title = {Pattern classification by geometrical learning of binary neural networks},
	volume = {1},
	year = {1993},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1993.714090}}

@inproceedings{203280,
	abstract = {A novel design technique for asynchronous binary neural networks is proposed. This design uses linear programming to design two architectures: (i) a fully connected network that reads a N-digit cue and classifies it into a category represented by a N-digit pattern: and (ii) a two-layer network (with lateral connections) that has M neurons in the first layer and L neurons in the second layer; the network reads an M-digit cue to the first layer and associates it with a second-layer L-digit pattern. In both cases, the objective function is a weighted sum of the number of errors that can be corrected by the network. A cue with this number of errors (or fewer) is guaranteed to converge to the correct pattern. An economical VLSI realization of the designed networks can be easily accomplished.&lt;&lt;ETX&gt;&gt;},
	author = {M. Kam and J. C. Chow and R. Fischl},
	booktitle = {29th IEEE Conference on Decision and Control},
	date-added = {2018-11-21 23:33:12 +1300},
	date-modified = {2018-11-21 23:33:20 +1300},
	doi = {10.1109/CDC.1990.203280},
	keywords = {BNN, linear programming;neural nets;asynchronous binary neural networks;linear programming;design technique;fully connected network;two-layer network;lateral connections;Neural networks;Linear programming;Neurons;Algorithm design and analysis;Error correction;Convergence;Neurofeedback;Hamming distance;Very large scale integration;Stochastic processes},
	month = {Dec},
	pages = {2766-2767 vol.5},
	title = {Design of two architectures of asynchronous binary neural networks using linear programming},
	year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1109/CDC.1990.203280}}

@inproceedings{1259700,
	abstract = {Learning problem for neural networks has widely been investigated in last two decades. Kim and Park [J.H. Kim et al., Jan. 1995] proposed one approach based on geometric technique, called "expand and truncate learning (ETL)". ETL is proposed to construct a three-layer binary neural network (BNN) for training a Boolean function of n (Boolean) variables. It is claimed by Kim and Park in [J.H. Kim et al., Jan. 1995] that, neural networks constructed according to this technique are much smaller. This paper investigates usefulness of these ideas for data classification. Data classification in real world involves multiple classes. For solving this problem, there are many techniques based on statistical principles, clustering approaches, etc. Application of binary neural networks for multiple outputs are important in practice. We propose a method for construction of a binary neural net based on generalization of ETL to more than two classes. Our method simplifies the resulting neural network architecture.},
	author = {Yi Xu and N. S. Chaudhari},
	booktitle = {Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)},
	date-added = {2018-11-21 23:18:59 +1300},
	date-modified = {2018-11-21 23:19:06 +1300},
	doi = {10.1109/ICMLC.2003.1259700},
	keywords = {BNN; neural nets;pattern classification;learning (artificial intelligence);Boolean functions;binary neural networks;geometric technique;expand and truncate learning;Boolean function;data classification;core vertex;neural network training;Neural networks;Neurons;Artificial neural networks;Power line communications;Machine learning algorithms;Application software;Boolean functions;Pattern classification;Function approximation;Pattern matching},
	month = {Nov},
	pages = {1343-1348 Vol.3},
	title = {Application of binary neural networks for classification},
	volume = {3},
	year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICMLC.2003.1259700}}

@inproceedings{480879,
	abstract = {Hidden periodicities play a very important role in the regulatory and structural functioning of genomic DNA strands. Primarily, it concerns the fundamental three-periodicity inherent to protein coding regions in all taxonomic groups, two-periodicity in introns of eukaryots as well as periodicities related to helix and chromatin pitches, while the other periodicities appear to be species specific. Rather roughly (and without sharp boundary) the underlying periodicities may be divided by two groups. In the first case the periodicities are due to particular nucleotides (or very short oligomers) quasi-regularly positioned in a seemingly random background. This type of regularity can be identified via either standard frequency analysis or more elaborate Fourier methods. For the second group a periodicity is related to the quasi-random replacements in initially complete repeating motifs (situation typical, e.g., for modifications of satellites). In the last case the statistical reconstruction of underlying repeats is a much less trivial task. The authors show that this problem can successfully be solved with multi-symbol extension of energy-minimizing neural networks (EMNN). The reconstruction of underlying motifs may shed additional light on the evolutionary and functional modifications in various genomes.},
	author = {V. R. Chechetkin and A. A. Ezhov and L. A. Knizhnikova and A. Y. Turygin},
	booktitle = {The Second International Symposium on Neuroinformatics and Neurocomputers},
	date-added = {2018-11-21 23:03:36 +1300},
	date-modified = {2018-11-21 23:03:46 +1300},
	doi = {10.1109/ISNINC.1995.480879},
	keywords = {NN; DNA;neural nets;sequences;genomic DNA sequences;neural networks;fundamental three-periodicity;protein coding regions;taxonomic groups;two-periodicity;introns;eukaryots;chromatin pitches;helix pitches;nucleotides;quasi-random replacements;statistical reconstruction;energy-minimizing neural networks;Intelligent networks;Genomics;Bioinformatics;DNA;Sequences;Neural networks;Neurons;Proteins;Technological innovation;Frequency},
	month = {Sept},
	pages = {346-352},
	title = {Study of underlying repeats in genomic DNA sequences with neural networks},
	year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNINC.1995.480879}}

@inproceedings{5380141,
	abstract = {In this paper we propose a new method to analyze the similarity/dissimilarity of DNA sequences which can be used in human identification field. This method is based on the graphical representation proposed by Randic et al [M. Randic, M. Vracko, L. Nella, P. Dejan, Chem. (2003)]. Instead of calculating the leading eigenvalues of the matrix for graphical representation we smooth the zigzag curve and calculate its curvature. Similarity between DNA sequences are decided by neural network. Our method is useful for human identification in criminal investigations and in genetic disease. Our results verify the validity of our method.},
	author = {R. Rafeh and M. Mesgar},
	booktitle = {2009 Second International Conference on Computer and Electrical Engineering},
	date-added = {2018-11-21 22:54:01 +1300},
	date-modified = {2018-11-21 22:54:11 +1300},
	doi = {10.1109/ICCEE.2009.132},
	keywords = {NN, biology computing;DNA;eigenvalues and eigenfunctions;forensic science;matrix algebra;medical computing;neural nets;neural network;human identification;DNA sequences;graphical representation;criminal investigations;genetic disease;matrix eigenvalues;Neural networks;Humans;Sequences;Computer networks;DNA computing;Eigenvalues and eigenfunctions;Artificial neural networks;Genetics;Diseases;Forensics;University Timetabling;Optimization Problems;Constraint Programming;Linear Programming},
	month = {Dec},
	pages = {64-67},
	title = {Neural Network in Human Identification by DNA Sequences},
	volume = {2},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCEE.2009.132}}

@inproceedings{497852,
	abstract = {The four nitrogenous bases of DNA spell out the recipes from which proteins are made. A gene typically contains five thousand or so bases but often only a small percentage of these are protein coding. Computer based prediction systems are increasingly relied upon as submissions to the major genetic databases are growing exponentially. Several systems exist to locate coding regions (exons) and noncoding regions (introns) within genomic DNA; the common models used are neural networks and Markov chains (M. Borodovsky and J. McIninch (1993), A. Krogh et al. (1994). One of the most successful programs is called GRAIL. Currently, two versions of GRAIL are available: GRAIL-I (E. Uberbacher and R. Mural (1991), and GRAIL-II (Y. Xu et al. (1994). In GRAIL-I, a neural network receives its inputs from seven statistical measures taken on a 99 base window. Performance is improved in GRAIL-II by the addition of variable length windows, neural nets trained to locate intron/exon boundaries, and a number of steps designed to evaluate candidate exons and eliminate improbable ones. Both versions of GRAIL predict coding regions in human DNA. A simulation of GRAIL-I was carried out with the goal of improving classification performance without resorting to the additional measures used in GRAIL-II. The intention was then to supplement the resulting module with modules based on physiochemical measures of DNA (such as melting profiles, twist and wedge angles) to enable precise exon prediction in plant sequences.},
	author = {L. Roberts and N. Steele and C. Reeves and G. J. King},
	booktitle = {1995 Fourth International Conference on Artificial Neural Networks},
	date-added = {2018-11-21 22:10:13 +1300},
	date-modified = {2018-11-21 22:10:22 +1300},
	doi = {10.1049/cp:19950589},
	issn = {0537-9989},
	keywords = {NN; biology computing;DNA;genetics;neural nets;learning (artificial intelligence);proteins;neural network training;coding region identification;genomic DNA;nitrogenous bases;Markov chains;computer based prediction systems;exons;introns;GRAIL;GRAIL-I;GRAIL-II;statistical measures;variable length windows;intron/exon boundaries;human DNA;classification performance;physiochemical measures;precise exon prediction;plant sequences;Biomedical computing;DNA;Genetics;Neural networks;Learning systems;Proteins},
	month = {June},
	pages = {399-403},
	title = {Training neural networks to identify coding regions in genomic DNA},
	year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1049/cp:19950589}}

@article{Stanley:2002:ENN:638553.638554,
	acmid = {638554},
	address = {Cambridge, MA, USA},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	date-added = {2018-11-14 20:01:01 +1300},
	date-modified = {2018-11-14 20:01:09 +1300},
	doi = {10.1162/106365602320169811},
	issn = {1063-6560},
	issue_date = {Summer 2002},
	journal = {Evol. Comput.},
	keywords = {Evolutionary, competing conventions, genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
	month = jun,
	number = {2},
	numpages = {29},
	pages = {99--127},
	publisher = {MIT Press},
	title = {Evolving Neural Networks Through Augmenting Topologies},
	url = {http://dx.doi.org/10.1162/106365602320169811},
	volume = {10},
	year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/106365602320169811}}

@article{2018arXiv180402464M,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402464M},
	archiveprefix = {arXiv},
	author = {{Miconi}, T. and {Clune}, J. and {Stanley}, K.~O.},
	date-added = {2018-11-13 19:47:52 +1300},
	date-modified = {2018-11-13 19:48:02 +1300},
	eprint = {1804.02464},
	journal = {ArXiv e-prints},
	keywords = {DNN, Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	month = apr,
	title = {{Differentiable plasticity: training plastic neural networks with backpropagation}},
	year = 2018}

@inproceedings{10.1007/3-540-61108-8_27,
	abstract = {In this paper, evolution strategies (ESs) --- a class of evolutionary algorithms using normally distributed mutations, recombination, deterministic selection of the $\mu$>1 best offspring individuals, and the principle of self-adaptation for the collective on-line learning of strategy parameters --- are described by demonstrating their differences to genetic algorithms. By comparison of the algorithms, it is argued that the application of canonical genetic algorithms for continuous parameter optimization problems implies some difficulties caused by the encoding of continuous object variables by binary strings and the constant mutation rate used in genetic algorithms. Because they utilize a problem-adequate representation and a suitable self-adaptive step size control guaranteeing linear convergence for strictly convex problems, evolution strategies are argued to be more adequate for continuous problems.},
	address = {Berlin, Heidelberg},
	author = {B{\"a}ck, Thomas},
	booktitle = {Artificial Evolution},
	date-added = {2018-11-13 16:18:55 +1300},
	date-modified = {2018-11-13 16:19:16 +1300},
	editor = {Alliot, Jean-Marc and Lutton, Evelyne and Ronald, Edmund and Schoenauer, Marc and Snyers, Dominique},
	isbn = {978-3-540-49948-0},
	keywords = {Evolutionary, evolution strategies, Genetic Algorithm},
	pages = {1--20},
	publisher = {Springer Berlin Heidelberg},
	title = {Evolution strategies: An alternative evolutionary algorithm},
	year = {1996}}

@inproceedings{4424711,
	abstract = {Differential evolution has shown to be a very powerful, yet simple, population-based optimization approach. The nature of its reproduction operator limits its application to continuous-valued search spaces. However, a simple discretization procedure can be used to convert floating-point solution vectors into discrete-valued vectors. This paper considers three approaches in which differential evolution can be used to solve problems with binary-valued parameters. The first approach is based on a homomorphous mapping, while the second approach interprets the floating-point solution vector as a vector of probabilities, used to decide on the appropriate binary value. The third approach normalizes solution vectors and then discretize these normalized vectors to form a bitstring. Empirical results are provided to illustrate the efficiency of both methods in comparison with particle swarm optimizers.},
	author = {A. P. Engelbrecht and G. Pampara},
	booktitle = {2007 IEEE Congress on Evolutionary Computation},
	date-added = {2018-11-13 15:23:49 +1300},
	date-modified = {2018-11-13 15:24:01 +1300},
	doi = {10.1109/CEC.2007.4424711},
	issn = {1089-778X},
	keywords = {Evolutionary; evolutionary computation;optimisation;binary differential evolution;population-based optimization;reproduction operator;continuous-valued search space;simple discretization procedure;floating-point solution vectors;discrete-valued vectors;binary-valued parameters;homomorphous mapping;normalized vectors;particle swarm optimizers;Optimization methods;Genetic mutations;Particle swarm optimization;Probability density function;Arithmetic;Biological cells;Difference equations;Differential equations;Stochastic processes;Discrete transforms},
	month = {Sept},
	pages = {1942-1947},
	title = {Binary differential evolution strategies},
	year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2007.4424711}}

@article{Wierstra:2014:NES:2627435.2638566,
	acmid = {2638566},
	author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J\"{u}rgen},
	date-added = {2018-11-13 15:10:29 +1300},
	date-modified = {2018-11-13 15:10:50 +1300},
	issn = {1532-4435},
	issue_date = {January 2014},
	journal = {J. Mach. Learn. Res.},
	keywords = {Evolutionary, black-box optimization, evolution strategies, natural gradient, sampling, stochastic search},
	month = jan,
	number = {1},
	numpages = {32},
	pages = {949--980},
	publisher = {JMLR.org},
	title = {Natural Evolution Strategies},
	url = {http://dl.acm.org/citation.cfm?id=2627435.2638566},
	volume = {15},
	year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBaLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L1Jlb3JnYW5pc2luZyBBcnRpZmljaWFsIE5ldXJhbCBOZXR3b3JrIFRvcG9sb2dpZXMuYmliTxECJAAAAAACJAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1Jlb3JnYW5pc2luZyBBcnRpZiNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAxFdm9sdXRpb25hcnkAAgBwLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkV2b2x1dGlvbmFyeTpSZW9yZ2FuaXNpbmcgQXJ0aWZpY2lhbCBOZXVyYWwgTmV0d29yayBUb3BvbG9naWVzLmJpYgAOAGwANQBSAGUAbwByAGcAYQBuAGkAcwBpAG4AZwAgAEEAcgB0AGkAZgBpAGMAaQBhAGwAIABOAGUAdQByAGEAbAAgAE4AZQB0AHcAbwByAGsAIABUAG8AcABvAGwAbwBnAGkAZQBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBuVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9Fdm9sdXRpb25hcnkvUmVvcmdhbmlzaW5nIEFydGlmaWNpYWwgTmV1cmFsIE5ldHdvcmsgVG9wb2xvZ2llcy5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAgQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKp},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2627435.2638566}}

@article{Sehnke:2010aa,
	abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradient by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the parameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
	author = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
	booktitle = {The 18th International Conference on Artificial Neural Networks, ICANN 2008},
	da = {2010/05/01/},
	date-added = {2018-11-13 15:07:22 +1300},
	date-modified = {2018-11-13 15:07:34 +1300},
	doi = {https://doi.org/10.1016/j.neunet.2009.12.004},
	isbn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Evolutionary, Policy gradients; Stochastic optimisation; Reinforcement learning; Robotics; Control},
	number = {4},
	pages = {551--559},
	title = {Parameter-exploring policy gradients},
	ty = {JOUR},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608009003220},
	volume = {23},
	year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608009003220},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2009.12.004}}

@article{Williams1992,
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	author = {Williams, Ronald J.},
	date-added = {2018-11-13 15:01:38 +1300},
	date-modified = {2018-11-13 15:02:10 +1300},
	day = {01},
	doi = {10.1007/BF00992696},
	issn = {1573-0565},
	journal = {Machine Learning},
	keywords = {Evolutionary, Gradient Methods, Reinforcement Learning, REINFORCE},
	month = {May},
	number = {3},
	pages = {229--256},
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	url = {https://doi.org/10.1007/BF00992696},
	volume = {8},
	year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00992696}}

@inproceedings{Morse:2016:SEO:2908812.2908916,
	acmid = {2908916},
	address = {New York, NY, USA},
	author = {Morse, Gregory and Stanley, Kenneth O.},
	booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
	date-added = {2018-11-12 23:49:41 +1300},
	date-modified = {2018-11-12 23:49:51 +1300},
	doi = {10.1145/2908812.2908916},
	isbn = {978-1-4503-4206-3},
	keywords = {Evolutionary, artificial intelligence, deep learning, machine learning, neural networks, pattern recognition and classification},
	location = {Denver, Colorado, USA},
	numpages = {8},
	pages = {477--484},
	publisher = {ACM},
	series = {GECCO '16},
	title = {Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks},
	url = {http://doi.acm.org/10.1145/2908812.2908916},
	year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2908812.2908916},
	Bdsk-Url-2 = {https://doi.org/10.1145/2908812.2908916}}

@article{2018arXiv180703247L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703247L},
	archiveprefix = {arXiv},
	author = {{Liu}, R. and {Lehman}, J. and {Molino}, P. and {Petroski Such}, F. and {Frank}, E. and {Sergeev}, A. and {Yosinski}, J.},
	date-added = {2018-11-12 20:13:43 +1300},
	date-modified = {2018-11-12 20:14:06 +1300},
	eprint = {1807.03247},
	journal = {ArXiv e-prints},
	keywords = {CNN, Computer Vision, Pattern Recognition, Machine Learning, Statistics, Machine Learning},
	month = jul,
	primaryclass = {cs.CV},
	title = {{An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution}},
	year = 2018}

@article{2017arXiv171206564Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171206564Z},
	archiveprefix = {arXiv},
	author = {{Zhang}, X. and {Clune}, J. and {Stanley}, K.~O.},
	date-added = {2018-11-12 19:47:24 +1300},
	date-modified = {2018-11-12 19:47:36 +1300},
	eprint = {1712.06564},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary, Neural and Evolutionary Computing},
	month = dec,
	title = {{On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent}},
	year = 2017}

@article{Assuncao2018,
	abstract = {Deep evolutionary network structured representation (DENSER) is a novel evolutionary approach for the automatic generation of deep neural networks (DNNs) which combines the principles of genetic algorithms (GAs) with those of dynamic structured grammatical evolution (DSGE). The GA-level encodes the macro structure of evolution, i.e., the layers, learning, and/or data augmentation methods (among others); the DSGE-level specifies the parameters of each GA evolutionary unit and the valid range of the parameters. The use of a grammar makes DENSER a general purpose framework for generating DNNs: one just needs to adapt the grammar to be able to deal with different network and layer types, problems, or even to change the range of the parameters. DENSER is tested on the automatic generation of convolutional neural networks (CNNs) for the CIFAR-10 dataset, with the best performing networks reaching accuracies of up to 95.22{\%}. Furthermore, we take the fittest networks evolved on the CIFAR-10, and apply them to classify MNIST, Fashion-MNIST, SVHN, Rectangles, and CIFAR-100. The results show that the DNNs discovered by DENSER during evolution generalise, are robust, and scale. The most impressive result is the 78.75{\%} classification accuracy on the CIFAR-100 dataset, which, to the best of our knowledge, sets a new state-of-the-art on methods that seek to automatically design CNNs.},
	author = {Assun{\c{c}}ao, Filipe and Louren{\c{c}}o, Nuno and Machado, Penousal and Ribeiro, Bernardete},
	date-added = {2018-11-12 17:44:52 +1300},
	date-modified = {2018-11-12 17:45:26 +1300},
	day = {27},
	doi = {10.1007/s10710-018-9339-y},
	issn = {1573-7632},
	journal = {Genetic Programming and Evolvable Machines},
	keywords = {Evolutionary, Deep Neural Network, Neuroevolution},
	month = {Sep},
	title = {DENSER: deep evolutionary network structured representation},
	url = {https://doi.org/10.1007/s10710-018-9339-y},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-018-9339-y}}

@article{Mirjalili_2012,
	author = {SeyedAli Mirjalili and Siti Zaiton Mohd Hashim},
	date-added = {2018-11-09 11:12:56 +1300},
	date-modified = {2018-11-09 11:13:25 +1300},
	doi = {10.7763/ijmlc.2012.v2.114},
	journal = {International Journal of Machine Learning and Computing},
	keywords = {Other, Learning algorithm, Physics},
	pages = {204--208},
	publisher = {{EJournal} Publishing},
	title = {{BMOA}: Binary Magnetic Optimization Algorithm},
	url = {https://doi.org/10.7763%2Fijmlc.2012.v2.114},
	year = 2012,
	Bdsk-Url-1 = {https://doi.org/10.7763%2Fijmlc.2012.v2.114},
	Bdsk-Url-2 = {https://doi.org/10.7763/ijmlc.2012.v2.114}}

@inproceedings{8056823,
	abstract = {Convolutional neural networks (CNNs) are deployed in a wide range of image recognition, scene segmentation and object detection applications. Achieving state of the art accuracy in CNNs often results in large models and complex topologies that require significant compute resources to complete in a timely manner. Binarised neural networks (BNNs) have been proposed as an optimised variant of CNNs, which constrain the weights and activations to +1 or -1 and thus offer compact models and lower computational complexity per operation. This paper presents a high performance BNN accelerator on the Intel{\textregistered}Xeon+FPGA{\texttrademark} platform. The proposed accelerator is designed to take advantage of the Xeon+FPGA system in a way that a specialised FPGA architecture can be targeted for the most compute intensive parts of the BNN whilst other parts of the topology can be handled by the Xeon{\texttrademark} CPU. The implementation is evaluated by comparing the raw compute performance and energy efficiency for key layers in standard CNN topologies against an Nvidia Titan X Pascal GPU and other published FPGA BNN accelerators. The results show that our single-package integrated Arria{\texttrademark} 10 FPGA accelerator coupled with a high-end Xeon CPU can offer comparable performance and better energy efficiency than a high-end discrete Titan X GPU card. In addition, our solution delivers the best performance compared to previous BNN FPGA implementations.},
	author = {D. J. M. Moss and E. Nurvitadhi and J. Sim and A. Mishra and D. Marr and S. Subhaschandra and P. H. W. Leong},
	booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
	date-added = {2018-11-05 22:28:08 +1300},
	date-modified = {2018-12-06 14:59:09 +1300},
	doi = {10.23919/FPL.2017.8056823},
	issn = {1946-1488},
	keywords = {BNN, CNN, FPGA, computational complexity;field programmable gate arrays;graphics processing units;image recognition;neural nets;object detection;high performance binary neural networks;convolutional neural networks;CNN;Xeon CPU;Intel Xeon+FPGA platform;FPGA BNN accelerators;Nvidia Titan X Pascal GPU;specialised FPGA architecture;Xeon+FPGA system;high performance BNN accelerator;lower computational complexity;complex topologies;object detection applications;scene segmentation;image recognition;Field programmable gate arrays;Graphics processing units;Topology;Computer architecture;Performance evaluation;Network topology;IP networks},
	month = {Sept},
	pages = {1-4},
	title = {High performance binary neural networks on the Xeon-FPGA platform},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/FPL.2017.8056823}}

@article{2017arXiv170402081J,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170402081J},
	archiveprefix = {arXiv},
	author = {{Javad Shafiee}, M. and {Barshan}, E. and {Wong}, A.},
	date-added = {2018-10-29 11:34:29 +0000},
	date-modified = {2018-10-29 11:34:29 +0000},
	eprint = {1704.02081},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	month = apr,
	title = {{Evolution in Groups: A deeper look at synaptic cluster driven evolution of deep neural networks}},
	year = 2017}

@article{2018arXiv180905989W,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180905989W},
	archiveprefix = {arXiv},
	author = {{Wong}, A. and {Javad Shafiee}, M. and {Chwyl}, B. and {Li}, F.},
	date-added = {2018-10-29 06:56:37 +0000},
	date-modified = {2018-10-29 06:57:00 +0000},
	eprint = {1809.05989},
	journal = {ArXiv e-prints},
	keywords = {NN; Neural and Evolutionary Computing; Artificial Intelligence, Computer Vision; Pattern Recognition},
	month = sep,
	title = {{FermiNets: Learning generative machines to generate efficient neural networks via generative synthesis}},
	year = 2018}

@article{2016arXiv160604393J,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604393J},
	archiveprefix = {arXiv},
	author = {{Javad Shafiee}, M. and {Mishra}, A. and {Wong}, A.},
	date-added = {2018-10-29 06:49:10 +0000},
	date-modified = {2018-10-29 06:49:40 +0000},
	eprint = {1606.04393},
	journal = {ArXiv e-prints},
	keywords = {NN; Computer Vision; Pattern Recognition; Machine Learning; Neural and Evolutionary Computing; Statistics; Machine Learning},
	month = jun,
	primaryclass = {cs.CV},
	title = {{Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks}},
	year = 2016}

@article{2015arXiv151108228K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151108228K},
	archiveprefix = {arXiv},
	author = {{Kaiser}, {\L}. and {Sutskever}, I.},
	date-added = {2018-10-28 08:28:38 +0000},
	date-modified = {2018-10-28 08:28:51 +0000},
	eprint = {1511.08228},
	journal = {ArXiv e-prints},
	keywords = {NN; Machine Learning, Neural and Evolutionary Computing},
	month = nov,
	title = {{Neural GPUs Learn Algorithms}},
	year = 2015}

@article{2015arXiv150205477S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205477S},
	archiveprefix = {arXiv},
	author = {{Schulman}, J. and {Levine}, S. and {Moritz}, P. and {Jordan}, M.~I. and {Abbeel}, P.},
	date-added = {2018-10-26 09:31:15 +0000},
	date-modified = {2018-10-26 09:31:24 +0000},
	eprint = {1502.05477},
	journal = {ArXiv e-prints},
	keywords = {DNN; Machine Learning},
	month = feb,
	title = {{Trust Region Policy Optimization}},
	year = 2015,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBfLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRGVjZW50cmFsaXplZC9EaXN0cmlidXRlZDpEZWNlbnRyYWxpemVkIGFuZCBBc3luY2hyb25vdXMgQWxnb3JpdGhtcy5iaWJPEQI6AAAAAAI6AAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRGlzdHJpYnV0ZWQvRGVjZW50I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAADURlY2VudHJhbGl6ZWQAAAIAdS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpEZWNlbnRyYWxpemVkOkRpc3RyaWJ1dGVkL0RlY2VudHJhbGl6ZWQgYW5kIEFzeW5jaHJvbm91cyBBbGdvcml0aG1zLmJpYgAADgB0ADkARABpAHMAdAByAGkAYgB1AHQAZQBkAC8ARABlAGMAZQBuAHQAcgBhAGwAaQB6AGUAZAAgAGEAbgBkACAAQQBzAHkAbgBjAGgAcgBvAG4AbwB1AHMAIABBAGwAZwBvAHIAaQB0AGgAbQBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBzVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9EZWNlbnRyYWxpemVkL0Rpc3RyaWJ1dGVkOkRlY2VudHJhbGl6ZWQgYW5kIEFzeW5jaHJvbm91cyBBbGdvcml0aG1zLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAhgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAALE}}

@article{2016arXiv160909106H,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160909106H},
	archiveprefix = {arXiv},
	author = {{Ha}, D. and {Dai}, A. and {Le}, Q.~V.},
	date-added = {2018-10-26 09:08:27 +0000},
	date-modified = {2018-10-26 09:08:35 +0000},
	eprint = {1609.09106},
	journal = {ArXiv e-prints},
	keywords = {CNN; Machine Learning},
	month = sep,
	title = {{HyperNetworks}},
	year = 2016}

@inproceedings{7780459,
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8; deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	author = {K. He and X. Zhang and S. Ren and J. Sun},
	booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-10-25 23:41:31 +0000},
	date-modified = {2018-12-01 13:39:34 +1300},
	doi = {10.1109/CVPR.2016.90},
	issn = {1063-6919},
	keywords = {CNN, image classification; AI; neural nets;object detection, RESNET},
	month = {June},
	pages = {770-778},
	title = {Deep Residual Learning for Image Recognition},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2016.90}}

@article{726791,
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
	date-added = {2018-10-25 01:06:25 +0000},
	date-modified = {2018-10-25 01:06:33 +0000},
	doi = {10.1109/5.726791},
	issn = {0018-9219},
	journal = {Proceedings of the IEEE},
	keywords = {NN; optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
	month = {Nov},
	number = {11},
	pages = {2278-2324},
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	year = {1998},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBVLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0hhbW1pbmcgQ2x1c3RlcmluZy0gQSBOZXcgQXBwcm9hY2ggdG8gUnVsZSBFeHRyYWN0aW9uLmJpYk8RAhwAAAAAAhwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IYW1taW5nIENsdXN0ZXJpbmcjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADQk5OAAACAGsvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Qk5OOkhhbW1pbmcgQ2x1c3RlcmluZy0gQSBOZXcgQXBwcm9hY2ggdG8gUnVsZSBFeHRyYWN0aW9uLmJpYgAADgB0ADkASABhAG0AbQBpAG4AZwAgAEMAbAB1AHMAdABlAHIAaQBuAGcALQAgAEEAIABOAGUAdwAgAEEAcABwAHIAbwBhAGMAaAAgAHQAbwAgAFIAdQBsAGUAIABFAHgAdAByAGEAYwB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBpVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9CTk4vSGFtbWluZyBDbHVzdGVyaW5nLSBBIE5ldyBBcHByb2FjaCB0byBSdWxlIEV4dHJhY3Rpb24uYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAB8AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAApw=},
	Bdsk-Url-1 = {https://doi.org/10.1109/5.726791}}

@inproceedings{7966159,
	abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
	author = {X. Chen and X. Hu and H. Zhou and N. Xu},
	booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
	date-added = {2018-10-24 07:44:36 +0000},
	date-modified = {2018-10-24 07:44:36 +0000},
	doi = {10.1109/IJCNN.2017.7966159},
	issn = {2161-4407},
	keywords = {application specific integrated circuits;convolution;field programmable gate arrays;fixed point arithmetic;floating point arithmetic;neural nets;FxpNet;deep convolutional neural network;fixed-point representation;bit-width arithmetics;forward pass;backward pass;floating-point values;binarized neural networks;quantized neural networks;fixed-point primal weights;low resolution fixed-point values;fixed-point primal parameters;FPGAs;ASICs;integer batch normalization;IBN;fixed-point ADAM;FxpADAM;CIFAR-10 dataset;12-bit primal parameters;12-bit gradients;Quantization (signal);Training;Field programmable gate arrays;Neural networks;Convolution;Kernel;Acceleration},
	month = {May},
	pages = {2494-2501},
	title = {FxpNet: Training a deep convolutional neural network in fixed-point representation},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2017.7966159}}

@inproceedings{Nakahara:2018:LYB:3174243.3174266,
	acmid = {3174266},
	address = {New York, NY, USA},
	author = {Nakahara, Hiroki and Yonekawa, Haruyoshi and Fujii, Tomoya and Sato, Shimpei},
	booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-10-22 09:26:07 +0000},
	date-modified = {2018-10-22 09:26:17 +0000},
	doi = {10.1145/3174243.3174266},
	isbn = {978-1-4503-5614-5},
	keywords = {BNN; binarized deep neural network, convolutional deep neural network, object detection},
	location = {Monterey, CALIFORNIA, USA},
	numpages = {10},
	pages = {31--40},
	publisher = {ACM},
	series = {FPGA '18},
	title = {A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA},
	url = {http://doi.acm.org/10.1145/3174243.3174266},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3174243.3174266},
	Bdsk-Url-2 = {https://doi.org/10.1145/3174243.3174266}}

@inproceedings{Guo2018FBNAAF,
	author = {P. Guo and H. Ma and Ruizhi Chen and Pin Li and Shaolin Xie and Donglin Wang},
	booktitle = {FPL 2018},
	date-added = {2018-10-22 09:01:32 +0000},
	date-modified = {2018-10-22 09:11:30 +0000},
	keywords = {BNN, Neural Network, FPGA, Accelerator},
	organization = {Trinity College, Dublin},
	title = {FBNA: A Fully Binarized Neural Network Accelerator},
	volume = {1},
	year = {2018}}

@inproceedings{Yang:2018:FOB:3218603.3218615,
	acmid = {3218615},
	address = {New York, NY, USA},
	articleno = {50},
	author = {Yang, Li and He, Zhezhi and Fan, Deliang},
	booktitle = {Proceedings of the International Symposium on Low Power Electronics and Design},
	date-added = {2018-10-22 05:09:45 +0000},
	date-modified = {2018-10-22 05:09:59 +0000},
	doi = {10.1145/3218603.3218615},
	isbn = {978-1-4503-5704-3},
	keywords = {BNN; Binarized convolutional neural network (BNN), Convolutional neural network (CNN), field-programmable gate array (FPGA)},
	location = {Seattle, WA, USA},
	numpages = {6},
	pages = {50:1--50:6},
	publisher = {ACM},
	series = {ISLPED '18},
	title = {A Fully Onchip Binarized Convolutional Neural Network FPGA Impelmentation with Accurate Inference},
	url = {http://doi.acm.org/10.1145/3218603.3218615},
	year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3218603.3218615},
	Bdsk-Url-2 = {https://doi.org/10.1145/3218603.3218615}}

@article{2018arXiv180903368P,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180903368P},
	archiveprefix = {arXiv},
	author = {{Peters}, J.~W.~T. and {Welling}, M.},
	date-added = {2018-10-22 01:43:56 +0000},
	date-modified = {2018-10-22 01:44:09 +0000},
	eprint = {1809.03368},
	journal = {ArXiv e-prints},
	keywords = {BNN; Machine Learning; Statistics},
	month = sep,
	title = {{Probabilistic Binary Neural Networks}},
	year = 2018}

@article{2018arXiv180909244B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180909244B},
	archiveprefix = {arXiv},
	author = {{Baluja}, S. and {Marwood}, D. and {Covell}, M. and {Johnston}, N.},
	date-added = {2018-10-22 01:40:27 +0000},
	date-modified = {2018-10-22 01:40:43 +0000},
	eprint = {1809.09244},
	journal = {ArXiv e-prints},
	keywords = {BNN; Machine Learning; Statistics},
	month = sep,
	title = {{No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference}},
	year = 2018}

@article{2018arXiv181002068F,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181002068F},
	archiveprefix = {arXiv},
	author = {{Fu}, C. and {Zhu}, S. and {Su}, H. and {Lee}, C.-E. and {Zhao}, J.},
	date-added = {2018-10-22 01:15:11 +0000},
	date-modified = {2018-10-22 01:15:49 +0000},
	eprint = {1810.02068},
	journal = {ArXiv e-prints},
	keywords = {BNN; Machine Learning; Artificial Intelligence; Hardware Architecture; Computer Vision; Pattern Recognition; Statistics},
	month = oct,
	title = {{Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA}},
	year = 2018}

@article{2017arXiv170302660R,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170302660R},
	archiveprefix = {arXiv},
	author = {{Rajeswaran}, A. and {Lowrey}, K. and {Todorov}, E. and {Kakade}, S.},
	date-added = {2018-10-19 10:46:17 +0000},
	date-modified = {2018-10-19 10:46:39 +0000},
	eprint = {1703.02660},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary, Machine Learning, Artificial Intelligence, Robotics, Systems and Control},
	month = mar,
	title = {{Towards Generalization and Simplicity in Continuous Control}},
	year = 2017}

@article{2017arXiv171206567P,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171206567P},
	archiveprefix = {arXiv},
	author = {{Petroski Such}, F. and {Madhavan}, V. and {Conti}, E. and {Lehman}, J. and {Stanley}, K.~O. and {Clune}, J.},
	date-added = {2018-10-19 10:41:55 +0000},
	date-modified = {2019-01-11 17:06:46 +1300},
	eprint = {1712.06567},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary, Neural and Evolutionary Computing, Machine Learning; Uber},
	month = dec,
	title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
	year = 2017}

@article{2018arXiv180307055M,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180307055M},
	archiveprefix = {arXiv},
	author = {{Mania}, H. and {Guy}, A. and {Recht}, B.},
	date-added = {2018-10-19 10:39:53 +0000},
	date-modified = {2018-10-19 10:40:20 +0000},
	eprint = {1803.07055},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary; Machine Learning, Artificial Intelligence, Mathematics, Optimization and Control, Statistics, Machine Learning},
	month = mar,
	title = {{Simple random search provides a competitive approach to reinforcement learning}},
	year = 2018}

@article{2017arXiv171003748B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171003748B},
	archiveprefix = {arXiv},
	author = {{Bansal}, T. and {Pachocki}, J. and {Sidor}, S. and {Sutskever}, I. and {Mordatch}, I.},
	date-added = {2018-10-19 10:33:38 +0000},
	date-modified = {2018-10-19 10:34:00 +0000},
	eprint = {1710.03748},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary; Artificial Intelligence},
	month = oct,
	primaryclass = {cs.AI},
	title = {{Emergent Complexity via Multi-Agent Competition}},
	year = 2017}

@inproceedings{pmlr-v37-schaul15,
	abstract = {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
	address = {Lille, France},
	author = {Tom Schaul and Daniel Horgan and Karol Gregor and David Silver},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	date-added = {2018-10-19 10:22:36 +0000},
	date-modified = {2018-10-19 10:23:02 +0000},
	editor = {Francis Bach and David Blei},
	keywords = {Evolutionary, Google, DeepMind, Neural Network},
	month = {07--09 Jul},
	pages = {1312--1320},
	pdf = {http://proceedings.mlr.press/v37/schaul15.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Universal Value Function Approximators},
	url = {http://proceedings.mlr.press/v37/schaul15.html},
	volume = {37},
	year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v37/schaul15.html}}

@article{2018arXiv180403867P,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403867P},
	archiveprefix = {arXiv},
	author = {{Prabhu}, A. and {Batchu}, V. and {Gajawada}, R. and {Aurobindo Munagala}, S. and {Namboodiri}, A.},
	date-added = {2018-10-13 07:48:52 +0000},
	date-modified = {2018-10-13 07:49:08 +0000},
	eprint = {1804.03867},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Vision; Pattern Recognition},
	month = apr,
	primaryclass = {cs.CV},
	title = {{Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory}},
	year = 2018}

@inproceedings{bmxnet,
	acmid = {3129393},
	address = {New York, NY, USA},
	annote = {https://arxiv.org/abs/1705.09864
https://github.com/hpi-xnor/BMXNet
},
	author = {Yang, Haojin and Fritzsche, Martin and Bartz, Christian and Meinel, Christoph},
	booktitle = {Proceedings of the 2017 ACM on Multimedia Conference},
	date-added = {2018-10-13 07:42:15 +0000},
	date-modified = {2018-10-13 07:45:31 +0000},
	doi = {10.1145/3123266.3129393},
	isbn = {978-1-4503-4906-2},
	keywords = {BNN; binary neural networks, computer vision, machine learning, open source},
	location = {Mountain View, California, USA},
	numpages = {4},
	pages = {1209--1212},
	publisher = {ACM},
	series = {MM '17},
	title = {BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet},
	url = {http://doi.acm.org/10.1145/3123266.3129393},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3123266.3129393},
	Bdsk-Url-2 = {https://doi.org/10.1145/3123266.3129393}}

@inproceedings{8429420,
	abstract = {Deep neural networks have achieved impressive results in computer vision and machine learning. Unfortunately, state-of-the-art networks are extremely compute-and memory-intensive which makes them unsuitable for mW-devices such as IoT end-nodes. Aggressive quantization of these networks dramatically reduces the computation and memory footprint. Binary-weight neural networks (BWNs) follow this trend, pushing weight quantization to the limit. Hardware accelerators for BWNs presented up to now have focused on core efficiency, disregarding I/O bandwidth and system-level efficiency that are crucial for deployment of accelerators in ultra-low power devices. We present Hyperdrive: a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel binary-weight streaming approach, and capable of handling high-resolution images by virtue of its systolic-scalable architecture. We achieve a 5.9 TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than state-of-the-art BNN accelerators, even if our core uses resource-intensive FP16 arithmetic for increased robustness.},
	author = {R. Andri and L. Cavigelli and D. Rossi and L. Benini},
	booktitle = {2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},
	date-added = {2018-10-12 06:40:47 +0000},
	date-modified = {2018-10-12 06:40:54 +0000},
	doi = {10.1109/ISVLSI.2018.00099},
	issn = {2159-3477},
	keywords = {BNN; computer vision;feedforward neural nets;Internet of Things;learning (artificial intelligence);low-power electronics;microprocessor chips;hardware accelerators;core efficiency;I/O bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable architecture;state-of-the-art BNN accelerators;resource-intensive FP16 arithmetic;TOp/s/W system-level efficiency;binary-weight streaming approach;BWN;hyperdrive;weight quantization;binary-weight neural networks;memory footprint;aggressive quantization;mW-devices;memory-intensive;machine learning;computer vision;impressive results;deep neural networks;mW IoT end-nodes;systolically scalable binary-weight CNN inference engine;Frequency modulation;Computer architecture;Quantization (signal);Neural networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary Weights Neural Networks;IoT},
	month = {July},
	pages = {509-515},
	title = {Hyperdrive: A Systolically Scalable Binary-Weight CNN Inference Engine for mW IoT End-Nodes},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISVLSI.2018.00099}}

@inproceedings{8457633,
	abstract = {This paper proposes ReBNet, an end-to-end framework for training reconfigurable binary neural networks on software and developing efficient accelerators for execution on FPGA. Binary neural networks offer an intriguing opportunity for deploying large-scale deep learning models on resource-constrained devices. Binarization reduces the memory footprint and replaces the power-hungry matrix-multiplication with light-weight XnorPopcount operations. However, binary networks suffer from a degraded accuracy compared to their fixed-point counterparts. We show that the state-of-the-art methods for optimizing binary networks accuracy, significantly increase the implementation cost and complexity. To compensate for the degraded accuracy while adhering to the simplicity of binary networks, we devise the first reconfigurable scheme that can adjust the classification accuracy based on the application. Our proposition improves the classification accuracy by representing features with multiple levels of residual binarization. Unlike previous methods, our approach does not exacerbate the area cost of the hardware accelerator. Instead, it provides a tradeoff between throughput and accuracy while the area overhead of multi-level binarization is negligible.},
	author = {M. Ghasemzadeh and M. Samragh and F. Koushanfar},
	booktitle = {2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	date-added = {2018-10-12 06:35:49 +0000},
	date-modified = {2018-10-12 06:36:55 +0000},
	doi = {10.1109/FCCM.2018.00018},
	issn = {2576-2621},
	keywords = {BNN; field programmable gate arrays;learning (artificial intelligence);matrix multiplication;neural nets;ReBNet;residual binarized neural network;large-scale deep learning models;power-hungry matrix-multiplication;light-weight XnorPopcount operations;fixed-point counterparts;FPGA;memory footprint;hardware accelerator;Hardware;Neural networks;Training;Field programmable gate arrays;Parallel processing;Cost function;Libraries;Deep neural networks;Reconfigurable computing;Domain customized computing;Binary neural network;Residual binarization},
	month = {April},
	pages = {57-64},
	title = {ReBNet: Residual Binarized Neural Network},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FCCM.2018.00018}}

@inproceedings{8461456,
	abstract = {With the ever growing popularity of deep learning, the tremendous complexity of deep neural networks is becoming problematic when one considers inference on resource constrained platforms. Binary networks have emerged as a potential solution, however, they exhibit a fundamentallimi-tation in realizing gradient-based learning as their activations are non-differentiable. Current work has so far relied on approximating gradients in order to use the back-propagation algorithm via the straight through estimator (STE). Such approximations harm the quality of the training procedure causing a noticeable gap in accuracy between binary neural networks and their full precision baselines. We present a novel method to train binary activated neural networks using true gradient-based learning. Our idea is motivated by the similarities between clipping and binary activation functions. We show that our method has minimal accuracy degradation with respect to the full precision baseline. Finally, we test our method on three benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show that continuous binarization using true gradient-based learning achieves an accuracy within 1.5% of the floating-point baseline, as compared to accuracy drops as high as 6% when training the same binary activated network using the STE.},
	author = {C. Sakr and J. Choi and Z. Wang and K. Gopalakrishnan and N. Shanbhag},
	booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	date-added = {2018-10-12 06:28:18 +0000},
	date-modified = {2018-10-12 06:28:24 +0000},
	doi = {10.1109/ICASSP.2018.8461456},
	issn = {2379-190X},
	keywords = {BNN; gradient methods;learning (artificial intelligence);neural nets;gradient-based training;deep binary activated neural networks;continuous binarization;deep learning;tremendous complexity;resource constrained platforms;training procedure;binary activation functions;minimal accuracy degradation;gradient-based learning;back-propagation algorithm;straight through estimator;floating-point baseline;STE;Training;Neural networks;Complexity theory;Machine learning;Stochastic processes;Perturbation methods;Approximation algorithms;deep learning;binary neural networks;activation functions},
	month = {April},
	pages = {2346-2350},
	title = {True Gradient-Based Training of Deep Binary Activated Neural Networks Via Continuous Binarization},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICASSP.2018.8461456}}

@inproceedings{8373076,
	abstract = {Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8-32x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations. We present XNORBIN, a flexible accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse supporting even non-trivial network topologies with large feature map volumes. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.},
	author = {A. A. Bahou and G. Karunaratne and R. Andri and L. Cavigelli and L. Benini},
	booktitle = {2018 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS)},
	date-added = {2018-10-12 06:22:47 +0000},
	date-modified = {2018-10-12 06:22:55 +0000},
	doi = {10.1109/CoolChips.2018.8373076},
	issn = {2473-4683},
	keywords = {BNN; embedded systems;low-power electronics;neural nets;power aware computing;binary convolutional neural networks;off-chip memory;low-power embedded systems;extreme quantization;flexible accelerator;aggressive data;nontrivial network topologies;feature map volumes;energy efficiency;hardware accelerator;binary CNN;weight binarization;collapsing energy-intensive sum-of-products;XNOR-and-popcount operations;Hardware;Convolutional neural networks;System-on-chip;Computational modeling;Computer architecture;Program processors},
	month = {April},
	pages = {1-3},
	title = {XNORBIN: A 95 TOp/s/W hardware accelerator for binary convolutional neural networks},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CoolChips.2018.8373076}}

@article{8103902,
	abstract = {Binary weight convolutional neural networks (BCNNs) can achieve near state-of-the-art classification accuracy and have far less computation complexity compared with traditional CNNs using high-precision weights. Due to their binary weights, BCNNs are well suited for vision-based Internet-of-Things systems being sensitive to power consumption. BCNNs make it possible to achieve very high throughput with moderate power dissipation. In this paper, an energy-efficient architecture for BCNNs is proposed. It fully exploits the binary weights and other hardware-friendly characteristics of BCNNs. A judicious processing schedule is proposed so that off-chip I/O access is minimized and activations are maximally reused. To significantly reduce the critical path delay, we introduce optimized compressor trees and approximate binary multipliers with two novel compensation schemes. The latter is able to save significant hardware resource, and almost no computation accuracy is compromised. Taking advantage of error resiliency of BCNNs, an innovative approximate adder is developed, which significantly reduces the silicon area and data path delay. Thorough error analysis and extensive experimental results on several data sets show that the approximate adders in the data path cause negligible accuracy loss. Moreover, algorithmic transformations for certain layers of BCNNs and a memory-efficient quantization scheme are incorporated to further reduce the energy cost and on-chip storage requirement. Finally, the proposed BCNN hardware architecture is implemented with the SMIC 130-nm technology. The postlayout results demonstrate that our design can achieve an energy efficiency over 2.0TOp/s/W when scaled to 65 nm, which is more than two times better than the prior art.},
	author = {Y. Wang and J. Lin and Z. Wang},
	date-added = {2018-10-12 06:20:05 +0000},
	date-modified = {2018-10-12 06:20:11 +0000},
	doi = {10.1109/TVLSI.2017.2767624},
	issn = {1063-8210},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {BNN;adders;convolution;energy conservation;error analysis;feedforward neural nets;multiplying circuits;neural chips;trees (mathematics);energy-efficient architecture;binary weight convolutional neural networks;high-precision weights;binary weights;approximate binary multipliers;BCNN hardware architecture;energy efficiency;classification accuracy;data path delay;processing schedule;off-chip I/O access;critical path delay;optimized compressor trees;approximate adder;error analysis;memory-efficient quantization;on-chip storage requirement;size 65.0 nm;Computer architecture;Hardware;Neural networks;Neurons;Adders;Quantization (signal);Convolution;Approximate computing;binary weight convolutional neural network (BCNN) architecture;convolutional neural network (CNN);deep learning;energy-efficient design;signal processing;VLSI architecture},
	month = {Feb},
	number = {2},
	pages = {280-293},
	title = {An Energy-Efficient Architecture for Binary Weight Convolutional Neural Networks},
	volume = {26},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TVLSI.2017.2767624}}

@inproceedings{8425178,
	abstract = {Deep learning has revolutionized computer vision and other fields since its big bang in 2012. However, it is challenging to deploy Deep Neural Networks (DNNs) into real-world applications due to their high computational complexity. Binary Neural Networks (BNNs) dramatically reduce computational complexity by replacing most arithmetic operations with bitwise operations. Existing implementations of BNNs have been focusing on GPU or FPGA, and using the conventional image-to-column method that doesn't perform well for binary convolution due to low arithmetic intensity and unfriendly pattern for bitwise operations. We propose BitFlow, a gemm-operator-network three-level optimization framework for fully exploiting the computing power of BNNs on CPU. BitFlow features a new class of algorithm named PressedConv for efficient binary convolution using locality-aware layout and vector parallelism. We evaluate BitFlow with the VGG network. On a single core of Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN implementations, and 11.5x speedup over counterpart full-precision DNNs. Over 64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart full-precision DNNs on GPU (GTX 1080).},
	author = {Y. Hu and J. Zhai and D. Li and Y. Gong and Y. Zhu and W. Liu and L. Su and J. Jin},
	booktitle = {2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	date-added = {2018-10-12 06:15:46 +0000},
	date-modified = {2018-10-12 06:16:05 +0000},
	doi = {10.1109/IPDPS.2018.00034},
	issn = {1530-2075},
	keywords = {BNN;computational complexity;field programmable gate arrays;graphics processing units;learning (artificial intelligence);multiprocessing systems;neural nets;optimisation;parallel processing;vector parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big bang;Deep Neural Networks;high computational complexity;Binary Neural Networks;BNNs;arithmetic operations;bitwise operations;image-to-column method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow features;efficient binary convolution;VGG network;counterpart full-precision DNNs;GPU;Convolution;Neural networks;Layout;Parallel processing;Acceleration;Graphics processing units;Machine learning;Network Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi},
	month = {May},
	pages = {244-253},
	title = {BitFlow: Exploiting Vector Parallelism for Binary Neural Networks on CPU},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPDPS.2018.00034}}

@article{8226999,
	abstract = {A versatile reconfigurable accelerator architecture for binary/ternary deep neural networks is presented. In-memory neural network processing without any external data accesses, sustained by the symmetry and simplicity of the computation of the binary/ternaty neural network, improves the energy efficiency dramatically. The prototype chip is fabricated, and it achieves 1.4 TOPS (tera operations per second) peak performance with 0.6-W power consumption at 400-MHz clock. The application examination is also conducted.},
	author = {K. Ando and K. Ueyoshi and K. Orimo and H. Yonekawa and S. Sato and H. Nakahara and S. Takamaeda-Yamazaki and M. Ikebe and T. Asai and T. Kuroda and M. Motomura},
	date-added = {2018-10-12 06:15:46 +0000},
	date-modified = {2018-10-12 06:15:58 +0000},
	doi = {10.1109/JSSC.2017.2778702},
	issn = {0018-9200},
	journal = {IEEE Journal of Solid-State Circuits},
	keywords = {BNN;low-power electronics;neural nets;random-access storage;reconfigurable architectures;deep neural network accelerator;binary/ternary deep neural networks;In-memory neural network processing;binary/ternaty neural network;BRein memory;single-chip binary/ternary reconfigurable in-memory;reconfigurable accelerator architecture;external data access;power 0.6 W;frequency 400 MHz;Biological neural networks;Random access memory;Memory management;Neurons;System-on-chip;Parallel processing;Binary neural networks;in-memory processing;near-memory processing;neural networks;reconfigurable array;ternary neural networks},
	month = {April},
	number = {4},
	pages = {983-994},
	title = {BRein Memory: A Single-Chip Binary/Ternary Reconfigurable in-Memory Deep Neural Network Accelerator Achieving 1.4 TOPS at 0.6 W},
	volume = {53},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2017.2778702}}

@inproceedings{8394726,
	abstract = {To achieve an advanced Internet of Things (IoT), it is necessary to combine artificial intelligence (AI) with IoT. Compact circuits that can operate AI functions will be useful for this purpose. Therefore, we propose stochastic weights binary neural networks (SWBNN). SWBNNs are more accurate than binary neural networks (BNN) with small circuits. BNNs can be realized with small circuits since binary calculation needs simpler circuits than real number calculation. However, BNNs have lower accuracy than networks with real numbers. Thus, the proposed SWBNNs are BNNs that behave stochastically, which makes them more accurate than BNNs. Moreover, SWBNNs can still be achieved with small circuits since they execute binary calculation. As a result, the accuracy for the test data of SWBNNs is closer to the accuracy for learning data than the accuracy for the test data of BNNs is. Especially when using the CIFAR10 database, the difference in the identification accuracy rate between learning data and test data decreased from 6% for BNNs to 2% for SWBNNs. From results of a field-programmable gate array (FPGA) implementation, circuits of SWBNNs are sufficiently small although they are 10% bigger than those of BNNs. Therefore, SWBNNs are more accurate than BNNs, and the circuit costs ofintroducing stochastic weights are low.},
	author = {Y. Fukuda and T. Kawahara},
	booktitle = {2018 7th International Symposium on Next Generation Electronics (ISNE)},
	date-added = {2018-10-12 06:02:12 +0000},
	date-modified = {2018-10-12 06:02:21 +0000},
	doi = {10.1109/ISNE.2018.8394726},
	issn = {2378-8607},
	keywords = {BNN;field programmable gate arrays;Internet of Things;learning (artificial intelligence);neural nets;stochastic processes;binary calculation;stochastic weights binary neural networks;IoT;BNN;SWBNN;FPGA;Internet of things;artificial intelligence;AI;binary neural networks;CIFAR10 database;field-programmable gate array implementation;Next generation networking;Neural networks;IoT;Neural-network;BNN;FPGA;stochastic},
	month = {May},
	pages = {1-3},
	title = {Stochastic weights binary neural networks on FPGA},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNE.2018.8394726}}

@inproceedings{8457667,
	abstract = {In binary convolutional neural networks (BCNN), arithmetic operations are replaced by bitwise operations and the required memory size is greatly reduced, which is a good opportunity to accelerate training or inference on FPGAs. This paper proposes a BCNN architecture with a single engine that achieves high resource utilization. The proposed design deploys a large number of processing elements in parallel to increase throughput, and a forwarding scheme to increase resource utilization on the existing engine. In addition, we demonstrate a novel reuse scheme to make fully-connected layers exploit the same engine. The proposed design is combined with an inference environment for comparison and implemented on a Xilinx XCVU190 FPGA. The implemented design uses 61k look-up tables (LUTs), 45k flip-flops (FFs), and 13.9Mbit block RAM (BRAM). In addition, it achieves 61.6 GOPS/kLUT at 240MHz, which is 1.16 times higher than that of the best prior BCNN design, even though it uses a single engine without optimal configurations on each layer.},
	author = {S. Kim and R. Rutenbar},
	booktitle = {2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	date-added = {2018-10-12 05:55:29 +0000},
	date-modified = {2018-10-12 05:55:36 +0000},
	doi = {10.1109/FCCM.2018.00052},
	issn = {2576-2621},
	keywords = {BNN; field programmable gate arrays;flip-flops;logic design;neural chips;neural net architecture;random-access storage;table lookup;BRAM;block RAM;flip-flops;look-up tables;inference environment;fully-connected layers;processing elements;BCNN design;memory size;bitwise operations;arithmetic operations;binary convolutional neural networks;effective resource utilization;accelerator design;Xilinx XCVU190 FPGA;forwarding scheme;high resource utilization;BCNN architecture;frequency 240.0 MHz;storage capacity 13.9 Mbit;Field programmable gate arrays;Resource management;Convolutional neural networks;Engines;Computer science;Acceleration;Machine learning;Binary convolutional neural networks;High resource utilization;FPGA},
	month = {April},
	pages = {218-218},
	title = {Accelerator Design with Effective Resource Utilization for Binary Convolutional Neural Networks on an FPGA},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FCCM.2018.00052}}

@article{6847217,
	abstract = {Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.},
	author = {L. Shao and F. Zhu and X. Li},
	date-added = {2018-10-11 23:28:26 +0000},
	date-modified = {2018-10-11 23:30:04 +0000},
	doi = {10.1109/TNNLS.2014.2330900},
	issn = {2162-237X},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {NN; image classification;learning (artificial intelligence);object recognition;visual categorization;transfer learning algorithms;object recognition;image classification;human action recognition;Knowledge transfer;Visualization;Training;Training data;Adaptation models;Learning systems;Testing;Action recognition;image classification;machine learning;object recognition;survey;transfer learning;visual categorization.;Action recognition;image classification;machine learning;object recognition;survey;transfer learning;visual categorization;Algorithms;Humans;Knowledge;Machine Learning;Models, Theoretical;Neural Networks (Computer);Surveys and Questionnaires;Transfer (Psychology);Visual Perception},
	month = {May},
	number = {5},
	pages = {1019-1034},
	title = {Transfer Learning for Visual Categorization: A Survey},
	volume = {26},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2014.2330900}}

@article{2016arXiv160204283L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160204283L},
	archiveprefix = {arXiv},
	author = {{Lacey}, G. and {Taylor}, G.~W. and {Areibi}, S.},
	date-added = {2018-10-10 00:44:17 +0000},
	date-modified = {2018-10-10 00:44:26 +0000},
	eprint = {1602.04283},
	journal = {ArXiv e-prints},
	keywords = {FPGA; Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	month = feb,
	primaryclass = {cs.DC},
	title = {{Deep Learning on FPGAs: Past, Present, and Future}},
	year = 2016}

@article{Gadea-Girones:2018aa,
	abstract = {In the optimization of deep neural networks (DNNs) via evolutionary algorithms (EAs) and the implementation of the training necessary for the creation of the objective function, there is often a trade-off between efficiency and flexibility. Pure software solutions implemented on general-purpose processors tend to be slow because they do not take advantage of the inherent parallelism of these devices, whereas hardware realizations based on heterogeneous platforms (combining central processing units (CPUs), graphics processing units (GPUs) and/or field-programmable gate arrays (FPGAs)) are designed based on different solutions using methodologies supported by different languages and using very different implementation criteria. This paper first presents a study that demonstrates the need for a heterogeneous (CPU-GPU-FPGA) platform to accelerate the optimization of artificial neural networks (ANNs) using genetic algorithms. Second, the paper presents implementations of the calculations related to the individuals evaluated in such an algorithm on different (CPU- and FPGA-based) platforms, but with the same source files written in OpenCL. The implementation of individuals on remote, low-cost FPGA systems on a chip (SoCs) is found to enable the achievement of good efficiency in terms of performance per watt.},
	an = {PMC5982427},
	author = {Gadea-Giron{\'e}s, Rafael and Colom-Palero, Ricardo and Herrero-Bosch, Vicente},
	date = {2018/05/},
	date-added = {2018-10-10 00:41:23 +0000},
	date-modified = {2018-10-10 00:41:45 +0000},
	db = {PMC},
	doi = {10.3390/s18051384},
	isbn = {1424-8220},
	j1 = {Sensors (Basel)},
	journal = {Sensors (Basel, Switzerland)},
	keywords = {Evolutionary; SoC; OpenCL},
	month = {05},
	number = {5},
	pages = {1384},
	publisher = {MDPI},
	title = {Optimization of Deep Neural Networks Using SoCs with OpenCL},
	ty = {JOUR},
	u1 = {29710875{$[$}pmid{$]$}; sensors-18-01384{$[$}PII{$]$}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982427/},
	volume = {18},
	year = {2018},
	year1 = {2018/04/30},
	year2 = {2018/03/08/received},
	year3 = {2018/04/27/accepted},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982427/},
	Bdsk-Url-2 = {https://doi.org/10.3390/s18051384}}

@inproceedings{2017arXiv171205877J,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171205877J},
	archiveprefix = {arXiv},
	author = {{Jacob}, B. and {Kligys}, S. and {Chen}, B. and {Zhu}, M. and {Tang}, M. and {Howard}, A. and {Adam}, H. and {Kalenichenko}, D.},
	booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-10-10 00:07:20 +0000},
	date-modified = {2018-12-04 23:27:14 +1300},
	eprint = {1712.05877},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science - Machine Learning, Statistics - Machine Learning},
	month = jun,
	title = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
	year = 2018}

@inproceedings{Soudry2014ExpectationBP,
	abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient
descent-based methods, such as BackPropagation (BP). Inference in probabilistic
graphical models is often done using variational Bayes methods, such as Expectation
Propagation (EP). We show how an EP based approach can also be used
to train deterministic MNNs. Specifically, we approximate the posterior of the
weights given the data using a ``mean-field'' factorized distribution, in an online
setting. Using online EP and the central limit theorem we find an analytical approximation
to the Bayes update of this posterior, as well as the resulting Bayes
estimates of the weights and outputs.
Despite a different origin, the resulting algorithm, Expectation BackPropagation
(EBP), is very similar to BP in form and efficiency. However, it has several additional
advantages: (1) Training is parameter-free, given initial conditions (prior)
and the MNN architecture. This is useful for large-scale problems, where parameter
tuning is a major challenge. (2) The weights can be restricted to have discrete
values. This is especially useful for implementing trained MNNs in precision limited
hardware chips, thus improving their speed and energy efficiency by several
orders of magnitude.
We test the EBP algorithm numerically in eight binary text classification tasks.
In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning
rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with
binary weights usually perform better than MNNs with continuous (real) weights
- if we average the MNN output using the inferred posterior.},
	annote = {https://www.semanticscholar.org/paper/Expectation-Backpropagation%3A-Parameter-Free-of-with-Soudry-Hubara/0b88ed755c4dcd0ac3d25842a5bbbe4ebcefeeec

https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf},
	author = {Daniel Soudry and Itay Hubara and Ron Meir},
	booktitle = {NIPS},
	date-added = {2018-10-09 23:57:10 +0000},
	date-modified = {2018-10-10 00:00:53 +0000},
	keywords = {DNN; Backpropagation; Discrete weight space; Continuos weight space},
	title = {Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights},
	year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBULi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vU3VydmV5IFJlcG9ydCBvbiBDcnlwdG9ncmFwaHkgQmFzZWQgb24gTmV1cmFsIE5ldHdvcmsuYmliTxECFgAAAAACFgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1N1cnZleSBSZXBvcnQgb24gQyNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAGovOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46U3VydmV5IFJlcG9ydCBvbiBDcnlwdG9ncmFwaHkgQmFzZWQgb24gTmV1cmFsIE5ldHdvcmsuYmliAA4AdAA5AFMAdQByAHYAZQB5ACAAUgBlAHAAbwByAHQAIABvAG4AIABDAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAgAEIAYQBzAGUAZAAgAG8AbgAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAaFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vU3VydmV5IFJlcG9ydCBvbiBDcnlwdG9ncmFwaHkgQmFzZWQgb24gTmV1cmFsIE5ldHdvcmsuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAHsAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAClQ==}}

@article{2018arXiv180607550Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607550Z},
	archiveprefix = {arXiv},
	author = {{Zhu}, S. and {Dong}, X. and {Su}, H.},
	date-added = {2018-10-09 23:07:05 +0000},
	date-modified = {2018-10-09 23:07:12 +0000},
	eprint = {1806.07550},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	month = jun,
	title = {{Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?}},
	year = 2018}

@article{2016arXiv160207360I,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160207360I},
	archiveprefix = {arXiv},
	author = {{Iandola}, F.~N. and {Han}, S. and {Moskewicz}, M.~W. and {Ashraf}, K. and {Dally}, W.~J. and {Keutzer}, K.},
	date-added = {2018-10-01 09:23:08 +0000},
	date-modified = {2018-10-01 09:23:15 +0000},
	eprint = {1602.07360},
	journal = {ArXiv e-prints},
	keywords = {CNN; Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	month = feb,
	primaryclass = {cs.CV},
	title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $\lt$0.5MB model size}},
	year = 2016}

@article{2017arXiv170507175P,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170507175P},
	archiveprefix = {arXiv},
	author = {{Pedersoli}, F. and {Tzanetakis}, G. and {Tagliasacchi}, A.},
	date-added = {2018-09-30 04:37:12 +0000},
	date-modified = {2018-09-30 04:37:18 +0000},
	eprint = {1705.07175},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, 62M45, I.2.6},
	month = may,
	primaryclass = {cs.DC},
	title = {{Espresso: Efficient Forward Propagation for BCNNs}},
	year = 2017}

@article{2018arXiv180700343A,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180700343A},
	archiveprefix = {arXiv},
	author = {{Agrawal}, A. and {Jaiswal}, A. and {Han}, B. and {Srinivasan}, G. and {Roy}, K.},
	date-added = {2018-09-30 04:23:17 +0000},
	date-modified = {2018-09-30 04:23:29 +0000},
	eprint = {1807.00343},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science; Emerging Technologies},
	month = jul,
	title = {{Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays}},
	year = 2018}

@article{2018arXiv180703010C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703010C},
	archiveprefix = {arXiv},
	author = {{Conti}, F. and {Davide Schiavone}, P. and {Benini}, L.},
	date-added = {2018-09-30 04:20:35 +0000},
	date-modified = {2018-09-30 04:20:42 +0000},
	eprint = {1807.03010},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science - Neural and Evolutionary Computing, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	month = jul,
	title = {{XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary Neural Network Inference}},
	year = 2018}

@article{2018arXiv180801990C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180801990C},
	archiveprefix = {arXiv},
	author = {{Cakir}, F. and {He}, K. and {Sclaroff}, S.},
	date-added = {2018-09-30 02:43:46 +0000},
	date-modified = {2018-10-09 23:14:42 +0000},
	eprint = {1808.01990},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	month = aug,
	title = {{Hashing with Binary Matrix Pursuit}},
	year = 2018}

@article{2017arXiv171107971W,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171107971W},
	archiveprefix = {arXiv},
	author = {{Wang}, X. and {Girshick}, R. and {Gupta}, A. and {He}, K.},
	date-added = {2018-09-30 02:43:14 +0000},
	date-modified = {2018-09-30 02:44:18 +0000},
	eprint = {1711.07971},
	journal = {ArXiv e-prints},
	keywords = {CNN; Computer Science - Computer Vision and Pattern Recognition},
	month = nov,
	primaryclass = {cs.CV},
	title = {{Non-local Neural Networks}},
	year = 2017}

@article{2018arXiv180802631Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180802631Z},
	archiveprefix = {arXiv},
	author = {{Zhuang}, B. and {Shen}, C. and {Reid}, I.},
	date-added = {2018-09-30 01:58:28 +0000},
	date-modified = {2018-09-30 01:58:33 +0000},
	eprint = {1808.02631},
	journal = {ArXiv e-prints},
	keywords = {BNN;Computer Science - Computer Vision and Pattern Recognition},
	month = aug,
	primaryclass = {cs.CV},
	title = {{Training Compact Neural Networks with Binary Weights and Low Precision Activations}},
	year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBkLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0RFTlNFUi0gRGVlcCBFdm9sdXRpb25hcnkgTmV0d29yayBTdHJ1Y3R1cmVkIFJlcHJlc2VudGF0aW9uLmJpYk8RAkwAAAAAAkwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9ERU5TRVItIERlZXAgRXZvbHUjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAMRXZvbHV0aW9uYXJ5AAIAei86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpFdm9sdXRpb25hcnk6REVOU0VSLSBEZWVwIEV2b2x1dGlvbmFyeSBOZXR3b3JrIFN0cnVjdHVyZWQgUmVwcmVzZW50YXRpb24uYmliAA4AgAA/AEQARQBOAFMARQBSAC0AIABEAGUAZQBwACAARQB2AG8AbAB1AHQAaQBvAG4AYQByAHkAIABOAGUAdAB3AG8AcgBrACAAUwB0AHIAdQBjAHQAdQByAGUAZAAgAFIAZQBwAHIAZQBzAGUAbgB0AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAeFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0RFTlNFUi0gRGVlcCBFdm9sdXRpb25hcnkgTmV0d29yayBTdHJ1Y3R1cmVkIFJlcHJlc2VudGF0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACLAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAts=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBiLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OL1ZlcnkgZGVlcCBjb252b2x1dGlvbmFsIG5ldHdvcmtzIGZvciBsYXJnZS1zY2FsZSBpbWFnZSByZWNvZ25pdGlvbi5iaWJPEQJOAAAAAAJOAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fVmVyeSBkZWVwIGNvbnZvbHV0I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAA0NOTgAAAgB4LzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkNOTjpWZXJ5IGRlZXAgY29udm9sdXRpb25hbCBuZXR3b3JrcyBmb3IgbGFyZ2Utc2NhbGUgaW1hZ2UgcmVjb2duaXRpb24uYmliAA4AjgBGAFYAZQByAHkAIABkAGUAZQBwACAAYwBvAG4AdgBvAGwAdQB0AGkAbwBuAGEAbAAgAG4AZQB0AHcAbwByAGsAcwAgAGYAbwByACAAbABhAHIAZwBlAC0AcwBjAGEAbABlACAAaQBtAGEAZwBlACAAcgBlAGMAbwBnAG4AaQB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgB2VXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4vVmVyeSBkZWVwIGNvbnZvbHV0aW9uYWwgbmV0d29ya3MgZm9yIGxhcmdlLXNjYWxlIGltYWdlIHJlY29nbml0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACJAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAts=}}

@article{2018arXiv180810631K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180810631K},
	archiveprefix = {arXiv},
	author = {{Krestinskaya}, O. and {Salama}, K.~N. and {Pappachen James}, A.},
	date-added = {2018-09-30 01:58:11 +0000},
	date-modified = {2018-09-30 01:58:18 +0000},
	eprint = {1808.10631},
	journal = {ArXiv e-prints},
	keywords = {BNN;Computer Science - Emerging Technologies, Computer Science - Artificial Intelligence},
	month = aug,
	title = {{Learning in Memristive Neural Network Architectures using Analog Backpropagation Circuits}},
	year = 2018}

@article{2018arXiv180910463B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180910463B},
	archiveprefix = {arXiv},
	author = {{Bethge}, J. and {Yang}, H. and {Bartz}, C. and {Meinel}, C.},
	date-added = {2018-09-30 01:49:26 +0000},
	date-modified = {2018-09-30 01:49:44 +0000},
	eprint = {1809.10463},
	journal = {ArXiv e-prints},
	keywords = {BNN;Computer Science; Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	month = sep,
	title = {{Learning to Train a Binary Neural Network}},
	year = 2018}

@article{2018JInst..13P7027D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018JInst..13P7027D},
	archiveprefix = {arXiv},
	author = {{Duarte}, J. and {Han}, S. and {Harris}, P. and {Jindariani}, S. and {Kreinar}, E. and {Kreis}, B. and {Ngadiuba}, J. and {Pierini}, M. and {Rivera}, R. and {Tran}, N. and {Wu}, Z.},
	date-added = {2018-09-30 01:22:32 +0000},
	date-modified = {2018-10-10 00:48:54 +0000},
	doi = {10.1088/1748-0221/13/07/P07027},
	eprint = {1804.06913},
	journal = {Journal of Instrumentation},
	keywords = {FPGA-Fin; NN; Physics},
	month = jul,
	pages = {P07027},
	primaryclass = {physics.ins-det},
	title = {{Fast inference of deep neural networks in FPGAs for particle physics}},
	volume = 13,
	year = 2018,
	Bdsk-Url-1 = {https://doi.org/10.1088/1748-0221/13/07/P07027}}

@inproceedings{8052915,
	abstract = {As a popular deep learning technique, convolutional neural network has been widely used in many tasks such as image classification and object recognition. Convolutional neural network exploits spatial correlations in the images by performing convolution operations in local receptive fields. Convolutional neural networks are preferred over fully connected neural networks because they have fewer weights and are easier to train. Many research works have been conducted to reduce the computational complexity and memory requirements of convolutional neural network, to make it applicable to the low-power embedded applications with limited memories. This paper presents the architecture design of convolutional neural network with binary weights and activations, also known as binary neural network, on an FPGA platform. Weights and input activations are binarized with only two values, +1 and -1. This reduces all the fixed point multiplication operations in convolutional layers and fully connected layers to 1-bit XNOR operations. The proposed design uses only on-chip memories. Furthermore, an efficient implementation of batch normalization operation is introduced. When evaluating the CIFAR-10 benchmark, the proposed FPGA design can achieve a processing rate of 332,158 images per second with with accuracy of 86.06% using 1-bit quantized weights and activations.},
	author = {Y. Zhou and S. Redkar and X. Huang},
	booktitle = {2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
	date-added = {2018-09-29 09:57:58 +0000},
	date-modified = {2018-09-29 09:58:39 +0000},
	doi = {10.1109/MWSCAS.2017.8052915},
	keywords = {BNN;field programmable gate arrays;fixed point arithmetic;image classification;learning (artificial intelligence);neural nets;object recognition;quantisation (signal);1-bit XNOR operation;CIFAR-10 benchmark;FPGA platform;batch normalization operation;computational complexity reduction;convolutional neural network;deep learning binary neural network;fixed point multiplication operation;local receptive fields;low-power embedded applications;memory requirement reduction;on-chip memories;spatial correlation;Biological neural networks;Convolution;Field programmable gate arrays;Hardware;Memory management;Training},
	month = {Aug},
	pages = {281-284},
	title = {Deep learning binary neural network on an FPGA},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/MWSCAS.2017.8052915}}

@article{5159360,
	abstract = {Implementing linearly nonseparable Boolean functions (non-LSBF) has been an important and yet challenging task due to the extremely high complexity of this kind of functions and the exponentially increasing percentage of the number of non-LSBF in the entire set of Boolean functions as the number of input variables increases. In this paper, an algorithm named DNA-like learning and decomposing algorithm (DNA-like LDA) is proposed, which is capable of effectively implementing non-LSBF. The novel algorithm first trains the DNA-like offset sequence and decomposes non-LSBF into logic XOR operations of a sequence of LSBF, and then determines the weight-threshold values of the multilayer perceptron (MLP) that perform both the decompositions of LSBF and the function mapping the hidden neurons to the output neuron. The algorithm is validated by two typical examples about the problem of approximating the circular region and the well-known &lt;i&gt;n&lt;/i&gt; -bit parity Boolean function (PBF).},
	author = {F. Chen and G. Chen and Q. He and G. He and X. Xu},
	date-added = {2018-09-29 09:56:02 +0000},
	date-modified = {2018-09-29 09:56:11 +0000},
	doi = {10.1109/TNN.2009.2023122},
	issn = {1045-9227},
	journal = {IEEE Transactions on Neural Networks},
	keywords = {BNN; Boolean functions;learning (artificial intelligence);multilayer perceptrons;binary neural network;linearly nonseparable Boolean functions;nonLSBF;DNA-like learning and decomposing algorithm;DNA-like LDA;DNA-like offset sequence;logic XOR operation;weight-threshold value;multilayer perceptron;function mapping;parity Boolean function;Neural networks;Boolean functions;Neurons;Multilayer perceptrons;Linear discriminant analysis;Logic;Backpropagation algorithms;Input variables;Mathematics;Binary neural network;DNA-like learning and decomposing algorithm (DNA-like LDA);linearly nonseparable Boolean function (non-LSBF);multilayer perceptron (MLP);parity Boolean function (PBF);Algorithms;Artificial Intelligence;DNA;Linear Models;Neural Networks (Computer)},
	month = {Aug},
	number = {8},
	pages = {1293-1301},
	title = {Universal Perceptron and DNA-Like Learning Algorithm for Binary Neural Networks: Non-LSBF Implementation},
	volume = {20},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNN.2009.2023122}}

@inproceedings{7016218,
	abstract = {This paper presents a new method in forecasting Philippine Peso to US Dollar exchange rate. Compared to the conventional way, in which the Philippine Dealing System (PDS), as monitored by the Central Bank, determines the rate by analysing demand and supply, the use of artificial neural network, having consumer price index, inflation rate, lending interest rate and purchasing power of the peso as the inputs is presented in this paper. Though foreign exchange rates vary on a daily basis, the output of this paper is prediction of the average foreign exchange rate every month. Artificial Neural Network serves as a powerful tool in forecasting Philippine Peso to US Dollar exchange rate not requiring expert knowledge in banking and finance thus letting the public gain access to a helpful beacon which is the foreign exchange rate. However, the accuracy of the forecast using artificial neural network is highly dependent on the volume of the training data, in this paper, an alternative algorithm that will increase the accuracy of the conventional artificial neural network with limited volume of training data is presented and analyze.},
	author = {M. L. R. Torregoza and E. P. Dadios},
	booktitle = {2014 International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management (HNICEM)},
	date-added = {2018-09-29 02:58:04 +0000},
	date-modified = {2018-10-10 06:51:39 +0000},
	doi = {10.1109/HNICEM.2014.7016218},
	keywords = {Evolutionary;banking;forecasting theory;genetic algorithms;neural nets;pricing;hybrid genetic algorithm neural network;Philippine peso US dollar exchange rate forecasting;Philippine dealing system;PDS;central bank;demand and supply analysis;artificial neural network;consumer price index;interest rate;inflation rate;lending interest rate;purchasing power;foreign exchange rates;banking;training data;Exchange rates;Genetic algorithms;Artificial neural networks;Forecasting;Conferences;Economic indicators;Artificial Neural Network;forecasting;prediction;exchange rate;evolutionary algorithm},
	month = {Nov},
	pages = {1-5},
	title = {Comparison of neural network and hybrid genetic algorithm-neural network in forecasting of Philippine Peso-US Dollar exchange rate},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/HNICEM.2014.7016218}}

@inproceedings{7838429,
	abstract = {On-chip implementation of large-scale neural networks with emerging synaptic devices is attractive but challenging, primarily due to the pre-mature analog properties of today's resistive memory technologies. This work aims to realize a large-scale neural network using today's available binary RRAM devices for image recognition. We propose a methodology to binarize the neural network parameters with a goal of reducing the precision of weights and neurons to 1-bit for classification and &lt;;8-bit for online training. We experimentally demonstrate the binary neural network (BNN) on Tsinghua's 16 Mb RRAM macro chip fabricated in 130 nm CMOS process. Even under finite bit yield and endurance cycles, the system performance on MNIST handwritten digit dataset achieves ~96.5% accuracy for both classification and online training, close to ~97% accuracy by the ideal software implementation. This work reports the largest scale of the synaptic arrays and achieved the highest accuracy so far.},
	author = {S. Yu and Z. Li and P. Chen and H. Wu and B. Gao and D. Wang and W. Wu and H. Qian},
	booktitle = {2016 IEEE International Electron Devices Meeting (IEDM)},
	date-added = {2018-09-28 11:32:29 +0000},
	date-modified = {2018-09-28 11:32:37 +0000},
	doi = {10.1109/IEDM.2016.7838429},
	issn = {2156-017X},
	keywords = {BNN; CMOS integrated circuits;electronic engineering computing;image recognition;neural nets;resistive RAM;MNIST handwritten digit dataset;CMOS process;Tsinghua;BNN;image recognition;binary RRAM macrochip device;resistive memory technology;pre-mature analog property;synaptic device;large-scale binary neural network;word length 1 bit;storage capacity 16 Mbit;size 130 nm},
	month = {Dec},
	pages = {16.2.1-16.2.4},
	title = {Binary neural network with 16 Mb RRAM macro chip for classification and online training},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/IEDM.2016.7838429}}

@inproceedings{7033335,
	author = {O. P. Patel and A. Tiwari},
	booktitle = {2014 International Conference on Information Technology},
	date-added = {2018-09-28 10:26:33 +0000},
	date-modified = {2018-09-28 10:26:47 +0000},
	doi = {10.1109/ICIT.2014.29},
	keywords = {BNN; generalisation (artificial intelligence);learning (artificial intelligence);neural nets;optimisation;pattern classification;quantum computing;quantum based binary neural network learning algorithm;network structure optimisation;neurons;classification accuracy;hidden layer;training accuracy;generalization accuracy;QANN;Neurons;Accuracy;Training;Biological neural networks;Testing;Diabetes;Binary neural network;Quantum processing;Qubits;Back propagation learning},
	month = {Dec},
	pages = {270-274},
	title = {Quantum Inspired Binary Neural Network Algorithm},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIT.2014.29}}

@inproceedings{1206405,
	abstract = {This paper describes a 3D VLSI Chip for binary neural network classification applications. The 3D circuit includes three layers of MCM integrating 4 chips each making it a total of 12 chips integrated in a volume of (2 /spl times/ 2 /spl times/ 0.7)cm/sup 3/. The architecture is scalable, and real-time binary neural network classifier systems could be built with one, two or all twelve chip solutions. Each basic chip includes an on-chip control unit for programming options of the neural network topology and precision. The system is modular and presents easy expansibility without requiring extra devices. Experimental test results showed that a full recall operation is obtained in less than 1.2/spl mu/s for any topology with 4-bit or 8-bit precision while it is obtained in less than 2.2/spl mu/s for any 16-bit precision. As a consequence the 3D chip is a very powerful reconfigurable and a multiprecision neural chip exhibiting a significant speed of 1.25 GCPS.},
	author = {A. Bermak},
	booktitle = {Proceedings of the 2003 International Symposium on Circuits and Systems, 2003. ISCAS '03.},
	date-added = {2018-09-28 10:13:06 +0000},
	date-modified = {2018-12-06 14:26:04 +1300},
	doi = {10.1109/ISCAS.2003.1206405},
	keywords = {BNN; pattern classification;VLSI;multiprecision neural chip},
	month = {May},
	pages = {V-V},
	title = {A highly scalable 3D chip for binary neural network classification applications},
	volume = {5},
	year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCAS.2003.1206405}}

@article{2009arXiv0904.4587T,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2009arXiv0904.4587T},
	archiveprefix = {arXiv},
	author = {{Torres-Moreno}, J.-M. and {Gordon}, M.~B.},
	date-added = {2018-09-28 09:40:01 +0000},
	date-modified = {2018-09-28 09:40:24 +0000},
	eprint = {0904.4587},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science; Artificial Intelligence; Neural and Evolutionary Computing},
	month = apr,
	primaryclass = {cs.AI},
	title = {{Adaptive Learning with Binary Neurons}},
	year = 2009}

@inproceedings{616215,
	abstract = {An "evolutionary neural network (ENN)" is presented for the max cut problem of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem is to find a partition of V into two disjoint subsets such that the cut size be maximized. The cut size is the sum of weights on edges in E whose endpoints belong to different subsets. The ENN combines the evolutionary initialization scheme of the neural state into the energy minimization criteria of the binary neural network. The performance of ENN is evaluated through simulations in randomly weighted complete graphs and unweighted random graphs with up to 1000 vertices. The results show that the evolutionary initialization scheme drastically improves the solution quality. ENN can always find better solutions than the maximum neural network, the mean field annealing, the simulated annealing, and the greedy algorithm.},
	author = {N. Funabiki and J. Kitamichi and S. Nishikawa},
	booktitle = {Proceedings of International Conference on Neural Networks (ICNN'97)},
	date-added = {2018-09-28 09:23:32 +0000},
	date-modified = {2018-10-10 06:34:05 +0000},
	doi = {10.1109/ICNN.1997.616215},
	keywords = {Evolutionary; neural nets;genetic algorithms;set theory;graph theory;minimisation;computational complexity;evolutionary neural network algorithm;ENN;max cut problems;undirected graph;NP-hard problem;partition;disjoint subsets;evolutionary initialization scheme;energy minimization criteria;binary neural network;randomly weighted complete graphs;unweighted random graphs;maximum neural network;mean field annealing;simulated annealing;greedy algorithm;Neural networks;Neurons;Simulated annealing;NP-hard problem;Equations;Multi-layer neural network;Computer networks;Minimization;Greedy algorithms;Approximation algorithms},
	month = {June},
	pages = {1260-1265 vol.2},
	title = {An evolutionary neural network algorithm for max cut problems},
	volume = {2},
	year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICNN.1997.616215}}

@inproceedings{4790104,
	abstract = {Analysis of the state space for the fully-connected binary neural network ("the Hopfield model") remains an important objective in utilizing the network in pattern recognition and associative information retrieval. Most of the research pertaining to the network's state space so far concentrated on stable-state enumeration and often it was assumed that the patterns which are to be stored are random. We discuss the case of deterministic known codewords whose storage is required, and show that for this important case bounds on the retrieval probabilities and convergence rates can be achieved. The main tool which we employ is Birth-and-Death Markov chains, describing the Hamming distance of the network's state from the stored patterns. The results are applicable to both the asynchronous network and to the Boltzmann machine, and can be utilized to compare codeword sets in terms of efficiency of their retrieval, when the neural network is used as a content addressable memory.},
	author = {M. Kam and R. Cheng and A. Guez},
	booktitle = {1988 American Control Conference},
	date-added = {2018-09-28 09:05:13 +0000},
	date-modified = {2018-09-28 09:05:27 +0000},
	doi = {10.23919/ACC.1988.4790104},
	keywords = {BNN;State-space methods;Neural networks;Information analysis;Pattern analysis;Hopfield neural networks;Pattern recognition;Information retrieval;Convergence;Hamming distance;Content based retrieval},
	month = {June},
	pages = {2276-2281},
	title = {On the State Space of the Binary Neural Network},
	year = {1988},
	Bdsk-Url-1 = {https://doi.org/10.23919/ACC.1988.4790104}}

@article{2016arXiv160602580F,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602580F},
	archiveprefix = {arXiv},
	author = {{Fernando}, C. and {Banarse}, D. and {Reynolds}, M. and {Besse}, F. and {Pfau}, D. and {Jaderberg}, M. and {Lanctot}, M. and {Wierstra}, D.},
	date-added = {2018-09-26 08:54:27 +0000},
	date-modified = {2018-10-10 07:00:44 +0000},
	eprint = {1606.02580},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary; Computer Science; Neural; Evolutionary Computing; Computer Vision; Pattern Recognition; Machine Learning;},
	month = jun,
	title = {{Convolution by Evolution: Differentiable Pattern Producing Networks}},
	year = 2016}

@article{2017arXiv170303864S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170303864S},
	archiveprefix = {arXiv},
	author = {{Salimans}, T. and {Ho}, J. and {Chen}, X. and {Sidor}, S. and {Sutskever}, I.},
	date-added = {2018-09-26 08:48:55 +0000},
	date-modified = {2019-01-11 17:05:39 +1300},
	eprint = {1703.03864},
	journal = {ArXiv e-prints},
	keywords = {Evolutionary; Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing; OpenAI},
	month = mar,
	primaryclass = {stat.ML},
	title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
	year = 2017}

@article{2018arXiv180801974T,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180801974T},
	archiveprefix = {arXiv},
	author = {{Tan}, C. and {Sun}, F. and {Kong}, T. and {Zhang}, W. and {Yang}, C. and {Liu}, C.},
	date-added = {2018-09-26 08:24:11 +0000},
	date-modified = {2018-10-10 00:49:11 +0000},
	eprint = {1808.01974},
	journal = {ArXiv e-prints},
	keywords = {NN; Computer Science - Machine Learning, Statistics - Machine Learning},
	month = aug,
	title = {{A Survey on Deep Transfer Learning}},
	year = 2018}

@article{2018arXiv180804752G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180804752G},
	archiveprefix = {arXiv},
	author = {{Guo}, Y.},
	date-added = {2018-09-26 07:51:55 +0000},
	date-modified = {2018-10-14 00:31:41 +0000},
	eprint = {1808.04752},
	journal = {ArXiv e-prints},
	keywords = {BNN; Machine Learning, Neural and Evolutionary Computing, Statistics,Machine Learning},
	month = aug,
	title = {{A Survey on Methods and Theories of Quantized Neural Networks}},
	year = 2018}

@article{2016arXiv161006918A,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006918A},
	archiveprefix = {arXiv},
	author = {{Abadi}, M. and {Andersen}, D.~G.},
	date-added = {2018-09-26 05:39:12 +0000},
	date-modified = {2019-01-11 15:27:35 +1300},
	eprint = {1610.06918},
	journal = {ArXiv e-prints},
	keywords = {DCGAN; Computer Science - Cryptography and Security, Computer Science - Machine Learning, Cryptonet},
	month = oct,
	primaryclass = {cs.CR},
	title = {{Learning to Protect Communications with Adversarial Neural Cryptography}},
	year = 2016}

@article{2018arXiv180605695W,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180605695W},
	archiveprefix = {arXiv},
	author = {{Wilson}, D.~G and {Cussat-Blanc}, S. and {Luga}, H. and {Miller}, J.~F},
	date-added = {2018-09-26 05:29:58 +0000},
	date-modified = {2018-09-26 05:29:58 +0000},
	eprint = {1806.05695},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	month = jun,
	title = {{Evolving simple programs for playing Atari games}},
	year = 2018}

@inproceedings{8393327,
	author = {B. Yang and W. Zhang and L. Gong and H. Ma},
	booktitle = {2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
	date-added = {2018-09-26 01:45:32 +0000},
	date-modified = {2018-10-10 07:02:14 +0000},
	doi = {10.1109/FSKD.2017.8393327},
	keywords = {Evolutionary; finance; exchange rates;forecasting theory;genetic algorithms;neural nets;stock markets;time series;trees (mathematics);CVFNT model;time series datasets;neural network;finance time series prediction;complex-valued flexible neural tree model;artificial bee colony;forecasting accuracy;genetic algorithm;Shanghai stock index;exchange rates;Neural networks;Time series analysis;Brain modeling;Predictive models;Forecasting;Data models;Indexes;evolutionary method;flexible neural tree;complex-valued;artificial bee colony},
	month = {July},
	pages = {54-58},
	title = {Finance time series prediction using complex-valued flexible neural tree model},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/FSKD.2017.8393327}}

@inproceedings{5432472,
	author = {L. Liu and M. Deng},
	booktitle = {2010 Third International Conference on Knowledge Discovery and Data Mining},
	date-added = {2018-09-26 01:39:28 +0000},
	date-modified = {2019-06-05 21:20:37 +1200},
	doi = {10.1109/WKDD.2010.148},
	keywords = {Evolutionary; cancer;genetic algorithms;medical image processing;neural nets;artificial neural network;breast cancer;women;adaptive genetic algorithm;macro-search capability;global optimization;computational cost;Wisions breast cancer data set;Artificial neural networks;Breast cancer;Genetic algorithms;Genetic mutations;Flowcharts;Economic forecasting;Space technology;Data mining;Conference management;Knowledge management;adaptive genetic algorithm;neural network;weights and thresholds;breast cancer},
	month = {Jan},
	pages = {593-596},
	title = {An Evolutionary Artificial Neural Network Approach for Breast Cancer Diagnosis},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/WKDD.2010.148}}

@inproceedings{Krizhevsky2999257,
	acmid = {2999257},
	address = {USA},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
	date-added = {2018-09-25 11:16:05 +0000},
	date-modified = {2018-12-01 13:40:11 +1300},
	keywords = {CNN, ImageNet, ILSVRC, Convolution, AlexNet},
	location = {Lake Tahoe, Nevada},
	numpages = {9},
	pages = {1097--1105},
	publisher = {Curran Associates Inc.},
	series = {NIPS'12},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
	year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2999134.2999257}}

@webpage{AIWinter-Andrey,
	author = {Kurenkov, Andrey},
	date-added = {2018-09-25 10:57:03 +0000},
	date-modified = {2018-10-10 09:00:52 +0000},
	keywords = {AI, Neural Networks, History},
	lastchecked = {25-Sep-2018},
	month = {September},
	title = {A 'Brief' History of Neural Nets and Deep Learning},
	url = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning},
	urldate = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning},
	year = {2018},
	Bdsk-Url-1 = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning}}

@article{Linnainmaa1976,
	abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
	author = {Linnainmaa, Seppo},
	date-added = {2018-09-25 08:37:46 +0000},
	date-modified = {2018-09-25 08:38:04 +0000},
	day = {01},
	doi = {10.1007/BF01931367},
	issn = {1572-9125},
	journal = {BIT Numerical Mathematics},
	keywords = {NN, Neural Networks, Backpropagation},
	month = {Jun},
	number = {2},
	pages = {146--160},
	title = {Taylor expansion of the accumulated rounding error},
	url = {https://doi.org/10.1007/BF01931367},
	volume = {16},
	year = {1976},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF01931367}}

@book{Minsky:1988:PEE:50066,
	address = {Cambridge, MA, USA},
	author = {Minsky, Marvin L. and Papert, Seymour A.},
	date-added = {2018-09-25 08:29:03 +0000},
	date-modified = {2018-09-25 08:29:27 +0000},
	isbn = {0-262-63111-3},
	keywords = {NN, Neural Networks},
	publisher = {MIT Press},
	title = {Perceptrons: Expanded Edition},
	year = {1988},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB+Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgR0EtYmFzZWQgZmxleGlibGUgbGVhcm5pbmcgYWxnb3JpdGhtIHdpdGggZXJyb3IgdG9sZXJhbmNlIGZvciBkaWdpdGFsIGJpbmFyeSBuZXVyYWwgbmV0d29ya3MuYmliTxECvgAAAAACvgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgR0EtYmFzZWQgZmxleGlibCNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANCTk4AAAIAlC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpCTk46QSBHQS1iYXNlZCBmbGV4aWJsZSBsZWFybmluZyBhbGdvcml0aG0gd2l0aCBlcnJvciB0b2xlcmFuY2UgZm9yIGRpZ2l0YWwgYmluYXJ5IG5ldXJhbCBuZXR3b3Jrcy5iaWIADgDGAGIAQQAgAEcAQQAtAGIAYQBzAGUAZAAgAGYAbABlAHgAaQBiAGwAZQAgAGwAZQBhAHIAbgBpAG4AZwAgAGEAbABnAG8AcgBpAHQAaABtACAAdwBpAHQAaAAgAGUAcgByAG8AcgAgAHQAbwBsAGUAcgBhAG4AYwBlACAAZgBvAHIAIABkAGkAZwBpAHQAYQBsACAAYgBpAG4AYQByAHkAIABuAGUAdQByAGEAbAAgAG4AZQB0AHcAbwByAGsAcwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAklVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgR0EtYmFzZWQgZmxleGlibGUgbGVhcm5pbmcgYWxnb3JpdGhtIHdpdGggZXJyb3IgdG9sZXJhbmNlIGZvciBkaWdpdGFsIGJpbmFyeSBuZXVyYWwgbmV0d29ya3MuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAKUAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADZw==}}

@inproceedings{6173439,
	abstract = {Hardware/software partitioning is a crucial problem in hardware/software co-design. In this paper, we deeply investigate genetic algorithm (GA) for hardware/software partitioning, our co-design targets a heterogeneous multicore system on chip (SoC) which consists of several different types of processing engines(PE), Communicating structure adopts NOC, We use GA for four task graphs to simulate the hardware/software partitioning, experiments show our method is an effective hardware/software partitioning algorithm.},
	author = {L. Luo and H. He and Q. Dou and W. Xu},
	booktitle = {2012 Second International Conference on Intelligent System Design and Engineering Application},
	date-added = {2018-09-09 11:15:43 +0000},
	date-modified = {2018-09-09 11:16:08 +0000},
	doi = {10.1109/ISdea.2012.501},
	keywords = {Evolutionary, genetic algorithms;graph theory;hardware-software codesign;multiprocessing systems;system-on-chip;hardware-software partitioning;genetic algorithm;hardware-software codesign;heterogeneous multicore system on chip;processing engines;communicating structure;NOC;task graphs;Software;Hardware;Partitioning algorithms;Software algorithms;Genetic algorithms;Heuristic algorithms;System-on-a-chip;Hardware/Software Partitioning;Genetic Algorithm(GA);Heterogeneous Multicore SOC},
	month = {Jan},
	pages = {1267-1270},
	title = {Hardware/Software Partitioning for Heterogeneous Multicore SoC Using Genetic Algorithm},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISdea.2012.501}}

@inproceedings{Zhang:2016:ECI:2934583.2934644,
	acmid = {2934644},
	address = {New York, NY, USA},
	author = {Zhang, Chen and Wu, Di and Sun, Jiayu and Sun, Guangyu and Luo, Guojie and Cong, Jason},
	booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
	date-added = {2018-08-25 11:20:53 +0000},
	date-modified = {2018-09-26 07:26:01 +0000},
	doi = {10.1145/2934583.2934644},
	isbn = {978-1-4503-4185-1},
	keywords = {CNN-FPGA, FPGA, CNN, Pipeline, Cluster},
	location = {San Francisco Airport, CA, USA},
	numpages = {6},
	pages = {326--331},
	publisher = {ACM},
	series = {ISLPED '16},
	title = {Energy-Efficient CNN Implementation on a Deeply Pipelined FPGA Cluster},
	url = {http://doi.acm.org/10.1145/2934583.2934644},
	year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2934583.2934644},
	Bdsk-Url-2 = {https://doi.org/10.1145/2934583.2934644}}

@article{2017arXiv170402019C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170402019C},
	archiveprefix = {arXiv},
	author = {{Chaudhuri}, R. and {Fiete}, I.},
	date-added = {2018-07-09 10:12:02 +0000},
	date-modified = {2018-07-09 10:12:09 +0000},
	eprint = {1704.02019},
	journal = {ArXiv e-prints},
	keywords = {BNN, Quantitative Biology - Neurons and Cognition, Computer Science - Neural and Evolutionary Computing},
	month = apr,
	primaryclass = {q-bio.NC},
	title = {{Associative content-addressable networks with exponentially many robust stable states}},
	year = 2017}

@article{32008,
	author = {M. Verleysen and B. Sirletti and A. M. Vandemeulebroecke and P. G. A. Jespers},
	date-added = {2018-07-09 10:11:47 +0000},
	date-modified = {2018-07-09 10:11:54 +0000},
	doi = {10.1109/4.32008},
	issn = {0018-9200},
	journal = {IEEE Journal of Solid-State Circuits},
	keywords = {BNN, VLSI;computerised pattern recognition;content-addressable storage;integrated memory circuits;learning systems;neural nets;programming;CAM;Hopfield neural network;VLSI circuit;analogue VLSI implementation;content-addressable memory;fully interconnected neural network;learning algorithm;pattern-recognition applications;programming;storage capacity;synapse weights;synaptic cells;Artificial intelligence;Artificial neural networks;Biological neural networks;Biology computing;Circuits;Hopfield neural networks;Neural networks;Neurons;Pattern recognition;Very large scale integration},
	month = {Jun},
	number = {3},
	pages = {562-569},
	title = {Neural networks for high-storage content-addressable memory: VLSI circuit and learning algorithm},
	volume = {24},
	year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1109/4.32008}}

@article{Brodsky:93,
	abstract = {The content-addressable network (CAN) is an efficient, intrinsically discrete training algorithm for binary-valued classification networks. The binary nature of the CAN network permits accelerated learning and significantly reduced hardware-implementation requirements. A multilayer optoelectronic CAN network employing matrix--vector multiplication was constructed. The network learned and correctly classified trained patterns, gaining a measure of fault tolerance by learning associative solutions to optical hardware imperfections. Operation of this system is possible owing to the reduced hardware accuracy requirements of the CAN learning algorithm.},
	author = {Stephen A. Brodsky and Gary C. Marsden and Clark C. Guest},
	date-added = {2018-07-05 08:41:33 +0000},
	date-modified = {2018-07-05 08:41:42 +0000},
	doi = {10.1364/AO.32.001338},
	journal = {Appl. Opt.},
	keywords = {BNN, Cylindrical lenses; Light valves; Neural networks; Optical components; Optical neural systems; Parallel processing},
	month = {Mar},
	number = {8},
	pages = {1338--1345},
	publisher = {OSA},
	title = {Optical matrix--vector implementation of the content-addressable network},
	url = {http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338},
	volume = {32},
	year = {1993},
	Bdsk-Url-1 = {http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338},
	Bdsk-Url-2 = {https://doi.org/10.1364/AO.32.001338}}

@article{LMNN-UoE-UK,
	adsurl = {http://homepages.inf.ed.ac.uk/rbf/EUCOGNITION/BRIEFINGS/billings.pdf},
	archiveprefix = {UoE-UK},
	author = {{Billings}, Guy},
	date-added = {2018-06-22 10:47:53 +0000},
	date-modified = {2018-06-22 10:47:53 +0000},
	eprint = {1502.04390},
	journal = {Euroinformatics Doctoral Training Centre},
	keywords = {NN, Learning, Numerical Analysis},
	primaryclass = {cs.LG},
	title = {{Learning and Memory in Neural Networks}},
	year = 2004}

@article{PETERSON1989475,
	author = {Carsten Peterson and Eric Hartman},
	date-added = {2018-06-22 10:47:31 +0000},
	date-modified = {2018-12-06 13:49:53 +1300},
	doi = {https://doi.org/10.1016/0893-6080(89)90045-2},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {NN, Neural network, Bidirectional, Generalization, Content addressable memory, Mean field theory, Learning algorithm},
	number = {6},
	pages = {475 - 494},
	title = {Explorations of the mean field theory learning algorithm},
	url = {http://www.sciencedirect.com/science/article/pii/0893608089900452},
	volume = {2},
	year = {1989},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0893608089900452},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(89)90045-2}}

@article{2018arXiv180305900V,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180305900V},
	archiveprefix = {arXiv},
	author = {{Venieris}, S.~I. and {Kouris}, A. and {Bouganis}, C.-S.},
	date-added = {2018-06-22 10:47:07 +0000},
	date-modified = {2018-06-22 10:47:16 +0000},
	eprint = {1803.05900},
	journal = {ArXiv e-prints},
	keywords = {FPGA-NN, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Learning},
	month = mar,
	primaryclass = {cs.CV},
	title = {{Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey and Future Directions}},
	year = 2018}

@inproceedings{Suda:2016:TOF:2847263.2847276,
	acmid = {2847276},
	address = {New York, NY, USA},
	author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
	booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-06-22 10:45:58 +0000},
	date-modified = {2018-06-22 10:46:11 +0000},
	doi = {10.1145/2847263.2847276},
	isbn = {978-1-4503-3856-1},
	keywords = {FPGA-NN, convolutional neural networks, fpga, opencl, optimization},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {16--25},
	publisher = {ACM},
	series = {FPGA '16},
	title = {Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks},
	url = {http://doi.acm.org/10.1145/2847263.2847276},
	year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2847263.2847276},
	Bdsk-Url-2 = {https://doi.org/10.1145/2847263.2847276}}

@inproceedings{374468,
	author = {A. V. Krishnamoorthy and S. A. Brodsky and C. C. Guest and G. C. Marsden and M. Blume and G. Yayla and J. Merckle and S. C. Esener},
	booktitle = {Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on},
	date-added = {2018-06-22 10:44:13 +0000},
	date-modified = {2018-09-26 08:04:11 +0000},
	doi = {10.1109/ICNN.1994.374468},
	keywords = {BNN, learning (artificial intelligence);neural chips;neural net architecture;optical neural nets;3D optoelectronic neural system;D-STOP system;Accelerated Learning;connectivity;content addressable network learning algorithm;discrete learning algorithm;dual-scale topology optoelectronic processor neural network;fully-parallel neural networks;generic gradient-descent learning rules;hardware efficient learning;optoelectronic hardware tradeoffs;scalable optically interconnected neural network architecture;Backpropagation algorithms;Hardware;Integrated circuit interconnections;Neural networks;Neurons;Neurotransmitters;Optical interconnections;Optical network units;Optical transmitters;Power system interconnection},
	month = {Jun},
	pages = {1998-2003 vol.3},
	title = {Hardware efficient learning on a 3-D optoelectronic neural system},
	volume = {3},
	year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICNN.1994.374468}}

@inproceedings{226962,
	author = {S. A. Brodsky and C. C. Guest},
	booktitle = {[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
	date-added = {2018-06-22 10:43:49 +0000},
	date-modified = {2018-06-22 10:43:58 +0000},
	doi = {10.1109/IJCNN.1992.226962},
	keywords = {BNN, backpropagation;content-addressable storage;optical neural nets;VLSI;backpropagation;content addressable networks;discrete mappings;error-free solution;fast convergence rate;initialization;zero error solutions;Backpropagation algorithms;Computer errors;Costs;Error correction;Hardware;Optical computing;Optical devices;Optical fiber networks;Optical network units;Very large scale integration},
	month = {Jun},
	pages = {352-357 vol.2},
	title = {Content addressable networks for initialization of backpropagation with zero error solutions},
	volume = {2},
	year = {1992},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBtLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS1OTi9GLUNOTi0gQW4gRlBHQS1iYXNlZCBmcmFtZXdvcmsgZm9yIHRyYWluaW5nIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYk8RAngAAAAAAngAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9GLUNOTi0gQW4gRlBHQS1iYXMjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAHRlBHQS1OTgAAAgCDLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkZQR0EtTk46Ri1DTk4tIEFuIEZQR0EtYmFzZWQgZnJhbWV3b3JrIGZvciB0cmFpbmluZyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4AnABNAEYALQBDAE4ATgAtACAAQQBuACAARgBQAEcAQQAtAGIAYQBzAGUAZAAgAGYAcgBhAG0AZQB3AG8AcgBrACAAZgBvAHIAIAB0AHIAYQBpAG4AaQBuAGcAIABDAG8AbgB2AG8AbAB1AHQAaQBvAG4AYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAIFVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ZQR0EtTk4vRi1DTk4tIEFuIEZQR0EtYmFzZWQgZnJhbWV3b3JrIGZvciB0cmFpbmluZyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAJQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADEA==},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1992.226962}}

@inproceedings{5726804,
	author = {S. A. Brodsky and C. C. Guest},
	booktitle = {1990 IJCNN International Joint Conference on Neural Networks},
	date-added = {2018-06-22 10:43:21 +0000},
	date-modified = {2018-06-22 10:43:33 +0000},
	doi = {10.1109/IJCNN.1990.137846},
	keywords = {BNN, content-addressable storage;learning systems;neural nets;arbitrary bit-level significance;binary backpropagation;bit connection weights;content addressable memory;continuous backpropagation network learning model;local computation;pseudoanalog extension},
	month = {June},
	pages = {205-210 vol.3},
	title = {Binary backpropagation in content addressable memory},
	year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1990.137846}}

@article{31325,
	author = {M. Verleysen and B. Sirletti and A. Vandemeulebroecke and P. G. A. Jespers},
	date-added = {2018-06-22 10:42:54 +0000},
	date-modified = {2018-06-22 10:43:03 +0000},
	doi = {10.1109/31.31325},
	issn = {0098-4094},
	journal = {IEEE Transactions on Circuits and Systems},
	keywords = {BNN, CMOS integrated circuits;VLSI;content-addressable storage;neural nets;Hopfield neural network;VLSI;content-addressable memory;fully interconnected neural network;high-storage capacity;implementation;learning algorithm;neural networks;optimization;pattern recognition;programming algorithm;retrieval capabilities;small area;speed capabilities;CADCAM;Cams;Computer aided manufacturing;Content based retrieval;Hopfield neural networks;Integrated circuit interconnections;Neural networks;Neurons;Pattern recognition;Very large scale integration},
	month = {May},
	number = {5},
	pages = {762-766},
	title = {A high-storage capacity content-addressable memory and its learning algorithm},
	volume = {36},
	year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1109/31.31325}}

@article{2014arXiv1409.5185L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.5185L},
	archiveprefix = {arXiv},
	author = {{Lee}, C.-Y. and {Xie}, S. and {Gallagher}, P. and {Zhang}, Z. and {Tu}, Z.},
	date-added = {2018-06-11 04:57:33 +0000},
	date-modified = {2018-06-11 04:57:33 +0000},
	eprint = {1409.5185},
	journal = {ArXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	month = sep,
	primaryclass = {stat.ML},
	title = {{Deeply-Supervised Nets}},
	year = 2014}

@article{2013arXiv1306.0239T,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1306.0239T},
	archiveprefix = {arXiv},
	author = {{Tang}, Y.},
	date-added = {2018-06-11 04:57:10 +0000},
	date-modified = {2018-06-11 04:57:10 +0000},
	eprint = {1306.0239},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	month = jun,
	primaryclass = {cs.LG},
	title = {{Deep Learning using Linear Support Vector Machines}},
	year = 2013}

@article{2016arXiv160504711L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160504711L},
	archiveprefix = {arXiv},
	author = {{Li}, F. and {Zhang}, B. and {Liu}, B.},
	date-added = {2018-06-11 04:56:39 +0000},
	date-modified = {2018-06-11 04:56:39 +0000},
	eprint = {1605.04711},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = may,
	primaryclass = {cs.CV},
	title = {{Ternary Weight Networks}},
	year = 2016}

@article{BALDOMINOS201838,
	author = {Alejandro Baldominos and Yago Saez and Pedro Isasi},
	date-added = {2018-06-02 05:38:07 +0000},
	date-modified = {2018-06-02 05:38:25 +0000},
	doi = {https://doi.org/10.1016/j.neucom.2017.12.049},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {CNN, Neuroevolution, Evolutionary algorithms, Convolutional neural networks, Automatic topology design, Genetic algorithms, Grammatical evolution},
	pages = {38 - 52},
	title = {Evolutionary convolutional neural networks: An application to handwriting recognition},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231217319112},
	volume = {283},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217319112},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.12.049}}

@article{FERREIRA2018205,
	author = {Martha Dais Ferreira and D{\'e}bora Cristina Corr{\^e}a and Luis Gustavo Nonato and Rodrigo Fernandes de Mello},
	date-added = {2018-06-02 05:37:45 +0000},
	date-modified = {2018-06-02 05:37:53 +0000},
	doi = {https://doi.org/10.1016/j.eswa.2017.10.052},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {CNN, Convolutional neural network, Architecture assessment, Dynamical systems, Handwritten digit recognition, Face recognition, Object recognition},
	pages = {205 - 217},
	title = {Designing architectures of convolutional neural networks to solve practical problems},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417417307340},
	volume = {94},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417307340},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.10.052}}

@article{LI2018154,
	author = {Guoqi Li and Lei Deng and Lei Tian and Haotian Cui and Wentao Han and Jing Pei and Luping Shi},
	date-added = {2018-06-02 05:36:48 +0000},
	date-modified = {2018-06-02 05:37:00 +0000},
	doi = {https://doi.org/10.1016/j.neucom.2017.06.058},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {BNN, Deep learning, Neural network applications, Discrete state transition, Discrete weight space},
	pages = {154 - 162},
	title = {Training deep neural networks with discrete state transition},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231217311864},
	volume = {272},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217311864},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.06.058}}

@article{LIANG20181072,
	author = {Shuang Liang and Shouyi Yin and Leibo Liu and Wayne Luk and Shaojun Wei},
	date-added = {2018-06-02 05:36:24 +0000},
	date-modified = {2018-06-02 05:36:30 +0000},
	doi = {https://doi.org/10.1016/j.neucom.2017.09.046},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {BNN, Binarized neural network, Hardware accelerator, FPGA},
	pages = {1072 - 1086},
	title = {FP-BNN: Binarized neural network on FPGA},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231217315655},
	volume = {275},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217315655},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.09.046}}

@article{SHOEMAKER1991231,
	author = {Patrick A. Shoemaker and Michael J. Carlin and Randy L. Shimabukuro},
	date-added = {2018-06-02 05:35:50 +0000},
	date-modified = {2018-06-02 05:35:58 +0000},
	doi = {https://doi.org/10.1016/0893-6080(91)90007-R},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {BNN, Neural networks, Learning algorithms, Back propagation, Trinary, VLSI implementations, Nonvolatile weights},
	number = {2},
	pages = {231 - 241},
	title = {Back propagation learning with trinary quantization of weight updates},
	url = {http://www.sciencedirect.com/science/article/pii/089360809190007R},
	volume = {4},
	year = {1991},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/089360809190007R},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(91)90007-R}}

@article{DENG201849,
	author = {Lei Deng and Peng Jiao and Jing Pei and Zhenzhi Wu and Guoqi Li},
	date-added = {2018-06-02 05:35:28 +0000},
	date-modified = {2018-06-02 05:35:34 +0000},
	doi = {https://doi.org/10.1016/j.neunet.2018.01.010},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {BNN, GXNOR-Net, Discrete state transition, Ternary neural networks, Sparse binary networks},
	pages = {49 - 58},
	title = {GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608018300108},
	volume = {100},
	year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608018300108},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2018.01.010}}

@article{2017arXiv170300810S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170300810S},
	archiveprefix = {arXiv},
	author = {{Shwartz-Ziv}, R. and {Tishby}, N.},
	date-added = {2018-06-02 03:06:54 +0000},
	date-modified = {2018-06-02 03:07:05 +0000},
	eprint = {1703.00810},
	journal = {ArXiv e-prints},
	keywords = {DNN, Computer Science - Learning},
	month = mar,
	primaryclass = {cs.LG},
	title = {{Opening the Black Box of Deep Neural Networks via Information}},
	year = 2017}

@article{2016arXiv160606160Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160606160Z},
	archiveprefix = {arXiv},
	author = {{Zhou}, S. and {Wu}, Y. and {Ni}, Z. and {Zhou}, X. and {Wen}, H. and {Zou}, Y.},
	date-added = {2018-05-30 01:32:32 +0000},
	date-modified = {2018-08-31 09:56:59 +0000},
	eprint = {1606.06160},
	journal = {ArXiv e-prints},
	keywords = {BNN, Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	month = jun,
	title = {{DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients}},
	year = 2016}

@article{2017arXiv170602379L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602379L},
	annote = {Same paper as:

Towards a Deeper Understanding of Training Quantized Neural Networks
Published in https://www.padl.ws
Principled Approaches to Deep Learning
ICML 2017, Sydney, Australia
August 10, 2017
https://www.padl.ws/papers/Paper%2016.pdf},
	archiveprefix = {arXiv},
	author = {{Li}, H. and {De}, S. and {Xu}, Z. and {Studer}, C. and {Samet}, H. and {Goldstein}, T.},
	date-added = {2018-05-30 01:03:06 +0000},
	date-modified = {2018-11-28 15:14:52 +1300},
	eprint = {1706.02379},
	journal = {ArXiv e-prints},
	keywords = {DNN, Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	month = jun,
	primaryclass = {cs.LG},
	title = {{Training Quantized Nets: A Deeper Understanding}},
	year = 2017}

@techreport{abdelouahab:hal-01695375,
	author = {ABDELOUAHAB, Kamel and Pelcat, Maxime and Berry, Fran{\c c}ois and S{\'e}rot, Jocelyn},
	date-added = {2018-05-28 11:51:04 +0000},
	date-modified = {2018-05-28 11:51:35 +0000},
	hal_id = {hal-01695375},
	hal_version = {v2},
	institution = {{Universit{\'e} Clermont Auvergne ; Institut Pascal, Clermont Ferrand ; IETR/INSA Rennes}},
	keywords = {FPGA-NN, FPGA, CNN, HDL, Hardware},
	month = Jan,
	pdf = {https://hal.archives-ouvertes.fr/hal-01695375/file/hal-accelerating-cnn.pdf},
	title = {{Accelerating CNN inference on FPGAs: A Survey}},
	type = {Research Report},
	url = {https://hal.archives-ouvertes.fr/hal-01695375},
	year = {2018},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-01695375}}

@article{HASSABIS2017245,
	author = {Demis Hassabis and Dharshan Kumaran and Christopher Summerfield and Matthew Botvinick},
	date-added = {2018-05-25 05:39:29 +0000},
	date-modified = {2018-05-25 05:39:29 +0000},
	doi = {https://doi.org/10.1016/j.neuron.2017.06.011},
	issn = {0896-6273},
	journal = {Neuron},
	keywords = {artificial intelligence, brain, cognition, neural network, learning},
	number = {2},
	pages = {245 - 258},
	title = {Neuroscience-Inspired Artificial Intelligence},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627317305093},
	volume = {95},
	year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0896627317305093},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neuron.2017.06.011}}

@article{2016arXiv161200796K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200796K},
	archiveprefix = {arXiv},
	author = {{Kirkpatrick}, J. and {Pascanu}, R. and {Rabinowitz}, N. and {Veness}, J. and {Desjardins}, G. and {Rusu}, A.~A. and {Milan}, K. and {Quan}, J. and {Ramalho}, T. and {Grabska-Barwinska}, A. and {Hassabis}, D. and {Clopath}, C. and {Kumaran}, D. and {Hadsell}, R.},
	date-added = {2018-05-25 05:39:08 +0000},
	date-modified = {2018-05-25 05:39:08 +0000},
	eprint = {1612.00796},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	month = dec,
	primaryclass = {cs.LG},
	title = {{Overcoming catastrophic forgetting in neural networks}},
	year = 2016}

@article{2015arXiv151004189N,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151004189N},
	archiveprefix = {arXiv},
	author = {{N{\o}kland}, A.},
	date-added = {2018-05-21 00:54:01 +0000},
	date-modified = {2018-05-21 00:54:01 +0000},
	eprint = {1510.04189},
	journal = {ArXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	month = oct,
	primaryclass = {stat.ML},
	title = {{Improving Back-Propagation by Adding an Adversarial Gradient}},
	year = 2015}

@article{2015arXiv150303562C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150303562C},
	archiveprefix = {arXiv},
	author = {{Cheng}, Z. and {Soudry}, D. and {Mao}, Z. and {Lan}, Z.},
	date-added = {2018-05-19 11:32:04 +0000},
	date-modified = {2018-10-09 23:48:40 +0000},
	eprint = {1503.03562},
	journal = {ArXiv e-prints},
	keywords = {BNN; Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	month = mar,
	title = {{Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation}},
	year = 2015}

@article{2016arXiv161103530Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161103530Z},
	archiveprefix = {arXiv},
	author = {{Zhang}, C. and {Bengio}, S. and {Hardt}, M. and {Recht}, B. and {Vinyals}, O.},
	date-added = {2018-05-19 05:48:30 +0000},
	date-modified = {2018-05-19 05:48:30 +0000},
	eprint = {1611.03530},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning},
	month = nov,
	primaryclass = {cs.LG},
	title = {{Understanding deep learning requires rethinking generalization}},
	year = 2016}

@article{2017arXiv170107875A,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107875A},
	archiveprefix = {arXiv},
	author = {{Arjovsky}, M. and {Chintala}, S. and {Bottou}, L.},
	date-added = {2018-05-19 04:48:30 +0000},
	date-modified = {2018-05-19 04:48:30 +0000},
	eprint = {1701.07875},
	journal = {ArXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	month = jan,
	primaryclass = {stat.ML},
	title = {{Wasserstein GAN}},
	year = 2017}

@article{2015arXiv151202479M,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151202479M},
	archiveprefix = {arXiv},
	author = {{Montavon}, G. and {Bach}, S. and {Binder}, A. and {Samek}, W. and {M{\"u}ller}, K.-R.},
	date-added = {2018-05-16 10:38:31 +0000},
	date-modified = {2018-05-16 10:38:31 +0000},
	eprint = {1512.02479},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	month = dec,
	primaryclass = {cs.LG},
	title = {{Explaining NonLinear Classification Decisions with Deep Taylor Decomposition}},
	year = 2015}

@article{journal.pone.0130140,
	abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Gr{\'e}goire AND Klauschen, Frederick AND M{\"u}ller, Klaus-Robert AND Samek, Wojciech},
	date-added = {2018-05-16 09:58:57 +0000},
	date-modified = {2018-05-16 09:59:52 +0000},
	doi = {10.1371/journal.pone.0130140},
	journal = {PLOS ONE},
	keywords = {DNN, Taylor Series, MNIST, ImageNet, Pixel-wise},
	month = {07},
	number = {7},
	pages = {1-46},
	publisher = {Public Library of Science},
	title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
	url = {https://doi.org/10.1371/journal.pone.0130140},
	volume = {10},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0130140}}

@article{kim51bitwise,
	author = {Kim, Minje and Smaragdis, Paris},
	date-added = {2018-05-16 00:48:16 +0000},
	date-modified = {2018-10-10 08:59:49 +0000},
	journal = {Urbana},
	keywords = {BNN, QaD, IBM, Signals, Denoise},
	pages = {61801},
	title = {Bitwise Neural Networks for Efficient Single-Channel Source Separation},
	volume = {51},
	year = {2018}}

@article{2018arXiv180409154G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180409154G},
	archiveprefix = {arXiv},
	author = {{Giacomello}, E. and {Lanzi}, P.~L. and {Loiacono}, D.},
	date-added = {2018-05-15 11:53:51 +0000},
	date-modified = {2018-05-15 11:53:51 +0000},
	eprint = {1804.09154},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
	month = apr,
	primaryclass = {cs.LG},
	title = {{DOOM Level Generation using Generative Adversarial Networks}},
	year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCBLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vT24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliTxECzAAAAAACzAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H09uIFBpeGVsLVdpc2UgRXhwbCNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAJcvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46T24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliAAAOAM4AZgBPAG4AIABQAGkAeABlAGwALQBXAGkAcwBlACAARQB4AHAAbABhAG4AYQB0AGkAbwBuAHMAIABmAG8AcgAgAE4AbwBuAC0ATABpAG4AZQBhAHIAIABDAGwAYQBzAHMAaQBmAGkAZQByACAARABlAGMAaQBzAGkAbwBuAHMAIABiAHkAIABMAGEAeQBlAHIALQBXAGkAcwBlACAAUgBlAGwAZQB2AGEAbgBjAGUAIABQAHIAbwBwAGEAZwBhAHQAaQBvAG4ALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAJVVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL09uIFBpeGVsLVdpc2UgRXhwbGFuYXRpb25zIGZvciBOb24tTGluZWFyIENsYXNzaWZpZXIgRGVjaXNpb25zIGJ5IExheWVyLVdpc2UgUmVsZXZhbmNlIFByb3BhZ2F0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAqAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAN4},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OL05vbi1sb2NhbCBOZXVyYWwgTmV0d29ya3MuYmliTxEBrAAAAAABrAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HU5vbi1sb2NhbCBOZXVyYWwgTmV0d29ya3MuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANDTk4AAAIATy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk46Tm9uLWxvY2FsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4APAAdAE4AbwBuAC0AbABvAGMAYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAE1Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi9Ob24tbG9jYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAYAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIQ}}

@inproceedings{5234726,
	author = {X. Chen and Q. Ma and T. Alkharobi},
	booktitle = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
	date-added = {2018-05-15 11:27:59 +0000},
	date-modified = {2018-05-15 11:27:59 +0000},
	doi = {10.1109/ICCSIT.2009.5234726},
	keywords = {Fourier series;data mining;pattern clustering;radial basis function networks;Fourier component neural network;Gauss series clustering neural network;Taylor component neural network;Taylor series;mining;radial basis function neuron;Computer science;Educational institutions;Electronic mail;Gaussian processes;Information science;Input variables;Neural networks;Neurons;Taylor series;Transfer functions;Fourier component neural network;Gauss series Clustering neural network;Taylor component neural network;Taylor series neural network;prediction;stock price},
	month = {Aug},
	pages = {291-294},
	title = {New neural networks based on Taylor series and their research},
	year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCSIT.2009.5234726}}

@inproceedings{li2013,
	abstract = {This study adopts popular back-propagation neural network to make one-period-ahead prediction of the stock price. A model based on Taylor series by using both fundamental and technical indicators EPS and MACD as input data is built for an empirical study. Leading Taiwanese companies in non-hi-tech industry such as Formosa Plastics, Yieh Phui Steel, Evergreen Marine, and Chang Hwa Bank are picked as targets to analyze their reasonable prices and moving trends. The performance of this model shows remarkable return and high accuracy in making long/short strategies.},
	author = {Li, Jung Bin and Wu, Chien Ho},
	booktitle = {Innovation for Applied Science and Technology},
	date-added = {2018-05-15 11:27:42 +0000},
	date-modified = {2019-04-29 12:00:28 +1200},
	doi = {10.4028/www.scientific.net/AMM.284-287.3020},
	keywords = {NN-Fin; Neural Network (NN), BPN, Taylor Series, Price Forecast},
	month = {3},
	pages = {3020--3024},
	publisher = {Trans Tech Publications},
	series = {Applied Mechanics and Materials},
	title = {An Efficient Neural Network Model with Taylor Series-Based Data Pre-Processing for Stock Price Forecast},
	volume = {284},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.4028/www.scientific.net/AMM.284-287.3020}}

@article{2016arXiv160205897D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160205897D},
	archiveprefix = {arXiv},
	author = {{Daniely}, A. and {Frostig}, R. and {Singer}, Y.},
	date-added = {2018-05-15 11:27:25 +0000},
	date-modified = {2018-05-15 11:27:25 +0000},
	eprint = {1602.05897},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Statistics - Machine Learning},
	month = feb,
	primaryclass = {cs.LG},
	title = {{Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity}},
	year = 2016}

@article{10.1002-mma.2641,
	abstract = {This paper focuses on learning algorithms for approximating functional data that are chosen from some Hilbert spaces. An effective algorithm, called Hilbert parallel overrelaxation backpropagation (HPORBP) algorithm, is proposed for training the Hilbert feedforward neural networks that are extensions of feedforward neural networks from Euclidean space to some Hilbert spaces. Furthermore, the convergence of the iterative HPORBP algorithm is analyzed, and a deterministic convergence theorem is proposed for the HPORBP algorithm on the basis of the perturbation results of Mangasarian and Solodov. Some experimental results of learning functional data on some Hilbert spaces illustrate the convergence theorem and show that the proposed HPORBP algorithm has a better accuracy than the Hilbert backpropagation algorithm. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
	author = {Zhao Jianwei},
	date-added = {2018-05-15 11:26:53 +0000},
	date-modified = {2018-10-10 08:58:50 +0000},
	doi = {10.1002/mma.2641},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mma.2641},
	journal = {Mathematical Methods in the Applied Sciences},
	keywords = {functional data, feedforward neural network, learning algorithm, convergence},
	month = {November},
	number = {17},
	pages = {2111-2121},
	title = {Functional data learning by Hilbert feedforward neural networks},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	volume = {35},
	year = {2012},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Bdsk-Url-2 = {https://doi.org/10.1002/mma.2641}}

@article{CASTRO2005967,
	abstract = {The Fuzzy ARTMAP algorithm has been proven to be one of the premier neural network architectures for classification problems. One of the properties of Fuzzy ARTMAP, which can be both an asset and a liability, is its capacity to produce new nodes (templates) on demand to represent classification categories. This property allows Fuzzy ARTMAP to automatically adapt to the database without having to a priori specify its network size. On the other hand, it has the undesirable side effect that large databases might produce a large network size (node proliferation) that can dramatically slow down the training speed of the algorithm. To address the slow convergence speed of Fuzzy ARTMAP for large database problems, we propose the use of space-filling curves, specifically the Hilbert space-filling curves (HSFC). Hilbert space-filling curves allow us to divide the problem into smaller sub-problems, each focusing on a smaller than the original dataset. For learning each partition of data, a different Fuzzy ARTMAP network is used. Through this divide-and-conquer approach we are avoiding the node proliferation problem, and consequently we speedup Fuzzy ARTMAP's training. Results have been produced for a two-class, 16-dimensional Gaussian data, and on the Forest database, available at the UCI repository. Our results indicate that the Hilbert space-filling curve approach reduces the time that it takes to train Fuzzy ARTMAP without affecting the generalization performance attained by Fuzzy ARTMAP trained on the original large dataset. Given that the resulting smaller datasets that the HSFC approach produces can independently be learned by different Fuzzy ARTMAP networks, we have also implemented and tested a parallel implementation of this approach on a Beowulf cluster of workstations that further speeds up Fuzzy ARTMAP's convergence to a solution for large database problems.},
	author = {Jos{\'e} Castro and Michael Georgiopoulos and Ronald Demara and Avelino Gonzalez},
	date-added = {2018-05-15 11:26:41 +0000},
	date-modified = {2018-05-15 11:26:41 +0000},
	doi = {https://doi.org/10.1016/j.neunet.2005.01.007},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Fuzzy-ARTMAP, Hilbert space-filling curve, Data mining, Data-partitioning},
	number = {7},
	pages = {967 - 984},
	title = {Data-partitioning using the Hilbert space filling curves: Effect on the speed of convergence of Fuzzy ARTMAP for large database problems},
	url = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	volume = {18},
	year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2005.01.007}}

@article{2018arXiv180501934C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180501934C},
	archiveprefix = {arXiv},
	author = {{Chen}, C. and {Chen}, Q. and {Xu}, J. and {Koltun}, V.},
	date-added = {2018-05-15 11:26:25 +0000},
	date-modified = {2018-05-15 11:26:25 +0000},
	eprint = {1805.01934},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Learning},
	month = may,
	primaryclass = {cs.CV},
	title = {{Learning to See in the Dark}},
	year = 2018}

@inproceedings{2017arXiv171111294L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171111294L},
	archiveprefix = {arXiv},
	author = {{Lin}, X. and {Zhao}, C. and {Pan}, W.},
	booktitle = {31st Conference on Neural Information Processing Systems},
	date-added = {2018-05-15 11:26:04 +0000},
	date-modified = {2018-11-26 21:20:19 +1300},
	eprint = {1711.11294},
	journal = {NIPS 2017},
	keywords = {BNN; Computer Science; Learning; Statistics; Machine Learning},
	month = nov,
	primaryclass = {cs.LG},
	title = {{Towards Accurate Binary Convolutional Neural Network}},
	year = 2017}

@article{2017arXiv170906206Y,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170906206Y},
	archiveprefix = {arXiv},
	author = {{Yin}, S. and {Venkataramanaiah}, S.~K. and {Chen}, G.~K. and {Krishnamurthy}, R. and {Cao}, Y. and {Chakrabarti}, C. and {Seo}, J.-s.},
	date-added = {2018-05-15 11:24:06 +0000},
	date-modified = {2018-10-11 08:40:58 +0000},
	eprint = {1709.06206},
	journal = {ArXiv e-prints},
	keywords = {SNN; Neural and Evolutionary Computing},
	month = sep,
	title = {{Algorithm and Hardware Design of Discrete-Time Spiking Neural Networks Based on Back Propagation with Binary Activations}},
	year = 2017}

@inproceedings{5949465,
	author = {J. Ranhel and C. V. Lima and J. L. R. Monteiro and J. E. Kogler and M. L. Netto},
	booktitle = {2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI)},
	date-added = {2018-05-15 11:24:06 +0000},
	date-modified = {2018-05-15 11:24:06 +0000},
	doi = {10.1109/FOCI.2011.5949465},
	keywords = {neural nets;storage management;PNG attributes;binary counters;bistable memory;parallel computing;polychronous group;spiking neural network;Biological system modeling;Computational modeling;Delay;Fires;Firing;Kernel;Neurons;bistable neural memory;neural counters;neural hierarchical organization;neural stack counter;polychronization;spiking neural networks},
	month = {April},
	pages = {66-73},
	title = {Bistable memory and binary counters in spiking neural network},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCI.2011.5949465}}

@conference{2016arXiv161105128Y,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161105128Y},
	archiveprefix = {arXiv},
	author = {{Yang}, T.-J. and {Chen}, Y.-H. and {Sze}, V.},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2018-05-11 02:51:58 +0000},
	date-modified = {2018-11-26 20:59:17 +1300},
	eprint = {1611.05128},
	journal = {ArXiv e-prints},
	keywords = {CNN; Computer Vision; Pattern Recognition},
	month = jul,
	primaryclass = {cs.CV},
	title = {{Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning}},
	year = 2017}

@conference{Chen2018UnderstandingTL,
	author = {Yu-hsin Chen and Tien-Ju Yang and Joel S. Emer and Vivienne Sze},
	booktitle = {SysML Conference},
	date-added = {2018-05-11 00:57:52 +0000},
	date-modified = {2018-11-26 19:52:57 +1300},
	keywords = {NN, DNN, Hardware, FPGA, ASIC},
	month = {February},
	title = {Understanding the Limitations of Existing Energy-Efficient Design Approaches for Deep Neural Networks},
	year = {2018}}

@article{2016arXiv161207625S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161207625S},
	archiveprefix = {arXiv},
	author = {{Sze}, V. and {Chen}, Y.-H. and {Emer}, J. and {Suleiman}, A. and {Zhang}, Z.},
	date-added = {2018-05-10 05:20:22 +0000},
	date-modified = {2018-05-10 05:20:22 +0000},
	eprint = {1612.07625},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = dec,
	primaryclass = {cs.CV},
	title = {{Hardware for Machine Learning: Challenges and Opportunities}},
	year = 2016}

@article{2018arXiv180403230Y,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403230Y},
	archiveprefix = {arXiv},
	author = {{Yang}, T.-J. and {Howard}, A. and {Chen}, B. and {Zhang}, X. and {Go}, A. and {Sze}, V. and {Adam}, H.},
	date-added = {2018-05-10 04:40:55 +0000},
	date-modified = {2018-05-10 04:40:55 +0000},
	eprint = {1804.03230},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = apr,
	primaryclass = {cs.CV},
	title = {{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications}},
	year = 2018}

@article{7738524,
	author = {Y. H. Chen and T. Krishna and J. S. Emer and V. Sze},
	date-added = {2018-05-10 03:54:03 +0000},
	date-modified = {2018-10-17 00:13:32 +0000},
	doi = {10.1109/JSSC.2016.2616357},
	issn = {0018-9200},
	journal = {IEEE Journal of Solid-State Circuits},
	keywords = {CNN; DRAM chips;data flow computing;energy conservation;feedforward neural nets;learning (artificial intelligence);neural net architecture;power aware computing;reconfigurable architectures;AI systems;AlexNet;CNN shapes;DRAM accesses;Eyeriss;MAC;RS dataflow reconfiguration;;convolutional layers;data movement energy cost;dataflow processing;deep convolutional neural networks;energy efficiency;energy-efficient reconfigurable accelerator;multiply and accumulation;off-chip DRAM;reconfiguring architecture;row stationary;spatial architecture;Clocks;Computer architecture;Hardware;Neural networks;Random access memory;Shape;Throughput;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
	month = {Jan},
	number = {1},
	pages = {127-138},
	title = {Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
	volume = {52},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2016.2616357}}

@article{2014arXiv1412.6980K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	archiveprefix = {arXiv},
	author = {{Kingma}, D.~P. and {Ba}, J.},
	date-added = {2018-05-09 11:47:33 +0000},
	date-modified = {2018-05-09 11:47:33 +0000},
	eprint = {1412.6980},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning},
	month = dec,
	primaryclass = {cs.LG},
	title = {{Adam: A Method for Stochastic Optimization}},
	year = 2014}

@inproceedings{8302078,
	author = {X. Xu and J. Amaro and S. Caulfield and A. Forembski and G. Falcao and D. Moloney},
	booktitle = {2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
	date-added = {2018-05-09 11:20:24 +0000},
	date-modified = {2018-09-26 08:02:09 +0000},
	doi = {10.1109/CISP-BMEI.2017.8302078},
	keywords = {computer vision;convolution;feedforward neural nets;image classification;image processing;multiprocessing systems;object recognition;stereo image processing;CNN;3D volumetric representation;CNNs;Movidius Neural Compute Stick;USB;VOLA;Volumetric Accelerator;computational requirements;computer vision;dedicated CNN hardware blocks;low-power processing unit;synthetic 3D voxelized point-clouds generation method;trained model;training data;volumetric data;voxelized point-clouds classification;Computational modeling;Graphics processing units;Object recognition;Solid modeling;Task analysis;Three-dimensional displays;Training;Convolutional Neural Networks;Embedded Systems;Point-clouds},
	month = {Oct},
	pages = {1-7},
	title = {Convolutional neural network on neural compute stick for voxelized point-clouds classification},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CISP-BMEI.2017.8302078}}

@article{2015arXiv151003009L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151003009L},
	archiveprefix = {arXiv},
	author = {{Lin}, Z. and {Courbariaux}, M. and {Memisevic}, R. and {Bengio}, Y.},
	date-added = {2018-05-09 11:05:15 +0000},
	date-modified = {2018-05-09 11:05:15 +0000},
	eprint = {1510.03009},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	month = oct,
	primaryclass = {cs.LG},
	title = {{Neural Networks with Few Multiplications}},
	year = 2015}

@inproceedings{7929192,
	author = {E. Nurvitadhi and D. Sheffield and Jaewoong Sim and A. Mishra and G. Venkatesh and D. Marr},
	booktitle = {2016 International Conference on Field-Programmable Technology (FPT)},
	date-added = {2018-05-06 09:18:40 +0000},
	date-modified = {2020-07-18 21:53:19 +1200},
	doi = {10.1109/FPT.2016.7929192},
	keywords = {BNN, application specific integrated circuits;field programmable gate arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria 10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural networks;deep neural network;hardware acceleration;Biological neural networks;Field programmable gate arrays;Graphics processing units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep learning;FPGA;GPU;binarized neural networks;data analytics;hardware accelerator; FPGA, GPU},
	month = {Dec},
	pages = {77-84},
	title = {Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2016.7929192}}

@inproceedings{Zhao:2017:ABC:3020078.3021741,
	acmid = {3021741},
	annote = {Location
Monterey, California, USA

Address
New York, NY, USA},
	author = {Zhao, Ritchie and Song, Weinan and Zhang, Wentao and Xing, Tianwei and Lin, Jeng-Hau and Srivastava, Mani and Gupta, Rajesh and Zhang, Zhiru},
	booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-05-06 08:57:34 +0000},
	date-modified = {2020-07-21 14:13:17 +1200},
	doi = {10.1145/3020078.3021741},
	isbn = {978-1-4503-4354-1},
	keywords = {FPGAs, binarized, binarized convolutional networks, deep learning, high-level synthesis, reconfigurable computing},
	numpages = {10},
	pages = {15--24},
	publisher = {ACM},
	series = {FPGA '17},
	title = {Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs},
	url = {http://doi.acm.org/10.1145/3020078.3021741},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021741},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021741}}

@article{2016arXiv160106071K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160106071K},
	archiveprefix = {arXiv},
	author = {{Kim}, M. and {Smaragdis}, P.},
	date-added = {2018-05-06 05:27:19 +0000},
	date-modified = {2018-05-06 05:27:19 +0000},
	eprint = {1601.06071},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	month = jan,
	primaryclass = {cs.LG},
	title = {{Bitwise Neural Networks}},
	year = 2016}

@inbook{Baez2011,
	abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a ``cobordism'': a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and ``quantum topology''. But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of ``closed symmetric monoidal category''. We assume no prior knowledge of category theory, proof theory or computer science.},
	address = {Berlin, Heidelberg},
	author = {Baez, J. and Stay, M.},
	booktitle = {New Structures for Physics},
	date-added = {2018-04-29 11:38:03 +0000},
	date-modified = {2018-04-29 11:38:23 +0000},
	doi = {10.1007/978-3-642-12821-9_2},
	editor = {Coecke, Bob},
	isbn = {978-3-642-12821-9},
	keywords = {Category Theory, Physics, Logic, Computability},
	pages = {95--172},
	publisher = {Springer Berlin Heidelberg},
	title = {Physics, Topology, Logic and Computation: A Rosetta Stone},
	url = {https://doi.org/10.1007/978-3-642-12821-9_2},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-12821-9_2}}

@article{2017arXiv170404865J,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170404865J},
	archiveprefix = {arXiv},
	author = {{Juefei-Xu}, F. and {Naresh Boddeti}, V. and {Savvides}, M.},
	date-added = {2018-04-28 05:48:26 +0000},
	date-modified = {2018-04-28 05:48:26 +0000},
	eprint = {1704.04865},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	month = apr,
	primaryclass = {cs.CV},
	title = {{Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking}},
	year = 2017}

@article{5607329,
	abstract = {This paper presents the development and implementation of a generalized backpropagation multilayer perceptron (MLP) architecture described in VLSI hardware description language (VHDL). The development of hardware platforms has been complicated by the high hardware cost and quantity of the arithmetic operations required in online artificial neural networks (ANNs), i.e., general purpose ANNs with learning capability. Besides, there remains a dearth of hardware platforms for design space exploration, fast prototyping, and testing of these networks. Our general purpose architecture seeks to fill that gap and at the same time serve as a tool to gain a better understanding of issues unique to ANNs implemented in hardware, particularly using field programmable gate array (FPGA). The challenge is thus to find an architecture that minimizes hardware costs, while maximizing performance, accuracy, and parameterization. This work describes a platform that offers a high degree of parameterization, while maintaining generalized network design with performance comparable to other hardware-based MLP implementations. Application of the hardware implementation of ANN with backpropagation learning algorithm for a realistic application is also presented.},
	author = {A. Gomperts and A. Ukil and F. Zurfluh},
	date-added = {2018-04-28 04:52:24 +0000},
	date-modified = {2018-04-28 04:52:24 +0000},
	doi = {10.1109/TII.2010.2085006},
	issn = {1551-3203},
	journal = {IEEE Transactions on Industrial Informatics},
	keywords = {backpropagation;field programmable gate arrays;hardware description languages;multilayer perceptrons;FPGA;VLSI hardware description language;arithmetic operation;artificial neural network;backpropagation multilayer perceptron;fast prototyping;field programmable gate array;general purpose neural network;hardware-based MLP;learning capability;online application;space exploration;Backpropagation;NIR spectra calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware implementation;multilayer perceptron;neural network;spectroscopy},
	month = {Feb},
	number = {1},
	pages = {78-89},
	title = {Development and Implementation of Parameterized FPGA-Based General Purpose Neural Networks for Online Applications},
	volume = {7},
	year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TII.2010.2085006}}

@inproceedings{Sun:2018:FPR:3201607.3201741,
	acmid = {3201741},
	address = {Piscataway, NJ, USA},
	author = {Sun, Xiaoyu and Peng, Xiaochen and Chen, Pai-Yu and Liu, Rui and Seo, Jae-sun and Yu, Shimeng},
	booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
	date-added = {2018-04-28 04:34:43 +0000},
	date-modified = {2018-10-15 10:21:08 +0000},
	keywords = {BNN, P-BNN, CSM, MNIST},
	location = {Jeju, Republic of Korea},
	numpages = {6},
	pages = {574--579},
	publisher = {IEEE Press},
	series = {ASPDAC '18},
	title = {Fully Parallel RRAM Synaptic Array for Implementing Binary Neural Network with (+1, -1) Weights and (+1, 0) Neurons},
	url = {http://dl.acm.org/citation.cfm?id=3201607.3201741},
	year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3201607.3201741}}

@conference{8541786,
	adsurl = {http://hdl.handle.net/1854/LU-8541786},
	archiveprefix = {arXiv},
	author = {{Leroux}, S. and {Bohez}, S. and {Verbelen}, T. and {Vankeirsbilck}, B. and {Simoens}, P. and {Dhoedt}, B.},
	booktitle = {31st Conference on Neural Information Processing Systems},
	date-added = {2018-04-28 03:56:37 +0000},
	date-modified = {2018-12-06 11:17:15 +1300},
	eprint = {8541786},
	journal = {NIPS 2017},
	keywords = {BNN; Computer Science; Neural and Evolutionary Computing; Computer Vision; Pattern Recognition},
	month = Dec,
	organization = {NIPS},
	pages = {1-4},
	title = {{Transfer Learning with Binary Neural Networks}},
	year = 2017}

@article{2017arXiv170905306G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170905306G},
	archiveprefix = {arXiv},
	author = {{Guan}, T. and {Zeng}, X. and {Seok}, M.},
	date-added = {2018-04-28 03:26:42 +0000},
	date-modified = {2018-10-11 08:41:32 +0000},
	eprint = {1709.05306},
	journal = {ArXiv e-prints},
	keywords = {BNN; Neural and Evolutionary Computing},
	month = sep,
	title = {{Recursive Binary Neural Network Learning Model with 2.28b/Weight Storage Requirement}},
	year = 2017}

@inproceedings{McDanel:2017:EBN:3108009.3108031,
	acmid = {3108031},
	address = {USA},
	author = {McDanel, Bradley and Teerapittayanon, Surat and Kung, H.T.},
	booktitle = {Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks},
	date-added = {2018-04-28 01:31:18 +0000},
	date-modified = {2018-04-28 01:34:56 +0000},
	isbn = {978-0-9949886-1-4},
	keywords = {BNN, eBNN, CNN, MNIST, CIFAR},
	location = {Uppsala, Sweden},
	numpages = {6},
	pages = {168--173},
	publisher = {Junction Publishing},
	series = {EWSN \&\#8217;17},
	title = {Embedded Binarized Neural Networks},
	url = {http://dl.acm.org/citation.cfm?id=3108009.3108031},
	year = {2017},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3108009.3108031}}

@inproceedings{Tang2017HowTT,
	annote = {https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14619},
	author = {Wei Tang and Gang Hua and Liang Wang},
	booktitle = {AAAI},
	date-added = {2018-04-26 22:22:30 +0000},
	date-modified = {2020-10-16 23:44:30 +1300},
	keywords = {BNN, NN, Training},
	title = {How to Train a Compact Binary Neural Network with High Accuracy?},
	year = {2017}}

@inproceedings{Wei:2017:ASA:3061639.3062207,
	acmid = {3062207},
	address = {New York, NY, USA},
	articleno = {29},
	author = {Wei, Xuechao and Yu, Cody Hao and Zhang, Peng and Chen, Youxiang and Wang, Yuxin and Hu, Han and Liang, Yun and Cong, Jason},
	booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
	date-added = {2018-04-25 23:12:26 +0000},
	date-modified = {2018-04-26 22:23:20 +0000},
	doi = {10.1145/3061639.3062207},
	isbn = {978-1-4503-4927-7},
	keywords = {CNN, FPGA, Linear Algebra, Systolic Array},
	location = {Austin, TX, USA},
	numpages = {6},
	pages = {29:1--29:6},
	publisher = {ACM},
	series = {DAC '17},
	title = {Automated Systolic Array Architecture Synthesis for High Throughput CNN Inference on FPGAs},
	url = {http://doi.acm.org/10.1145/3061639.3062207},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3061639.3062207},
	Bdsk-Url-2 = {https://doi.org/10.1145/3061639.3062207}}

@article{1998adap.org..6001P,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/1998adap.org..6001P},
	author = {{Pang}, X. and {Werbos}, P.},
	date-added = {2018-04-24 23:59:42 +0000},
	date-modified = {2018-04-24 23:59:42 +0000},
	journal = {Advances in Astrophysics},
	keywords = {Adaptation, Noise, and Self-Organizing Systems, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Neurons and Cognition},
	month = jun,
	title = {{Neural network design for J function approximation in dynamic programming}},
	year = 1998}

@article{2018arXiv180200438Z,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200438Z},
	archiveprefix = {arXiv},
	author = {{Zohouri}, H.~R. and {Podobas}, A. and {Matsuoka}, S.},
	date-added = {2018-04-24 23:57:57 +0000},
	date-modified = {2018-04-24 23:57:57 +0000},
	eprint = {1802.00438},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	month = feb,
	primaryclass = {cs.DC},
	title = {{Combined Spatial and Temporal Blocking for High-Performance Stencil Computation on FPGAs Using OpenCL}},
	year = 2018}

@article{2017arXiv170703049M,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170703049M},
	archiveprefix = {arXiv},
	author = {{Mostafa}, H. and {Pedroni}, B. and {Sheik}, S. and {Cauwenberghs}, G.},
	date-added = {2018-04-24 23:57:22 +0000},
	date-modified = {2018-06-22 10:46:48 +0000},
	eprint = {1707.03049},
	journal = {ArXiv e-prints},
	keywords = {FPGA-NN, Computer Science - Neural and Evolutionary Computing},
	month = jun,
	title = {{Hardware-efficient on-line learning through pipelined truncated-error backpropagation in binary-state networks}},
	year = 2017}

@article{2018arXiv180303790S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303790S},
	archiveprefix = {arXiv},
	author = {{Shen}, J. and {Qiao}, Y. and {Huang}, Y. and {Wen}, M. and {Zhang}, C.},
	date-added = {2018-04-24 23:57:08 +0000},
	date-modified = {2018-04-24 23:57:08 +0000},
	eprint = {1803.03790},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Hardware Architecture},
	month = mar,
	title = {{Towards a Multi-array Architecture for Accelerating Large-scale Matrix Multiplication on FPGAs}},
	year = 2018}

@article{2016arXiv161200694H,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200694H},
	archiveprefix = {arXiv},
	author = {{Han}, S. and {Kang}, J. and {Mao}, H. and {Hu}, Y. and {Li}, X. and {Li}, Y. and {Xie}, D. and {Luo}, H. and {Yao}, S. and {Wang}, Y. and {Yang}, H. and {Dally}, W.~J.},
	date-added = {2018-04-24 10:49:59 +0000},
	date-modified = {2018-04-24 10:49:59 +0000},
	eprint = {1612.00694},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computation and Language},
	month = dec,
	primaryclass = {cs.CL},
	title = {{ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA}},
	year = 2016}

@article{2011arXiv1107.1831H,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1107.1831H},
	archiveprefix = {arXiv},
	author = {{Homescu}, C.},
	date-added = {2018-04-15 11:10:18 +0000},
	date-modified = {2018-04-15 11:10:18 +0000},
	eprint = {1107.1831},
	journal = {ArXiv e-prints},
	keywords = {Quantitative Finance - Computational Finance},
	month = jul,
	primaryclass = {q-fin.CP},
	title = {{Adjoints and Automatic (Algorithmic) Differentiation in Computational Finance}},
	year = 2011}

@inproceedings{1188677,
	author = {A. A. Gaffar and O. Mencer and W. Luk and P. Y. K. Cheung and N. Shirazi},
	booktitle = {2002 IEEE International Conference on Field-Programmable Technology, 2002. (FPT). Proceedings.},
	date-added = {2018-04-15 10:59:42 +0000},
	date-modified = {2018-04-15 10:59:42 +0000},
	doi = {10.1109/FPT.2002.1188677},
	keywords = {FIR filters;VLSI;circuit CAD;circuit optimisation;data flow graphs;differentiation;digital filters;digital signal processing chips;discrete Fourier transforms;field programmable gate arrays;floating point arithmetic;high level synthesis;integrated circuit design;sensitivity analysis;DFT implementation;FIR filter implementation;FPGA;VLSI circuits;arithmetic operations;automatic bitwidth analysis;automatic differentiation;dataflow graph representation;discrete Fourier transform implementation;floating-point bitwidth analysis;floating-point designs;high-level programming;high-level synthesis;mathematical method;precision analysis;sensitivity analysis;user-defined numerical constraints;Arithmetic;Automatic programming;Circuits;Data analysis;Discrete Fourier transforms;Field programmable gate arrays;Finite impulse response filter;High level synthesis;Sensitivity analysis;Very large scale integration},
	month = {Dec},
	pages = {158-165},
	title = {Floating-point bitwidth analysis via automatic differentiation},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2002.1188677}}

@inproceedings{Elliott-2009-beautiful-differentiation,
	author = {Conal Elliott},
	date-added = {2018-04-13 05:07:34 +0000},
	date-modified = {2018-04-13 05:07:34 +0000},
	journal = {International Conference on Functional Programming (ICFP)},
	keywords = {Automatic Differentiation, Vector Spaces, Haskell},
	month = sep,
	number = {ICFP},
	title = {Beautiful Differentiation},
	url = {http://conal.net/papers/beautiful-differentiation/},
	year = {2009},
	Bdsk-Url-1 = {http://conal.net/papers/beautiful-differentiation/}}

@article{Elliott-2017-compiling-to-categories,
	articleno = {48},
	author = {Conal Elliott},
	date-added = {2018-04-07 11:10:44 +0000},
	date-modified = {2018-04-07 11:10:44 +0000},
	doi = {http://dx.doi.org/10.1145/3110271},
	journal = {Proc. ACM Program. Lang.},
	keywords = {Automatic Differentiation, Category Theory, Haskell},
	month = sep,
	number = {ICFP},
	numpages = {24},
	title = {Compiling To Categories},
	url = {http://conal.net/papers/compiling-to-categories},
	volume = {1},
	year = {2017},
	Bdsk-Url-1 = {http://conal.net/papers/compiling-to-categories},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3110271}}

@inproceedings{Elliott-2018-ad-icfp,
	abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm can be specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.
},
	author = {Conal Elliott},
	booktitle = {Proceedings of the ACM on Programming Languages (ICFP)},
	date-added = {2018-04-06 11:42:18 +0000},
	date-modified = {2018-12-03 19:46:22 +1300},
	eprint = {1804.00746},
	keywords = {Haskell, Automatic Differentiation, Math, Category Theory},
	month = {March},
	title = {The simple essence of automatic differentiation},
	url = {http://conal.net/papers/essence-of-ad/},
	volume = {abs/1804.00746},
	year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.00746},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1804.00746},
	Bdsk-Url-3 = {http://conal.net/papers/essence-of-ad/}}

@article{Karczmarczuk2001,
	abstract = {We present a purely functional implementation of the computational differentiation tools---the well known numeric (i.e., not symbolic) techniques which permit one to compute point-wise derivatives of functions defined by computer programs economically and exactly (with machine precision). We show how the use of lazy evaluation permits a transparent and elegant construction of the entire infinite tower of derivatives of higher order for any expressions present in the program. The formalism may be useful in various problems of scientific computing which often demand a hard and ungracious human preprocessing before writing the final code. Some concrete examples are given.},
	author = {Karczmarczuk, Jerzy},
	date-added = {2018-04-06 04:46:21 +0000},
	date-modified = {2018-04-06 04:49:58 +0000},
	day = {01},
	doi = {10.1023/A:1011501232197},
	issn = {1573-0557},
	journal = {Higher-Order and Symbolic Computation},
	keywords = {Mathematics, Derivates, Automatic Differentiation, Computer Science},
	month = {Mar},
	number = {1},
	pages = {35--57},
	title = {Functional Differentiation of Computer Programs},
	url = {https://doi.org/10.1023/A:1011501232197},
	volume = {14},
	year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1011501232197}}

@article{2014arXiv1404.7456G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1404.7456G},
	archiveprefix = {arXiv},
	author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A.},
	date-added = {2018-04-04 09:50:26 +0000},
	date-modified = {2018-09-26 08:04:03 +0000},
	eprint = {1404.7456},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Symbolic Computation, Statistics - Machine Learning, Automatic Differentiation, Automatic Differentiation, Automatic Differentiation, G.1.4, I.2.6},
	month = apr,
	primaryclass = {cs.LG},
	title = {{Automatic Differentiation of Algorithms for Machine Learning}},
	year = 2014}

@article{Gremse:2016aa,
	abstract = {Many scientific problems such as classifier training or medical image reconstruction can be expressed as minimization of differentiable real-valued cost functions and solved with iterative gradient-based methods. Adjoint algorithmic differentiation (AAD) enables automated computation of gradients of such cost functions implemented as computer programs. To backpropagate adjoint derivatives, excessive memory is potentially required to store the intermediate partial derivatives on a dedicated data structure, referred to as the ``tape''. Parallelization is difficult because threads need to synchronize their accesses during taping and backpropagation. This situation is aggravated for many-core architectures, such as Graphics Processing Units (GPUs), because of the large number of light-weight threads and the limited memory size in general as well as per thread. We show how these limitations can be mediated if the cost function is expressed using GPU-accelerated vector and matrix operations which are recognized as intrinsic functions by our AAD software. We compare this approach with naive and vectorized implementations for CPUs. We use four increasingly complex cost functions to evaluate the performance with respect to memory consumption and gradient computation times. Using vectorization, CPU and GPU memory consumption could be substantially reduced compared to the naive reference implementation, in some cases even by an order of complexity. The vectorization allowed usage of optimized parallel libraries during forward and reverse passes which resulted in high speedups for the vectorized CPU version compared to the naive reference implementation. The GPU version achieved an additional speedup of 7.5 $\pm$4.4, showing that the processing power of GPUs can be utilized for AAD using this concept. Furthermore, we show how this software can be systematically extended for more complex problems such as nonlinear absorption reconstruction for fluorescence-mediated tomography.},
	an = {PMC4772124},
	author = {Gremse, Felix and H{\"o}fter, Andreas and Razik, Lukas and Kiessling, Fabian and Naumann, Uwe},
	date = {2016/03/01},
	date-added = {2018-04-04 08:10:03 +0000},
	date-modified = {2018-04-04 08:10:23 +0000},
	db = {PMC},
	doi = {10.1016/j.cpc.2015.10.027},
	isbn = {0010-4655},
	j1 = {Comput Phys Commun},
	journal = {Computer physics communications},
	keywords = {GPU, Automatic Differentiation, Math, Neural Network},
	month = {03},
	pages = {300--311},
	title = {GPU-Accelerated Adjoint Algorithmic Differentiation},
	ty = {JOUR},
	u1 = {26941443{$[$}pmid{$]$}},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	volume = {200},
	year = {2016},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Bdsk-Url-2 = {https://dx.doi.org/10.1016/j.cpc.2015.10.027}}

@article{6701396,
	author = {D. Neil and S. C. Liu},
	date-added = {2018-03-20 09:15:46 +0000},
	date-modified = {2018-03-20 09:15:46 +0000},
	doi = {10.1109/TVLSI.2013.2294916},
	issn = {1063-8210},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {field programmable gate arrays;neural nets;CPU;MNIST handwritten digit classification;Minitaur;event-driven FPGA;event-driven neural network accelerator;field-programmable gate array-based system;neural networks;newsgroups classification data;robotics;spiking deep network;spiking network accelerator;Biological neural networks;Clocks;Computer architecture;Field programmable gate arrays;Mathematical model;Neurons;Performance evaluation;Deep belief networks;field programmable arrays;machine learning;neural networks;restricted Boltzmann machines;spiking neural networks},
	month = {Dec},
	number = {12},
	pages = {2621-2628},
	title = {Minitaur, an Event-Driven FPGA-Based Spiking Network Accelerator},
	volume = {22},
	year = {2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2013.2294916}}

@book{Make-Your-Own-Neural-Network,
	adsurl = {https://www.amazon.co.uk/Make-Your-Own-Neural-Network/dp/1530826608},
	author = {{Rashid}, Tariq},
	date-added = {2018-03-03 10:33:43 +0000},
	date-modified = {2018-03-03 10:33:43 +0000},
	keywords = {Computer Science, Neural Network, Python},
	month = March,
	title = {{Make Your Own Neural Network}},
	year = 2016}

@article{798320,
	author = {C. Elliott},
	date-added = {2018-02-26 09:50:52 +0000},
	date-modified = {2018-02-26 09:50:52 +0000},
	doi = {10.1109/32.798320},
	issn = {0098-5589},
	journal = {IEEE Transactions on Software Engineering},
	keywords = {computer animation;multimedia computing;simulation languages;Fran;Haskell;declarative host language;embedded domain-specific vocabulary;embedded modeling language approach;growth;interactive 3D animation;interactive multimedia animation;modeled animation;motion;Animation;Automatic programming;Computer graphics;Computer languages;Domain specific languages;Functional programming;Programming profession;Shape;Vocabulary;Writing},
	month = {May},
	number = {3},
	pages = {291-308},
	title = {An embedded modeling language approach to interactive 3D and multimedia animation},
	volume = {25},
	year = {1999},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/32.798320}}

@techreport{Hudak94vs.ada,
	author = {Paul Hudak and Mark P. Jones},
	date-added = {2018-02-26 09:38:35 +0000},
	date-modified = {2018-02-26 09:38:35 +0000},
	keywords = {DARPA, haskell, Cpp, Awk},
	title = {vs. Ada vs. C++ vs. Awk vs. ... An Experiment in Software Prototyping Productivity Available from http://www.haskell.org/papers/NSWC/jfp.ps},
	year = {1994}}

@inproceedings{Totoo:2012:HVF:2364474.2364483,
	acmid = {2364483},
	address = {New York, NY, USA},
	author = {Totoo, Prabhat and Deligiannis, Pantazis and Loidl, Hans-Wolfgang},
	booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	date-added = {2018-02-26 08:51:00 +0000},
	date-modified = {2018-02-26 08:51:00 +0000},
	doi = {10.1145/2364474.2364483},
	isbn = {978-1-4503-1577-7},
	keywords = {barnes-hut, f\#, haskell, n-body, parallelism, scala},
	location = {Copenhagen, Denmark},
	numpages = {12},
	pages = {49--60},
	publisher = {ACM},
	series = {FHPC '12},
	title = {Haskell vs. F\# vs. Scala: A High-level Language Features and Parallelism Support Comparison},
	url = {http://doi.acm.org/10.1145/2364474.2364483},
	year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2364474.2364483},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364483}}

@inproceedings{6209130,
	author = {M. Fenwick and C. Sesanker and M. R. Schiller and H. J. Ellis and M. L. Hinman and J. Vyas and M. R. Gryk},
	booktitle = {2012 Ninth International Conference on Information Technology - New Generations},
	date-added = {2018-02-26 08:28:41 +0000},
	date-modified = {2018-02-26 08:28:41 +0000},
	doi = {10.1109/ITNG.2012.21},
	keywords = {Java;LISP;bioinformatics;functional programming;learning (artificial intelligence);public domain software;software engineering;Haskell;Java;LISP;Python;algorithm development;bioinformatics;complex data operations;complex mathematical notions;data processing;functional computing;functional languages;functional programming accessibility;functional programming techniques;learning curve;learning resources;machine learning;multilanguage source-code repository;open-source Sandbox;scientific communities;software integration;Bioinformatics;Data visualization;Functional programming;Nuclear magnetic resonance;Proteins;Schedules;Transient analysis;Clojure;Haskell;Java;LISP;NMR;bioinformatics;functional-programming},
	month = {April},
	pages = {89-94},
	title = {An Open-Source Sandbox for Increasing the Accessibility of Functional Programming to the Bioinformatics and Scientific Communities},
	year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBrLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vR2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWJPEQJyAAAAAAJyAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fR2FuZyBvZiBHQU5zLSBHZW5lI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABURDR0FOAAACAIEvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RENHQU46R2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWIAAA4AnABNAEcAYQBuAGcAIABvAGYAIABHAEEATgBzAC0AIABHAGUAbgBlAHIAYQB0AGkAdgBlACAAQQBkAHYAZQByAHMAYQByAGkAYQBsACAATgBlAHQAdwBvAHIAawBzACAAdwBpAHQAaAAgAE0AYQB4AGkAbQB1AG0AIABNAGEAcgBnAGkAbgAgAFIAYQBuAGsAaQBuAGcALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAH9Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0RDR0FOL0dhbmcgb2YgR0FOcy0gR2VuZXJhdGl2ZSBBZHZlcnNhcmlhbCBOZXR3b3JrcyB3aXRoIE1heGltdW0gTWFyZ2luIFJhbmtpbmcuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACSAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAwg=},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ITNG.2012.21}}

@inproceedings{Pop:2010:ERH:1863543.1863595,
	acmid = {1863595},
	address = {New York, NY, USA},
	author = {Pop, Iustin},
	booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
	date-added = {2018-02-26 08:27:07 +0000},
	date-modified = {2018-02-26 08:27:07 +0000},
	doi = {10.1145/1863543.1863595},
	isbn = {978-1-60558-794-3},
	keywords = {ganeti, haskell, python, system administration},
	location = {Baltimore, Maryland, USA},
	numpages = {6},
	pages = {369--374},
	publisher = {ACM},
	series = {ICFP '10},
	title = {Experience Report: Haskell As a Reagent: Results and Observations on the Use of Haskell in a Python Project},
	url = {http://doi.acm.org/10.1145/1863543.1863595},
	year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1863543.1863595},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1863543.1863595}}

@inproceedings{Nanz:2015:CSP:2818754.2818848,
	acmid = {2818848},
	address = {Piscataway, NJ, USA},
	author = {Nanz, Sebastian and Furia, Carlo A.},
	booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
	date-added = {2018-02-26 07:03:13 +0000},
	date-modified = {2018-04-15 11:11:54 +0000},
	isbn = {978-1-4799-1934-5},
	keywords = {Languages, Haskell, C++},
	location = {Florence, Italy},
	numpages = {11},
	pages = {778--788},
	publisher = {IEEE Press},
	series = {ICSE '15},
	title = {A Comparative Study of Programming Languages in Rosetta Code},
	url = {http://dl.acm.org/citation.cfm?id=2818754.2818848},
	year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2818754.2818848}}

@article{2010arXiv1009.0305R,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1009.0305R},
	archiveprefix = {arXiv},
	author = {{Rabah}, S. and {Li}, J. and {Liu}, M. and {Lai}, Y.},
	date-added = {2018-02-22 23:19:52 +0000},
	date-modified = {2018-02-22 23:19:52 +0000},
	eprint = {1009.0305},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Programming Languages, D.3},
	month = sep,
	primaryclass = {cs.PL},
	title = {{Comparative Studies of 10 Programming Languages within 10 Diverse Criteria -- a Team 7 COMP6411-S10 Term Report}},
	year = 2010}

@article{2017arXiv171109846J,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171109846J},
	archiveprefix = {arXiv},
	author = {{Jaderberg}, M. and {Dalibard}, V. and {Osindero}, S. and {Czarnecki}, W.~M. and {Donahue}, J. and {Razavi}, A. and {Vinyals}, O. and {Green}, T. and {Dunning}, I. and {Simonyan}, K. and {Fernando}, C. and {Kavukcuoglu}, K.},
	date-added = {2018-02-21 06:49:04 +0000},
	date-modified = {2018-02-21 06:49:04 +0000},
	eprint = {1711.09846},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	month = nov,
	primaryclass = {cs.LG},
	title = {{Population Based Training of Neural Networks}},
	year = 2017}

@article{2014arXiv1410.5401G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1410.5401G},
	archiveprefix = {arXiv},
	author = {{Graves}, A. and {Wayne}, G. and {Danihelka}, I.},
	date-added = {2018-02-21 06:07:10 +0000},
	date-modified = {2018-02-21 06:07:10 +0000},
	eprint = {1410.5401},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	month = oct,
	title = {{Neural Turing Machines}},
	year = 2014}

@article{2015arXiv150308895S,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150308895S},
	archiveprefix = {arXiv},
	author = {{Sukhbaatar}, S. and {Szlam}, A. and {Weston}, J. and {Fergus}, R.},
	date-added = {2018-02-21 05:45:01 +0000},
	date-modified = {2018-02-21 05:45:01 +0000},
	eprint = {1503.08895},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	month = mar,
	title = {{End-To-End Memory Networks}},
	year = 2015}

@article{2014arXiv1409.0473B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.0473B},
	archiveprefix = {arXiv},
	author = {{Bahdanau}, D. and {Cho}, K. and {Bengio}, Y.},
	date-added = {2018-02-21 05:29:04 +0000},
	date-modified = {2018-02-21 05:29:04 +0000},
	eprint = {1409.0473},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	month = sep,
	primaryclass = {cs.CL},
	title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
	year = 2014}

@article{2015arXiv150203044X,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150203044X},
	archiveprefix = {arXiv},
	author = {{Xu}, K. and {Ba}, J. and {Kiros}, R. and {Cho}, K. and {Courville}, A. and {Salakhutdinov}, R. and {Zemel}, R. and {Bengio}, Y.},
	date-added = {2018-02-21 04:55:23 +0000},
	date-modified = {2018-02-21 04:55:23 +0000},
	eprint = {1502.03044},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	month = feb,
	primaryclass = {cs.LG},
	title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
	year = 2015}

@article{2015arXiv150301007J,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150301007J},
	archiveprefix = {arXiv},
	author = {{Joulin}, A. and {Mikolov}, T.},
	date-added = {2018-02-21 04:03:18 +0000},
	date-modified = {2018-02-21 04:03:18 +0000},
	eprint = {1503.01007},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	month = mar,
	title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
	year = 2015}

@article{2014arXiv1406.6247M,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.6247M},
	archiveprefix = {arXiv},
	author = {{Mnih}, V. and {Heess}, N. and {Graves}, A. and {Kavukcuoglu}, K.},
	date-added = {2018-02-21 04:01:10 +0000},
	date-modified = {2018-02-21 04:01:10 +0000},
	eprint = {1406.6247},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	month = jun,
	primaryclass = {cs.LG},
	title = {{Recurrent Models of Visual Attention}},
	year = 2014}

@article{2013arXiv1308.0850G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1308.0850G},
	archiveprefix = {arXiv},
	author = {{Graves}, A.},
	date-added = {2018-02-21 03:44:05 +0000},
	date-modified = {2018-02-21 03:44:05 +0000},
	eprint = {1308.0850},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	month = aug,
	title = {{Generating Sequences With Recurrent Neural Networks}},
	year = 2013}

@article{2015arXiv150204390D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204390D},
	archiveprefix = {arXiv},
	author = {{Dauphin}, Y.~N. and {de Vries}, H. and {Bengio}, Y.},
	date-added = {2018-02-21 02:45:02 +0000},
	date-modified = {2018-02-21 02:45:02 +0000},
	eprint = {1502.04390},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Numerical Analysis},
	month = feb,
	primaryclass = {cs.LG},
	title = {{Equilibrated adaptive learning rates for non-convex optimization}},
	year = 2015}

@article{2016arXiv160604934K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604934K},
	archiveprefix = {arXiv},
	author = {{Kingma}, D.~P. and {Salimans}, T. and {Jozefowicz}, R. and {Chen}, X. and {Sutskever}, I. and {Welling}, M.},
	date-added = {2018-02-20 10:42:29 +0000},
	date-modified = {2018-02-20 10:42:29 +0000},
	eprint = {1606.04934},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	month = jun,
	primaryclass = {cs.LG},
	title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
	year = 2016}

@article{2015arXiv150204623G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204623G},
	archiveprefix = {arXiv},
	author = {{Gregor}, K. and {Danihelka}, I. and {Graves}, A. and {Jimenez Rezende}, D. and {Wierstra}, D.},
	date-added = {2018-02-20 10:42:05 +0000},
	date-modified = {2018-02-20 10:42:05 +0000},
	eprint = {1502.04623},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	month = feb,
	primaryclass = {cs.CV},
	title = {{DRAW: A Recurrent Neural Network For Image Generation}},
	year = 2015}

@article{2016arXiv160603657C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603657C},
	archiveprefix = {arXiv},
	author = {{Chen}, X. and {Duan}, Y. and {Houthooft}, R. and {Schulman}, J. and {Sutskever}, I. and {Abbeel}, P.},
	date-added = {2018-02-20 00:17:44 +0000},
	date-modified = {2018-02-20 00:17:44 +0000},
	eprint = {1606.03657},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	month = jun,
	primaryclass = {cs.LG},
	title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	year = 2016}

@article{2016arXiv160509674H,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160509674H},
	archiveprefix = {arXiv},
	author = {{Houthooft}, R. and {Chen}, X. and {Duan}, Y. and {Schulman}, J. and {De Turck}, F. and {Abbeel}, P.},
	date-added = {2018-02-19 23:51:31 +0000},
	date-modified = {2018-02-19 23:51:31 +0000},
	eprint = {1605.09674},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	month = may,
	primaryclass = {cs.LG},
	title = {{VIME: Variational Information Maximizing Exploration}},
	year = 2016}

@article{2016arXiv160603476H,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603476H},
	archiveprefix = {arXiv},
	author = {{Ho}, J. and {Ermon}, S.},
	date-added = {2018-02-19 10:11:33 +0000},
	date-modified = {2018-02-19 10:11:33 +0000},
	eprint = {1606.03476},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	month = jun,
	primaryclass = {cs.LG},
	title = {{Generative Adversarial Imitation Learning}},
	year = 2016}

@article{SalimansGZCRC16,
	archiveprefix = {arXiv},
	author = {Tim Salimans and Ian J. Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
	bibsource = {dblp computer science bibliography, http://dblp.org},
	biburl = {http://dblp.org/rec/bib/journals/corr/SalimansGZCRC16},
	date-added = {2018-02-19 09:53:01 +0000},
	date-modified = {2018-10-29 11:30:01 +0000},
	eprint = {1606.03498},
	journal = {CoRR},
	keywords = {DCGAN, MNIST, Semi-Supervised learning, minibatch},
	timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	title = {Improved Techniques for Training GANs},
	url = {http://arxiv.org/abs/1606.03498},
	volume = {abs/1606.03498},
	year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1606.03498}}

@article{2013arXiv1312.6114K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
	archiveprefix = {arXiv},
	author = {{Kingma}, D.~P and {Welling}, M.},
	date-added = {2018-02-19 09:25:50 +0000},
	date-modified = {2018-02-19 09:25:50 +0000},
	eprint = {1312.6114},
	journal = {ArXiv e-prints},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	month = dec,
	primaryclass = {stat.ML},
	title = {{Auto-Encoding Variational Bayes}},
	year = 2013,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCHLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0Egc3ltbWV0cmljIGtleSBjcnlwdG9ncmFwaHkgdXNpbmcgZ2VuZXRpYyBhbGdvcml0aG0gYW5kIGVycm9yIGJhY2sgcHJvcGFnYXRpb24gbmV1cmFsIG5ldHdvcmsuYmliTxEC2gAAAAAC2gACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0Egc3ltbWV0cmljIGtleSBjciNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAxFdm9sdXRpb25hcnkAAgCdLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkV2b2x1dGlvbmFyeTpBIHN5bW1ldHJpYyBrZXkgY3J5cHRvZ3JhcGh5IHVzaW5nIGdlbmV0aWMgYWxnb3JpdGhtIGFuZCBlcnJvciBiYWNrIHByb3BhZ2F0aW9uIG5ldXJhbCBuZXR3b3JrLmJpYgAADgDGAGIAQQAgAHMAeQBtAG0AZQB0AHIAaQBjACAAawBlAHkAIABjAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAgAHUAcwBpAG4AZwAgAGcAZQBuAGUAdABpAGMAIABhAGwAZwBvAHIAaQB0AGgAbQAgAGEAbgBkACAAZQByAHIAbwByACAAYgBhAGMAawAgAHAAcgBvAHAAYQBnAGEAdABpAG8AbgAgAG4AZQB1AHIAYQBsACAAbgBlAHQAdwBvAHIAawAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAm1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0Egc3ltbWV0cmljIGtleSBjcnlwdG9ncmFwaHkgdXNpbmcgZ2VuZXRpYyBhbGdvcml0aG0gYW5kIGVycm9yIGJhY2sgcHJvcGFnYXRpb24gbmV1cmFsIG5ldHdvcmsuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACuAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA4w=}}

@article{DBLP:journals/corr/OordKK16,
	archiveprefix = {arXiv},
	author = {A{\"{a}}ron van den Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
	bibsource = {dblp computer science bibliography, http://dblp.org},
	biburl = {http://dblp.org/rec/bib/journals/corr/OordKK16},
	date-added = {2018-02-19 09:01:51 +0000},
	date-modified = {2018-02-19 09:05:45 +0000},
	eprint = {1601.06759},
	journal = {CoRR},
	keywords = {RNN, PixelRNN, BiLSTM, MNIST},
	timestamp = {Wed, 07 Jun 2017 14:40:22 +0200},
	title = {Pixel Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1601.06759},
	volume = {abs/1601.06759},
	year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1601.06759}}

@article{DBLP:journals/corr/KulkarniWKT15,
	archiveprefix = {arXiv},
	author = {Tejas D. Kulkarni and Will Whitney and Pushmeet Kohli and Joshua B. Tenenbaum},
	bibsource = {dblp computer science bibliography, http://dblp.org},
	biburl = {http://dblp.org/rec/bib/journals/corr/KulkarniWKT15},
	date-added = {2018-02-19 04:44:04 +0000},
	date-modified = {2018-02-19 04:44:43 +0000},
	eprint = {1503.03167},
	journal = {CoRR},
	keywords = {CNN, IGN, SGVB, DC-IGN},
	timestamp = {Wed, 07 Jun 2017 14:40:29 +0200},
	title = {Deep Convolutional Inverse Graphics Network},
	url = {http://arxiv.org/abs/1503.03167},
	volume = {abs/1503.03167},
	year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.03167}}

@article{2017arXiv170100160G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
	archiveprefix = {arXiv},
	author = {{Goodfellow}, I.},
	date-added = {2018-02-16 06:51:27 +0000},
	date-modified = {2018-02-16 06:51:27 +0000},
	eprint = {1701.00160},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning},
	month = dec,
	primaryclass = {cs.LG},
	title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
	year = 2017}

@article{2017arXiv170900199H,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170900199H},
	archiveprefix = {arXiv},
	author = {{Hadad}, N. and {Wolf}, L. and {Shahar}, M.},
	date-added = {2018-02-07 01:32:29 +0000},
	date-modified = {2018-02-07 01:32:29 +0000},
	eprint = {1709.00199},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	month = sep,
	primaryclass = {cs.LG},
	title = {{Two-Step Disentanglement for Financial Data}},
	year = 2017}

@inproceedings{10.1007/3-540-28438-9_2,
	abstract = {Backwards calculation of derivatives -- sometimes called the reverse mode, the full adjoint method, or backpropagation -- has been developed and applied in many fields. This paper reviews several strands of history, advanced capabilities and types of application -- particularly those which are crucial to the development of brain-like capabilities in intelligent control and artificial intelligence.},
	address = {Berlin, Heidelberg},
	author = {Werbos, Paul J.},
	booktitle = {Automatic Differentiation: Applications, Theory, and Implementations},
	date-added = {2018-02-07 01:11:34 +0000},
	date-modified = {2018-04-15 11:11:31 +0000},
	editor = {B{\"u}cker, Martin and Corliss, George and Naumann, Uwe and Hovland, Paul and Norris, Boyana},
	isbn = {978-3-540-28438-3},
	keywords = {NN, Backpropagation},
	pages = {15--34},
	publisher = {Springer Berlin Heidelberg},
	title = {Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities},
	year = {2006}}

@article{2017arXiv170107274L,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107274L},
	archiveprefix = {arXiv},
	author = {{Li}, Y.},
	date-added = {2018-02-06 09:19:00 +0000},
	date-modified = {2018-02-06 09:19:00 +0000},
	eprint = {1701.07274},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning},
	month = jan,
	primaryclass = {cs.LG},
	title = {{Deep Reinforcement Learning: An Overview}},
	year = 2017}

@incollection{NIPS2017_6917,
	author = {XIAO, SHUAI and Farajtabar, Mehrdad and Ye, Xiaojing and Yan, Junchi and Yang, Xiaokang and Song, Le and Zha, Hongyuan},
	booktitle = {Advances in Neural Information Processing Systems 30},
	date-added = {2018-02-06 09:12:15 +0000},
	date-modified = {2018-02-06 09:13:20 +0000},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	keywords = {DNN, Generative Adversarial NN, Point processes},
	pages = {3250--3259},
	publisher = {Curran Associates, Inc.},
	title = {Wasserstein Learning of Deep Generative Point Process Models},
	url = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf},
	year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf}}

@book{TheWayOfTheTurtle,
	author = {Curtis M. Faith},
	date-added = {2018-02-06 08:38:52 +0000},
	date-modified = {2018-02-06 08:40:50 +0000},
	keywords = {Finance, Economics Personal, Professional Development},
	month = {March},
	number = {9780071486644},
	publisher = {McGraw-Hill Osborne Media},
	title = {Way of the Turtle},
	year = {2007}}

@article{2015arXiv150205767G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205767G},
	archiveprefix = {arXiv},
	author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A. and {Andreyevich Radul}, A. and {Siskind}, J.~M.},
	date-added = {2018-02-06 01:01:08 +0000},
	date-modified = {2018-09-26 08:04:03 +0000},
	eprint = {1502.05767},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Automatic Differentiation, Automatic Differentiation, Automatic Differentiation, G.1.4, I.2.6},
	month = feb,
	primaryclass = {cs.SC},
	title = {{Automatic differentiation in machine learning: a survey}},
	year = 2015}

@misc{2015arXiv151106434R-sc,
	author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	date-added = {2018-02-05 09:50:12 +0000},
	date-modified = {2018-02-23 00:38:59 +0000},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	month = nov,
	title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	url = {https://github.com/Newmu/dcgan_code},
	year = 2015,
	Bdsk-Url-1 = {https://github.com/Newmu/dcgan_code}}

@article{2015arXiv151106434R,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
	archiveprefix = {arXiv},
	author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	date-added = {2018-02-05 09:50:12 +0000},
	date-modified = {2018-02-05 09:50:12 +0000},
	eprint = {1511.06434},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	month = nov,
	primaryclass = {cs.LG},
	title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	year = 2015}

@misc{Grenade-Github-Code,
	address = {San Francisco, CA, USA},
	author = {Campbell, Huw},
	date-added = {2018-02-05 09:27:10 +0000},
	date-modified = {2018-02-23 00:39:26 +0000},
	day = {24},
	keywords = {functional programming, DNN, CNN, sourcecode},
	month = {June},
	publisher = {Github},
	title = {Grenade},
	url = {https://github.com/HuwCampbell/grenade},
	year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB1Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRGVjZW50cmFsaXplZC9CaXRXb3JrZXIsIGEgRGVjZW50cmFsaXplZCBEaXN0cmlidXRlZCBDb21wdXRpbmcgU3lzdGVtIEJhc2VkIG9uIEJpdFRvcnJlbnQuYmliTxECkgAAAAACkgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0JpdFdvcmtlciwgYSBEZWNlbiNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAA1EZWNlbnRyYWxpemVkAAACAIsvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RGVjZW50cmFsaXplZDpCaXRXb3JrZXIsIGEgRGVjZW50cmFsaXplZCBEaXN0cmlidXRlZCBDb21wdXRpbmcgU3lzdGVtIEJhc2VkIG9uIEJpdFRvcnJlbnQuYmliAAAOAKAATwBCAGkAdABXAG8AcgBrAGUAcgAsACAAYQAgAEQAZQBjAGUAbgB0AHIAYQBsAGkAegBlAGQAIABEAGkAcwB0AHIAaQBiAHUAdABlAGQAIABDAG8AbQBwAHUAdABpAG4AZwAgAFMAeQBzAHQAZQBtACAAQgBhAHMAZQBkACAAbwBuACAAQgBpAHQAVABvAHIAcgBlAG4AdAAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAiVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRGVjZW50cmFsaXplZC9CaXRXb3JrZXIsIGEgRGVjZW50cmFsaXplZCBEaXN0cmlidXRlZCBDb21wdXRpbmcgU3lzdGVtIEJhc2VkIG9uIEJpdFRvcnJlbnQuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACcAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAzI=},
	Bdsk-Url-1 = {https://github.com/HuwCampbell/grenade}}

@inproceedings{Zhang:2015:OFA:2684746.2689060,
	acmid = {2689060},
	address = {New York, NY, USA},
	author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	date-added = {2018-02-05 09:06:58 +0000},
	date-modified = {2018-09-26 08:04:22 +0000},
	doi = {10.1145/2684746.2689060},
	isbn = {978-1-4503-3315-3},
	keywords = {Acceleration, convolutional neural network, fpga, roofline model},
	location = {Monterey, California, USA},
	numpages = {10},
	pages = {161--170},
	publisher = {ACM},
	series = {FPGA '15},
	title = {Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks},
	url = {http://doi.acm.org/10.1145/2684746.2689060},
	year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689060},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2684746.2689060}}

@article{2016arXiv161102450W,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161102450W},
	archiveprefix = {arXiv},
	author = {{Wang}, D. and {An}, J. and {Xu}, K.},
	date-added = {2018-02-05 09:06:58 +0000},
	date-modified = {2018-06-22 10:46:32 +0000},
	eprint = {1611.02450},
	journal = {ArXiv e-prints},
	keywords = {FPGA-NN, Computer Science - Hardware Architecture},
	month = nov,
	title = {{PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks}},
	year = 2016}

@article{anonymous2018wavelet,
	author = {Anonymous},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-12-02 12:41:25 +1300},
	journal = {International Conference on Learning Representations},
	keywords = {CNN, Wavelet, MIST, MathConvNet},
	title = {Wavelet Pooling for Convolutional Neural Networks},
	url = {https://openreview.net/forum?id=rkhlb8lCZ},
	year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCALi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0ZQR0EtQmFzZWQgSGlnaC1QZXJmb3JtYW5jZSBQYXJhbGxlbCBBcmNoaXRlY3R1cmUgZm9yIEhvbW9tb3JwaGljIENvbXB1dGluZyBvbiBFbmNyeXB0ZWQgRGF0YS5iaWJPEQLCAAAAAALCAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRlBHQS1CYXNlZCBIaWdoLVBlI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABkNyeXB0bwACAJYvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Q3J5cHRvOkZQR0EtQmFzZWQgSGlnaC1QZXJmb3JtYW5jZSBQYXJhbGxlbCBBcmNoaXRlY3R1cmUgZm9yIEhvbW9tb3JwaGljIENvbXB1dGluZyBvbiBFbmNyeXB0ZWQgRGF0YS5iaWIADgDEAGEARgBQAEcAQQAtAEIAYQBzAGUAZAAgAEgAaQBnAGgALQBQAGUAcgBmAG8AcgBtAGEAbgBjAGUAIABQAGEAcgBhAGwAbABlAGwAIABBAHIAYwBoAGkAdABlAGMAdAB1AHIAZQAgAGYAbwByACAASABvAG0AbwBtAG8AcgBwAGgAaQBjACAAQwBvAG0AcAB1AHQAaQBuAGcAIABvAG4AIABFAG4AYwByAHkAcAB0AGUAZAAgAEQAYQB0AGEALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAJRVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NyeXB0by9GUEdBLUJhc2VkIEhpZ2gtUGVyZm9ybWFuY2UgUGFyYWxsZWwgQXJjaGl0ZWN0dXJlIGZvciBIb21vbW9ycGhpYyBDb21wdXRpbmcgb24gRW5jcnlwdGVkIERhdGEuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAKcAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADbQ==},
	Bdsk-Url-1 = {https://openreview.net/forum?id=rkhlb8lCZ}}

@book{Nayak:2017aa,
	author = {Nayak, Sarat and Bihari Misra, Bijan and Behera, Dr. H.},
	date = {2017/01/01},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.4018/978-1-5225-0788-8.ch022},
	journal = {Nature-Inspired Computing: Concepts, Methodologies, Tools, and Applications},
	keywords = {HONN, Pi-Sigma, Genetic Algorithms},
	month = {01},
	n2 = {This chapter presents two higher order neural networks (HONN) for efficient prediction of stock market behavior. The models include Pi-Sigma, and Sigma-Pi higher order neural network models. Along with the traditional gradient descent learning, how the evolutionary computation technique such as genetic algorithm (GA) can be used effectively for the learning process is also discussed here. The learning process is made adaptive to handle the noise and uncertainties associated with stock market data. Further, different prediction approaches are discussed here and application of HONN for time series forecasting is illustrated with real life data taken from a number of stock markets across the globe.},
	title = {Adaptive Hybrid Higher Order Neural Networks for Prediction of Stock Market Behavior},
	ty = {BOOK},
	year = {2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.4018/978-1-5225-0788-8.ch022}}

@article{CHONG2017187,
	abstract = {We offer a systematic analysis of the use of deep learning networks for stock market analysis and prediction. Its ability to extract features from a large set of raw data without relying on prior knowledge of predictors makes deep learning potentially attractive for stock market prediction at high frequencies. Deep learning algorithms vary considerably in the choice of network structure, activation function, and other model parameters, and their performance is known to depend heavily on the method of data representation. Our study attempts to provides a comprehensive and objective assessment of both the advantages and drawbacks of deep learning algorithms for stock market analysis and prediction. Using high-frequency intraday stock returns as input data, we examine the effects of three unsupervised feature extraction methods---principal component analysis, autoencoder, and the restricted Boltzmann machine---on the network's overall ability to predict future market behavior. Empirical results suggest that deep neural networks can extract additional information from the residuals of the autoregressive model and improve prediction performance; the same cannot be said when the autoregressive model is applied to the residuals of the network. Covariance estimation is also noticeably improved when the predictive network is applied to covariance-based market structure analysis. Our study offers practical insights and potentially useful directions for further investigation into how deep learning networks can be effectively used for stock market analysis and prediction.},
	author = {Eunsuk Chong and Chulwoo Han and Frank C. Park},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {https://doi.org/10.1016/j.eswa.2017.04.030},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Stock market prediction, Deep learning, Multilayer neural network, Covariance estimation},
	pages = {187 - 205},
	title = {Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	volume = {83},
	year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.04.030}}

@inproceedings{HuangY.2016Etmt,
	author = {Huang, Y. and Huang, K. and Wang, Y. and Zhang, H. and Guan, J. and Zhou, S.},
	copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	isbn = {9783319422961},
	issn = {03029743},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	keywords = {Convolutional Neural Network ; Deep Neural Network ; Financial Trend Prediction ; Twitter Mood},
	pages = {449--460},
	publisher = {Springer Verlag},
	title = {Exploiting twitter moods to boost financial trend prediction based on deep network models},
	volume = {9773},
	year = {2016}}

@article{diartificial,
	author = {Di Persio, Luca and Honchar, Oleksandr},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	journal = {International Journal of Economics and Management Systems},
	keywords = {CNN, RNN, LSTM},
	month = {January},
	pages = {5},
	title = {Artificial neural networks approach to the forecast of stock market price movements},
	volume = {1},
	year = {2016}}

@techreport{ghoshal2017reading,
	author = {Ghoshal, Sid and Roberts, Stephen},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	institution = {Technical report},
	keywords = {CNN, RNN, Technical Analysis},
	title = {Reading the Tea Leaves: A Neural Network Perspective on Technical Trading},
	year = {2017}}

@article{aggarwal2017deep,
	author = {Aggarwal, Saurabh and Aggarwal, Somya},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	journal = {International Journal of Computer Applications},
	keywords = {DNN, Finance, Portfolio, LSTM},
	number = {2},
	publisher = {Foundation of Computer Science},
	title = {Deep Investment in Financial Markets using Deep Learning Models},
	volume = {162},
	year = {2017}}

@article{di2016artificial,
	author = {Di Persio, Luca and Honchar, Oleksandr},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	journal = {International Journal of Circuits, Systems and Signal Processing},
	keywords = {CNN, MLP, LSTM, Wavelet},
	pages = {403--413},
	title = {Artificial Neural Networks architectures for stock price prediction: comparisons and applications},
	volume = {10},
	year = {2016}}

@article{dixon2016classification,
	author = {Dixon, Matthew Francis and Klabjan, Diego and Bang, Jin Hoon},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	keywords = {DNN, Futures, Finance},
	title = {Classification-based Financial Markets Prediction using Deep Neural Networks},
	year = {2016}}

@misc{essay59381,
	author = {M. {Kooijman}},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	keywords = {Haskell, functional programming},
	month = {December},
	title = {Haskell as a higher order structural hardware description language},
	url = {http://essay.utwente.nl/59381/},
	year = {2009},
	Bdsk-Url-1 = {http://essay.utwente.nl/59381/}}

@conference{L:08,
	author = {Philip H.W. Leong},
	booktitle = {Proc. 4th IEEE International Symposium on Electronic Design, Test and Applications},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	keywords = {FPGA, Trends},
	location = {Hong Kong},
	note = {\textbf{Invited}},
	pages = {137--141},
	title = {Recent Trends in {FPGA} Architectures and Applications},
	url = {rtfpga_delta08.pdf},
	year = {2008},
	Bdsk-Url-1 = {rtfpga_delta08.pdf}}

@inproceedings{Pike:2009:RYO:1596638.1596646,
	acmid = {1596646},
	address = {New York, NY, USA},
	author = {Pike, Lee and Brown, Geoffrey and Goodloe, Alwyn},
	booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1145/1596638.1596646},
	isbn = {978-1-60558-508-6},
	keywords = {emulation, functional programming, physical-layer protocol testing},
	location = {Edinburgh, Scotland},
	numpages = {8},
	pages = {61--68},
	publisher = {ACM},
	series = {Haskell '09},
	title = {Roll Your Own Test Bed for Embedded Real-time Protocols: A Haskell Experience},
	url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliTxEBrAAAAAABrAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HkVuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAE8vOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46RW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliAAAOAD4AHgBFAG4AZAAtAFQAbwAtAEUAbgBkACAATQBlAG0AbwByAHkAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAE1Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL0VuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAYAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIQ},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1596638.1596646}}

@inproceedings{Schrage:2005:HRD:1088348.1088351,
	acmid = {1088351},
	address = {New York, NY, USA},
	author = {Schrage, Martijn M. and van IJzendoorn, Arjan and van der Gaag, Linda C.},
	booktitle = {Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1145/1088348.1088351},
	isbn = {1-59593-071-X},
	keywords = {application, bayesian networks, graphical user interface, haskell, wxHaskell},
	location = {Tallinn, Estonia},
	numpages = {10},
	pages = {17--26},
	publisher = {ACM},
	series = {Haskell '05},
	title = {Haskell Ready to Dazzle the Real World},
	url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1088348.1088351}}

@article{Sezer:2017aa,
	author = {Sezer, Omer Berat and Ozbayoglu, Murat and Dogdu, Erdogan},
	booktitle = {Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 --November 1, 2017, Chicago, Illinois, USA},
	da = {2017/01/01/},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {https://doi.org/10.1016/j.procs.2017.09.031},
	isbn = {1877-0509},
	journal = {Procedia Computer Science},
	keywords = {Stock Trading; Stock Market; Deep Neural-Network; Evolutionary Algorithms; Technical Analysis},
	number = {Supplement C},
	pages = {473--480},
	title = {A Deep Neural-Network Based Stock Trading System Based on Evolutionary Optimized Technical Analysis Parameters},
	ty = {JOUR},
	url = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	volume = {114},
	year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2017.09.031}}

@inproceedings{Kablan:2009aa,
	author = {A. Kablan},
	booktitle = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/ADVCOMP.2009.23},
	journal = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	journal1 = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	keywords = {decision making; expert systems; financial data processing; fuzzy reasoning; neural nets; pattern recognition; stock markets; adaptive neuro fuzzy inference systems; automated trading strategy; decision making; efficient market hypothesis; expert system; financial forecasting; financial markets; financial time series; fuzzy reasoning; high frequency financial trading; neural networks; pattern recognition; Adaptive systems; Economic forecasting; Expert systems; Finance; Frequency; Fuzzy reasoning; Fuzzy systems; Humans; Neural networks; Pattern recognition; efficient market hypothesis; financial prediction; high frequency trading; neuro-fuzzy inference system},
	pages = {105--110},
	title = {Adaptive Neuro Fuzzy Inference Systems for High Frequency Financial Trading and Forecasting},
	ty = {CONF},
	year = {2009},
	year1 = {11-16 Oct. 2009},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ADVCOMP.2009.23}}

@article{2003cond.mat..4469K,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2003cond.mat..4469K},
	author = {{Kondratenko}, V.~V. and {Kuperin}, Y.~A},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	eprint = {cond-mat/0304469},
	journal = {eprint arXiv:cond-mat/0304469},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Quantitative Finance - Statistical Finance},
	month = apr,
	title = {{Using Recurrent Neural Networks To Forecasting of Forex}},
	year = 2003}

@article{2015arXiv151207108G,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151207108G},
	archiveprefix = {arXiv},
	author = {{Gu}, J. and {Wang}, Z. and {Kuen}, J. and {Ma}, L. and {Shahroudy}, A. and {Shuai}, B. and {Liu}, T. and {Wang}, X. and {Wang}, L. and {Wang}, G. and {Cai}, J. and {Chen}, T.},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	eprint = {1512.07108},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	month = dec,
	primaryclass = {cs.CV},
	title = {{Recent Advances in Convolutional Neural Networks}},
	year = 2015}

@article{Niedermeier:2014aa,
	abstract = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	author = {Niedermeier,A.},
	date = {2014/8/29},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.3990/1.9789036537322},
	isbn = {978-90-365-3732-2},
	keywords = {IR-91607; EWI-25011; METIS-304761},
	m3 = {PhD Thesis - Research UT, graduation UT},
	month = {8},
	n2 = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	title = {A fine-grained parallel dataflow-inspired architecture for streaming applications},
	ty = {THES},
	u2 = {10.3990/1.9789036537322},
	year = {2014},
	year1 = {2014/8/29},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036537322}}

@inbook{Smit:2010aa,
	abstract = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	annote = {eemcs-eprint-19169},
	author = {Smit,Gerardus Johannes Maria and Kuper,Jan and Baaij,C. P. R.},
	booktitle = {Dagstuhl Seminar on Dynamically Reconfigurable Architectures},
	date = {2010/12/14},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.4230/OASIcs.WCET.2010.136},
	keywords = {IR-75334; METIS-275806; Hardware design; EC Grant Agreement nr.: FP7/248465; Streaming Applications; EWI-19169; mathematical specification},
	m3 = {Conference contribution},
	month = {12},
	n2 = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	pages = {11},
	publisher = {Internationales Begegnungs- und Forschungszentrum fur Informatik (IBFI)},
	title = {A mathematical approach towards hardware design},
	title1 = {Dagstuhl Seminar Proceedings},
	ty = {CHAP},
	u2 = {10.4230/OASIcs.WCET.2010.136},
	year = {2010},
	year1 = {2010/12/14},
	Bdsk-Url-1 = {https://dx.doi.org/10.4230/OASIcs.WCET.2010.136}}

@article{Wester:2015aa,
	abstract = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	author = {Wester,Rinse},
	date = {2015/7/3},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.3990/1.9789036538879},
	isbn = {978-90-365-3887-9},
	keywords = {Higher-order functions; EWI-26125; IR-96278; METIS-310874; transformations; Hardware design},
	m3 = {PhD Thesis - Research UT, graduation UT},
	month = {7},
	n2 = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	publisher = {Universiteit Twente},
	title = {A transformation-based approach to hardware design using higher-order functions},
	ty = {THES},
	u2 = {10.3990/1.9789036538879},
	year = {2015},
	year1 = {2015/7/3},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCRLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0EgRGVlcCBOZXVyYWwtTmV0d29yayBCYXNlZCBTdG9jayBUcmFkaW5nIFN5c3RlbSBCYXNlZCBvbiBFdm9sdXRpb25hcnkgT3B0aW1pemVkIFRlY2huaWNhbCBBbmFseXNpcyBQYXJhbWV0ZXJzLnJpc08RAwgAAAAAAwgAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BIERlZXAgTmV1cmFsLU5ldHcjRkZGRkZGRkYucmlzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGTk4tRmluAAIApy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTi1GaW46QSBEZWVwIE5ldXJhbC1OZXR3b3JrIEJhc2VkIFN0b2NrIFRyYWRpbmcgU3lzdGVtIEJhc2VkIG9uIEV2b2x1dGlvbmFyeSBPcHRpbWl6ZWQgVGVjaG5pY2FsIEFuYWx5c2lzIFBhcmFtZXRlcnMucmlzAAAOAOYAcgBBACAARABlAGUAcAAgAE4AZQB1AHIAYQBsAC0ATgBlAHQAdwBvAHIAawAgAEIAYQBzAGUAZAAgAFMAdABvAGMAawAgAFQAcgBhAGQAaQBuAGcAIABTAHkAcwB0AGUAbQAgAEIAYQBzAGUAZAAgAG8AbgAgAEUAdgBvAGwAdQB0AGkAbwBuAGEAcgB5ACAATwBwAHQAaQBtAGkAegBlAGQAIABUAGUAYwBoAG4AaQBjAGEAbAAgAEEAbgBhAGwAeQBzAGkAcwAgAFAAYQByAGEAbQBlAHQAZQByAHMALgByAGkAcwAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAKVVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9BIERlZXAgTmV1cmFsLU5ldHdvcmsgQmFzZWQgU3RvY2sgVHJhZGluZyBTeXN0ZW0gQmFzZWQgb24gRXZvbHV0aW9uYXJ5IE9wdGltaXplZCBUZWNobmljYWwgQW5hbHlzaXMgUGFyYW1ldGVycy5yaXMAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkALgAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADxA==},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB+Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0FkYXB0aXZlIE5ldXJvIEZ1enp5IEluZmVyZW5jZSBTeXN0ZW1zIGZvciBIaWdoIEZyZXF1ZW5jeSBGaW5hbmNpYWwgVHJhZGluZyBhbmQgRm9yZWNhc3RpbmcucmlzTxECugAAAAACugACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0FkYXB0aXZlIE5ldXJvIEZ1eiNGRkZGRkZGRi5yaXMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgCULzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpBZGFwdGl2ZSBOZXVybyBGdXp6eSBJbmZlcmVuY2UgU3lzdGVtcyBmb3IgSGlnaCBGcmVxdWVuY3kgRmluYW5jaWFsIFRyYWRpbmcgYW5kIEZvcmVjYXN0aW5nLnJpcwAOAMAAXwBBAGQAYQBwAHQAaQB2AGUAIABOAGUAdQByAG8AIABGAHUAegB6AHkAIABJAG4AZgBlAHIAZQBuAGMAZQAgAFMAeQBzAHQAZQBtAHMAIABmAG8AcgAgAEgAaQBnAGgAIABGAHIAZQBxAHUAZQBuAGMAeQAgAEYAaQBuAGEAbgBjAGkAYQBsACAAVAByAGEAZABpAG4AZwAgAGEAbgBkACAARgBvAHIAZQBjAGEAcwB0AGkAbgBnAC4AcgBpAHMADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCSVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi1GaW4vQWRhcHRpdmUgTmV1cm8gRnV6enkgSW5mZXJlbmNlIFN5c3RlbXMgZm9yIEhpZ2ggRnJlcXVlbmN5IEZpbmFuY2lhbCBUcmFkaW5nIGFuZCBGb3JlY2FzdGluZy5yaXMAEwABLwAAFQACAAv//wAAAAgADQAaACQApQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAANj},
	Bdsk-File-3 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBaLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL1VzaW5nIFJlY3VycmVudCBOZXVyYWwgTmV0d29ya3MgVG8gRm9yZWNhc3Rpbmcgb2YgRm9yZXguYmliTxECKgAAAAACKgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1VzaW5nIFJlY3VycmVudCBOZSNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgBwLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpVc2luZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIFRvIEZvcmVjYXN0aW5nIG9mIEZvcmV4LmJpYgAOAHgAOwBVAHMAaQBuAGcAIABSAGUAYwB1AHIAcgBlAG4AdAAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzACAAVABvACAARgBvAHIAZQBjAGEAcwB0AGkAbgBnACAAbwBmACAARgBvAHIAZQB4AC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBuVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi1GaW4vVXNpbmcgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3JrcyBUbyBGb3JlY2FzdGluZyBvZiBGb3JleC5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAgQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKv},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036538879}}

@inproceedings{Wester:2012aa,
	author = {R. Wester and C. Baaij and J. Kuper},
	booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/FPL.2012.6339258},
	isbn = {1946-147X},
	journal = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	journal1 = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	keywords = {field programmable gate arrays; functional languages; hardware description languages; logic design; mathematical analysis; particle filtering (numerical methods); program compilers; C\&{\#}x03BB;aSH HDL; DSP application; FPGA; Haskell; adequate abstraction mechanisms; functional hardware description language; higher level abstraction mechanism; higher-order function; mathematical definition; particle filtering; polymorphism; two step hardware design method; Atmospheric measurements; Design methodology; Equations; Hardware; Mathematical model; Particle measurements; Systematics},
	pages = {181--188},
	title = {A two step hardware design method using C\&{\#}x03BB;aSH},
	ty = {CONF},
	year = {2012},
	year1 = {29-31 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339258}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5faB,
	abstract = {Functional hardware description languages are a class of hardware description languages that emphasize on the ability to express higher level structural properties, such a parameterization and regularity. Due to such features as higher-order functions and polymorphism, parameterization in functional hardware description languages is more natural than the parameterization support found in the more traditional hardware description languages, like VHDL and Verilog. We de- velop a new functional hardware description language, CλasH, that borrows both the syntax and semantics from the general-purpose functional programming language Haskell.},
	author = {C.P.R. Baaij},
	booktitle = {ClasH - From Haskell To Hardware},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	keywords = {Haskell, CLaSH},
	month = {12},
	pages = {101},
	publisher = {University of Twente},
	title = {ClasH - From Haskell To Hardware},
	year = {2009}}

@misc{essay70777,
	abstract = {ClaSH is a functional hardware description language (HDL) developed at the CAES
group of the University of Twente. ClaSH borrows both the syntax and semantics from the general-purpose functional programming language Haskell, meaning that circuit designers can define their circuits with regular Haskell syntax.

In this thesis, research is done on the co-simulation of ClaSH and traditional HDLs. The Verilog Procedural Interface (VPI), as defined in the IEEE 1364 standard, is used to set-up the communication and to control a Verilog simulator. An implementation is made, as will be described in this thesis, to show the practical feasibility of co-simulation of ClaSH and Verilog.},
	author = {J.G.J. {Verheij}},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	keywords = {CLaSH, Haskell, Simulation, HDL},
	month = {August},
	title = {Co-simulation between C$\lambda$aSH and traditional HDLs},
	url = {http://essay.utwente.nl/70777/},
	year = {2016},
	Bdsk-Url-1 = {http://essay.utwente.nl/70777/}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5fa,
	abstract = {As embedded systems are becoming increasingly complex, the design process and verification have become very time-consuming. Additionally, specifying hardware manually in a low-level hardware description language like VHDL is usually an error-prone task. In our group, a tool (the ClaSH compiler) was developed to generate fully synthesisable VHDL code from a specification given in the functional programming language Haskell. In this paper, we present a comparison between two implementations of the same design by using ClaSH and hand-written VHDL. The design is a simple dataflow processor. As measures of interest area, performance, power consumption and source lines of code (SLOC) are used. The obtained results indicate that the ClaSH -generated VHDL code as well as the netlist after synthesis and place and route are functionally correct. The placed and routed hand-written VHDL code has also the correct behaviour. Furthermore, a similar performance is achieved. The power consumption is even lower for the ClaSH implementation. The SLOC for ClaSH is considerably smaller and it is possible to specify the design in a much higher level of abstraction compared to VHDL.},
	author = {A. Niedermeier and Rinse Wester and Rinse Wester and C.P.R. Baaij and Jan Kuper and Smit, {Gerardus Johannes Maria}},
	booktitle = {Proceedings of the Workshop on PROGram for Research on Embedded Systems and Software (PROGRESS 2010)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	isbn = {978-90-73461-67-3},
	keywords = {METIS-277454, IR-75095, EWI-18902},
	month = {11},
	pages = {216--221},
	publisher = {Technology Foundation (STW)},
	title = {Comparing CλaSH and VHDL by implementing a dataflow processor},
	year = {2010}}

@inproceedings{6339201,
	author = {B. N. Uchevler and K. Svarstad},
	booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/FPL.2012.6339201},
	issn = {1946-147X},
	keywords = {circuit complexity;functional languages;hardware description languages;high level synthesis;program compilers;program verification;reconfigurable architectures;CLaSH;RT level VHDL;digital circuit description;digital circuit verification;dynamic reconfigurable system modeling;electronic design complexity;formal verification;functional HDL;high-level Haskell description translation;higher-order functions;parametrization;partial evaluation technique;polymorphism;run-time reconfigurable systems;synthesis tool chain;Communications technology;Consumer electronics;Educational institutions;Field programmable gate arrays;Hardware;Mathematical model;Unified modeling language},
	month = {Aug},
	pages = {481-482},
	title = {Modeling of dynamic reconfigurable systems with Haskell},
	year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBnLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYk8RAmIAAAAAAmIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BIFB5dGhvbmljIEFwcHJvYWMjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAERlBHQQACAH0vOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RlBHQTpBIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYgAADgCWAEoAQQAgAFAAeQB0AGgAbwBuAGkAYwAgAEEAcABwAHIAbwBhAGMAaAAgAGYAbwByACAAUgBhAHAAaQBkACAASABhAHIAZAB3AGEAcgBlACAAUAByAG8AdABvAHQAeQBwAGkAbgBnACAAYQBuAGQAIABJAG4AcwB0AHIAdQBtAGUAbgB0AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAe1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAjgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAL0},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339201}}

@inbook{fff28539e56047c4adca5cc3c9c29cec,
	abstract = {CλaSH, a functional hardware description language based on Haskell, has several abstraction mechanisms that allow a hardware designer to describe architectures in a short and concise way. In this paper we evaluate CλaSH on a complex DSP application, a Polyphase Filter Bank as it is used in the ASTRON APERTIF project. The Polyphase Filter Bank is implemented in two steps: first in Haskell as being close to a standard mathematical specification, then in CλaSH which is derived from the Haskell formulation by applying only minor changes. We show that the CλaSH formulation can be directly mapped to hardware, thus exploiting the parallelism and concurrency that is present in the original mathematical specification.},
	author = {Rinse Wester and Dimitrios Sarakiotis and Eric Kooistra and Jan Kuper},
	booktitle = {Communicating Process Architectures 2012},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	isbn = {978-0-9565409-5-9},
	keywords = {EWI-22586, EC Grant Agreement nr.: FP7/248465, Specification, METIS-289800, APERTIF Project, CλaSH, IR-82307},
	month = {8},
	note = {eemcs-eprint-22586},
	pages = {53--64},
	publisher = {Open Channel Publishing},
	title = {Specification of APERTIF Polyphase Filter Bank in CλaSH},
	year = {2012}}

@inproceedings{6523639,
	author = {B. N. Uchevler and K. Svarstad and J. Kuper and C. Baaij},
	booktitle = {International Symposium on Quality Electronic Design (ISQED)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/ISQED.2013.6523639},
	issn = {1948-3287},
	keywords = {field programmable gate arrays;hardware description languages;logic design;CLaSH;FPGA design;RT level;Suzaku-sz410 board;digital circuit verification;dynamic reconfigurable designs;formal verification;functional HDL;functional programming abstractions;high-level Haskell descriptions;high-level descriptions;high-level structures;higher-order functions;partial evaluation implementation technique;run-time reconfigurable systems;synthesizable VHDL;system-level modelling;Consumer electronics;Digital signal processing;Field programmable gate arrays;Finite impulse response filters;Hardware;Software;Unified modeling language;Functional HDL;Partial Evaluation;Run-Time Reconfiguration;Self-Reconfiguration},
	month = {March},
	pages = {379-385},
	title = {System-level modelling of dynamic reconfigurable designs using functional programming abstractions},
	year = {2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ISQED.2013.6523639}}

@inproceedings{Oancea:2012:FSG:2364474.2364484,
	acmid = {2364484},
	address = {New York, NY, USA},
	author = {Oancea, Cosmin E. and Andreetta, Christian and Berthold, Jost and Frisch, Alain and Henglein, Fritz},
	booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1145/2364474.2364484},
	isbn = {978-1-4503-1577-7},
	keywords = {autoparallelization, functional language, memory coalescing, strength reduction, tiling},
	location = {Copenhagen, Denmark},
	numpages = {12},
	pages = {61--72},
	publisher = {ACM},
	series = {FHPC '12},
	title = {Financial Software on GPUs: Between Haskell and Fortran},
	url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364484}}

@inproceedings{Funie:2014aa,
	author = {A. I. Funie and M. Salmon and W. Luk},
	booktitle = {2014 13th International Conference on Machine Learning and Applications},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/ICMLA.2014.11},
	journal = {2014 13th International Conference on Machine Learning and Applications},
	journal1 = {2014 13th International Conference on Machine Learning and Applications},
	keywords = {economics; field programmable gate arrays; foreign exchange trading; genetic algorithms; particle swarm optimisation; economic value; field programmable gate array technology; financial markets; foreign exchange market data; genetic programming; high frequency trading strategies; hybrid evolutionary algorithm; monitor market stability; particle swarm optimisation; Algorithm design and analysis; Genetics; Noise; Prediction algorithms; Sociology; Statistics; Testing},
	pages = {29--34},
	title = {A Hybrid Genetic-Programming Swarm-Optimisation Approach for Examining the Nature and Stability of High Frequency Trading Strategies},
	ty = {CONF},
	year = {2014},
	year1 = {3-6 Dec. 2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICMLA.2014.11}}

@inproceedings{Lockwood:2012aa,
	author = {J. W. Lockwood and A. Gupte and N. Mehta and M. Blott and T. English and K. Vissers},
	booktitle = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/HOTI.2012.15},
	isbn = {1550-4794},
	journal = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	journal1 = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	keywords = {IP networks; electronic engineering computing; electronic trading; field programmable gate arrays; local area networks; memory protocols; microprocessor chips; network interfaces; Ethernet line rate; FPGA IP library; FPGA hardware; HFT platform; I-O interface; alternative hybrid architecture; bit rate 10 Gbit/s; computers software; custom 1U FPGA appliance; electronic trading; financial protocol parser; fixed end-to-end latency; hardware acceleration; high-frequency trading platform; high-performance network adapter; low-latency library; memory interface; pre-built infrastructure; software implementation; time 1 mus; Field programmable gate arrays; Hardware; IP networks; Libraries; Protocols; Registers; Software; Algorithmic; FPGA; HFT; latency; trading},
	pages = {9--16},
	title = {A Low-Latency Library in FPGA Hardware for High-Frequency Trading (HFT)},
	ty = {CONF},
	year = {2012},
	year1 = {22-24 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/HOTI.2012.15}}

@inproceedings{Zoican:2016aa,
	author = {S. Zoican and M. Vochin},
	booktitle = {2016 International Conference on Communications (COMM)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/ICComm.2016.7528289},
	journal = {2016 International Conference on Communications (COMM)},
	journal1 = {2016 International Conference on Communications (COMM)},
	keywords = {financial data processing; computing system; financial market literature; high frequency trading applications; high frequency trading financial applications; high processing speed; high-frequency traders; low latency technology; low network latency; medium cost technology; network architectures; optimal trading speed; Bandwidth; Computer architecture; Computers; Graphics processing units; Instruction sets; Parallel processing; Servers; computer unified device architecture; high frequency trading algorithms; network latency},
	pages = {139--144},
	title = {Computing system and network architectures in high frequency trading financial applications},
	ty = {CONF},
	year = {2016},
	year1 = {9-10 June 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICComm.2016.7528289}}

@inproceedings{Litz:2011:DPE:2088256.2088268,
	acmid = {2088268},
	address = {New York, NY, USA},
	author = {Litz, Heiner and Leber, Christian and Geib, Benjamin},
	booktitle = {Proceedings of the Fourth Workshop on High Performance Computational Finance},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1145/2088256.2088268},
	isbn = {978-1-4503-1108-3},
	keywords = {DSL, FAST, FIX, FPGA, decoder, domain specific language, high throughput, low latency, stock, trading},
	location = {Seattle, Washington, USA},
	numpages = {8},
	pages = {31--38},
	publisher = {ACM},
	series = {WHPCF '11},
	title = {DSL Programmable Engine for High Frequency Trading Acceleration},
	url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2088256.2088268}}

@inproceedings{Woods:2008aa,
	author = {N. A. Woods and T. VanCourt},
	booktitle = {2008 International Conference on Field Programmable Logic and Applications},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-09-26 08:04:22 +0000},
	doi = {10.1109/FPL.2008.4629954},
	isbn = {1946-147X},
	journal = {2008 International Conference on Field Programmable Logic and Applications},
	journal1 = {2008 International Conference on Field Programmable Logic and Applications},
	keywords = {Monte Carlo methods; field programmable gate arrays; financial data processing; FPGA acceleration; finance; multicore processor; pricing simulations; quasiMonte Carlo methods; Acceleration; Computational modeling; Field programmable gate arrays; Finance; Monte Carlo methods; Multicore processing; Pricing; Runtime; Security; Yield estimation},
	pages = {335--340},
	title = {FPGA acceleration of quasi-Monte Carlo in finance},
	ty = {CONF},
	year = {2008},
	year1 = {8-10 Sept. 2008},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2008.4629954}}

@article{Tian:2010:HQC:1862648.1862656,
	acmid = {1862656},
	address = {New York, NY, USA},
	articleno = {26},
	author = {Tian, Xiang and Benkrid, Khaled},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1145/1862648.1862656},
	issn = {1936-7406},
	issue_date = {November 2010},
	journal = {ACM Trans. Reconfigurable Technol. Syst.},
	keywords = {CPU, FPGA, GPU, Maxwell, Quasi-Monte Carlo simulations, option pricing},
	month = nov,
	number = {4},
	numpages = {22},
	pages = {26:1--26:22},
	publisher = {ACM},
	title = {High-Performance Quasi-Monte Carlo Financial Simulation: FPGA vs. GPP vs. GPU},
	url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	volume = {3},
	year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1862648.1862656}}

@inproceedings{Dvorak:2014aa,
	author = {M. Dvo{\v r}{\'a}k and J. Ko{\v r}enek},
	booktitle = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/DDECS.2014.6868785},
	journal = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	journal1 = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	keywords = {electronic trading; field programmable gate arrays; logic design; FPGA; QDR SRAM; algorithmic trading; best bid price; best offer price; financial instrument; hardware architecture; hardware market state; high frequency trading; lookup latency-memory utilization trade-off; low latency book handling; low latency trading system; market data processing; storage capacity 144 Mbit; Algorithm design and analysis; Feeds; Field programmable gate arrays; Hardware; Instruments; Memory management},
	pages = {175--178},
	title = {Low latency book handling in FPGA for high frequency trading},
	ty = {CONF},
	year = {2014},
	year1 = {23-25 April 2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBfLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRXF1aWxpYnJhdGVkIGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzIGZvciBub24tY29udmV4IG9wdGltaXphdGlvbi5iaWJPEQJEAAAAAAJEAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRXF1aWxpYnJhdGVkIGFkYXB0I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAAk5OAAIAdS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTjpFcXVpbGlicmF0ZWQgYWRhcHRpdmUgbGVhcm5pbmcgcmF0ZXMgZm9yIG5vbi1jb252ZXggb3B0aW1pemF0aW9uLmJpYgAADgCKAEQARQBxAHUAaQBsAGkAYgByAGEAdABlAGQAIABhAGQAYQBwAHQAaQB2AGUAIABsAGUAYQByAG4AaQBuAGcAIAByAGEAdABlAHMAIABmAG8AcgAgAG4AbwBuAC0AYwBvAG4AdgBlAHgAIABvAHAAdABpAG0AaQB6AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAc1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRXF1aWxpYnJhdGVkIGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzIGZvciBub24tY29udmV4IG9wdGltaXphdGlvbi5iaWIAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAIYAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACzg==},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/DDECS.2014.6868785}}

@article{Thomas:2013aa,
	author = {D. B. Thomas and W. Luk},
	booktitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/TVLSI.2012.2228017},
	isbn = {1063-8210},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	journal1 = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {Gaussian distribution; adders; field programmable gate arrays; graphics processing units; random number generation; shift registers; table lookup; Virtex-5 FPGA; adders; block-memory resources; field-programmable gate array; frequency 1.2 GHz; frequency 400 MHz; graphics processing unit; independent Gaussian samples; logic resources; lookup-tables; multiplierless algorithm; multivariate Gaussian distribution; multivariate Gaussian vectors; multivariate generator; numerical simulation; pair-wise correlations; random number generation; read-only memories; registers; scalar Gaussian generator; uniform distribution; Covariance matrix; Field programmable gate arrays; Generators; Matrix decomposition; Standards; Table lookup; Vectors; Field-programmable gate array (FPGA); Monte Carlo simulation; multivariate samples; random number generation},
	number = {12},
	pages = {2193--2205},
	title = {Multiplierless Algorithm for Multivariate Gaussian Random Number Generation in FPGAs},
	ty = {JOUR},
	vo = {21},
	volume = {21},
	year = {2013},
	year1 = {Dec. 2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2012.2228017}}

@article{2017arXiv171105860H-2,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105860H},
	archiveprefix = {arXiv},
	author = {{Hao}, Y.},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	eprint = {1711.05860},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Neural and Evolutionary Computing},
	month = nov,
	primaryclass = {cs.CV},
	title = {{A General Neural Network Hardware Architecture on FPGA}},
	year = 2017}

@article{2017arXiv170808917D,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170808917D},
	archiveprefix = {arXiv},
	author = {{Ding}, C. and {Liao}, S. and {Wang}, Y. and {Li}, Z. and {Liu}, N. and {Zhuo}, Y. and {Wang}, C. and {Qian}, X. and {Bai}, Y. and {Yuan}, G. and {Ma}, X. and {Zhang}, Y. and {Tang}, J. and {Qiu}, Q. and {Lin}, X. and {Yuan}, B.},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	eprint = {1708.08917},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	month = aug,
	primaryclass = {cs.CV},
	title = {{CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices}},
	year = 2017}

@inproceedings{Zhao:2016aa,
	author = {Wenlai Zhao and Haohuan Fu and W. Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	booktitle = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-09-26 08:03:08 +0000},
	doi = {10.1109/ASAP.2016.7760779},
	journal = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	journal1 = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	keywords = {field programmable gate arrays; floating point arithmetic; neural nets; 32bit floating-point arithmetic; FPGA-based framework; bandwidth resources; convolutional neural networks; hardware resources; streaming datapath; Bandwidth; Computational modeling; Convolution; Field programmable gate arrays; Neural networks; Runtime; Training},
	pages = {107--114},
	title = {F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	ty = {CONF},
	year = {2016},
	year1 = {6-8 July 2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBWLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXNPEQIcAAAAAAIcAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fQWNjZWxlcmF0aW5nIExhcmdlI0ZGRkZGRkZGLnJpcwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABEZQR0EAAgBsLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkZQR0E6QWNjZWxlcmF0aW5nIExhcmdlLVNjYWxlIEhQQyBBcHBsaWNhdGlvbnMgVXNpbmcgRlBHQXMucmlzAA4AdAA5AEEAYwBjAGUAbABlAHIAYQB0AGkAbgBnACAATABhAHIAZwBlAC0AUwBjAGEAbABlACAASABQAEMAIABBAHAAcABsAGkAYwBhAHQAaQBvAG4AcwAgAFUAcwBpAG4AZwAgAEYAUABHAEEAcwAuAHIAaQBzAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAalVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXMAEwABLwAAFQACAAv//wAAAAgADQAaACQAfQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKd},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAxLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vV2Fzc2Vyc3RlaW4gR0FOLmJpYk8RAYoAAAAAAYoAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xNXYXNzZXJzdGVpbiBHQU4uYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAFRENHQU4AAAIARy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpEQ0dBTjpXYXNzZXJzdGVpbiBHQU4uYmliAAAOACgAEwBXAGEAcwBzAGUAcgBzAHQAZQBpAG4AIABHAEEATgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIARVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vV2Fzc2Vyc3RlaW4gR0FOLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAWAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAHm},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ASAP.2016.7760779}}

@inproceedings{Abdelouahab:2017:WTH:3131885.3131937,
	acmid = {3131937},
	address = {New York, NY, USA},
	author = {Abdelouahab, Kamel and Pelcat, Maxime and Berry, Francois},
	booktitle = {Proceedings of the 11th International Conference on Distributed Smart Cameras},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-09-26 08:04:22 +0000},
	doi = {10.1145/3131885.3131937},
	isbn = {978-1-4503-5487-5},
	keywords = {FPGA, TanH, Harware, Acceleration},
	location = {Stanford, CA, USA},
	numpages = {3},
	pages = {199--201},
	publisher = {ACM},
	series = {ICDSC 2017},
	title = {Why TanH is a Hardware Friendly Activation Function for CNNs},
	url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/3131885.3131937}}

@article{2017arXiv170304691B,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170304691B},
	archiveprefix = {arXiv},
	author = {{Borovykh}, A. and {Bohte}, S. and {Oosterlee}, C.~W.},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-10-24 07:44:45 +0000},
	eprint = {1703.04691},
	journal = {ArXiv e-prints},
	keywords = {BNN; Statistics; Machine Learning},
	month = mar,
	primaryclass = {stat.ML},
	title = {{Conditional Time Series Forecasting with Convolutional Neural Networks}},
	year = 2017,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB0Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OLUZQR0EvT3B0aW1pemluZyBGUEdBLWJhc2VkIEFjY2VsZXJhdG9yIERlc2lnbiBmb3IgRGVlcCBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWJPEQKQAAAAAAKQAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fT3B0aW1pemluZyBGUEdBLWJhI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAACENOTi1GUEdBAAIAii86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk4tRlBHQTpPcHRpbWl6aW5nIEZQR0EtYmFzZWQgQWNjZWxlcmF0b3IgRGVzaWduIGZvciBEZWVwIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgAOAKgAUwBPAHAAdABpAG0AaQB6AGkAbgBnACAARgBQAEcAQQAtAGIAYQBzAGUAZAAgAEEAYwBjAGUAbABlAHIAYQB0AG8AcgAgAEQAZQBzAGkAZwBuACAAZgBvAHIAIABEAGUAZQBwACAAQwBvAG4AdgBvAGwAdQB0AGkAbwBuAGEAbAAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCIVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4tRlBHQS9PcHRpbWl6aW5nIEZQR0EtYmFzZWQgQWNjZWxlcmF0b3IgRGVzaWduIGZvciBEZWVwIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACbAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAy8=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB6Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OLUZQR0EvUGlwZUNOTi0gQW4gT3BlbkNMLUJhc2VkIE9wZW4tU291cmNlIEZQR0EgQWNjZWxlcmF0b3IgZm9yIENvbnZvbHV0aW9uIE5ldXJhbCBOZXR3b3Jrcy5iaWJPEQKoAAAAAAKoAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fUGlwZUNOTi0gQW4gT3BlbkNMI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAACENOTi1GUEdBAAIAkC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk4tRlBHQTpQaXBlQ05OLSBBbiBPcGVuQ0wtQmFzZWQgT3Blbi1Tb3VyY2UgRlBHQSBBY2NlbGVyYXRvciBmb3IgQ29udm9sdXRpb24gTmV1cmFsIE5ldHdvcmtzLmJpYgAOALQAWQBQAGkAcABlAEMATgBOAC0AIABBAG4AIABPAHAAZQBuAEMATAAtAEIAYQBzAGUAZAAgAE8AcABlAG4ALQBTAG8AdQByAGMAZQAgAEYAUABHAEEAIABBAGMAYwBlAGwAZQByAGEAdABvAHIAIABmAG8AcgAgAEMAbwBuAHYAbwBsAHUAdABpAG8AbgAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCOVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4tRlBHQS9QaXBlQ05OLSBBbiBPcGVuQ0wtQmFzZWQgT3Blbi1Tb3VyY2UgRlBHQSBBY2NlbGVyYXRvciBmb3IgQ29udm9sdXRpb24gTmV1cmFsIE5ldHdvcmtzLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAChAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA00=}}

@inproceedings{Ding:2015:DLE:2832415.2832572,
	acmid = {2832572},
	author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	isbn = {978-1-57735-738-4},
	keywords = {Deep learning, CNN, NN, S&P},
	location = {Buenos Aires, Argentina},
	numpages = {7},
	pages = {2327--2333},
	publisher = {AAAI Press},
	series = {IJCAI'15},
	title = {Deep Learning for Event-driven Stock Prediction},
	url = {http://dl.acm.org/citation.cfm?id=2832415.2832572},
	year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2832415.2832572}}

@inproceedings{Chen:2016aa,
	author = {J. F. Chen and W. L. Chen and C. P. Huang and S. H. Huang and A. P. Chen},
	booktitle = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-10-10 00:34:11 +0000},
	doi = {10.1109/CCBD.2016.027},
	journal = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	journal1 = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	keywords = {CNN-FIN; decision support systems; feature extraction; feedforward neural nets; financial data processing; learning (artificial intelligence); stock markets; time series; FinTech; Taiwan Stock Index Futures; artificial intelligence; deep convolutional neural networks; deep learning; feature extraction; financial markets; financial time-series data analysis; historical datasets; intelligent trading decision support system; multimedia fields; next financial technology generation; numerical features; planar feature representation; time-series data prediction; time-series data processing; trading simulation application; Data models; Feature extraction; Machine learning; Market research; Neural networks; Time series analysis; Training; Deep learning; convolutional neural networks; data visualization; machine learning; trend prediction},
	pages = {87--92},
	title = {Financial Time-Series Data Analysis Using Deep Convolutional Neural Networks},
	ty = {CONF},
	year = {2016},
	year1 = {16-18 Nov. 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CCBD.2016.027}}

@inproceedings{Tsantekidis:2017aa,
	abstract = {In today's financial markets, where most trades are performed in their entirety by electronic means and the largest fraction of them is completely automated, an opportunity has risen from analyzing this vast amount of transactions. Since all the transactions are recorded in great detail, investors can analyze all the generated data and detect repeated patterns of the price movements. Being able to detect them in advance, allows them to take profitable positions or avoid anomalous events in the financial markets. In this work we proposed a deep learning methodology, based on Convolutional Neural Networks (CNNs), that predicts the price movements of stocks, using as input large-scale, high-frequency time-series derived from the order book of financial exchanges. The dataset that we use contains more than 4 million limit order events and our comparison with other methods, like Multilayer Neural Networks and Support Vector Machines, shows that CNNs are better suited for this kind of task.},
	author = {A. Tsantekidis and N. Passalis and A. Tefas and J. Kanniainen and M. Gabbouj and A. Iosifidis},
	booktitle = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {10.1109/CBI.2017.23},
	journal = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	journal1 = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	keywords = {economic forecasting; feedforward neural nets; learning (artificial intelligence); pricing; stock markets; time series; CNN; convolutional neural networks; deep learning methodology; financial exchanges; financial markets; input large-scale high-frequency time-series; limit order book; price movements; stock price forecasting; stock price movement prediction; transaction analysis; Convolution; Data models; Machine learning; Market research; Mathematical model; Neural networks; Support vector machines; Convolutional Neural Networks; Large scale financial data; Limit Orderbook},
	pages = {7--12},
	title = {Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks},
	ty = {CONF},
	vo = {01},
	volume = {01},
	year = {2017},
	year1 = {24-27 July 2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CBI.2017.23}}

@article{Gunduz:2017aa,
	author = {Gunduz, Hakan and Yaslan, Yusuf and Cataltepe, Zehra},
	da = {2017/12/01/},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	doi = {https://doi.org/10.1016/j.knosys.2017.09.023},
	isbn = {0950-7051},
	journal = {Knowledge-Based Systems},
	keywords = {Stock market prediction; Deep learning; Borsa Istanbul; Convolutional neural networks; CNN; Feature selection; Feature correlations},
	number = {Supplement C},
	pages = {138--148},
	title = {Intraday prediction of Borsa Istanbul using convolutional neural networks and feature correlations},
	ty = {JOUR},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	volume = {137},
	year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.knosys.2017.09.023}}

@article{2016arXiv160306995C,
	adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160306995C},
	archiveprefix = {arXiv},
	author = {{Cui}, Z. and {Chen}, W. and {Chen}, Y.},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	eprint = {1603.06995},
	journal = {ArXiv e-prints},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	month = mar,
	primaryclass = {cs.CV},
	title = {{Multi-Scale Convolutional Neural Networks for Time Series Classification}},
	year = 2016}

@article{2017arXiv171105860H,
	adsnote = {International Journal of Computer Science and Information Technologies (IJCSIT{\textregistered}) is published using an open access publishing model, which makes the full-text of all peer-reviewed papers freely available online with no subscription or registration barriers.},
	adsurl = {http://ijcsit.com/docs/Volume%207/vol7issue5/ijcsit20160705014.pdf},
	archiveprefix = {pdf},
	author = {{Bhandare}, Ashwin, {Bhide}, Maithili, {Gokhale}, Pranav, {Chandavarkar}, Rohan,},
	date-added = {2018-02-05 09:05:50 +0000},
	date-modified = {2018-02-05 09:05:50 +0000},
	eprint = {1711.05860},
	journal = {International Journal of Computer Science and Information Technologies},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	month = 5,
	primaryclass = {cs.CV},
	title = {{Applications of Convolutional Neural Networks}},
	year = 2016,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBiLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0RlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWJPEQJKAAAAAAJKAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRGVlcCBJbnZlc3RtZW50IGluI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABk5OLUZpbgACAHgvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk4tRmluOkRlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWIADgCIAEMARABlAGUAcAAgAEkAbgB2AGUAcwB0AG0AZQBuAHQAIABpAG4AIABGAGkAbgBhAG4AYwBpAGEAbAAgAE0AYQByAGsAZQB0AHMAIAB1AHMAaQBuAGcAIABEAGUAZQBwACAATABlAGEAcgBuAGkAbgBnACAATQBvAGQAZQBsAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAHZVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9EZWVwIEludmVzdG1lbnQgaW4gRmluYW5jaWFsIE1hcmtldHMgdXNpbmcgRGVlcCBMZWFybmluZyBNb2RlbHMuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAIkAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAC1w==}}

@inproceedings{FPT201902,
	author = {Longyu Ma and Chiu-Wing Sham},
	booktitle = {2019 International Conference on Field-Programmable Technology (ICFPT)},
	issn = {null},
	keywords = {FPGAGroup},
	month = {Dec},
	pages = {391-394},
	title = {{SoC-FPGA-Based Implementation of Iris Recognition Enhanced by QC-LDPC Codes}},
	year = {2019}}

@inproceedings{ICSAI201901,
	author = {Xinchao Zhong and Chiu-Wing Sham},
	booktitle = {2019 International Conference on Systems and Informatics (ICSAI)},
	issn = {null},
	keywords = {FPGAGroup},
	month = {Nov},
	pages = {678-681},
	title = {{An SoC for Qi-compliant Wireless Power Transmitter With Enhanced EMI Performance}},
	year = {2019}}

@inproceedings{GCCE201902,
	author = {Longyu Ma and Hong-fu Chou and Chiu-Wing Sham},
	booktitle = {2019 IEEE Global Conference on Consumer Electronics (GCCE)},
	issn = {null},
	keywords = {FPGAGroup},
	month = {Nov},
	pages = {921-922},
	title = {{A Novel Data Packing Technique for QC-LDPC Decoder Architecture applied to NAND Flash Controller}},
	year = {2019}}

@inproceedings{GCCE201901,
	author = {Xinchao Zhong and Chiu-Wing Sham},
	booktitle = {2019 IEEE Global Conference on Consumer Electronics (GCCE)},
	issn = {null},
	keywords = {FPGAGroup},
	month = {Nov},
	pages = {921-922},
	title = {{A Visible Light Communication Based Integrated Circuit for Mobile Payment}},
	year = {2019}}

@article{8936394,
	author = {S. {Jiang} and P. W. {Zhang} and F. C. M. {Lau} and {Chiu-Wing Sham}},
	doi = {10.1109/TCSII.2019.2960613},
	issn = {1558-3791},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {FPGAGroup},
	pages = {1-1},
	title = {{An Ultimate-Shannon-Limit-Approaching Gbps Throughput Encoder/Decoder System}},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCSII.2019.2960613}}

@article{8906146,
	author = {L. {Ma} and {Chiu-Wing Sham} and J. {Sun} and R. V. {Tenorio}},
	doi = {10.1109/TCSII.2019.2953700},
	issn = {1558-3791},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {FPGAGroup;FPGA;Dynamic Partial Reconfiguration;Partial Reconfiguration;LDPC.},
	pages = {1-1},
	title = {{A Real-Time Flexible Telecommunication Decoding Architecture using FPGA Partial Reconfiguration}},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCSII.2019.2953700}}

@article{8815836,
	author = {Z. {Xu} and L. {Wang} and S. {Hong} and F. C. M. {Lau} and {Chiu-Wing Sham}},
	doi = {10.1109/LWC.2019.2937766},
	journal = {IEEE Wireless Communications Letters},
	keywords = {FPGAGroup;JSCC;DP-LDPC codes;belief propagation;shuffled scheduling.},
	month = {Dec},
	number = {6},
	pages = {1696-1699},
	title = {{Joint Shuffled Scheduling Decoding Algorithm for DP-LDPC Codes-Based JSCC Systems}},
	volume = {8},
	year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/LWC.2019.2937766}}

@inproceedings{8625371,
	author = {S. {Jiang} and P. W. {Zhang} and F. C. M. {Lau} and {Chiu-Wing Sham} and K. {Huang}},
	booktitle = {2018 IEEE 10th International Symposium on Turbo Codes Iterative Information Processing (ISTC)},
	doi = {10.1109/ISTC.2018.8625371},
	keywords = {FPGAGroup,channel coding;decoding;error statistics;field programmable gate arrays;Hadamard codes;parity check codes;turbo codes;low-rate channel code;code rate equals;turbo-Hadamard encoder-decoder system;THC systems;FPGA board;bit error rate;noise figure -1.6 dB;word length 6.0 bit;noise figure 0.15 dB;noise figure -0.3 dB;Error correction;Error correction codes;Decoding;Convolutional codes;Field programmable gate arrays;Bit error rate;Program processors},
	month = {Dec},
	pages = {1-5},
	title = {{A Turbo-Hadamard Encoder/Decoder System with Hundreds of Mbps Throughput}},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISTC.2018.8625371}}

@inproceedings{8587580,
	author = {Chun-Yan {Lo} and Francis C. M. {Lau} and {Chiu-Wing Sham}},
	booktitle = {2018 International Conference on Advanced Technologies for Communications (ATC)},
	doi = {10.1109/ATC.2018.8587580},
	issn = {2162-1039},
	keywords = {FPGAGroup;C++ language;convolutional neural nets;field programmable gate arrays;fixed point arithmetic;handwritten character recognition;image classification;C++ programming;4bit fixed-point arithmetic;FPGA;handwritten digits classification;floating-point classifier;8bit additions;fixed-point convolutional neural network classifier;image classification;Training;Field programmable gate arrays;Testing;Standards;Table lookup;Adders;Frequency measurement},
	month = {Oct},
	pages = {105-109},
	title = {{Fixed-Point Implementation of Convolutional Neural Networks for Image Classification}},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ATC.2018.8587580}}

@inproceedings{8587568,
	author = {Longyu {Ma} and {Chiu-Wing Sham}},
	booktitle = {2018 International Conference on Advanced Technologies for Communications (ATC)},
	doi = {10.1109/ATC.2018.8587568},
	issn = {2162-1039},
	keywords = {FPGAGroup;decoding;error correction codes;parity check codes;QC-LDPC decoder;low density parity check codes;error correction performance;optimized BPU;variable nodes;check nodes;optimized Block Processing Unit;logic complexity;convincing trade-off;Shannon's capacity limit;error correction codes;layered LDPC code decoder;optimized layer architecture;Decoding;Parity check codes;Complexity theory;Table lookup;Manganese;Error correction},
	month = {Oct},
	pages = {287-291},
	title = {{Optimized Layer Architecture for Layered LDPC Code Decoder}},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ATC.2018.8587568}}

@article{8425687,
	author = {Hong-Fu {Chou} and {Chiu-Wing Sham}},
	doi = {10.1109/LCOMM.2018.2863363},
	issn = {1089-7798},
	journal = {IEEE Communications Letters},
	keywords = {FPGAGroup;channel coding;error statistics;evolutionary computation;iterative decoding;parity check codes;runlength codes;optimization approach;RLL-constrained LDPC coded recording system;deliberate flipping;run-length-limited constraint;high error coding rate;correcting capability;RLL bit error;iterative decoding;hard error bits;differential evolution approach;unequal error protection LDPC code;optimal LDPC code distribution;density evolution;RLL flipped system;Decoding;Probability density function;Iterative decoding;Optimization;Error correction codes;Equalizers;Parity check codes;iterative decoding;partial response channels;error correction codes},
	month = {Oct},
	number = {10},
	pages = {1976-1979},
	title = {{An Optimization Approach for an RLL-Constrained LDPC Coded Recording System Using Deliberate Flipping}},
	volume = {22},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/LCOMM.2018.2863363}}

@article{8233210,
	author = {Sheng {Jiang} and F. {Mo} and Francis C. M. {Lau} and {Chiu-Wing Sham}},
	doi = {10.1109/TCSII.2017.2785779},
	issn = {1549-7747},
	journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
	keywords = {FPGAGroup;decoding;matrix algebra;parity check codes;identity matrix;tree-permutation-matrix;tree-permutation-matrix;TPM-LDPC decoders;girth-10 TPM-LDPC code;base matrix;girth-8 TPM-LDPC code;anti-diagonal matrix;sub-matrices termed tree-permutation matrices;LDPC code type;parity-check matrices;semiregular style;random structure;low-density parity-check codes;Parity check codes;Decoding;Complexity theory;Field programmable gate arrays;Simulation;Throughput;FPGA implementation;low-density parity-check code;tree-permutation matrix},
	month = {Aug},
	number = {8},
	pages = {1019-1023},
	title = {{Tree-Permutation-Matrix Based LDPC Codes}},
	volume = {65},
	year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCSII.2017.2785779}}

@inproceedings{8304001,
	author = {Sheng {Jiang} and Francis C. M. {Lau} and {Chiu-Wing Sham}},
	booktitle = {2017 23rd Asia-Pacific Conference on Communications (APCC)},
	doi = {10.23919/APCC.2017.8304001},
	keywords = {FPGAGroup;computational complexity;Hadamard codes;Hamming codes;maximum likelihood decoding;search problems;error performance;puncturing patterns;minimum Hamming distance property;cross-correlation property;punctured codewords;punctured Hadamard codes;a-posteriori-probability decoding;Error correction;Error correction codes;Hamming distance;Decoding;Magnetohydrodynamics;Error analysis;Correlation},
	month = {Dec},
	pages = {1-5},
	title = {{Design and error performance of punctured hadamard codes}},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/APCC.2017.8304001}}

@inproceedings{8304002,
	author = {Pengwei {Zhang} and Francis C. M. {Lau} and {Chiu-Wing Sham}},
	booktitle = {2017 23rd Asia-Pacific Conference on Communications (APCC)},
	doi = {10.23919/APCC.2017.8304002},
	keywords = {FPGAGroup;clocks;field programmable gate arrays;Golay codes;maximum likelihood decoding;parallel architectures;PIMLD decoder;parallel architecture;FPGA;high-throughput low-latency extended golay decoder;imperfect maximum likelihood decoding method;frequency 500.0 MHz;Clocks;Maximum likelihood decoding;Hardware;Throughput;Field programmable gate arrays;Generators},
	month = {Dec},
	pages = {1-4},
	title = {{Design of a high-throughput low-latency extended golay decoder}},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/APCC.2017.8304002}}

@inproceedings{8368811,
	author = {Hong-Fu {Chou} and {Chiu-Wing Sham}},
	booktitle = {2017 International SoC Design Conference (ISOCC)},
	doi = {10.1109/ISOCC.2017.8368811},
	keywords = {FPGAGroup;error statistics;iterative decoding;parity check codes;runlength codes;Unequal protection approach;deliberate flipping approach;bit errors;correcting capability;RLL flipping errors;Unequal Protection Low-Density Parity-Check code;regular inter-leaver;high error protection capability;run-length-limited code;iterative decoding scheme;RLL-constrained LDPC coded recording system;Decoding;Error correction codes;Iterative decoding;Complexity theory;Optical recording;Labeling},
	month = {Nov},
	pages = {25-26},
	title = {{Unequal protection approach for RLL-constrained LDPC coded recording system using deliberate flipping}},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISOCC.2017.8368811}}

@inproceedings{7890139,
	author = {Francis C. M. {Lau} and F. {Mo} and W. M. {Tarn} and {Chiu-Wing Sham}},
	booktitle = {2017 19th International Conference on Advanced Communication Technology (ICACT)},
	doi = {10.23919/ICACT.2017.7890139},
	keywords = {FPGAGroup;cyclic codes;error statistics;matrix algebra;parity check codes;statistical analysis;FPGA simulations;bit error rate;same code length;circulant permutation matrix;random permutation matrix;hardware requirement;error performance;CC-QC-LDPC codes;quasicyclic low-density parity-check codes;random-permutation-matrix;cyclically-coupled LDPC codes;Parity check codes;Decoding;Throughput;Complexity theory;Bit error rate;Field programmable gate arrays;Hardware;Cyclically-coupled quasi-cyclic low-density parity-check;FPGA simulations;QC-LDPC;random-permutation-matrix-based CC-LDPC;RC-CC-QCLDPC},
	month = {Feb},
	pages = {497-500},
	title = {{Random-permutation-matrix-based cyclically-coupled LDPC codes}},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/ICACT.2017.7890139}}

@inproceedings{7890046,
	author = {Ting-Wai {Siu} and {Chiu-Wing Sham} and Francis C. M. {Lau}},
	booktitle = {2017 19th International Conference on Advanced Communication Technology (ICACT)},
	doi = {10.23919/ICACT.2017.7890046},
	keywords = {FPGAGroup;circuit complexity;fast Fourier transforms;field programmable gate arrays;memory architecture;operating frequency improvement;FPGA;pipeline large-FFT processor;circuit complexity reduction;large N-point Radix-2;single-path delay feedback architecture;ROM;system clock frequency;Altera Cyclone IV FPGA;EP4CGX22;frequency 231.11 MHz;frequency 215.75 MHz;Read only memory;Delays;Field programmable gate arrays;Registers;Pipelines;Clocks;Radiation detectors;Fast Fourier Transform implementation;FPGA;1K-FFT;4K-FFT},
	month = {Feb},
	pages = {5-9},
	title = {{Operating frequency improvement on FPGA implementation of a pipeline large-FFT processor}},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/ICACT.2017.7890046}}

@inproceedings{7804048,
	author = {Qing {Lu} and {Chiu-Wing Sham} and Francis C. M. {Lau}},
	booktitle = {2016 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
	doi = {10.1109/APCCAS.2016.7804048},
	keywords = {FPGAGroup;block codes;decoding;disc drives;error correction;field programmable gate arrays;parity check codes;FPGA-based implementations;decoder architecture;LDPC block codes;BCH codes;low-density parity-check codes;SSD;solid-state drives;error-correction engine;capacity scaling;bit rate 1.47 Gbit/s;frequency 100 MHz;storage capacity 1 Mbit;Decoding;Parity check codes;Throughput;Reliability;Complexity theory;Circuits and systems;Clocks},
	month = {Oct},
	pages = {625-628},
	title = {{On using the cyclically-coupled QC-LDPC codes in future SSDs}},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/APCCAS.2016.7804048}}

@inproceedings{7764787,
	author = {Francis C. M. {Lau} and F. {Mo} and Qing {Lu} and W. M. {Tam} and {Chiu-Wing Sham}},
	booktitle = {2016 International Conference on Advanced Technologies for Communications (ATC)},
	doi = {10.1109/ATC.2016.7764787},
	issn = {2162-1039},
	keywords = {FPGAGroup;cyclic codes;parity check codes;cyclically-coupled quasi-cyclic LDPC block codes;CC-QC-LDPC block codes;bit error performance;bit error rate;Parity check codes;Bit error rate;Decoding;Block codes;Complexity theory;Field programmable gate arrays;Signal to noise ratio},
	month = {Oct},
	pages = {27-31},
	title = {{Novel types of cyclically-coupled quasi-cyclic LDPC block codes}},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ATC.2016.7764787}}

@inproceedings{7427981,
	author = {Qing {Lu} and {Chiu-Wing Sham} and Francis C. M. {Lau}},
	booktitle = {2016 21st Asia and South Pacific Design Automation Conference (ASP-DAC)},
	doi = {10.1109/ASPDAC.2016.7427981},
	issn = {2153-697X},
	keywords = {FPGAGroup;cyclic codes;decoding;field programmable gate arrays;parity check codes;wireless LAN;multimode QC-LDPC decoder rapid prototyping;802.11n/ac Wi-Fi standard;on-the-fly reconfigurable ability;FPGA;low-density parity-check code;frequency 382 MHz to 1852 MHz;Decoding;Field programmable gate arrays;Throughput;Standards;Parity check codes;Schedules;Hardware},
	month = {Jan},
	pages = {19-20},
	title = {{Rapid prototyping of multi-mode QC-LDPC decoder for 802.11n/ac standard}},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASPDAC.2016.7427981}}

@article{7370816,
	author = {Qing {Lu} and J. {Fan} and {Chiu-Wing Sham} and W. M. {Tam} and Francis C. M. {Lau}},
	doi = {10.1109/TCSI.2015.2510619},
	issn = {1549-8328},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {FPGAGroup;cyclic codes;field programmable gate arrays;iterative decoding;parity check codes;random-access storage;throughput hardware-efficient decoder;cyclical coupled quasi-cyclic low-density parity-check codes;RAM-based decoder architecture;CC-QC-LDPC codes;field-programmable gate array;FPGA;iteration decoding;frequency 100 MHz;Decoding;Block codes;Bit error rate;Iterative decoding;Throughput;Complexity theory;Cyclically-coupled QC-LDPC code;decoder architecture;FPGA implementation;QC-LDPC code},
	month = {Jan},
	number = {1},
	pages = {134-145},
	title = {{A 3.0 Gb/s Throughput Hardware-Efficient Decoder for Cyclically-Coupled QC-LDPC Codes}},
	volume = {63},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCSI.2015.2510619}}

@inproceedings{7393120,
	author = {Qing {Lu} and {Chiu-Wing Sham} and Francis C. M. {Lau}},
	booktitle = {2015 International Conference on Field Programmable Technology (FPT)},
	doi = {10.1109/FPT.2015.7393120},
	keywords = {fFPGAGroup;ield programmable gate arrays;games of skill;hardware-software codesign;Monte Carlo methods;tree searching;Trax player;two-player game;FPGA-based artificial intelligence;Supertrax;heuristics;multi-level pattern recognition;Monte-Carlo Tree Search;path-based scheduling;architecture-algorithm co-design;Yttrium;Games;Software algorithms;Computer architecture;Pattern recognition;Monte Carlo methods;Color},
	month = {Dec},
	pages = {264-267},
	title = {{An architecture-algorithm co-design of artificial intelligence for Trax player}},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2015.7393120}}

@inproceedings{7388304,
	author = {Qing {Lu} and Z. {Shen} and {Chiu-Wing Sham} and Francis C. M. {Lau}},
	booktitle = {2015 International Conference on Advanced Technologies for Communications (ATC)},
	doi = {10.1109/ATC.2015.7388304},
	issn = {2162-1020},
	keywords = {FPGAGroup;channel coding;decoding;field programmable gate arrays;parity check codes;telecommunication network reliability;telecommunication network routing;parallel-routing network;single-parity-check decoder reliability inference;SPC code;channel decoder;disparate kernel;LUT-based method;field-programmable gate array;FPGA;Decoding;Kernel;Reliability;Complexity theory;Table lookup;Delays;Wires},
	month = {Oct},
	pages = {127-132},
	title = {{A parallel-routing network for reliability inferences of single-parity-check decoder}},
	year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/ATC.2015.7388304}}

@inproceedings{7032733,
	author = {Qing {Lu} and J. {Fan} and {Chiu-Wing Sham} and Francis C. M. {Lau}},
	booktitle = {2014 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
	doi = {10.1109/APCCAS.2014.7032733},
	keywords = {FPGAGroup;AWGN;field programmable gate arrays;noise generators;table lookup;high throughput Gaussian noise generator;digital additive white Gaussian noise generator;AWGN generator;hardware platform simulation;communication discipline;software ideas;field programmable gate arrays;architecture design;high-precision small-error Gaussian noise generators;academic applications;data resolution;straightforward architecture;Gaussian lookup table;FPGA implementation;Generators;Hardware;Throughput;Field programmable gate arrays;Computer architecture;AWGN},
	month = {Nov},
	pages = {117-120},
	title = {{A high throughput Gaussian noise generator}},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/APCCAS.2014.7032733}}

@inproceedings{6903345,
	author = {Hongxia {Zhou} and {Chiu-Wing Sham} and Hailong {Yao}},
	booktitle = {2014 IEEE Computer Society Annual Symposium on VLSI},
	doi = {10.1109/ISVLSI.2014.62},
	issn = {2159-3469},
	keywords = {FPGAGroup;integrated circuit layout;VLSI;slicing floorplans;handling symmetry;general placement constraints;polish expression representation;VLSI circuits;Routing;Vegetation;Runtime;Linear programming;Very large scale integration;Educational institutions;placement constraints;symmetry;slicing floorplan;polish expression;routability},
	month = {July},
	pages = {112-117},
	title = {{Slicing Floorplans with Handling Symmetry and General Placement Constraints}},
	year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISVLSI.2014.62}}

@inproceedings{6643571,
	author = {Hongxia {Zhou} and {Chiu-Wing Sham} and Hailong {Yao}},
	booktitle = {Fifth Asia Symposium on Quality Electronic Design (ASQED 2013)},
	doi = {10.1109/ASQED.2013.6643571},
	keywords = {FPGAGroup;analogue integrated circuits;integrated circuit design;integrated circuit layout;mixed analogue-digital integrated circuits;VLSI;dummy nodes;constraints edges;expansion process;congestion probability model;subsequent channel routing;net congestion probability;routability driven placement;very-large-scale integration;VLSI;placement design;mixed signal circuits;analog circuits;congestion oriented approach;Routing;Probabilistic logic;Compaction;Arrays;Educational institutions;Very large scale integration;Pins},
	month = {Aug},
	pages = {97-102},
	title = {{Congestion-oriented approach in placement for analog and mixed-signal circuits}},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASQED.2013.6643571}}

@inproceedings{6523615,
	author = {Jingwei {Lu} ; and {Chiu-Wing Sham}},
	booktitle = {International Symposium on Quality Electronic Design (ISQED)},
	doi = {10.1109/ISQED.2013.6523615},
	issn = {1948-3287},
	keywords = {FPGAGroup;integer programming;integrated circuit design;linear programming;network routing;network topology;search problems;storage management chips;integer linear programming;average memory reduction;FGR1.1;NTUgr;NTHU2.0;FastRoute3.0;maze routing algorithm;memory efficiency;dynamic topology update technique;routing path;high memory cost;fundamental physical design problem;LMgr;bending-aware optimum path search;low-memory global router;Routing;Topology;Algorithm design and analysis;Heuristic algorithms;Steiner trees;Cost function;Joining processes},
	month = {March},
	pages = {231-238},
	title = {{LMgr: A low-M emory global router with dynamic topology update and bending-aware optimum path search}},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISQED.2013.6523615}}

@article{6481477,
	author = {{Chiu-Wing Sham} and Xu {Chen} and Francis C. M. {Lau} and Y. {Zhao} and W. M. {Tam}},
	doi = {10.1109/TCSI.2012.2230506},
	issn = {1549-8328},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	keywords = {FPGAGroup;convolutional codes;decoding;field programmable gate arrays;parity check codes;pipeline processing;telecommunication computing;bit-energy-to-noise power-spectral-density ratio;Altera Stratix FPGA;field-programmable gate array;embedded memory;data-width;large memory block;pipelining processors;simple address controller;dynamic message storage;quasi cyclic structure;quasi cyclic LDPC block code;LDPCCC;decoder architecture;low-density parity-check convolutional code;QC-LDPC convolutional code;byte rate 2.0 GByte/s;frequency 100 MHz;gain 3.55 dB;Decoding;Program processors;Throughput;Complexity theory;Block codes;Iterative decoding;Decoder architecture;FPGA implementation;LDPC convolutional code;QC-LDPC convolutional code},
	month = {July},
	number = {7},
	pages = {1857-1869},
	title = {{A 2.0 Gb/s Throughput Decoder for QC-LDPC Convolutional Codes}},
	volume = {60},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCSI.2012.2230506}}

@inproceedings{6419075,
	author = {{Chiu-Wing Sham} and Xu {Chen} and W. M. {Tam} and Y. {Zhao} and Francis C. M. {Lau}},
	booktitle = {2012 IEEE Asia Pacific Conference on Circuits and Systems},
	doi = {10.1109/APCCAS.2012.6419075},
	keywords = {FPGAGroup;channel coding;cyclic codes;decoding;field programmable gate arrays;parity check codes;layered QC-LDPC decoder architecture;high-throughput long-distance communication system;optical transmission system;net coding gain;NCG;forward error correction;FEC system;channel coding scheme;high code rate;extremely low error floor;quasi-cyclic low-density parity-check codes;FPGA;Decoding;Parity check codes;Complexity theory;Throughput;Computer architecture;Random access memory;Forward error correction},
	month = {Dec},
	pages = {475-478},
	title = {{A layered QC-LDPC decoder architecture for high speed communication system}},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/APCCAS.2012.6419075}}

@article{6046234,
	author = {Jingwei {Lu} and W. {Chow} and {Chiu-Wing Sham}},
	doi = {10.1109/TVLSI.2011.2168834},
	issn = {1063-8210},
	journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	keywords = {FPGAGroup;clocks;network synthesis;network topology;SPICE;power-aware gated clock tree synthesis;slew-aware gated clock tree synthesis;dynamic power usage;PACTS;PSACTS;Elmore RC model;topology;clock gates;SPICE estimation;Clocks;Logic gates;Capacitance;Power demand;Topology;Design automation;Integrated circuit modeling;Clock gating;clock tree synthesis;design automation},
	month = {Nov},
	number = {11},
	pages = {2094-2103},
	title = {{Fast Power- and Slew-Aware Gated Clock Tree Synthesis}},
	volume = {20},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TVLSI.2011.2168834}}

@inproceedings{5543037,
	author = {W. {Tang} and C. H. {Ho} and {Chiu-Wing Sham} and K. F. C. {Yiu}},
	booktitle = {The 2010 International Conference on Green Circuits and Systems},
	doi = {10.1109/ICGCS.2010.5543037},
	keywords = {FPGAGroup;adaptive filters;crosstalk;echo suppression;field programmable gate arrays;frequency-domain analysis;multiprocessing systems;power consumption;telecommunication equipment;voice communication;low-power reconfigurable acceleration;robust frequency-domain echo cancellation;hands-free operation;telecommunication equipment;acoustic echo control;hardware architecture;adaptive algorithm;double-talk situation;bitwidth optimization;echo-canceller;microprocessor;Xilinx XC4VFX60 FPGA;Core 2 Duo PC;power consumption;Acceleration;Robustness;Echo cancellers;Field programmable gate arrays;Hardware;Computer architecture;Mobile handsets;Telecommunication control;Adaptive algorithm;Frequency domain analysis},
	month = {June},
	pages = {361-364},
	title = {{Low-power reconfigurable acceleration of robust frequency-domain echo cancellation on FPGA}},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICGCS.2010.5543037}}

@inproceedings{5419838,
	author = {Jingwei {Lu} and Wing-Kai {Chow} and {Chiu-Wing Sham} and Evangeline F.Y. {Young}},
	booktitle = {2010 15th Asia and South Pacific Design Automation Conference (ASP-DAC)},
	doi = {10.1109/ASPDAC.2010.5419838},
	issn = {2153-6961},
	keywords = {FPGAGroup;buffer circuits;capacitance;clocks;digital integrated circuits;integrated circuit design;nanotechnology;network topology;VLSI;clock network synthesis;nanometer-scale VLSI physical design;digital circuit;clock skew;process-voltage-and temperature variation;dual-MST geometric matching;topology construction;tree structure;recursive buffer insertion;blockage handling;buffer distribution;capacitance;Clocks;Network synthesis;Synthesizers;Very large scale integration;Digital circuits;Voltage;Temperature;Minimization;Network topology;Tree data structures},
	month = {Jan},
	pages = {467-473},
	title = {{A dual-MST approach for clock network synthesis}},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASPDAC.2010.5419838}}

@inproceedings{1688966,
	author = {{Chiu-Wing Sham} and Evangeline F.Y. {Young} and C. {Chu}},
	booktitle = {2006 43rd ACM/IEEE Design Automation Conference},
	doi = {10.1109/DAC.2006.229406},
	issn = {0738-100X},
	keywords = {FPGAGroup;circuit optimisation;integer programming;integrated circuit layout;linear programming;VLSI;optimal cell flipping;placed circuit;independent cells;strictly solvable cells;conditionally solvable cells;mixed integer linear programming;floorplanning;placement algorithm;Simulated annealing;Mixed integer linear programming;Integer linear programming;Benchmark testing;Algorithm design and analysis;Very large scale integration;Circuit simulation;Linear programming;Binary decision diagrams;Tiles;Algorithms;Design;Floorplanning;Placement;Wirelength;Orientation;Flipping},
	month = {July},
	pages = {1109-1114},
	title = {{Optimal cell flipping in placement and floorplanning}},
	year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/DAC.2006.229406}}

@inproceedings{1466534,
	author = {{Chiu-Wing Sham} and Evangeline F.Y. {Young}},
	booktitle = {Proceedings of the ASP-DAC 2005. Asia and South Pacific Design Automation Conference, 2005.},
	doi = {10.1109/ASPDAC.2005.1466534},
	issn = {2153-6961},
	keywords = {FPGAGroup;VLSI;network routing;circuit optimisation;congestion prediction;routability optimization;area minimization;VLSI technology;shortest Manhattan distance routes;global routing;maze router;estimation accuracy improvement;Routing;Integrated circuit interconnections;Tiles;Computer science;Predictive models;Very large scale integration;Shape;Wiring;Design optimization;Prediction methods},
	month = {Jan},
	pages = {1107-1110 Vol. 2},
	title = {{Congestion prediction in floorplanning}},
	volume = {2},
	year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASPDAC.2005.1466534}}

@inproceedings{1354023,
	author = {{Chiu-Wing Sham} and Evangeline F.Y. {Young}},
	booktitle = {The 2004 47th Midwest Symposium on Circuits and Systems, 2004. MWSCAS '04.},
	doi = {10.1109/MWSCAS.2004.1354023},
	keywords = {FPGAGroup;integrated circuit layout;integrated circuit interconnections;circuit optimisation;simulated annealing;linear programming;area reduction;interconnect optimized floorplan;deadspace utilization;simulated annealing;cost function;interconnect cost;linear programming;compaction ratio;over congestion prevention;Integrated circuit interconnections;Cost function;Shape;Constraint optimization;Computer science;Computational modeling;Simulated annealing;Linear programming;Compaction;Very large scale integration},
	month = {July},
	pages = {I-445},
	title = {{Area reduction on interconnect optimized floorplan using deadspace utilization}},
	volume = {1},
	year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/MWSCAS.2004.1354023}}

@inproceedings{998375,
	author = {{Chiu-Wing Sham} and {Wai-Chiu Wong} and E. R. Y. {Young}},
	booktitle = {Proceedings 2002 Design, Automation and Test in Europe Conference and Exhibition},
	doi = {10.1109/DATE.2002.998375},
	issn = {1530-1591},
	keywords = {FPGAGroup;integrated circuit layout;circuit layout CAD;delay estimation;VLSI;dynamic programming;circuit optimisation;table lookup;probability;routability;congestion model;delay;blocked grids;dynamic programming;table lookup;interconnect costs;buffer block planning;probability;buffer locations;VLSI;floorplanning;Delay;Routing;Integrated circuit interconnections;Timing;Dynamic programming;Very large scale integration;Shape;Transistors;Clocks;Minimization},
	month = {March},
	pages = {696-701},
	title = {Congestion estimation with buffer planning in floorplan design},
	year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/DATE.2002.998375}}

@article{LU2012121,
	abstract = {In nanometer-scale VLSI physical design, clock tree becomes a major concern on determining the total performance of the chip. Both the clock skew and the PVT (process, voltage and temperature) variations contribute a lot to the behavior of the digital circuits. Previous works mainly focused on skew and wirelength minimization. However, it may lead to negative influence on the variation factors. In this paper, a novel clock tree synthesizer is proposed for performance improvement. Several algorithms are introduced to tackle the issues accordingly. A dual-MST geometric approach of perfect matching is developed for symmetric clock tree construction. In addition, a special technique of buffer sizing is also introduced. These two techniques can help balancing the tree structure in order to reduce the variation effect. An iterative buffer insertion technique and the dual-MZ blockage handling technique are also presented. They are developed for proper distribution of buffers and connection of wires, so the dynamic power consumption can be reduced. Additionally, slew table construction and internal nodes relocation are involved to satisfy the slew rate constraint and further reduce the clock skew. Experimental results show that the performance of our synthesizer is better than those of the previous works.},
	author = {Jingwei Lu and Wing-Kai Chow and Chiu-Wing Sham},
	doi = {https://doi.org/10.1016/j.vlsi.2011.11.001},
	issn = {0167-9260},
	journal = {Integration},
	keywords = {FPGAGroup;Clock tree synthesis, Process variations},
	number = {2},
	pages = {121 - 131},
	title = {A new clock network synthesizer for modern VLSI designs},
	url = {http://www.sciencedirect.com/science/article/pii/S0167926011000903},
	volume = {45},
	year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167926011000903},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.vlsi.2011.11.001}}

@inproceedings{9184436,
	author = {C. Y. {Lo} and Chiu-Wing {Sham}},
	booktitle = {2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS)},
	pages = {403-406},
	title = {Energy Efficient Fixed-point Inference System of Convolutional Neural Network},
	year = {2020}}
