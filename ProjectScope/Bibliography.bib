%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for RHVT at 2018-12-12 20:01:30 +1300 


%% Saved with string encoding Unicode (UTF-8) 



@book{Sher2013NeuroEvol,
	Abstract = {This chapter discusses the numerous reasons for why one might wish to study the subject of neuroevolution. I cover a number of different applications of such a system, giving examples and scenarios of a neuroevolutionary system being applied within a variety of different fields. A discussion then follows on where all of this research is heading, and what the next step within this field might be. Finally, a whirlwind introduction of the book is given, with a short summary of what is covered in every chapter.},
	Address = {New York, NY},
	Author = {Sher, Gene I.},
	Booktitle = {Handbook of Neuroevolution Through Erlang},
	Date-Added = {2018-12-12 19:57:45 +1300},
	Date-Modified = {2018-12-12 19:59:29 +1300},
	Doi = {10.1007/978-1-4614-4463-3_1},
	Isbn = {978-1-4614-4463-3},
	Keywords = {Evolutionary, Neuroevolution, Erlang, Finance},
	Pages = {1--39},
	Publisher = {Springer New York},
	Title = {Handbook of Neuroevolution Through Erlang},
	Url = {https://doi.org/10.1007/978-1-4614-4463-3_1},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4614-4463-3_1}}

@inproceedings{10.1007/978-3-642-33860-1_3,
	Abstract = {The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), has now been around for over 20 years. The main appeal of this approach is that, because it does not rely on gradient information (e.g. backpropagation), it can potentially harness the universal function approximation capability of neural networks to solve reinforcement learning (RL) tasks, where there is no ``teacher'' (i.e. no targets or examples of correct behavior). Instead of incrementally adjusting the synaptic weights of a single network, the space of network parameters is searched directly according to principles inspired by natural selection: (1) encode a population of networks as strings, or genomes, (2) transform them into networks, (3) evaluate them on the task, (4) generate new, hopefully better, nets by recombining those that are most ``fit'', (5) goto step 2 until a solution is found. By evolving neural networks, NE can cope naturally with tasks that have continuous inputs and outputs, and, by evolving networks with feedback connections (recurrent networks), it can tackle more general tasks that require memory.},
	Address = {Berlin, Heidelberg},
	Author = {Gomez, Faustino},
	Booktitle = {Theory and Practice of Natural Computing},
	Date-Added = {2018-12-12 18:32:49 +1300},
	Date-Modified = {2018-12-12 18:33:16 +1300},
	Editor = {Dediu, Adrian-Horia and Mart{\'\i}n-Vide, Carlos and Truthe, Bianca},
	Isbn = {978-3-642-33860-1},
	Keywords = {Evolutionary, Book, Scalable, Reinforcement Learning},
	Pages = {27--29},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Scalable Neuroevolution for Reinforcement Learning},
	Year = {2012}}

@inproceedings{10.1007/11893295_130,
	Abstract = {This paper addresses the problem of accelerating large artificial neural networks (ANN), whose topology and weights can evolve via the use of a genetic algorithm. The proposed digital hardware architecture is capable of processing any evolved network topology, whilst at the same time providing a good trade off between throughput, area and power consumption. The latter is vital for a longer battery life on mobile devices. The architecture uses multiple parallel arithmetic units in each processing element (PE). Memory partitioning and data caching are used to minimise the effects of PE pipeline stalling. A first order minimax polynomial approximation scheme, tuned via a genetic algorithm, is used for the activation function generator. Efficient arithmetic circuitry, which leverages modified Booth recoding, column compressors and carry save adders, is adopted throughout the design.},
	Address = {Berlin, Heidelberg},
	Author = {Larkin, Daniel and Kinane, Andrew and O'Connor, Noel},
	Booktitle = {Neural Information Processing},
	Date-Added = {2018-12-12 18:30:15 +1300},
	Date-Modified = {2018-12-12 18:31:02 +1300},
	Editor = {King, Irwin and Wang, Jun and Chan, Lai-Wan and Wang, DeLiang},
	Isbn = {978-3-540-46485-3},
	Keywords = {Evolutionary, Book, Neuroevolution, Processing Element},
	Pages = {1178--1188},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Towards Hardware Acceleration of Neuroevolution for Multimedia Processing Applications on Mobile Devices},
	Year = {2006}}

@article{7307180,
	Abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyze the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The paper also highlights important open research challenges in the field.},
	Author = {S. Risi and J. Togelius},
	Date-Added = {2018-12-12 16:15:08 +1300},
	Date-Modified = {2018-12-12 16:15:18 +1300},
	Doi = {10.1109/TCIAIG.2015.2494596},
	Issn = {1943-068X},
	Journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	Keywords = {evolutionary; computer games;learning (artificial intelligence);neural nets;neuroevolution;NE;computer game;artificial neural network training;evolutionary algorithm;Games;Artificial intelligence;Evolutionary computation;Genetic algorithms;Biological neural networks;Network topology;Evolutionary algorithms;neural networks;neuroevolution},
	Month = {March},
	Number = {1},
	Pages = {25-41},
	Title = {Neuroevolution in Games: State of the Art and Open Challenges},
	Volume = {9},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCIAIG.2015.2494596}}

@inproceedings{4250190,
	Abstract = {Appropriate topology and connection weight are two very important properties a neural network must have in order to successfully perform data classification. In this paper, we propose a hybrid training scheme Learning-NEAT (L-NEAT) for data classification problem. L-NEAT simplifies evolution by dividing the complete problem domain into sub tasks and learn the sub tasks by incorporating back propagation rule into the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. The new algorithm combines the strength of searching for topology and weights from NEAT and back propagation respectively while overcoming problems associated with direct use of NEAT. We claim that L-NEAT can produce neural network for classification problem effectively and efficiently. Empirical evaluation shows that L-NEAT evolves classifying neural network with good generalization ability. Its accuracy outperforms original NEAT.},
	Author = {L. Chen and D. Alahakoon},
	Booktitle = {2006 International Conference on Information and Automation},
	Date-Added = {2018-12-12 16:06:11 +1300},
	Date-Modified = {2018-12-12 16:06:25 +1300},
	Doi = {10.1109/ICINFA.2006.374100},
	Issn = {2151-1802},
	Keywords = {Evolutionary; backpropagation;neural nets;pattern classification;search problems;topology;neuroevolution of augmenting topologies;augmenting topology;data classification learning;neural network;learning-NEAT training scheme;backpropagation;search problem;Network topology;Artificial neural networks;Neural networks;Supervised learning;Biological cells;Information technology;Evolutionary computation;Unsupervised learning;Technological innovation},
	Month = {Dec},
	Pages = {367-371},
	Title = {NeuroEvolution of Augmenting Topologies with Learning for Data Classification},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICINFA.2006.374100}}

@inproceedings{7991745,
	Abstract = {The article describes the problematic issues of neuroevolution, i.e. a promising approach for solving complex problems of machine learning, neural networks, adaptive management and multi-agent systems, evolutionary robotics, gaming strategies, and computer art. The authors have suggested neuro evolutional algorithm and presented experiment results on a standard task: the balancing trolley with two flagpoles of different lengths.},
	Author = {S. Rodzin and O. Rodzina and L. Rodzina},
	Booktitle = {2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)},
	Date-Added = {2018-12-12 13:20:17 +1300},
	Date-Modified = {2018-12-12 13:20:28 +1300},
	Doi = {10.1109/ICAICT.2016.7991745},
	Issn = {2472-8586},
	Keywords = {Evolutionary;learning (artificial intelligence);multi-agent systems;neural nets;machine learning;neural networks;adaptive management;multiagent systems;evolutionary robotics;gaming strategies;computer art;neuro evolutional algorithm;balancing trolley;Biological neural networks;Neurons;Encoding;Sociology;Statistics;Topology;Head;Neuroevolution;reinforcement machine learning;optimization;evolutionary computation;fitness function},
	Month = {Oct},
	Pages = {1-4},
	Title = {Neuroevolution: Problems, algorithms, and experiments},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICAICT.2016.7991745}}

@inproceedings{8489184,
	Abstract = {Memetic algorithms have been a promising strategy to enhance neuroevolution in the past. Cooperative coevolution has been combined as memetic cooperative neuroevolution with application to chaotic time series prediction. Although the method has shown promising performance, there are limitations in the balance between global and local search. The previous study used a specific local search strategy for intensification that affected the diversity of solutions. In this study, we address this limitation by information (meme) collection strategies that maintains and refines a pool of memes during global search. We present two strategies where one is sequential and the other is concurrent meme collection implemented at different stages of evolution. In the majority of the given problems, the proposed strategies showed improvement in prediction accuracy over the related methods.},
	Author = {G. Wong and A. Sharma and R. Chandra},
	Booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
	Date-Added = {2018-12-12 13:05:44 +1300},
	Date-Modified = {2018-12-12 13:05:58 +1300},
	Doi = {10.1109/IJCNN.2018.8489184},
	Issn = {2161-4407},
	Keywords = {Evolutionary;search problems;social sciences;time series;memetic algorithms;neuroevolution;coevolution;chaotic time series prediction;global search;specific local search strategy;information collection strategies;concurrent meme collection;prediction accuracy;Memetics;Time series analysis;Neurons;Prediction algorithms;Sociology;Cooperative Coevolution;Memetic Algorithms;Time Series Prediction Global Search;Local Search;Neuroevolution},
	Month = {July},
	Pages = {1-6},
	Title = {Information Collection Strategies In Memetic Cooperative Neuroevolution For Time Series Prediction},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2018.8489184}}

@conference{:aa,
	Date-Added = {2018-12-12 13:05:20 +1300},
	Date-Modified = {2018-12-12 13:05:20 +1300},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCILi4vUmVmZXJlbmNlcy9QYXBlcnMvRXZvbHV0aW9uYXJ5L0luZm9ybWF0aW9uIENvbGxlY3Rpb24gU3RyYXRlZ2llcyBJbiBNZW1ldGljIENvb3BlcmF0aXZlIE5ldXJvZXZvbHV0aW9uIEZvciBUaW1lIFNlcmllcyBQcmVkaWN0aW9uLnBkZk8RAuIAAAAAAuIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9JbmZvcm1hdGlvbiBDb2xsZWMjRkZGRkZGRkYucGRmAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAMRXZvbHV0aW9uYXJ5AAIAni86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOlBhcGVyczpFdm9sdXRpb25hcnk6SW5mb3JtYXRpb24gQ29sbGVjdGlvbiBTdHJhdGVnaWVzIEluIE1lbWV0aWMgQ29vcGVyYXRpdmUgTmV1cm9ldm9sdXRpb24gRm9yIFRpbWUgU2VyaWVzIFByZWRpY3Rpb24ucGRmAA4AzgBmAEkAbgBmAG8AcgBtAGEAdABpAG8AbgAgAEMAbwBsAGwAZQBjAHQAaQBvAG4AIABTAHQAcgBhAHQAZQBnAGkAZQBzACAASQBuACAATQBlAG0AZQB0AGkAYwAgAEMAbwBvAHAAZQByAGEAdABpAHYAZQAgAE4AZQB1AHIAbwBlAHYAbwBsAHUAdABpAG8AbgAgAEYAbwByACAAVABpAG0AZQAgAFMAZQByAGkAZQBzACAAUAByAGUAZABpAGMAdABpAG8AbgAuAHAAZABmAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAnFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9QYXBlcnMvRXZvbHV0aW9uYXJ5L0luZm9ybWF0aW9uIENvbGxlY3Rpb24gU3RyYXRlZ2llcyBJbiBNZW1ldGljIENvb3BlcmF0aXZlIE5ldXJvZXZvbHV0aW9uIEZvciBUaW1lIFNlcmllcyBQcmVkaWN0aW9uLnBkZgATAAEvAAAVAAIAC///AAAACAANABoAJACvAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA5U=}}

@inproceedings{Han:2016:EEI:3001136.3001163,
	Acmid = {3001163},
	Address = {Piscataway, NJ, USA},
	Author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
	Booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
	Date-Added = {2018-12-11 11:22:33 +1300},
	Date-Modified = {2018-12-11 11:22:42 +1300},
	Doi = {10.1109/ISCA.2016.30},
	Isbn = {978-1-4673-8947-1},
	Keywords = {FPGA-NN, ASIC, algorithm-hardware co-design, deep learning, hardware acceleration, model compression},
	Location = {Seoul, Republic of Korea},
	Numpages = {12},
	Pages = {243--254},
	Publisher = {IEEE Press},
	Series = {ISCA '16},
	Title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
	Url = {https://doi.org/10.1109/ISCA.2016.30},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCA.2016.30}}

@inproceedings{shayer2018learning,
	Author = {Oran Shayer and Dan Levi and Ethan Fetaya},
	Booktitle = {International Conference on Learning Representations},
	Date-Added = {2018-12-09 16:39:21 +1300},
	Date-Modified = {2018-12-09 16:39:50 +1300},
	Keywords = {BNN, Discrete, MNIST, CIFAR, ImageNet},
	Title = {Learning Discrete Weights Using the Local Reparameterization Trick},
	Url = {https://openreview.net/forum?id=BySRH6CpW},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=BySRH6CpW}}

@inproceedings{wu2018training,
	Author = {Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},
	Booktitle = {International Conference on Learning Representations},
	Date-Added = {2018-12-09 16:29:45 +1300},
	Date-Modified = {2018-12-09 17:10:02 +1300},
	Keywords = {BNN; Binary, MNIST, CIFAR, ImageNet},
	Month = {Feb},
	Title = {Training and Inference with Integers in Deep Neural Networks},
	Url = {https://openreview.net/forum?id=HJGXzmspb},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=HJGXzmspb}}

@inbook{Moussa2006,
	Abstract = {Artificial Neural Networks (ANNs) are inherently parallel architectures which represent a natural fit for custom implementation on FPGAs. One important implementation issue is to determine the numerical precision format that allows an optimum tradeoff between precision and implementation areas. Standard single or double precision floating-point representations minimize quantization errors while requiring significant hardware resources. Less precise fixed-point representation may require less hardware resources but add quantization errors that may prevent learning from taking place, especially in regression problems. This chapter examines this issue and reports on a recent experiment where we implemented a Multi-layer perceptron (MLP) on an FPGA using both fixed and floating point precision. Results show that the fixed-point MLP implementation was over 12x greater in speed, over 13x smaller in area, and achieves far greater processing density compared to the floating-point FPGA-based MLP.},
	Address = {Boston, MA},
	Author = {Moussa, Medhat and Areibi, Shawki and Nichols, Kristian},
	Booktitle = {FPGA Implementations of Neural Networks},
	Date-Added = {2018-12-06 14:20:11 +1300},
	Date-Modified = {2018-12-06 14:20:23 +1300},
	Doi = {10.1007/0-387-28487-7_2},
	Editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Isbn = {978-0-387-28487-3},
	Keywords = {FPGA-NN, Book Chapter},
	Pages = {37--61},
	Publisher = {Springer US},
	Title = {On the Arithmetic Precision for Implementing Back-Propagation Networks on FPGA: A Case Study},
	Url = {https://doi.org/10.1007/0-387-28487-7_2},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_2}}

@inbook{Paul2006,
	Abstract = {Back propagation is a well known technique used in the implementation of artificial neural networks. The algorithm can be described essentially as a sequence of matrix vector multiplications and outer product operations interspersed with the application of a point wise non linear function. The algorithm is compute intensive and lends itself to a high degree of parallelism. These features motivate a systolic design of hardware to implement the Back Propagation algorithm. We present in this chapter a new systolic architecture for the complete back propagation algorithm. For a neural network with N input neurons, P hidden layer neurons and M output neurons, the proposed architecture with P processors, has a running time of (2N + 2M + P + max(M,P)) for each training set vector. This is the first such implementation of the back propagation algorithm which completely parallelizes the entire computation of learning phase. The array has been implemented on an Annapolis FPGA based coprocessor and it achieves very favorable performance with range of 5 GOPS. The proposed new design targets Virtex boards.},
	Address = {Boston, MA},
	Author = {Paul, Kolin and Rajopadhye, Sanjay},
	Booktitle = {FPGA Implementations of Neural Networks},
	Date-Added = {2018-12-06 14:17:29 +1300},
	Date-Modified = {2018-12-06 14:17:42 +1300},
	Doi = {10.1007/0-387-28487-7_5},
	Editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Isbn = {978-0-387-28487-3},
	Keywords = {FPGA-NN, Book Chapter},
	Pages = {137--165},
	Publisher = {Springer US},
	Title = {Back-Propagation Algorithm Achieving 5 Gops on the Virtex-E},
	Url = {https://doi.org/10.1007/0-387-28487-7_5},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_5}}

@inbook{Omondi2006,
	Abstract = {This introductory chapter reviews the basics of artificial-neural-network theory, discusses various aspects of the hardware implementation of neural networks (in both ASIC and FPGA technologies, with a focus on special features of artificial neural networks), and concludes with a brief note on performance-evaluation. Special points are the exploitation of the parallelism inherent in neural networks and the appropriate implementation of arithmetic functions, especially the sigmoid function. With respect to the sigmoid function, the chapter includes a significant contribution.},
	Address = {Boston, MA},
	Author = {Omondi, Amos R. and Rajapakse, Jagath C. and Bajger, Mariusz},
	Booktitle = {FPGA Implementations of Neural Networks},
	Date-Added = {2018-12-06 14:08:39 +1300},
	Date-Modified = {2018-12-06 14:17:53 +1300},
	Doi = {10.1007/0-387-28487-7_1},
	Editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Isbn = {978-0-387-28487-3},
	Keywords = {FPGA-NN, NN, Neurocomputers, Book Chapter},
	Pages = {1--36},
	Publisher = {Springer US},
	Title = {FPGA Neurocomputers},
	Url = {https://doi.org/10.1007/0-387-28487-7_1},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_1}}

@inproceedings{758889,
	Abstract = {A comparison between a bit-level and a conventional VLSI implementation of a binary neural network is presented. This network is based on Correlation Matrix Memory (CMM) that stores relationships between pairs of binary vectors. The bit-level architecture consists of an n/spl times/m array of bit-level processors holding the storage and computation elements. The conventional CMM architecture consists of a RAM memory holding the CMM storage and an array of counters. Since we are interested in the VLSI implementation of such networks, hardware complexities and speeds of both bit-level and conventional architecture were compared by using VLSI tools. It is shown that a significant speedup is achieved by using the bit-level architecture since the speed of this last configuration is not limited by the memory addressing delay. Moreover, the bit-level architecture is very simple and reduces the bus/routing, making the architecture suitable for VLSI implementation. The main drawback of such an approach compared to the conventional one is the demand for a high number of adders for dealing with a large number of inputs.},
	Author = {A. Bermak and J. Austin},
	Booktitle = {Proceedings of the Seventh International Conference on Microelectronics for Neural, Fuzzy and Bio-Inspired Systems},
	Date-Added = {2018-12-06 10:54:22 +1300},
	Date-Modified = {2018-12-06 10:54:32 +1300},
	Doi = {10.1109/MN.1999.758889},
	Keywords = {BNN; VLSI;neural chips;neural net architecture;CMOS digital integrated circuits;VLSI implementation;binary neural network;bit-level architecture;conventional architecture;correlation matrix memory;bit-level processors;RAM memory;counter array;hardware complexities;speed comparison;memory addressing delay;adders;VLSI digital design;Very large scale integration;Neural networks;Coordinate measuring machines;Computer architecture;Random access memory;Read-write memory;Counting circuits;Hardware;Added delay;Routing},
	Month = {April},
	Pages = {374-379},
	Title = {VLSI implementation of a binary neural network-two case studies},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1109/MN.1999.758889}}

@electronic{xnor.ai-webpage,
	Author = {Ali Farhadi; Mohammad Rastegari},
	Date-Added = {2018-12-06 10:26:37 +1300},
	Date-Modified = {2018-12-06 11:06:21 +1300},
	Keywords = {BNN, XNOR, Yolo},
	Lastchecked = {6/Dec/2018},
	Month = {Dec},
	Title = {xnor.ai Company},
	Url = {https://www.xnor.ai/},
	Urldate = {2018},
	Year = {2016}}

@inproceedings{8416941,
	Author = {H. Yonekawa and S. Sato and H. Nakahara},
	Booktitle = {2018 IEEE 48th International Symposium on Multiple-Valued Logic (ISMVL)},
	Date-Added = {2018-12-05 21:41:24 +1300},
	Date-Modified = {2018-12-05 22:14:52 +1300},
	Doi = {10.1109/ISMVL.2018.00038},
	Issn = {2378-2226},
	Keywords = {BNN; Neurons;Training;Two dimensional displays;Embedded systems;Character recognition;Computational modeling;Convolutional neural networks},
	Month = {May},
	Pages = {174-179},
	Title = {A Ternary Weight Binary Input Convolutional Neural Network: Realization on the Embedded Processor},
	Url = {doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038},
	Volume = {00},
	Year = {2018},
	Bdsk-Url-1 = {doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038},
	Bdsk-Url-2 = {https://doi.org/10.1109/ISMVL.2018.00038}}

@inproceedings{Faraone_2018_CVPR,
	Author = {Faraone, Julian and Fraser, Nicholas and Blott, Michaela and Leong, Philip H.W.},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-04 23:43:14 +1300},
	Date-Modified = {2018-12-05 16:06:33 +1300},
	Keywords = {BNN; Quantization, FPGA, LUT},
	Month = {June},
	Title = {SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks},
	Year = {2018}}

@inproceedings{Zhou_2018_CVPR,
	Author = {Zhou, Aojun and Yao, Anbang and Wang, Kuan and Chen, Yurong},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-04 23:14:06 +1300},
	Date-Modified = {2018-12-04 23:15:40 +1300},
	Keywords = {BNN; Quantization, Regularization},
	Month = {June},
	Title = {Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks},
	Year = {2018}}

@inproceedings{Wang_2018_CVPR,
	Author = {Wang, Peisong and Hu, Qinghao and Zhang, Yifan and Zhang, Chunjie and Liu, Yang and Cheng, Jian},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-03 22:57:17 +1300},
	Date-Modified = {2018-12-03 22:57:41 +1300},
	Keywords = {BNN; Quantization, Low-bit},
	Month = {June},
	Title = {Two-Step Quantization for Low-Bit Neural Networks},
	Year = {2018}}

@inproceedings{Xie_2017_ICCV,
	Author = {Xie, Lingxi and Yuille, Alan},
	Booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	Date-Added = {2018-12-03 22:12:21 +1300},
	Date-Modified = {2018-12-03 22:12:58 +1300},
	Keywords = {CNN, Genetic Algorithm, VGGNet, ResNet, DenseNet},
	Month = {Oct},
	Title = {Genetic CNN},
	Year = {2017}}

@inproceedings{Sholomon_2013_ICCV_Workshops,
	Author = {Sholomon, Dror and David, Omid and Netanyahu, Nathan S.},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-03 22:02:22 +1300},
	Date-Modified = {2018-12-03 22:03:08 +1300},
	Keywords = {Evolutionary, Genetic Algorithm, Jigzaw, Puzzle},
	Month = {June},
	Title = {A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles},
	Year = {2013}}

@article{2017arXiv171100205Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100205Z},
	Archiveprefix = {arXiv},
	Author = {{Zhuang}, B. and {Shen}, C. and {Tan}, M. and {Liu}, L. and {Reid}, I.},
	Date-Added = {2018-12-03 21:17:35 +1300},
	Date-Modified = {2018-12-03 21:21:48 +1300},
	Eprint = {1711.00205},
	Journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Keywords = {BNN; Computer Vision; Pattern Recognition},
	Month = Jun,
	Primaryclass = {cs.CV},
	Title = {{Towards Effective Low-bitwidth Convolutional Neural Networks}},
	Year = 2017}

@article{6134678,
	Abstract = {This paper proposes a set of adaptive learning rules for binary feedforward neural networks (BFNNs) by means of the sensitivity measure that is established to investigate the effect of a BFNN's weight variation on its output. The rules are based on three basic adaptive learning principles: the benefit principle, the minimal disturbance principle, and the burden-sharing principle. In order to follow the benefit principle and the minimal disturbance principle, a neuron selection rule and a weight adaptation rule are developed. Besides, a learning control rule is developed to follow the burden-sharing principle. The advantage of the rules is that they can effectively guide the BFNN's learning to conduct constructive adaptations and avoid destructive ones. With these rules, a sensitivity-based adaptive learning (SBALR) algorithm for BFNNs is presented. Experimental results on a number of benchmark data demonstrate that the SBALR algorithm has better learning performance than the Madaline rule II and backpropagation algorithms.},
	Author = {S. Zhong and X. Zeng and S. Wu and L. Han},
	Date-Added = {2018-12-02 16:32:30 +1300},
	Date-Modified = {2018-12-02 16:32:45 +1300},
	Doi = {10.1109/TNNLS.2011.2177860},
	Issn = {2162-237X},
	Journal = {IEEE Transactions on Neural Networks and Learning Systems},
	Keywords = {BNN; feedforward neural nets;learning (artificial intelligence);sensitivity analysis;sensitivity-based adaptive learning rules;binary feedforward neural network;sensitivity measurement;BFNN weight variation;adaptive learning principles;benefit principle;minimal disturbance principle;burden-sharing principle;neuron selection rule;weight adaptation rule;learning control rule;BFNN learning;benchmark data;SBALR algorithm;Madaline rule II;backpropagation algorithm;Neurons;Sensitivity;Training;Learning systems;Weight measurement;Biological neural networks;Feedforward neural networks;Adaptive learning algorithm;binary feedforward neural networks;learning rule;sensitivity;Artificial Intelligence;Databases, Factual;Neural Networks (Computer)},
	Month = {March},
	Number = {3},
	Pages = {480-491},
	Title = {Sensitivity-Based Adaptive Learning Rules for Binary Feedforward Neural Networks},
	Volume = {23},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2011.2177860}}

@article{7878541,
	Abstract = {Convolutional neural networks (CNNs) have revolutionized the world of computer vision over the last few years, pushing image classification beyond human accuracy. The computational effort of today's CNNs requires power-hungry parallel processors or GP-GPUs. Recent developments in CNN accelerators for system-on-chip integration have reduced energy consumption significantly. Unfortunately, even these highly optimized devices are above the power envelope imposed by mobile and deeply embedded applications and face hard limitations caused by CNN weight I/O and storage. This prevents the adoption of CNNs in future ultralow power Internet of Things end-nodes for near-sensor analytics. Recent algorithmic and theoretical advancements enable competitive classification accuracy even when limiting CNNs to binary (+1/-1) weights during training. These new findings bring major optimization opportunities in the arithmetic core by removing the need for expensive multiplications, as well as reducing I/O bandwidth and storage. In this paper, we present an accelerator optimized for binary-weight CNNs that achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate equivalent (MGE) or 1.9 mm<sup>2</sup>and with a power dissipation of 895 μW in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms the state-of-the-art in terms of energy and area efficiency achieving 61.2 TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.},
	Author = {R. Andri and L. Cavigelli and D. Rossi and L. Benini},
	Date-Added = {2018-12-02 15:33:17 +1300},
	Date-Modified = {2018-12-02 15:33:26 +1300},
	Doi = {10.1109/TCAD.2017.2682138},
	Issn = {0278-0070},
	Journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	Keywords = {BNN; CMOS logic circuits;computer vision;convolution;coprocessors;embedded systems;image classification;integrated circuit design;low-power electronics;neural nets;optimisation;power aware computing;system-on-chip;I/O storage;I/O bandwidth;algorithmic advancements;binary weights;competitive classification accuracy;hard limitations;deeply embedded applications;mobile embedded applications;power envelope;energy consumption;system-on-chip integration;CNN accelerators;GP-GPUs;power-hungry parallel processors;computational effort;human accuracy;image classification;computer vision;convolutional neural networks;ultralow power binary-weight CNN acceleration;power dissipation;binary-weight CNNs;accelerator;optimization opportunities;ASIC;binary weights;convolutional neural networks (CNNs);hardware accelerator;Internet of Things (IoT)},
	Month = {Jan},
	Number = {1},
	Pages = {48-60},
	Title = {YodaNN: An Architecture for Ultralow Power Binary-Weight CNN Acceleration},
	Volume = {37},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCAD.2017.2682138}}

@article{POSEWSKY2018151,
	Abstract = {Deep neural networks are an extremely successful and widely used technique for various pattern recognition and machine learning tasks. Due to power and resource constraints, these computationally intensive networks are difficult to implement in embedded systems. Yet, the number of applications that can benefit from the mentioned possibilities is rapidly rising. In this paper, we propose novel architectures for the inference of previously learned and arbitrary deep neural networks on FPGA-based SoCs that are able to overcome these limitations. Our key contributions include the reuse of previously transferred weight matrices across multiple input samples, which we refer to as batch processing, and the usage of compressed weight matrices, also known as pruning. An extensive evaluation of these optimizations is presented. Both techniques allow a significant mitigation of data transfers and speed-up the network inference by one order of magnitude. At the same time, we surpass the data throughput of fully-featured x86-based systems while only using a fraction of their energy consumption.},
	Author = {Thorbj{\"o}rn Posewsky and Daniel Ziener},
	Date-Added = {2018-12-02 13:07:55 +1300},
	Date-Modified = {2018-12-02 13:08:06 +1300},
	Doi = {https://doi.org/10.1016/j.micpro.2018.04.004},
	Issn = {0141-9331},
	Journal = {Microprocessors and Microsystems},
	Keywords = {FPGA-NN; Deep neural networks, Batch processing, Pruning, Compression, FPGA, Inference, Throughput optimizations, Fully-connected},
	Pages = {151 - 161},
	Title = {Throughput optimizations for FPGA-based deep neural network inference},
	Url = {http://www.sciencedirect.com/science/article/pii/S014193311730296X},
	Volume = {60},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S014193311730296X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.micpro.2018.04.004}}

@article{HAJDUK2018227,
	Abstract = {This brief paper presents two implementations of feed-forward artificial neural networks in FPGAs. The implementations differ in the FPGA resources requirement and calculations speed. Both implementations exercise floating point arithmetic, apply very high accuracy activation function realization, and enable easy alteration of the neural network's structure without the need of a re-implementation of the entire FPGA project.},
	Author = {Zbigniew Hajduk},
	Date-Added = {2018-12-02 12:40:03 +1300},
	Date-Modified = {2018-12-02 12:40:15 +1300},
	Doi = {https://doi.org/10.1016/j.neucom.2018.04.077},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {FPGA-NN, FPGA, Neural networks},
	Pages = {227 - 234},
	Title = {Reconfigurable FPGA implementation of neural networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231218305393},
	Volume = {308},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231218305393},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2018.04.077}}

@book{Omondi:2010:FIN:1941654,
	Author = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Date-Added = {2018-12-02 11:27:41 +1300},
	Date-Modified = {2018-12-02 11:28:42 +1300},
	Edition = {1st},
	Isbn = {1441939423, 9781441939425},
	Keywords = {FPGA-NN, ASIC, Book by Publications},
	Publisher = {Springer Publishing Company, Incorporated},
	Title = {FPGA Implementations of Neural Networks},
	Year = {2010}}

@inproceedings{Raina:2009:LDU:1553374.1553486,
	Acmid = {1553486},
	Address = {New York, NY, USA},
	Author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
	Booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	Date-Added = {2018-12-01 22:23:56 +1300},
	Date-Modified = {2018-12-01 22:24:24 +1300},
	Doi = {10.1145/1553374.1553486},
	Isbn = {978-1-60558-516-1},
	Keywords = {NN; GPU, Neural Network},
	Location = {Montreal, Quebec, Canada},
	Numpages = {8},
	Pages = {873--880},
	Publisher = {ACM},
	Series = {ICML '09},
	Title = {Large-scale Deep Unsupervised Learning Using Graphics Processors},
	Url = {http://doi.acm.org/10.1145/1553374.1553486},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1553374.1553486},
	Bdsk-Url-2 = {https://doi.org/10.1145/1553374.1553486}}

@proceedings{2016arXiv160207261S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160207261S},
	Archiveprefix = {arXiv},
	Author = {{Szegedy}, C. and {Ioffe}, S. and {Vanhoucke}, V. and {Alemi}, A.},
	Date-Added = {2018-12-01 20:45:24 +1300},
	Date-Modified = {2018-12-01 20:47:18 +1300},
	Eprint = {1602.07261},
	Journal = {Prceedings of the 31st AAAI Conference on Artificial Intelligence},
	Keywords = {CNN; Computer Vision; Pattern Recognition; Inception},
	Month = feb,
	Number = {31},
	Organization = {AAAI},
	Primaryclass = {cs.CV},
	Publisher = {AAAI},
	Title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
	Volume = {31},
	Year = 2017}

@article{2016arXiv160507678C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160507678C},
	Annote = {https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
https://towardsdatascience.com/neural-network-architectures-156e5bad51ba
},
	Archiveprefix = {arXiv},
	Author = {{Canziani}, A. and {Paszke}, A. and {Culurciello}, E.},
	Date-Added = {2018-12-01 20:05:07 +1300},
	Date-Modified = {2018-12-01 23:10:43 +1300},
	Eprint = {1605.07678},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
	Year = 2016}

@inproceedings{8099726,
	Abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
	Author = {G. Huang and Z. Liu and L. v. d. Maaten and K. Q. Weinberger},
	Booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-01 19:33:27 +1300},
	Date-Modified = {2018-12-01 19:33:50 +1300},
	Doi = {10.1109/CVPR.2017.243},
	Issn = {1063-6919},
	Keywords = {CNN; convolution;feedforward neural nets;learning (artificial intelligence);DenseNet;traditional convolutional networks;feature propagation;feature reuse;object recognition benchmark tasks;dense convolutional network;vanishing-gradient problem;Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation; DenseNet},
	Month = {July},
	Pages = {2261-2269},
	Title = {Densely Connected Convolutional Networks},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2017.243}}

@article{Simonyan14c,
	Author = {Simonyan, K. and Zisserman, A.},
	Date-Added = {2018-12-01 13:32:36 +1300},
	Date-Modified = {2018-12-01 13:33:51 +1300},
	Journal = {ILSVRC - CoRR},
	Keywords = {CNN; image classification, VGG network},
	Title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	Volume = {abs/1409.1556},
	Year = {2014}}

@electronic{AppleFaceDetectionURL,
	Author = {Computer Vision Machine Learning Team},
	Date-Added = {2018-11-29 21:01:15 +1300},
	Date-Modified = {2018-11-29 21:07:30 +1300},
	Keywords = {Neural Network, Vision Framework, Face recognition},
	Lastchecked = {29/Nov/2018},
	Month = {11},
	Title = {An On-device Deep Neural Network for Face Detection},
	Url = {https://machinelearning.apple.com/2017/11/16/face-detection.html},
	Urldate = {November 2017},
	Year = {2017},
	Bdsk-Url-1 = {https://machinelearning.apple.com/2017/11/16/face-detection.html}}

@inproceedings{5524599,
	Abstract = {Artificial neural networks play an important role in robot programming by demonstration. In this paper we present a method for artificial neural network training. The main idea of this method is to train the artificial neural network with all of the data, before the current training step, and at a certain step the network is already trained a huge number of times. Some features of the quality of neural network training, using this method, were presented in. Because the method uses all of the data before the current training step, in this paper, we are concerned about training time and computing time comportment of the neural network. A software application for obtaining training time based on the number of training steps was designed. This software application implements the training method on an unidirectional multi-layer neural network and prints into a graph the training time and computing time. The results obtained using the software application and important conclusions towards the training and computing time comportment are also presented.},
	Author = {M. Stoica and G. A. Calangiu and F. Sisak},
	Booktitle = {19th International Workshop on Robotics in Alpe-Adria-Danube Region (RAAD 2010)},
	Date-Added = {2018-11-29 19:30:41 +1300},
	Date-Modified = {2018-11-29 19:30:48 +1300},
	Doi = {10.1109/RAAD.2010.5524599},
	Keywords = {NN; graph theory;learning (artificial intelligence);neural nets;robot programming;neural network training;training steps;artificial neural networks;robot programming;unidirectional multi-layer neural network;graph;Time measurement;Neural networks;Robot kinematics;Service robots;Sliding mode control;Control systems;Biological system modeling;Humans;Biological neural networks;Artificial neural networks},
	Month = {June},
	Pages = {109-113},
	Title = {Measuring the time needed for training a neural network based on the number of training steps},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/RAAD.2010.5524599}}

@inproceedings{ijcai2017-316,
	Author = {Tao Lin and Tian Guo and Karl Aberer},
	Booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {IJCAI-17}},
	Date-Added = {2018-11-29 19:19:54 +1300},
	Date-Modified = {2018-11-29 19:20:41 +1300},
	Doi = {10.24963/ijcai.2017/316},
	Keywords = {NN; Neural Network, LSTM, Convolutional Neural Network, Hybrid},
	Pages = {2273--2279},
	Title = {Hybrid Neural Networks for Learning the Trend in Time Series},
	Url = {https://doi.org/10.24963/ijcai.2017/316},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2017/316}}

@article{SCHMIDHUBER201585,
	Abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	Author = {J{\"u}rgen Schmidhuber},
	Date-Added = {2018-11-28 23:30:32 +1300},
	Date-Modified = {2018-11-28 23:31:02 +1300},
	Doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {DNN; Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
	Pages = {85 - 117},
	Title = {Deep learning in neural networks: An overview},
	Url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	Volume = {61},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2014.09.003}}

@inproceedings{5981829,
	Abstract = {In this paper we present a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms - neuFlow - and a dataflow compiler - luaFlow - that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications. We present an application of the system on street scene analysis, segmenting 20 categories on 500 × 375 frames at 12 frames per second on our custom hardware neuFlow.},
	Author = {C. Farabet and B. Martini and B. Corda and P. Akselrod and E. Culurciello and Y. LeCun},
	Booktitle = {CVPR 2011 WORKSHOPS},
	Date-Added = {2018-11-28 17:19:47 +1300},
	Date-Modified = {2018-11-28 17:19:58 +1300},
	Doi = {10.1109/CVPRW.2011.5981829},
	Issn = {2160-7508},
	Keywords = {CNN-FPGA; computer vision;field programmable gate arrays;flow graphs;NeuFlow;runtime reconfigurable dataflow processor;scalable dataflow hardware architecture;dataflow compiler;luaFlow;flow graph representations;machine code;Xilinx Virtex 6 FPGA platform;laptop computer;Tiles;Computer architecture;Runtime;Field programmable gate arrays;Hardware;Convolvers;Feature extraction},
	Month = {June},
	Pages = {109-116},
	Title = {NeuFlow: A runtime reconfigurable dataflow processor for vision},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPRW.2011.5981829}}

@book{Hertz:1991:ITN:104000,
	Address = {Boston, MA, USA},
	Author = {Hertz, John and Krogh, Anders and Palmer, Richard G.},
	Date-Added = {2018-11-26 21:51:03 +1300},
	Date-Modified = {2018-11-26 21:52:11 +1300},
	Isbn = {0-201-50395-6},
	Keywords = {NN; Book, Neural Computation},
	Publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	Source = {ISBN 0-201-51560-1},
	Title = {Introduction to the Theory of Neural Computation},
	Year = {1991}}

@article{Silver:2017aa,
	Author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	Date = {2017/10/18/online},
	Date-Added = {2018-11-26 19:46:00 +1300},
	Date-Modified = {2018-11-26 19:46:20 +1300},
	Day = {18},
	Journal = {Nature},
	Keywords = {NN, Go, Google, AlphaGo},
	L3 = {10.1038/nature24270; https://www.nature.com/articles/nature24270#supplementary-information},
	M3 = {Article},
	Month = {10},
	Pages = {354 EP -},
	Publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN -},
	Title = {Mastering the game of Go without human knowledge},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature24270},
	Volume = {550},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature24270}}

@article{doi:10.1177/1555343417695197,
	Abstract = { Autonomous and semiautonomous vehicles are currently being developed by over14 companies. These vehicles may improve driving safety and convenience, or they may create new challenges for drivers, particularly with regard to situation awareness (SA) and autonomy interaction. I conducted a naturalistic driving study on the autonomy features in the Tesla Model S, recording my experiences over a 6-month period, including assessments of SA and problems with the autonomy. This preliminary analysis provides insights into the challenges that drivers may face in dealing with new autonomous automobiles in realistic driving conditions, and it extends previous research on human-autonomy interaction to the driving domain. Issues were found with driver training, mental model development, mode confusion, unexpected mode interactions, SA, and susceptibility to distraction. New insights into challenges with semiautonomous driving systems include increased variability in SA, the replacement of continuous control with serial discrete control, and the need for more complex decisions. Issues that deserve consideration in future research and a set of guidelines for driver interfaces of autonomous systems are presented and used to create recommendations for improving driver SA when interacting with autonomous vehicles. },
	Author = {Mica R. Endsley},
	Date-Added = {2018-11-26 19:43:09 +1300},
	Date-Modified = {2018-11-26 19:43:24 +1300},
	Doi = {10.1177/1555343417695197},
	Eprint = {https://doi.org/10.1177/1555343417695197},
	Journal = {Journal of Cognitive Engineering and Decision Making},
	Keywords = {NN, autopilot, tesla},
	Number = {3},
	Pages = {225-238},
	Title = {Autonomous Driving Systems: A Preliminary Naturalistic Study of the Tesla Model S},
	Url = {https://doi.org/10.1177/1555343417695197},
	Volume = {11},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1177/1555343417695197}}

@inproceedings{nirkin2018_faceswap,
	Author = {Nirkin, Yuval and Masi, Iacopo and Tran, Anh Tuan and Hassner, Tal and Medioni, G\'{e}rard},
	Booktitle = {IEEE Conference on Automatic Face and Gesture Recognition},
	Date-Added = {2018-11-26 19:30:06 +1300},
	Date-Modified = {2018-11-26 19:35:44 +1300},
	Keywords = {CNN, Faceswap, DeepFakes, CNN},
	Title = {On Face Segmentation, Face Swapping, and Face Perception},
	Year = {2018}}

@article{McCulloch1943,
	Abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	Author = {McCulloch, Warren S. and Pitts, Walter},
	Date-Added = {2018-11-25 21:59:10 +1300},
	Date-Modified = {2018-11-25 21:59:28 +1300},
	Day = {01},
	Doi = {10.1007/BF02478259},
	Issn = {1522-9602},
	Journal = {The bulletin of mathematical biophysics},
	Keywords = {NN, Neural network, mathematical definition},
	Month = {Dec},
	Number = {4},
	Pages = {115--133},
	Title = {A logical calculus of the ideas immanent in nervous activity},
	Url = {https://doi.org/10.1007/BF02478259},
	Volume = {5},
	Year = {1943},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02478259}}

@inproceedings{Qiu:2016:GDE:2847263.2847265,
	Acmid = {2847265},
	Address = {New York, NY, USA},
	Author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
	Booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 19:49:32 +1300},
	Date-Modified = {2018-11-25 19:49:42 +1300},
	Doi = {10.1145/2847263.2847265},
	Isbn = {978-1-4503-3856-1},
	Keywords = {FPGA-NN; bandwidth utilization, convolutional neural network (cnn), dynamic-precision data quantization, embedded fpga},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {26--35},
	Publisher = {ACM},
	Series = {FPGA '16},
	Title = {Going Deeper with Embedded FPGA Platform for Convolutional Neural Network},
	Url = {http://doi.acm.org/10.1145/2847263.2847265},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2847263.2847265},
	Bdsk-Url-2 = {https://doi.org/10.1145/2847263.2847265}}

@inproceedings{Zhang:2017:IPO:3020078.3021698,
	Acmid = {3021698},
	Address = {New York, NY, USA},
	Author = {Zhang, Jialiang and Li, Jing},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 19:34:22 +1300},
	Date-Modified = {2018-11-25 19:34:32 +1300},
	Doi = {10.1145/3020078.3021698},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGA-NN; convolutional neural networks, fpga, hardware accelerator, opencl},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {25--34},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network},
	Url = {http://doi.acm.org/10.1145/3020078.3021698},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021698},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021698}}

@inproceedings{Langhammer:2015:FDB:2684746.2689071,
	Acmid = {2689071},
	Address = {New York, NY, USA},
	Author = {Langhammer, Martin and Pasca, Bogdan},
	Booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 16:46:58 +1300},
	Date-Modified = {2018-11-25 16:47:11 +1300},
	Doi = {10.1145/2684746.2689071},
	Isbn = {978-1-4503-3315-3},
	Keywords = {FPGA, altera, arria10, dsp, floating-point, single-precision},
	Location = {Monterey, California, USA},
	Numpages = {9},
	Pages = {117--125},
	Publisher = {ACM},
	Series = {FPGA '15},
	Title = {Floating-Point DSP Block Architecture for FPGAs},
	Url = {http://doi.acm.org/10.1145/2684746.2689071},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689071},
	Bdsk-Url-2 = {https://doi.org/10.1145/2684746.2689071}}

@inproceedings{Aydonat:2017:ODL:3020078.3021738,
	Acmid = {3021738},
	Address = {New York, NY, USA},
	Author = {Aydonat, Utku and O'Connell, Shane and Capalija, Davor and Ling, Andrew C. and Chiu, Gordon R.},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 15:35:07 +1300},
	Date-Modified = {2018-11-25 15:35:29 +1300},
	Doi = {10.1145/3020078.3021738},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGA-NN, convolutional neural networks, deep neural networks},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {55--64},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {An OpenCL Deep Learning Accelerator on Arria 10},
	Url = {http://doi.acm.org/10.1145/3020078.3021738},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021738},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021738}}

@inproceedings{10.1007/11550822_51,
	Abstract = {In this paper, we analyze characteristics of GA-based learning method of Binary Neural Networks (BNN). First, we consider coding methods to a chromosome in a GA and discuss the necessary chromosome length for a learning of BNN. Then, we compare some selection methods in a GA. We show that the learning results can be obtained in the less number of generations by properly setting selection methods and parameters in a GA. We also show that the quality of the learning results can be almost the same as that of the conventional method. These results can be verified by numerical experiments.},
	Address = {Berlin, Heidelberg},
	Author = {Hirane, Tatsuya and Toryu, Tetsuya and Nakano, Hidehiro and Miyauchi, Arata},
	Booktitle = {Artificial Neural Networks: Biological Inspirations -- ICANN 2005},
	Date-Added = {2018-11-25 00:34:18 +1300},
	Date-Modified = {2018-11-25 14:47:15 +1300},
	Editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.{z}}ny, S{\l}awomir},
	Isbn = {978-3-540-28754-4},
	Keywords = {BNN, Genetic Algorithm,},
	Pages = {323--329},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Analysis for Characteristics of GA-Based Learning Method of Binary Neural Networks},
	Year = {2005}}

@inproceedings{Umuroglu:2017:FFF:3020078.3021744,
	Acmid = {3021744},
	Address = {New York, NY, USA},
	Author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 00:31:00 +1300},
	Date-Modified = {2018-11-25 00:31:11 +1300},
	Doi = {10.1145/3020078.3021744},
	Isbn = {978-1-4503-4354-1},
	Keywords = {BNN; FPGA, binarized neural network, binary neural network, hardware acceleration, neural networks, reconfigurable logic},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {65--74},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {FINN: A Framework for Fast, Scalable Binarized Neural Network Inference},
	Url = {http://doi.acm.org/10.1145/3020078.3021744},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021744},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021744}}

@article{Yamada1992,
	Abstract = {This paper presents an ultra-high-speed sorter based upon a simplified parallel sorting algorithm using a binary neural network which consists both of binary neurons and of AND-OR synaptic connections to solve sorting problems at two and only two clock cycles. Our simplified algorithm is based on the super parallel sorting algorithm proposed by Takefuji and Lee. Nevertheless, our algorithm does not need any adders, while Takefuji's algorithm needs n{\texttimes}(n−1) analog adders of which each has multiple input ports. For an example of the simplified parallel sorter, a hardware design and its implementation will be introduced in this paper, which performs a sorting operation at two clock cycles. Both results of a logic circuit simulation and of an algorithm simulation show the justice of our hardware implementation even if in the practical size of the problem.},
	Author = {Yamada, Manabu and Nakagawa, Tohru and Kitagawa, Hajime},
	Date-Added = {2018-11-25 00:23:46 +1300},
	Date-Modified = {2018-11-25 00:24:03 +1300},
	Day = {01},
	Doi = {10.1007/BF00228719},
	Issn = {1573-1979},
	Journal = {Analog Integrated Circuits and Signal Processing},
	Keywords = {BNN; Parallel processing; Sorting Algorithm},
	Month = {Nov},
	Number = {4},
	Pages = {389--393},
	Title = {A super parallel sorter using a binary neural network with AND-OR synaptic connections},
	Url = {https://doi.org/10.1007/BF00228719},
	Volume = {2},
	Year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00228719}}

@inproceedings{10.1007/978-3-642-42042-9_86,
	Abstract = {This paper studies application of the dynamic binary neural network to control signal of switching circuits. The network is characterized by the signum activation function and ternary weighting parameters. In the application, the teacher signal is one binary periodic orbit corresponding to the controls. The learning algorithm is based on the genetic algorithm. As an application object, we consider a control signal of the basic matrix converter. Performing a basic numerical experiment, we have confirmed that the teacher signal is stored successfully and is stabilized automatically.},
	Address = {Berlin, Heidelberg},
	Author = {Nakayama, Yuta and Kouzuki, Ryota and Saito, Toshimichi},
	Booktitle = {Neural Information Processing},
	Date-Added = {2018-11-25 00:10:15 +1300},
	Date-Modified = {2018-11-25 00:20:07 +1300},
	Editor = {Lee, Minho and Hirose, Akira and Hou, Zeng-Guang and Kil, Rhee Man},
	Isbn = {978-3-642-42042-9},
	Keywords = {BNN; Dynamic NN; Control signal, Genetic Algorithm},
	Pages = {697--704},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Application of the Dynamic Binary Neural Network to Switching Circuits},
	Year = {2013}}

@inproceedings{10.1007/978-3-642-59041-2_23,
	Abstract = {In the very near future large amounts of Remotely Sensed data will become available on a daily basis. Unfortunately, it is not clear if the processing methods are available to deal with this data in a timely fashion. This paper describes research towards an approach which will allow a user to perform a rapid pre-search of large amounts of image data for regions of interest based on texture. The method is based on a novel neural network architecture (ADAM) that is designed primarily for speed of operation by making use of computationally simple pre-processing and only uses Boolean operations in the weights of the network. To facilitate interactive use of the network, it is capable of rapid training. The paper outlines the neural network, its application to RS data in comparison with other methods, and briefly describes a fast hardware implementation of the network.},
	Address = {Berlin, Heidelberg},
	Author = {Austin, Jim},
	Booktitle = {Neurocomputation in Remote Sensing Data Analysis},
	Date-Added = {2018-11-24 23:37:04 +1300},
	Date-Modified = {2018-11-24 23:37:33 +1300},
	Editor = {Kanellopoulos, Ioannis and Wilkinson, Graeme G. and Roli, Fabio and Austin, James},
	Isbn = {978-3-642-59041-2},
	Keywords = {BNN, Image Segmentation, Data Analysis},
	Pages = {202--213},
	Publisher = {Springer Berlin Heidelberg},
	Title = {High Speed Image Segmentation Using a Binary Neural Network},
	Year = {1997}}

@inproceedings{10.1007/978-81-322-2208-8_43,
	Abstract = {In this paper, a quantum based binary neural network learning algorithm is proposed for solving two class problems. The proposed method constructively forms the neural network architecture and weights are decided by quantum computing concept. The use of quantum computing optimizes the network structure and the performance in terms of number of neurons at hidden layer and classification accuracy. This approach is compared with MTiling-real networks algorithm and it is found that there is a significant improvement in terms of number of neurons at the hidden layer, number of iterations, training accuracy and generalization accuracy.},
	Address = {New Delhi},
	Author = {Patel, Om Prakash and Tiwari, Aruna},
	Booktitle = {Computational Intelligence in Data Mining - Volume 2},
	Date-Added = {2018-11-24 23:17:08 +1300},
	Date-Modified = {2018-11-24 23:17:53 +1300},
	Editor = {Jain, Lakhmi C. and Behera, Himansu Sekhar and Mandal, Jyotsna Kumar and Mohapatra, Durga Prasad},
	Isbn = {978-81-322-2208-8},
	Keywords = {BNN; Quantum Algorithm; Neural Network},
	Pages = {473--482},
	Publisher = {Springer India},
	Title = {Quantum Based Learning with Binary Neural Network},
	Year = {2015}}

@incollection{NIPS2016_6573,
	Author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	Date-Added = {2018-11-24 21:41:12 +1300},
	Date-Modified = {2018-11-24 21:42:24 +1300},
	Editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	Keywords = {BNN; Neural Network},
	Pages = {4107--4115},
	Publisher = {Curran Associates, Inc.},
	Title = {Binarized Neural Networks},
	Url = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf}}

@incollection{NIPS2015_5647,
	Author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Date-Added = {2018-11-24 21:39:32 +1300},
	Date-Modified = {2018-11-24 21:39:53 +1300},
	Editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	Keywords = {BNN; Machine Learning; Computer Vision; Pattern Recognition; Neural and Evolutionary Computing},
	Pages = {3123--3131},
	Publisher = {Curran Associates, Inc.},
	Title = {BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
	Url = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf}}

@inproceedings{Nurvitadhi:2017:FBG:3020078.3021740,
	Acmid = {3021740},
	Address = {New York, NY, USA},
	Author = {Nurvitadhi, Eriko and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan and Subhaschandra, Suchit and Boudoukh, Guy},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-24 20:45:12 +1300},
	Date-Modified = {2018-11-24 20:45:24 +1300},
	Doi = {10.1145/3020078.3021740},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGA-NN; FPGA, GPU, accelerator, deep learning, intel stratix 10},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {5--14},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?},
	Url = {http://doi.acm.org/10.1145/3020078.3021740},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021740},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021740}}

@inproceedings{Cheng_2014_CVPR,
	Author = {Cheng, Ming-Ming and Zhang, Ziming and Lin, Wen-Yan and Torr, Philip},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-11-23 10:09:08 +1300},
	Date-Modified = {2018-11-23 10:09:18 +1300},
	Keywords = {CNN; BING},
	Month = {June},
	Title = {BING: Binarized Normed Gradients for Objectness Estimation at 300fps},
	Year = {2014}}

@article{bhandari2012design,
	Author = {Bhandari, Sachin and Tiwari, Aruna},
	Date-Added = {2018-11-22 18:18:02 +1300},
	Date-Modified = {2018-11-22 18:18:27 +1300},
	Keywords = {BNN; Clustering, ETL, Fuzzy logic},
	Title = {Design and implementation of binary neural network learning with fuzzy clustering},
	Year = {2012}}

@misc{Muselli99hammingclustering:,
	Author = {Marco Muselli and Diego Liberati},
	Date-Added = {2018-11-22 17:37:41 +1300},
	Date-Modified = {2018-11-22 17:37:52 +1300},
	Keywords = {BNN},
	Title = {Hamming Clustering: A New Approach to Rule Extraction},
	Year = {1999}}

@inproceedings{817981,
	Abstract = {A new constructive learning algorithm, called Hamming clustering (HC), for binary neural networks is proposed. It is able to generate a set of rules in if-then form underlying an unknown classification problem starting from a training set of samples. The performance of HC has been evaluated through a variety of artificial and real-world benchmarks. In particular, its application in the diagnosis of breast cancer has led to the derivation of a reduced set of rules solving the associated classification problem.},
	Author = {M. Muselli and D. Liberati},
	Booktitle = {1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)},
	Date-Added = {2018-11-22 16:46:03 +1300},
	Date-Modified = {2018-11-22 16:46:11 +1300},
	Doi = {10.1049/cp:19991161},
	Issn = {0537-9989},
	Keywords = {BNN; neural nets;learning algorithm;Hamming clustering;binary neural networks;breast cancer;pattern classification;patient diagnosis;Boolean function},
	Month = {Sept},
	Pages = {515-520 vol.2},
	Title = {Rule extraction from binary neural networks},
	Volume = {2},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1049/cp:19991161}}

@inproceedings{5178979,
	Abstract = {This paper presents a learning algorithm of digital binary neural networks for approximation of desired Boolean functions. In the learning, the genetic algorithms is used with flexible fitness that tolerates error: it is suitable to reduce the number of hidden neurons and to tolerate noise and outliers. We then apply the algorithm to design of cellular automata with rich spatio-temporal patterns and various applications. Performing basic numerical experiment, the algorithm efficiency is confirmed.},
	Author = {S. Kabeya and T. Abe and T. Saito},
	Booktitle = {2009 International Joint Conference on Neural Networks},
	Date-Added = {2018-11-22 11:37:15 +1300},
	Date-Modified = {2018-11-22 11:37:25 +1300},
	Doi = {10.1109/IJCNN.2009.5178979},
	Issn = {2161-4393},
	Keywords = {BNN; Boolean functions;cellular automata;function approximation;genetic algorithms;learning (artificial intelligence);neural nets;flexible learning algorithm;error tolerance;digital binary neural networks;Boolean function approximation;genetic algorithms;cellular automata;spatio-temporal patterns;Neural networks;Signal processing algorithms;Neurons;Boolean functions;Approximation algorithms;Genetic algorithms;Noise reduction;Nonlinear dynamical systems;USA Councils;Algorithm design and analysis},
	Month = {June},
	Pages = {1476-1480},
	Title = {A GA-based flexible learning algorithm with error tolerance for digital binary neural networks},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2009.5178979}}

@inproceedings{714090,
	Abstract = {This paper considers the use of binary neural networks for pattern classification. An expand-and-truncate learning (ETL) algorithm is used to determine the required number of neurons as well as the connecting weights in a three-layered feedforward network for classifying input patterns. The ETL algorithm is guaranteed to find a network for any binary-to-binary mappings. The ETL algorithm's performance in pattern classification is tested using a breast cancer database that have been used for benchmarking performance other machine learning methods.},
	Author = {C. H. Chu and J. H. Kim},
	Booktitle = {Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
	Date-Added = {2018-11-21 23:44:42 +1300},
	Date-Modified = {2018-11-21 23:44:55 +1300},
	Doi = {10.1109/IJCNN.1993.714090},
	Keywords = {BNN; feedforward neural nets;pattern classification;learning (artificial intelligence);medical diagnostic computing;geometrical learning;binary neural networks;pattern classification;expand-and-truncate learning algorithm;connecting weights;three-layered feedforward network;binary-to-binary mappings;breast cancer database;Pattern classification;Neural networks;Neurons;Machine learning algorithms;Testing;Breast cancer;Databases;Hamming distance;Computer networks;Joining processes},
	Month = {Oct},
	Pages = {1039-1042 vol.1},
	Title = {Pattern classification by geometrical learning of binary neural networks},
	Volume = {1},
	Year = {1993},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1993.714090}}

@inproceedings{203280,
	Abstract = {A novel design technique for asynchronous binary neural networks is proposed. This design uses linear programming to design two architectures: (i) a fully connected network that reads a N-digit cue and classifies it into a category represented by a N-digit pattern: and (ii) a two-layer network (with lateral connections) that has M neurons in the first layer and L neurons in the second layer; the network reads an M-digit cue to the first layer and associates it with a second-layer L-digit pattern. In both cases, the objective function is a weighted sum of the number of errors that can be corrected by the network. A cue with this number of errors (or fewer) is guaranteed to converge to the correct pattern. An economical VLSI realization of the designed networks can be easily accomplished.&lt;&lt;ETX&gt;&gt;},
	Author = {M. Kam and J. C. Chow and R. Fischl},
	Booktitle = {29th IEEE Conference on Decision and Control},
	Date-Added = {2018-11-21 23:33:12 +1300},
	Date-Modified = {2018-11-21 23:33:20 +1300},
	Doi = {10.1109/CDC.1990.203280},
	Keywords = {BNN, linear programming;neural nets;asynchronous binary neural networks;linear programming;design technique;fully connected network;two-layer network;lateral connections;Neural networks;Linear programming;Neurons;Algorithm design and analysis;Error correction;Convergence;Neurofeedback;Hamming distance;Very large scale integration;Stochastic processes},
	Month = {Dec},
	Pages = {2766-2767 vol.5},
	Title = {Design of two architectures of asynchronous binary neural networks using linear programming},
	Year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1109/CDC.1990.203280}}

@inproceedings{1259700,
	Abstract = {Learning problem for neural networks has widely been investigated in last two decades. Kim and Park [J.H. Kim et al., Jan. 1995] proposed one approach based on geometric technique, called "expand and truncate learning (ETL)". ETL is proposed to construct a three-layer binary neural network (BNN) for training a Boolean function of n (Boolean) variables. It is claimed by Kim and Park in [J.H. Kim et al., Jan. 1995] that, neural networks constructed according to this technique are much smaller. This paper investigates usefulness of these ideas for data classification. Data classification in real world involves multiple classes. For solving this problem, there are many techniques based on statistical principles, clustering approaches, etc. Application of binary neural networks for multiple outputs are important in practice. We propose a method for construction of a binary neural net based on generalization of ETL to more than two classes. Our method simplifies the resulting neural network architecture.},
	Author = {Yi Xu and N. S. Chaudhari},
	Booktitle = {Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)},
	Date-Added = {2018-11-21 23:18:59 +1300},
	Date-Modified = {2018-11-21 23:19:06 +1300},
	Doi = {10.1109/ICMLC.2003.1259700},
	Keywords = {BNN; neural nets;pattern classification;learning (artificial intelligence);Boolean functions;binary neural networks;geometric technique;expand and truncate learning;Boolean function;data classification;core vertex;neural network training;Neural networks;Neurons;Artificial neural networks;Power line communications;Machine learning algorithms;Application software;Boolean functions;Pattern classification;Function approximation;Pattern matching},
	Month = {Nov},
	Pages = {1343-1348 Vol.3},
	Title = {Application of binary neural networks for classification},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICMLC.2003.1259700}}

@inproceedings{480879,
	Abstract = {Hidden periodicities play a very important role in the regulatory and structural functioning of genomic DNA strands. Primarily, it concerns the fundamental three-periodicity inherent to protein coding regions in all taxonomic groups, two-periodicity in introns of eukaryots as well as periodicities related to helix and chromatin pitches, while the other periodicities appear to be species specific. Rather roughly (and without sharp boundary) the underlying periodicities may be divided by two groups. In the first case the periodicities are due to particular nucleotides (or very short oligomers) quasi-regularly positioned in a seemingly random background. This type of regularity can be identified via either standard frequency analysis or more elaborate Fourier methods. For the second group a periodicity is related to the quasi-random replacements in initially complete repeating motifs (situation typical, e.g., for modifications of satellites). In the last case the statistical reconstruction of underlying repeats is a much less trivial task. The authors show that this problem can successfully be solved with multi-symbol extension of energy-minimizing neural networks (EMNN). The reconstruction of underlying motifs may shed additional light on the evolutionary and functional modifications in various genomes.},
	Author = {V. R. Chechetkin and A. A. Ezhov and L. A. Knizhnikova and A. Y. Turygin},
	Booktitle = {The Second International Symposium on Neuroinformatics and Neurocomputers},
	Date-Added = {2018-11-21 23:03:36 +1300},
	Date-Modified = {2018-11-21 23:03:46 +1300},
	Doi = {10.1109/ISNINC.1995.480879},
	Keywords = {NN; DNA;neural nets;sequences;genomic DNA sequences;neural networks;fundamental three-periodicity;protein coding regions;taxonomic groups;two-periodicity;introns;eukaryots;chromatin pitches;helix pitches;nucleotides;quasi-random replacements;statistical reconstruction;energy-minimizing neural networks;Intelligent networks;Genomics;Bioinformatics;DNA;Sequences;Neural networks;Neurons;Proteins;Technological innovation;Frequency},
	Month = {Sept},
	Pages = {346-352},
	Title = {Study of underlying repeats in genomic DNA sequences with neural networks},
	Year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNINC.1995.480879}}

@inproceedings{5380141,
	Abstract = {In this paper we propose a new method to analyze the similarity/dissimilarity of DNA sequences which can be used in human identification field. This method is based on the graphical representation proposed by Randic et al [M. Randic, M. Vracko, L. Nella, P. Dejan, Chem. (2003)]. Instead of calculating the leading eigenvalues of the matrix for graphical representation we smooth the zigzag curve and calculate its curvature. Similarity between DNA sequences are decided by neural network. Our method is useful for human identification in criminal investigations and in genetic disease. Our results verify the validity of our method.},
	Author = {R. Rafeh and M. Mesgar},
	Booktitle = {2009 Second International Conference on Computer and Electrical Engineering},
	Date-Added = {2018-11-21 22:54:01 +1300},
	Date-Modified = {2018-11-21 22:54:11 +1300},
	Doi = {10.1109/ICCEE.2009.132},
	Keywords = {NN, biology computing;DNA;eigenvalues and eigenfunctions;forensic science;matrix algebra;medical computing;neural nets;neural network;human identification;DNA sequences;graphical representation;criminal investigations;genetic disease;matrix eigenvalues;Neural networks;Humans;Sequences;Computer networks;DNA computing;Eigenvalues and eigenfunctions;Artificial neural networks;Genetics;Diseases;Forensics;University Timetabling;Optimization Problems;Constraint Programming;Linear Programming},
	Month = {Dec},
	Pages = {64-67},
	Title = {Neural Network in Human Identification by DNA Sequences},
	Volume = {2},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCEE.2009.132}}

@inproceedings{497852,
	Abstract = {The four nitrogenous bases of DNA spell out the recipes from which proteins are made. A gene typically contains five thousand or so bases but often only a small percentage of these are protein coding. Computer based prediction systems are increasingly relied upon as submissions to the major genetic databases are growing exponentially. Several systems exist to locate coding regions (exons) and noncoding regions (introns) within genomic DNA; the common models used are neural networks and Markov chains (M. Borodovsky and J. McIninch (1993), A. Krogh et al. (1994). One of the most successful programs is called GRAIL. Currently, two versions of GRAIL are available: GRAIL-I (E. Uberbacher and R. Mural (1991), and GRAIL-II (Y. Xu et al. (1994). In GRAIL-I, a neural network receives its inputs from seven statistical measures taken on a 99 base window. Performance is improved in GRAIL-II by the addition of variable length windows, neural nets trained to locate intron/exon boundaries, and a number of steps designed to evaluate candidate exons and eliminate improbable ones. Both versions of GRAIL predict coding regions in human DNA. A simulation of GRAIL-I was carried out with the goal of improving classification performance without resorting to the additional measures used in GRAIL-II. The intention was then to supplement the resulting module with modules based on physiochemical measures of DNA (such as melting profiles, twist and wedge angles) to enable precise exon prediction in plant sequences.},
	Author = {L. Roberts and N. Steele and C. Reeves and G. J. King},
	Booktitle = {1995 Fourth International Conference on Artificial Neural Networks},
	Date-Added = {2018-11-21 22:10:13 +1300},
	Date-Modified = {2018-11-21 22:10:22 +1300},
	Doi = {10.1049/cp:19950589},
	Issn = {0537-9989},
	Keywords = {NN; biology computing;DNA;genetics;neural nets;learning (artificial intelligence);proteins;neural network training;coding region identification;genomic DNA;nitrogenous bases;Markov chains;computer based prediction systems;exons;introns;GRAIL;GRAIL-I;GRAIL-II;statistical measures;variable length windows;intron/exon boundaries;human DNA;classification performance;physiochemical measures;precise exon prediction;plant sequences;Biomedical computing;DNA;Genetics;Neural networks;Learning systems;Proteins},
	Month = {June},
	Pages = {399-403},
	Title = {Training neural networks to identify coding regions in genomic DNA},
	Year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1049/cp:19950589}}

@article{Stanley:2002:ENN:638553.638554,
	Acmid = {638554},
	Address = {Cambridge, MA, USA},
	Author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	Date-Added = {2018-11-14 20:01:01 +1300},
	Date-Modified = {2018-11-14 20:01:09 +1300},
	Doi = {10.1162/106365602320169811},
	Issn = {1063-6560},
	Issue_Date = {Summer 2002},
	Journal = {Evol. Comput.},
	Keywords = {Evolutionary, competing conventions, genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
	Month = jun,
	Number = {2},
	Numpages = {29},
	Pages = {99--127},
	Publisher = {MIT Press},
	Title = {Evolving Neural Networks Through Augmenting Topologies},
	Url = {http://dx.doi.org/10.1162/106365602320169811},
	Volume = {10},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/106365602320169811}}

@article{2018arXiv180402464M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402464M},
	Archiveprefix = {arXiv},
	Author = {{Miconi}, T. and {Clune}, J. and {Stanley}, K.~O.},
	Date-Added = {2018-11-13 19:47:52 +1300},
	Date-Modified = {2018-11-13 19:48:02 +1300},
	Eprint = {1804.02464},
	Journal = {ArXiv e-prints},
	Keywords = {DNN, Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = apr,
	Title = {{Differentiable plasticity: training plastic neural networks with backpropagation}},
	Year = 2018}

@inproceedings{10.1007/3-540-61108-8_27,
	Abstract = {In this paper, evolution strategies (ESs) --- a class of evolutionary algorithms using normally distributed mutations, recombination, deterministic selection of the $\mu$>1 best offspring individuals, and the principle of self-adaptation for the collective on-line learning of strategy parameters --- are described by demonstrating their differences to genetic algorithms. By comparison of the algorithms, it is argued that the application of canonical genetic algorithms for continuous parameter optimization problems implies some difficulties caused by the encoding of continuous object variables by binary strings and the constant mutation rate used in genetic algorithms. Because they utilize a problem-adequate representation and a suitable self-adaptive step size control guaranteeing linear convergence for strictly convex problems, evolution strategies are argued to be more adequate for continuous problems.},
	Address = {Berlin, Heidelberg},
	Author = {B{\"a}ck, Thomas},
	Booktitle = {Artificial Evolution},
	Date-Added = {2018-11-13 16:18:55 +1300},
	Date-Modified = {2018-11-13 16:19:16 +1300},
	Editor = {Alliot, Jean-Marc and Lutton, Evelyne and Ronald, Edmund and Schoenauer, Marc and Snyers, Dominique},
	Isbn = {978-3-540-49948-0},
	Keywords = {Evolutionary, evolution strategies, Genetic Algorithm},
	Pages = {1--20},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Evolution strategies: An alternative evolutionary algorithm},
	Year = {1996}}

@inproceedings{4424711,
	Abstract = {Differential evolution has shown to be a very powerful, yet simple, population-based optimization approach. The nature of its reproduction operator limits its application to continuous-valued search spaces. However, a simple discretization procedure can be used to convert floating-point solution vectors into discrete-valued vectors. This paper considers three approaches in which differential evolution can be used to solve problems with binary-valued parameters. The first approach is based on a homomorphous mapping, while the second approach interprets the floating-point solution vector as a vector of probabilities, used to decide on the appropriate binary value. The third approach normalizes solution vectors and then discretize these normalized vectors to form a bitstring. Empirical results are provided to illustrate the efficiency of both methods in comparison with particle swarm optimizers.},
	Author = {A. P. Engelbrecht and G. Pampara},
	Booktitle = {2007 IEEE Congress on Evolutionary Computation},
	Date-Added = {2018-11-13 15:23:49 +1300},
	Date-Modified = {2018-11-13 15:24:01 +1300},
	Doi = {10.1109/CEC.2007.4424711},
	Issn = {1089-778X},
	Keywords = {Evolutionary; evolutionary computation;optimisation;binary differential evolution;population-based optimization;reproduction operator;continuous-valued search space;simple discretization procedure;floating-point solution vectors;discrete-valued vectors;binary-valued parameters;homomorphous mapping;normalized vectors;particle swarm optimizers;Optimization methods;Genetic mutations;Particle swarm optimization;Probability density function;Arithmetic;Biological cells;Difference equations;Differential equations;Stochastic processes;Discrete transforms},
	Month = {Sept},
	Pages = {1942-1947},
	Title = {Binary differential evolution strategies},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2007.4424711}}

@article{Wierstra:2014:NES:2627435.2638566,
	Acmid = {2638566},
	Author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J\"{u}rgen},
	Date-Added = {2018-11-13 15:10:29 +1300},
	Date-Modified = {2018-11-13 15:10:50 +1300},
	Issn = {1532-4435},
	Issue_Date = {January 2014},
	Journal = {J. Mach. Learn. Res.},
	Keywords = {Evolutionary, black-box optimization, evolution strategies, natural gradient, sampling, stochastic search},
	Month = jan,
	Number = {1},
	Numpages = {32},
	Pages = {949--980},
	Publisher = {JMLR.org},
	Title = {Natural Evolution Strategies},
	Url = {http://dl.acm.org/citation.cfm?id=2627435.2638566},
	Volume = {15},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2627435.2638566}}

@article{Sehnke:2010aa,
	Abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradient by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the parameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
	Author = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
	Booktitle = {The 18th International Conference on Artificial Neural Networks, ICANN 2008},
	Da = {2010/05/01/},
	Date-Added = {2018-11-13 15:07:22 +1300},
	Date-Modified = {2018-11-13 15:07:34 +1300},
	Doi = {https://doi.org/10.1016/j.neunet.2009.12.004},
	Isbn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Evolutionary, Policy gradients; Stochastic optimisation; Reinforcement learning; Robotics; Control},
	Number = {4},
	Pages = {551--559},
	Title = {Parameter-exploring policy gradients},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S0893608009003220},
	Volume = {23},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608009003220},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2009.12.004}}

@article{Williams1992,
	Abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	Author = {Williams, Ronald J.},
	Date-Added = {2018-11-13 15:01:38 +1300},
	Date-Modified = {2018-11-13 15:02:10 +1300},
	Day = {01},
	Doi = {10.1007/BF00992696},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Keywords = {Evolutionary, Gradient Methods, Reinforcement Learning, REINFORCE},
	Month = {May},
	Number = {3},
	Pages = {229--256},
	Title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	Url = {https://doi.org/10.1007/BF00992696},
	Volume = {8},
	Year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00992696}}

@inproceedings{Morse:2016:SEO:2908812.2908916,
	Acmid = {2908916},
	Address = {New York, NY, USA},
	Author = {Morse, Gregory and Stanley, Kenneth O.},
	Booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
	Date-Added = {2018-11-12 23:49:41 +1300},
	Date-Modified = {2018-11-12 23:49:51 +1300},
	Doi = {10.1145/2908812.2908916},
	Isbn = {978-1-4503-4206-3},
	Keywords = {Evolutionary, artificial intelligence, deep learning, machine learning, neural networks, pattern recognition and classification},
	Location = {Denver, Colorado, USA},
	Numpages = {8},
	Pages = {477--484},
	Publisher = {ACM},
	Series = {GECCO '16},
	Title = {Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks},
	Url = {http://doi.acm.org/10.1145/2908812.2908916},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2908812.2908916},
	Bdsk-Url-2 = {https://doi.org/10.1145/2908812.2908916}}

@article{2018arXiv180703247L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703247L},
	Archiveprefix = {arXiv},
	Author = {{Liu}, R. and {Lehman}, J. and {Molino}, P. and {Petroski Such}, F. and {Frank}, E. and {Sergeev}, A. and {Yosinski}, J.},
	Date-Added = {2018-11-12 20:13:43 +1300},
	Date-Modified = {2018-11-12 20:14:06 +1300},
	Eprint = {1807.03247},
	Journal = {ArXiv e-prints},
	Keywords = {CNN, Computer Vision, Pattern Recognition, Machine Learning, Statistics, Machine Learning},
	Month = jul,
	Primaryclass = {cs.CV},
	Title = {{An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution}},
	Year = 2018}

@article{2017arXiv171206564Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171206564Z},
	Archiveprefix = {arXiv},
	Author = {{Zhang}, X. and {Clune}, J. and {Stanley}, K.~O.},
	Date-Added = {2018-11-12 19:47:24 +1300},
	Date-Modified = {2018-11-12 19:47:36 +1300},
	Eprint = {1712.06564},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary, Neural and Evolutionary Computing},
	Month = dec,
	Title = {{On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent}},
	Year = 2017}

@article{Assuncao2018,
	Abstract = {Deep evolutionary network structured representation (DENSER) is a novel evolutionary approach for the automatic generation of deep neural networks (DNNs) which combines the principles of genetic algorithms (GAs) with those of dynamic structured grammatical evolution (DSGE). The GA-level encodes the macro structure of evolution, i.e., the layers, learning, and/or data augmentation methods (among others); the DSGE-level specifies the parameters of each GA evolutionary unit and the valid range of the parameters. The use of a grammar makes DENSER a general purpose framework for generating DNNs: one just needs to adapt the grammar to be able to deal with different network and layer types, problems, or even to change the range of the parameters. DENSER is tested on the automatic generation of convolutional neural networks (CNNs) for the CIFAR-10 dataset, with the best performing networks reaching accuracies of up to 95.22{\%}. Furthermore, we take the fittest networks evolved on the CIFAR-10, and apply them to classify MNIST, Fashion-MNIST, SVHN, Rectangles, and CIFAR-100. The results show that the DNNs discovered by DENSER during evolution generalise, are robust, and scale. The most impressive result is the 78.75{\%} classification accuracy on the CIFAR-100 dataset, which, to the best of our knowledge, sets a new state-of-the-art on methods that seek to automatically design CNNs.},
	Author = {Assun{\c{c}}ao, Filipe and Louren{\c{c}}o, Nuno and Machado, Penousal and Ribeiro, Bernardete},
	Date-Added = {2018-11-12 17:44:52 +1300},
	Date-Modified = {2018-11-12 17:45:26 +1300},
	Day = {27},
	Doi = {10.1007/s10710-018-9339-y},
	Issn = {1573-7632},
	Journal = {Genetic Programming and Evolvable Machines},
	Keywords = {Evolutionary, Deep Neural Network, Neuroevolution},
	Month = {Sep},
	Title = {DENSER: deep evolutionary network structured representation},
	Url = {https://doi.org/10.1007/s10710-018-9339-y},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-018-9339-y}}

@article{Mirjalili_2012,
	Author = {SeyedAli Mirjalili and Siti Zaiton Mohd Hashim},
	Date-Added = {2018-11-09 11:12:56 +1300},
	Date-Modified = {2018-11-09 11:13:25 +1300},
	Doi = {10.7763/ijmlc.2012.v2.114},
	Journal = {International Journal of Machine Learning and Computing},
	Keywords = {Other, Learning algorithm, Physics},
	Pages = {204--208},
	Publisher = {{EJournal} Publishing},
	Title = {{BMOA}: Binary Magnetic Optimization Algorithm},
	Url = {https://doi.org/10.7763%2Fijmlc.2012.v2.114},
	Year = 2012,
	Bdsk-Url-1 = {https://doi.org/10.7763%2Fijmlc.2012.v2.114},
	Bdsk-Url-2 = {https://doi.org/10.7763/ijmlc.2012.v2.114}}

@inproceedings{8056823,
	Abstract = {Convolutional neural networks (CNNs) are deployed in a wide range of image recognition, scene segmentation and object detection applications. Achieving state of the art accuracy in CNNs often results in large models and complex topologies that require significant compute resources to complete in a timely manner. Binarised neural networks (BNNs) have been proposed as an optimised variant of CNNs, which constrain the weights and activations to +1 or -1 and thus offer compact models and lower computational complexity per operation. This paper presents a high performance BNN accelerator on the Intel{\textregistered}Xeon+FPGA{\texttrademark} platform. The proposed accelerator is designed to take advantage of the Xeon+FPGA system in a way that a specialised FPGA architecture can be targeted for the most compute intensive parts of the BNN whilst other parts of the topology can be handled by the Xeon{\texttrademark} CPU. The implementation is evaluated by comparing the raw compute performance and energy efficiency for key layers in standard CNN topologies against an Nvidia Titan X Pascal GPU and other published FPGA BNN accelerators. The results show that our single-package integrated Arria{\texttrademark} 10 FPGA accelerator coupled with a high-end Xeon CPU can offer comparable performance and better energy efficiency than a high-end discrete Titan X GPU card. In addition, our solution delivers the best performance compared to previous BNN FPGA implementations.},
	Author = {D. J. M. Moss and E. Nurvitadhi and J. Sim and A. Mishra and D. Marr and S. Subhaschandra and P. H. W. Leong},
	Booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-11-05 22:28:08 +1300},
	Date-Modified = {2018-12-06 14:59:09 +1300},
	Doi = {10.23919/FPL.2017.8056823},
	Issn = {1946-1488},
	Keywords = {BNN, CNN, FPGA, computational complexity;field programmable gate arrays;graphics processing units;image recognition;neural nets;object detection;high performance binary neural networks;convolutional neural networks;CNN;Xeon CPU;Intel Xeon+FPGA platform;FPGA BNN accelerators;Nvidia Titan X Pascal GPU;specialised FPGA architecture;Xeon+FPGA system;high performance BNN accelerator;lower computational complexity;complex topologies;object detection applications;scene segmentation;image recognition;Field programmable gate arrays;Graphics processing units;Topology;Computer architecture;Performance evaluation;Network topology;IP networks},
	Month = {Sept},
	Pages = {1-4},
	Title = {High performance binary neural networks on the Xeon-FPGA platform},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/FPL.2017.8056823}}

@article{2017arXiv170402081J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170402081J},
	Archiveprefix = {arXiv},
	Author = {{Javad Shafiee}, M. and {Barshan}, E. and {Wong}, A.},
	Date-Added = {2018-10-29 11:34:29 +0000},
	Date-Modified = {2018-10-29 11:34:29 +0000},
	Eprint = {1704.02081},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = apr,
	Title = {{Evolution in Groups: A deeper look at synaptic cluster driven evolution of deep neural networks}},
	Year = 2017}

@article{2018arXiv180905989W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180905989W},
	Archiveprefix = {arXiv},
	Author = {{Wong}, A. and {Javad Shafiee}, M. and {Chwyl}, B. and {Li}, F.},
	Date-Added = {2018-10-29 06:56:37 +0000},
	Date-Modified = {2018-10-29 06:57:00 +0000},
	Eprint = {1809.05989},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Neural and Evolutionary Computing; Artificial Intelligence, Computer Vision; Pattern Recognition},
	Month = sep,
	Title = {{FermiNets: Learning generative machines to generate efficient neural networks via generative synthesis}},
	Year = 2018}

@article{2016arXiv160604393J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604393J},
	Archiveprefix = {arXiv},
	Author = {{Javad Shafiee}, M. and {Mishra}, A. and {Wong}, A.},
	Date-Added = {2018-10-29 06:49:10 +0000},
	Date-Modified = {2018-10-29 06:49:40 +0000},
	Eprint = {1606.04393},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Computer Vision; Pattern Recognition; Machine Learning; Neural and Evolutionary Computing; Statistics; Machine Learning},
	Month = jun,
	Primaryclass = {cs.CV},
	Title = {{Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks}},
	Year = 2016}

@article{2015arXiv151108228K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151108228K},
	Archiveprefix = {arXiv},
	Author = {{Kaiser}, {\L}. and {Sutskever}, I.},
	Date-Added = {2018-10-28 08:28:38 +0000},
	Date-Modified = {2018-10-28 08:28:51 +0000},
	Eprint = {1511.08228},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Machine Learning, Neural and Evolutionary Computing},
	Month = nov,
	Title = {{Neural GPUs Learn Algorithms}},
	Year = 2015}

@article{2015arXiv150205477S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205477S},
	Archiveprefix = {arXiv},
	Author = {{Schulman}, J. and {Levine}, S. and {Moritz}, P. and {Jordan}, M.~I. and {Abbeel}, P.},
	Date-Added = {2018-10-26 09:31:15 +0000},
	Date-Modified = {2018-10-26 09:31:24 +0000},
	Eprint = {1502.05477},
	Journal = {ArXiv e-prints},
	Keywords = {DNN; Machine Learning},
	Month = feb,
	Title = {{Trust Region Policy Optimization}},
	Year = 2015}

@article{2017arXiv170301041R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170301041R},
	Archiveprefix = {arXiv},
	Author = {{Real}, E. and {Moore}, S. and {Selle}, A. and {Saxena}, S. and {Suematsu}, Y.~L. and {Tan}, J. and {Le}, Q. and {Kurakin}, A.},
	Date-Added = {2018-10-26 09:15:55 +0000},
	Date-Modified = {2018-10-26 09:16:25 +0000},
	Eprint = {1703.01041},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Neural and Evolutionary Computing; Artificial Intelligence; Computer Vision; Pattern Recognition; Distributed, Parallel, and Cluster Computing},
	Month = mar,
	Title = {{Large-Scale Evolution of Image Classifiers}},
	Year = 2017}

@article{2016arXiv160909106H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160909106H},
	Archiveprefix = {arXiv},
	Author = {{Ha}, D. and {Dai}, A. and {Le}, Q.~V.},
	Date-Added = {2018-10-26 09:08:27 +0000},
	Date-Modified = {2018-10-26 09:08:35 +0000},
	Eprint = {1609.09106},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Machine Learning},
	Month = sep,
	Title = {{HyperNetworks}},
	Year = 2016}

@inproceedings{7780459,
	Abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8; deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	Author = {K. He and X. Zhang and S. Ren and J. Sun},
	Booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-10-25 23:41:31 +0000},
	Date-Modified = {2018-12-01 13:39:34 +1300},
	Doi = {10.1109/CVPR.2016.90},
	Issn = {1063-6919},
	Keywords = {CNN, image classification; AI; neural nets;object detection, RESNET},
	Month = {June},
	Pages = {770-778},
	Title = {Deep Residual Learning for Image Recognition},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2016.90}}

@article{726791,
	Abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	Author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
	Date-Added = {2018-10-25 01:06:25 +0000},
	Date-Modified = {2018-10-25 01:06:33 +0000},
	Doi = {10.1109/5.726791},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Keywords = {NN; optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
	Month = {Nov},
	Number = {11},
	Pages = {2278-2324},
	Title = {Gradient-based learning applied to document recognition},
	Volume = {86},
	Year = {1998},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBVLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0hhbW1pbmcgQ2x1c3RlcmluZy0gQSBOZXcgQXBwcm9hY2ggdG8gUnVsZSBFeHRyYWN0aW9uLmJpYk8RAhwAAAAAAhwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IYW1taW5nIENsdXN0ZXJpbmcjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADQk5OAAACAGsvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Qk5OOkhhbW1pbmcgQ2x1c3RlcmluZy0gQSBOZXcgQXBwcm9hY2ggdG8gUnVsZSBFeHRyYWN0aW9uLmJpYgAADgB0ADkASABhAG0AbQBpAG4AZwAgAEMAbAB1AHMAdABlAHIAaQBuAGcALQAgAEEAIABOAGUAdwAgAEEAcABwAHIAbwBhAGMAaAAgAHQAbwAgAFIAdQBsAGUAIABFAHgAdAByAGEAYwB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBpVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9CTk4vSGFtbWluZyBDbHVzdGVyaW5nLSBBIE5ldyBBcHByb2FjaCB0byBSdWxlIEV4dHJhY3Rpb24uYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAB8AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAApw=},
	Bdsk-Url-1 = {https://doi.org/10.1109/5.726791}}

@inproceedings{7966159,
	Abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
	Author = {X. Chen and X. Hu and H. Zhou and N. Xu},
	Booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
	Date-Added = {2018-10-24 07:44:36 +0000},
	Date-Modified = {2018-10-24 07:44:36 +0000},
	Doi = {10.1109/IJCNN.2017.7966159},
	Issn = {2161-4407},
	Keywords = {application specific integrated circuits;convolution;field programmable gate arrays;fixed point arithmetic;floating point arithmetic;neural nets;FxpNet;deep convolutional neural network;fixed-point representation;bit-width arithmetics;forward pass;backward pass;floating-point values;binarized neural networks;quantized neural networks;fixed-point primal weights;low resolution fixed-point values;fixed-point primal parameters;FPGAs;ASICs;integer batch normalization;IBN;fixed-point ADAM;FxpADAM;CIFAR-10 dataset;12-bit primal parameters;12-bit gradients;Quantization (signal);Training;Field programmable gate arrays;Neural networks;Convolution;Kernel;Acceleration},
	Month = {May},
	Pages = {2494-2501},
	Title = {FxpNet: Training a deep convolutional neural network in fixed-point representation},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2017.7966159}}

@inproceedings{Nakahara:2018:LYB:3174243.3174266,
	Acmid = {3174266},
	Address = {New York, NY, USA},
	Author = {Nakahara, Hiroki and Yonekawa, Haruyoshi and Fujii, Tomoya and Sato, Shimpei},
	Booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-10-22 09:26:07 +0000},
	Date-Modified = {2018-10-22 09:26:17 +0000},
	Doi = {10.1145/3174243.3174266},
	Isbn = {978-1-4503-5614-5},
	Keywords = {BNN; binarized deep neural network, convolutional deep neural network, object detection},
	Location = {Monterey, CALIFORNIA, USA},
	Numpages = {10},
	Pages = {31--40},
	Publisher = {ACM},
	Series = {FPGA '18},
	Title = {A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA},
	Url = {http://doi.acm.org/10.1145/3174243.3174266},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3174243.3174266},
	Bdsk-Url-2 = {https://doi.org/10.1145/3174243.3174266}}

@inproceedings{Guo2018FBNAAF,
	Author = {P. Guo and H. Ma and Ruizhi Chen and Pin Li and Shaolin Xie and Donglin Wang},
	Booktitle = {FPL 2018},
	Date-Added = {2018-10-22 09:01:32 +0000},
	Date-Modified = {2018-10-22 09:11:30 +0000},
	Keywords = {BNN, Neural Network, FPGA, Accelerator},
	Organization = {Trinity College, Dublin},
	Title = {FBNA: A Fully Binarized Neural Network Accelerator},
	Volume = {1},
	Year = {2018}}

@inproceedings{Yang:2018:FOB:3218603.3218615,
	Acmid = {3218615},
	Address = {New York, NY, USA},
	Articleno = {50},
	Author = {Yang, Li and He, Zhezhi and Fan, Deliang},
	Booktitle = {Proceedings of the International Symposium on Low Power Electronics and Design},
	Date-Added = {2018-10-22 05:09:45 +0000},
	Date-Modified = {2018-10-22 05:09:59 +0000},
	Doi = {10.1145/3218603.3218615},
	Isbn = {978-1-4503-5704-3},
	Keywords = {BNN; Binarized convolutional neural network (BNN), Convolutional neural network (CNN), field-programmable gate array (FPGA)},
	Location = {Seattle, WA, USA},
	Numpages = {6},
	Pages = {50:1--50:6},
	Publisher = {ACM},
	Series = {ISLPED '18},
	Title = {A Fully Onchip Binarized Convolutional Neural Network FPGA Impelmentation with Accurate Inference},
	Url = {http://doi.acm.org/10.1145/3218603.3218615},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3218603.3218615},
	Bdsk-Url-2 = {https://doi.org/10.1145/3218603.3218615}}

@article{2018arXiv180903368P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180903368P},
	Archiveprefix = {arXiv},
	Author = {{Peters}, J.~W.~T. and {Welling}, M.},
	Date-Added = {2018-10-22 01:43:56 +0000},
	Date-Modified = {2018-10-22 01:44:09 +0000},
	Eprint = {1809.03368},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning; Statistics},
	Month = sep,
	Title = {{Probabilistic Binary Neural Networks}},
	Year = 2018}

@article{2018arXiv180909244B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180909244B},
	Archiveprefix = {arXiv},
	Author = {{Baluja}, S. and {Marwood}, D. and {Covell}, M. and {Johnston}, N.},
	Date-Added = {2018-10-22 01:40:27 +0000},
	Date-Modified = {2018-10-22 01:40:43 +0000},
	Eprint = {1809.09244},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning; Statistics},
	Month = sep,
	Title = {{No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference}},
	Year = 2018}

@article{2018arXiv181002068F,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181002068F},
	Archiveprefix = {arXiv},
	Author = {{Fu}, C. and {Zhu}, S. and {Su}, H. and {Lee}, C.-E. and {Zhao}, J.},
	Date-Added = {2018-10-22 01:15:11 +0000},
	Date-Modified = {2018-10-22 01:15:49 +0000},
	Eprint = {1810.02068},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning; Artificial Intelligence; Hardware Architecture; Computer Vision; Pattern Recognition; Statistics},
	Month = oct,
	Title = {{Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA}},
	Year = 2018}

@article{2017arXiv170302660R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170302660R},
	Archiveprefix = {arXiv},
	Author = {{Rajeswaran}, A. and {Lowrey}, K. and {Todorov}, E. and {Kakade}, S.},
	Date-Added = {2018-10-19 10:46:17 +0000},
	Date-Modified = {2018-10-19 10:46:39 +0000},
	Eprint = {1703.02660},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary, Machine Learning, Artificial Intelligence, Robotics, Systems and Control},
	Month = mar,
	Title = {{Towards Generalization and Simplicity in Continuous Control}},
	Year = 2017}

@article{2017arXiv171206567P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171206567P},
	Archiveprefix = {arXiv},
	Author = {{Petroski Such}, F. and {Madhavan}, V. and {Conti}, E. and {Lehman}, J. and {Stanley}, K.~O. and {Clune}, J.},
	Date-Added = {2018-10-19 10:41:55 +0000},
	Date-Modified = {2018-10-19 10:42:15 +0000},
	Eprint = {1712.06567},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary, Neural and Evolutionary Computing, Machine Learning},
	Month = dec,
	Title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
	Year = 2017}

@article{2018arXiv180307055M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180307055M},
	Archiveprefix = {arXiv},
	Author = {{Mania}, H. and {Guy}, A. and {Recht}, B.},
	Date-Added = {2018-10-19 10:39:53 +0000},
	Date-Modified = {2018-10-19 10:40:20 +0000},
	Eprint = {1803.07055},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Machine Learning, Artificial Intelligence, Mathematics, Optimization and Control, Statistics, Machine Learning},
	Month = mar,
	Title = {{Simple random search provides a competitive approach to reinforcement learning}},
	Year = 2018}

@article{2017arXiv171003748B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171003748B},
	Archiveprefix = {arXiv},
	Author = {{Bansal}, T. and {Pachocki}, J. and {Sidor}, S. and {Sutskever}, I. and {Mordatch}, I.},
	Date-Added = {2018-10-19 10:33:38 +0000},
	Date-Modified = {2018-10-19 10:34:00 +0000},
	Eprint = {1710.03748},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Artificial Intelligence},
	Month = oct,
	Primaryclass = {cs.AI},
	Title = {{Emergent Complexity via Multi-Agent Competition}},
	Year = 2017}

@inproceedings{pmlr-v37-schaul15,
	Abstract = {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
	Address = {Lille, France},
	Author = {Tom Schaul and Daniel Horgan and Karol Gregor and David Silver},
	Booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	Date-Added = {2018-10-19 10:22:36 +0000},
	Date-Modified = {2018-10-19 10:23:02 +0000},
	Editor = {Francis Bach and David Blei},
	Keywords = {Evolutionary, Google, DeepMind, Neural Network},
	Month = {07--09 Jul},
	Pages = {1312--1320},
	Pdf = {http://proceedings.mlr.press/v37/schaul15.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Universal Value Function Approximators},
	Url = {http://proceedings.mlr.press/v37/schaul15.html},
	Volume = {37},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v37/schaul15.html}}

@article{2018arXiv180403867P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403867P},
	Archiveprefix = {arXiv},
	Author = {{Prabhu}, A. and {Batchu}, V. and {Gajawada}, R. and {Aurobindo Munagala}, S. and {Namboodiri}, A.},
	Date-Added = {2018-10-13 07:48:52 +0000},
	Date-Modified = {2018-10-13 07:49:08 +0000},
	Eprint = {1804.03867},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Vision; Pattern Recognition},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory}},
	Year = 2018}

@inproceedings{bmxnet,
	Acmid = {3129393},
	Address = {New York, NY, USA},
	Annote = {https://arxiv.org/abs/1705.09864
https://github.com/hpi-xnor/BMXNet
},
	Author = {Yang, Haojin and Fritzsche, Martin and Bartz, Christian and Meinel, Christoph},
	Booktitle = {Proceedings of the 2017 ACM on Multimedia Conference},
	Date-Added = {2018-10-13 07:42:15 +0000},
	Date-Modified = {2018-10-13 07:45:31 +0000},
	Doi = {10.1145/3123266.3129393},
	Isbn = {978-1-4503-4906-2},
	Keywords = {BNN; binary neural networks, computer vision, machine learning, open source},
	Location = {Mountain View, California, USA},
	Numpages = {4},
	Pages = {1209--1212},
	Publisher = {ACM},
	Series = {MM '17},
	Title = {BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet},
	Url = {http://doi.acm.org/10.1145/3123266.3129393},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3123266.3129393},
	Bdsk-Url-2 = {https://doi.org/10.1145/3123266.3129393}}

@inproceedings{8429420,
	Abstract = {Deep neural networks have achieved impressive results in computer vision and machine learning. Unfortunately, state-of-the-art networks are extremely compute-and memory-intensive which makes them unsuitable for mW-devices such as IoT end-nodes. Aggressive quantization of these networks dramatically reduces the computation and memory footprint. Binary-weight neural networks (BWNs) follow this trend, pushing weight quantization to the limit. Hardware accelerators for BWNs presented up to now have focused on core efficiency, disregarding I/O bandwidth and system-level efficiency that are crucial for deployment of accelerators in ultra-low power devices. We present Hyperdrive: a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel binary-weight streaming approach, and capable of handling high-resolution images by virtue of its systolic-scalable architecture. We achieve a 5.9 TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than state-of-the-art BNN accelerators, even if our core uses resource-intensive FP16 arithmetic for increased robustness.},
	Author = {R. Andri and L. Cavigelli and D. Rossi and L. Benini},
	Booktitle = {2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},
	Date-Added = {2018-10-12 06:40:47 +0000},
	Date-Modified = {2018-10-12 06:40:54 +0000},
	Doi = {10.1109/ISVLSI.2018.00099},
	Issn = {2159-3477},
	Keywords = {BNN; computer vision;feedforward neural nets;Internet of Things;learning (artificial intelligence);low-power electronics;microprocessor chips;hardware accelerators;core efficiency;I/O bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable architecture;state-of-the-art BNN accelerators;resource-intensive FP16 arithmetic;TOp/s/W system-level efficiency;binary-weight streaming approach;BWN;hyperdrive;weight quantization;binary-weight neural networks;memory footprint;aggressive quantization;mW-devices;memory-intensive;machine learning;computer vision;impressive results;deep neural networks;mW IoT end-nodes;systolically scalable binary-weight CNN inference engine;Frequency modulation;Computer architecture;Quantization (signal);Neural networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary Weights Neural Networks;IoT},
	Month = {July},
	Pages = {509-515},
	Title = {Hyperdrive: A Systolically Scalable Binary-Weight CNN Inference Engine for mW IoT End-Nodes},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISVLSI.2018.00099}}

@inproceedings{8457633,
	Abstract = {This paper proposes ReBNet, an end-to-end framework for training reconfigurable binary neural networks on software and developing efficient accelerators for execution on FPGA. Binary neural networks offer an intriguing opportunity for deploying large-scale deep learning models on resource-constrained devices. Binarization reduces the memory footprint and replaces the power-hungry matrix-multiplication with light-weight XnorPopcount operations. However, binary networks suffer from a degraded accuracy compared to their fixed-point counterparts. We show that the state-of-the-art methods for optimizing binary networks accuracy, significantly increase the implementation cost and complexity. To compensate for the degraded accuracy while adhering to the simplicity of binary networks, we devise the first reconfigurable scheme that can adjust the classification accuracy based on the application. Our proposition improves the classification accuracy by representing features with multiple levels of residual binarization. Unlike previous methods, our approach does not exacerbate the area cost of the hardware accelerator. Instead, it provides a tradeoff between throughput and accuracy while the area overhead of multi-level binarization is negligible.},
	Author = {M. Ghasemzadeh and M. Samragh and F. Koushanfar},
	Booktitle = {2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	Date-Added = {2018-10-12 06:35:49 +0000},
	Date-Modified = {2018-10-12 06:36:55 +0000},
	Doi = {10.1109/FCCM.2018.00018},
	Issn = {2576-2621},
	Keywords = {BNN; field programmable gate arrays;learning (artificial intelligence);matrix multiplication;neural nets;ReBNet;residual binarized neural network;large-scale deep learning models;power-hungry matrix-multiplication;light-weight XnorPopcount operations;fixed-point counterparts;FPGA;memory footprint;hardware accelerator;Hardware;Neural networks;Training;Field programmable gate arrays;Parallel processing;Cost function;Libraries;Deep neural networks;Reconfigurable computing;Domain customized computing;Binary neural network;Residual binarization},
	Month = {April},
	Pages = {57-64},
	Title = {ReBNet: Residual Binarized Neural Network},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FCCM.2018.00018}}

@inproceedings{8461456,
	Abstract = {With the ever growing popularity of deep learning, the tremendous complexity of deep neural networks is becoming problematic when one considers inference on resource constrained platforms. Binary networks have emerged as a potential solution, however, they exhibit a fundamentallimi-tation in realizing gradient-based learning as their activations are non-differentiable. Current work has so far relied on approximating gradients in order to use the back-propagation algorithm via the straight through estimator (STE). Such approximations harm the quality of the training procedure causing a noticeable gap in accuracy between binary neural networks and their full precision baselines. We present a novel method to train binary activated neural networks using true gradient-based learning. Our idea is motivated by the similarities between clipping and binary activation functions. We show that our method has minimal accuracy degradation with respect to the full precision baseline. Finally, we test our method on three benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show that continuous binarization using true gradient-based learning achieves an accuracy within 1.5% of the floating-point baseline, as compared to accuracy drops as high as 6% when training the same binary activated network using the STE.},
	Author = {C. Sakr and J. Choi and Z. Wang and K. Gopalakrishnan and N. Shanbhag},
	Booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	Date-Added = {2018-10-12 06:28:18 +0000},
	Date-Modified = {2018-10-12 06:28:24 +0000},
	Doi = {10.1109/ICASSP.2018.8461456},
	Issn = {2379-190X},
	Keywords = {BNN; gradient methods;learning (artificial intelligence);neural nets;gradient-based training;deep binary activated neural networks;continuous binarization;deep learning;tremendous complexity;resource constrained platforms;training procedure;binary activation functions;minimal accuracy degradation;gradient-based learning;back-propagation algorithm;straight through estimator;floating-point baseline;STE;Training;Neural networks;Complexity theory;Machine learning;Stochastic processes;Perturbation methods;Approximation algorithms;deep learning;binary neural networks;activation functions},
	Month = {April},
	Pages = {2346-2350},
	Title = {True Gradient-Based Training of Deep Binary Activated Neural Networks Via Continuous Binarization},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICASSP.2018.8461456}}

@inproceedings{8373076,
	Abstract = {Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8-32x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations. We present XNORBIN, a flexible accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse supporting even non-trivial network topologies with large feature map volumes. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.},
	Author = {A. A. Bahou and G. Karunaratne and R. Andri and L. Cavigelli and L. Benini},
	Booktitle = {2018 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS)},
	Date-Added = {2018-10-12 06:22:47 +0000},
	Date-Modified = {2018-10-12 06:22:55 +0000},
	Doi = {10.1109/CoolChips.2018.8373076},
	Issn = {2473-4683},
	Keywords = {BNN; embedded systems;low-power electronics;neural nets;power aware computing;binary convolutional neural networks;off-chip memory;low-power embedded systems;extreme quantization;flexible accelerator;aggressive data;nontrivial network topologies;feature map volumes;energy efficiency;hardware accelerator;binary CNN;weight binarization;collapsing energy-intensive sum-of-products;XNOR-and-popcount operations;Hardware;Convolutional neural networks;System-on-chip;Computational modeling;Computer architecture;Program processors},
	Month = {April},
	Pages = {1-3},
	Title = {XNORBIN: A 95 TOp/s/W hardware accelerator for binary convolutional neural networks},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CoolChips.2018.8373076}}

@article{8103902,
	Abstract = {Binary weight convolutional neural networks (BCNNs) can achieve near state-of-the-art classification accuracy and have far less computation complexity compared with traditional CNNs using high-precision weights. Due to their binary weights, BCNNs are well suited for vision-based Internet-of-Things systems being sensitive to power consumption. BCNNs make it possible to achieve very high throughput with moderate power dissipation. In this paper, an energy-efficient architecture for BCNNs is proposed. It fully exploits the binary weights and other hardware-friendly characteristics of BCNNs. A judicious processing schedule is proposed so that off-chip I/O access is minimized and activations are maximally reused. To significantly reduce the critical path delay, we introduce optimized compressor trees and approximate binary multipliers with two novel compensation schemes. The latter is able to save significant hardware resource, and almost no computation accuracy is compromised. Taking advantage of error resiliency of BCNNs, an innovative approximate adder is developed, which significantly reduces the silicon area and data path delay. Thorough error analysis and extensive experimental results on several data sets show that the approximate adders in the data path cause negligible accuracy loss. Moreover, algorithmic transformations for certain layers of BCNNs and a memory-efficient quantization scheme are incorporated to further reduce the energy cost and on-chip storage requirement. Finally, the proposed BCNN hardware architecture is implemented with the SMIC 130-nm technology. The postlayout results demonstrate that our design can achieve an energy efficiency over 2.0TOp/s/W when scaled to 65 nm, which is more than two times better than the prior art.},
	Author = {Y. Wang and J. Lin and Z. Wang},
	Date-Added = {2018-10-12 06:20:05 +0000},
	Date-Modified = {2018-10-12 06:20:11 +0000},
	Doi = {10.1109/TVLSI.2017.2767624},
	Issn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {BNN;adders;convolution;energy conservation;error analysis;feedforward neural nets;multiplying circuits;neural chips;trees (mathematics);energy-efficient architecture;binary weight convolutional neural networks;high-precision weights;binary weights;approximate binary multipliers;BCNN hardware architecture;energy efficiency;classification accuracy;data path delay;processing schedule;off-chip I/O access;critical path delay;optimized compressor trees;approximate adder;error analysis;memory-efficient quantization;on-chip storage requirement;size 65.0 nm;Computer architecture;Hardware;Neural networks;Neurons;Adders;Quantization (signal);Convolution;Approximate computing;binary weight convolutional neural network (BCNN) architecture;convolutional neural network (CNN);deep learning;energy-efficient design;signal processing;VLSI architecture},
	Month = {Feb},
	Number = {2},
	Pages = {280-293},
	Title = {An Energy-Efficient Architecture for Binary Weight Convolutional Neural Networks},
	Volume = {26},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TVLSI.2017.2767624}}

@inproceedings{8425178,
	Abstract = {Deep learning has revolutionized computer vision and other fields since its big bang in 2012. However, it is challenging to deploy Deep Neural Networks (DNNs) into real-world applications due to their high computational complexity. Binary Neural Networks (BNNs) dramatically reduce computational complexity by replacing most arithmetic operations with bitwise operations. Existing implementations of BNNs have been focusing on GPU or FPGA, and using the conventional image-to-column method that doesn't perform well for binary convolution due to low arithmetic intensity and unfriendly pattern for bitwise operations. We propose BitFlow, a gemm-operator-network three-level optimization framework for fully exploiting the computing power of BNNs on CPU. BitFlow features a new class of algorithm named PressedConv for efficient binary convolution using locality-aware layout and vector parallelism. We evaluate BitFlow with the VGG network. On a single core of Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN implementations, and 11.5x speedup over counterpart full-precision DNNs. Over 64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart full-precision DNNs on GPU (GTX 1080).},
	Author = {Y. Hu and J. Zhai and D. Li and Y. Gong and Y. Zhu and W. Liu and L. Su and J. Jin},
	Booktitle = {2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	Date-Added = {2018-10-12 06:15:46 +0000},
	Date-Modified = {2018-10-12 06:16:05 +0000},
	Doi = {10.1109/IPDPS.2018.00034},
	Issn = {1530-2075},
	Keywords = {BNN;computational complexity;field programmable gate arrays;graphics processing units;learning (artificial intelligence);multiprocessing systems;neural nets;optimisation;parallel processing;vector parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big bang;Deep Neural Networks;high computational complexity;Binary Neural Networks;BNNs;arithmetic operations;bitwise operations;image-to-column method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow features;efficient binary convolution;VGG network;counterpart full-precision DNNs;GPU;Convolution;Neural networks;Layout;Parallel processing;Acceleration;Graphics processing units;Machine learning;Network Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi},
	Month = {May},
	Pages = {244-253},
	Title = {BitFlow: Exploiting Vector Parallelism for Binary Neural Networks on CPU},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPDPS.2018.00034}}

@article{8226999,
	Abstract = {A versatile reconfigurable accelerator architecture for binary/ternary deep neural networks is presented. In-memory neural network processing without any external data accesses, sustained by the symmetry and simplicity of the computation of the binary/ternaty neural network, improves the energy efficiency dramatically. The prototype chip is fabricated, and it achieves 1.4 TOPS (tera operations per second) peak performance with 0.6-W power consumption at 400-MHz clock. The application examination is also conducted.},
	Author = {K. Ando and K. Ueyoshi and K. Orimo and H. Yonekawa and S. Sato and H. Nakahara and S. Takamaeda-Yamazaki and M. Ikebe and T. Asai and T. Kuroda and M. Motomura},
	Date-Added = {2018-10-12 06:15:46 +0000},
	Date-Modified = {2018-10-12 06:15:58 +0000},
	Doi = {10.1109/JSSC.2017.2778702},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {BNN;low-power electronics;neural nets;random-access storage;reconfigurable architectures;deep neural network accelerator;binary/ternary deep neural networks;In-memory neural network processing;binary/ternaty neural network;BRein memory;single-chip binary/ternary reconfigurable in-memory;reconfigurable accelerator architecture;external data access;power 0.6 W;frequency 400 MHz;Biological neural networks;Random access memory;Memory management;Neurons;System-on-chip;Parallel processing;Binary neural networks;in-memory processing;near-memory processing;neural networks;reconfigurable array;ternary neural networks},
	Month = {April},
	Number = {4},
	Pages = {983-994},
	Title = {BRein Memory: A Single-Chip Binary/Ternary Reconfigurable in-Memory Deep Neural Network Accelerator Achieving 1.4 TOPS at 0.6 W},
	Volume = {53},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2017.2778702}}

@inproceedings{8394726,
	Abstract = {To achieve an advanced Internet of Things (IoT), it is necessary to combine artificial intelligence (AI) with IoT. Compact circuits that can operate AI functions will be useful for this purpose. Therefore, we propose stochastic weights binary neural networks (SWBNN). SWBNNs are more accurate than binary neural networks (BNN) with small circuits. BNNs can be realized with small circuits since binary calculation needs simpler circuits than real number calculation. However, BNNs have lower accuracy than networks with real numbers. Thus, the proposed SWBNNs are BNNs that behave stochastically, which makes them more accurate than BNNs. Moreover, SWBNNs can still be achieved with small circuits since they execute binary calculation. As a result, the accuracy for the test data of SWBNNs is closer to the accuracy for learning data than the accuracy for the test data of BNNs is. Especially when using the CIFAR10 database, the difference in the identification accuracy rate between learning data and test data decreased from 6% for BNNs to 2% for SWBNNs. From results of a field-programmable gate array (FPGA) implementation, circuits of SWBNNs are sufficiently small although they are 10% bigger than those of BNNs. Therefore, SWBNNs are more accurate than BNNs, and the circuit costs ofintroducing stochastic weights are low.},
	Author = {Y. Fukuda and T. Kawahara},
	Booktitle = {2018 7th International Symposium on Next Generation Electronics (ISNE)},
	Date-Added = {2018-10-12 06:02:12 +0000},
	Date-Modified = {2018-10-12 06:02:21 +0000},
	Doi = {10.1109/ISNE.2018.8394726},
	Issn = {2378-8607},
	Keywords = {BNN;field programmable gate arrays;Internet of Things;learning (artificial intelligence);neural nets;stochastic processes;binary calculation;stochastic weights binary neural networks;IoT;BNN;SWBNN;FPGA;Internet of things;artificial intelligence;AI;binary neural networks;CIFAR10 database;field-programmable gate array implementation;Next generation networking;Neural networks;IoT;Neural-network;BNN;FPGA;stochastic},
	Month = {May},
	Pages = {1-3},
	Title = {Stochastic weights binary neural networks on FPGA},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNE.2018.8394726}}

@inproceedings{8457667,
	Abstract = {In binary convolutional neural networks (BCNN), arithmetic operations are replaced by bitwise operations and the required memory size is greatly reduced, which is a good opportunity to accelerate training or inference on FPGAs. This paper proposes a BCNN architecture with a single engine that achieves high resource utilization. The proposed design deploys a large number of processing elements in parallel to increase throughput, and a forwarding scheme to increase resource utilization on the existing engine. In addition, we demonstrate a novel reuse scheme to make fully-connected layers exploit the same engine. The proposed design is combined with an inference environment for comparison and implemented on a Xilinx XCVU190 FPGA. The implemented design uses 61k look-up tables (LUTs), 45k flip-flops (FFs), and 13.9Mbit block RAM (BRAM). In addition, it achieves 61.6 GOPS/kLUT at 240MHz, which is 1.16 times higher than that of the best prior BCNN design, even though it uses a single engine without optimal configurations on each layer.},
	Author = {S. Kim and R. Rutenbar},
	Booktitle = {2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	Date-Added = {2018-10-12 05:55:29 +0000},
	Date-Modified = {2018-10-12 05:55:36 +0000},
	Doi = {10.1109/FCCM.2018.00052},
	Issn = {2576-2621},
	Keywords = {BNN; field programmable gate arrays;flip-flops;logic design;neural chips;neural net architecture;random-access storage;table lookup;BRAM;block RAM;flip-flops;look-up tables;inference environment;fully-connected layers;processing elements;BCNN design;memory size;bitwise operations;arithmetic operations;binary convolutional neural networks;effective resource utilization;accelerator design;Xilinx XCVU190 FPGA;forwarding scheme;high resource utilization;BCNN architecture;frequency 240.0 MHz;storage capacity 13.9 Mbit;Field programmable gate arrays;Resource management;Convolutional neural networks;Engines;Computer science;Acceleration;Machine learning;Binary convolutional neural networks;High resource utilization;FPGA},
	Month = {April},
	Pages = {218-218},
	Title = {Accelerator Design with Effective Resource Utilization for Binary Convolutional Neural Networks on an FPGA},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FCCM.2018.00052}}

@article{6847217,
	Abstract = {Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.},
	Author = {L. Shao and F. Zhu and X. Li},
	Date-Added = {2018-10-11 23:28:26 +0000},
	Date-Modified = {2018-10-11 23:30:04 +0000},
	Doi = {10.1109/TNNLS.2014.2330900},
	Issn = {2162-237X},
	Journal = {IEEE Transactions on Neural Networks and Learning Systems},
	Keywords = {NN; image classification;learning (artificial intelligence);object recognition;visual categorization;transfer learning algorithms;object recognition;image classification;human action recognition;Knowledge transfer;Visualization;Training;Training data;Adaptation models;Learning systems;Testing;Action recognition;image classification;machine learning;object recognition;survey;transfer learning;visual categorization.;Action recognition;image classification;machine learning;object recognition;survey;transfer learning;visual categorization;Algorithms;Humans;Knowledge;Machine Learning;Models, Theoretical;Neural Networks (Computer);Surveys and Questionnaires;Transfer (Psychology);Visual Perception},
	Month = {May},
	Number = {5},
	Pages = {1019-1034},
	Title = {Transfer Learning for Visual Categorization: A Survey},
	Volume = {26},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2014.2330900}}

@article{2016arXiv160204283L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160204283L},
	Archiveprefix = {arXiv},
	Author = {{Lacey}, G. and {Taylor}, G.~W. and {Areibi}, S.},
	Date-Added = {2018-10-10 00:44:17 +0000},
	Date-Modified = {2018-10-10 00:44:26 +0000},
	Eprint = {1602.04283},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA; Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{Deep Learning on FPGAs: Past, Present, and Future}},
	Year = 2016}

@article{Gadea-Girones:2018aa,
	Abstract = {In the optimization of deep neural networks (DNNs) via evolutionary algorithms (EAs) and the implementation of the training necessary for the creation of the objective function, there is often a trade-off between efficiency and flexibility. Pure software solutions implemented on general-purpose processors tend to be slow because they do not take advantage of the inherent parallelism of these devices, whereas hardware realizations based on heterogeneous platforms (combining central processing units (CPUs), graphics processing units (GPUs) and/or field-programmable gate arrays (FPGAs)) are designed based on different solutions using methodologies supported by different languages and using very different implementation criteria. This paper first presents a study that demonstrates the need for a heterogeneous (CPU-GPU-FPGA) platform to accelerate the optimization of artificial neural networks (ANNs) using genetic algorithms. Second, the paper presents implementations of the calculations related to the individuals evaluated in such an algorithm on different (CPU- and FPGA-based) platforms, but with the same source files written in OpenCL. The implementation of individuals on remote, low-cost FPGA systems on a chip (SoCs) is found to enable the achievement of good efficiency in terms of performance per watt.},
	An = {PMC5982427},
	Author = {Gadea-Giron{\'e}s, Rafael and Colom-Palero, Ricardo and Herrero-Bosch, Vicente},
	Date = {2018/05/},
	Date-Added = {2018-10-10 00:41:23 +0000},
	Date-Modified = {2018-10-10 00:41:45 +0000},
	Db = {PMC},
	Doi = {10.3390/s18051384},
	Isbn = {1424-8220},
	J1 = {Sensors (Basel)},
	Journal = {Sensors (Basel, Switzerland)},
	Keywords = {Evolutionary; SoC; OpenCL},
	Month = {05},
	Number = {5},
	Pages = {1384},
	Publisher = {MDPI},
	Title = {Optimization of Deep Neural Networks Using SoCs with OpenCL},
	Ty = {JOUR},
	U1 = {29710875{$[$}pmid{$]$}; sensors-18-01384{$[$}PII{$]$}},
	Url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982427/},
	Volume = {18},
	Year = {2018},
	Year1 = {2018/04/30},
	Year2 = {2018/03/08/received},
	Year3 = {2018/04/27/accepted},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982427/},
	Bdsk-Url-2 = {https://doi.org/10.3390/s18051384}}

@inproceedings{2017arXiv171205877J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171205877J},
	Archiveprefix = {arXiv},
	Author = {{Jacob}, B. and {Kligys}, S. and {Chen}, B. and {Zhu}, M. and {Tang}, M. and {Howard}, A. and {Adam}, H. and {Kalenichenko}, D.},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-10-10 00:07:20 +0000},
	Date-Modified = {2018-12-04 23:27:14 +1300},
	Eprint = {1712.05877},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = jun,
	Title = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
	Year = 2018}

@inproceedings{Soudry2014ExpectationBP,
	Abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient
descent-based methods, such as BackPropagation (BP). Inference in probabilistic
graphical models is often done using variational Bayes methods, such as Expectation
Propagation (EP). We show how an EP based approach can also be used
to train deterministic MNNs. Specifically, we approximate the posterior of the
weights given the data using a ``mean-field'' factorized distribution, in an online
setting. Using online EP and the central limit theorem we find an analytical approximation
to the Bayes update of this posterior, as well as the resulting Bayes
estimates of the weights and outputs.
Despite a different origin, the resulting algorithm, Expectation BackPropagation
(EBP), is very similar to BP in form and efficiency. However, it has several additional
advantages: (1) Training is parameter-free, given initial conditions (prior)
and the MNN architecture. This is useful for large-scale problems, where parameter
tuning is a major challenge. (2) The weights can be restricted to have discrete
values. This is especially useful for implementing trained MNNs in precision limited
hardware chips, thus improving their speed and energy efficiency by several
orders of magnitude.
We test the EBP algorithm numerically in eight binary text classification tasks.
In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning
rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with
binary weights usually perform better than MNNs with continuous (real) weights
- if we average the MNN output using the inferred posterior.},
	Annote = {https://www.semanticscholar.org/paper/Expectation-Backpropagation%3A-Parameter-Free-of-with-Soudry-Hubara/0b88ed755c4dcd0ac3d25842a5bbbe4ebcefeeec

https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf},
	Author = {Daniel Soudry and Itay Hubara and Ron Meir},
	Booktitle = {NIPS},
	Date-Added = {2018-10-09 23:57:10 +0000},
	Date-Modified = {2018-10-10 00:00:53 +0000},
	Keywords = {DNN; Backpropagation; Discrete weight space; Continuos weight space},
	Title = {Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights},
	Year = {2014}}

@article{2018arXiv180607550Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607550Z},
	Archiveprefix = {arXiv},
	Author = {{Zhu}, S. and {Dong}, X. and {Su}, H.},
	Date-Added = {2018-10-09 23:07:05 +0000},
	Date-Modified = {2018-10-09 23:07:12 +0000},
	Eprint = {1806.07550},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	Month = jun,
	Title = {{Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?}},
	Year = 2018}

@article{2016arXiv160207360I,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160207360I},
	Archiveprefix = {arXiv},
	Author = {{Iandola}, F.~N. and {Han}, S. and {Moskewicz}, M.~W. and {Ashraf}, K. and {Dally}, W.~J. and {Keutzer}, K.},
	Date-Added = {2018-10-01 09:23:08 +0000},
	Date-Modified = {2018-10-01 09:23:15 +0000},
	Eprint = {1602.07360},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $\lt$0.5MB model size}},
	Year = 2016}

@article{2017arXiv170507175P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170507175P},
	Archiveprefix = {arXiv},
	Author = {{Pedersoli}, F. and {Tzanetakis}, G. and {Tagliasacchi}, A.},
	Date-Added = {2018-09-30 04:37:12 +0000},
	Date-Modified = {2018-09-30 04:37:18 +0000},
	Eprint = {1705.07175},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, 62M45, I.2.6},
	Month = may,
	Primaryclass = {cs.DC},
	Title = {{Espresso: Efficient Forward Propagation for BCNNs}},
	Year = 2017}

@article{2018arXiv180700343A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180700343A},
	Archiveprefix = {arXiv},
	Author = {{Agrawal}, A. and {Jaiswal}, A. and {Han}, B. and {Srinivasan}, G. and {Roy}, K.},
	Date-Added = {2018-09-30 04:23:17 +0000},
	Date-Modified = {2018-09-30 04:23:29 +0000},
	Eprint = {1807.00343},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science; Emerging Technologies},
	Month = jul,
	Title = {{Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays}},
	Year = 2018}

@article{2018arXiv180703010C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703010C},
	Archiveprefix = {arXiv},
	Author = {{Conti}, F. and {Davide Schiavone}, P. and {Benini}, L.},
	Date-Added = {2018-09-30 04:20:35 +0000},
	Date-Modified = {2018-09-30 04:20:42 +0000},
	Eprint = {1807.03010},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Neural and Evolutionary Computing, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	Month = jul,
	Title = {{XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary Neural Network Inference}},
	Year = 2018}

@article{2018arXiv180801990C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180801990C},
	Archiveprefix = {arXiv},
	Author = {{Cakir}, F. and {He}, K. and {Sclaroff}, S.},
	Date-Added = {2018-09-30 02:43:46 +0000},
	Date-Modified = {2018-10-09 23:14:42 +0000},
	Eprint = {1808.01990},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = aug,
	Title = {{Hashing with Binary Matrix Pursuit}},
	Year = 2018}

@article{2017arXiv171107971W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171107971W},
	Archiveprefix = {arXiv},
	Author = {{Wang}, X. and {Girshick}, R. and {Gupta}, A. and {He}, K.},
	Date-Added = {2018-09-30 02:43:14 +0000},
	Date-Modified = {2018-09-30 02:44:18 +0000},
	Eprint = {1711.07971},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Primaryclass = {cs.CV},
	Title = {{Non-local Neural Networks}},
	Year = 2017}

@article{2018arXiv180802631Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180802631Z},
	Archiveprefix = {arXiv},
	Author = {{Zhuang}, B. and {Shen}, C. and {Reid}, I.},
	Date-Added = {2018-09-30 01:58:28 +0000},
	Date-Modified = {2018-09-30 01:58:33 +0000},
	Eprint = {1808.02631},
	Journal = {ArXiv e-prints},
	Keywords = {BNN;Computer Science - Computer Vision and Pattern Recognition},
	Month = aug,
	Primaryclass = {cs.CV},
	Title = {{Training Compact Neural Networks with Binary Weights and Low Precision Activations}},
	Year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBkLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0RFTlNFUi0gRGVlcCBFdm9sdXRpb25hcnkgTmV0d29yayBTdHJ1Y3R1cmVkIFJlcHJlc2VudGF0aW9uLmJpYk8RAkwAAAAAAkwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9ERU5TRVItIERlZXAgRXZvbHUjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAMRXZvbHV0aW9uYXJ5AAIAei86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpFdm9sdXRpb25hcnk6REVOU0VSLSBEZWVwIEV2b2x1dGlvbmFyeSBOZXR3b3JrIFN0cnVjdHVyZWQgUmVwcmVzZW50YXRpb24uYmliAA4AgAA/AEQARQBOAFMARQBSAC0AIABEAGUAZQBwACAARQB2AG8AbAB1AHQAaQBvAG4AYQByAHkAIABOAGUAdAB3AG8AcgBrACAAUwB0AHIAdQBjAHQAdQByAGUAZAAgAFIAZQBwAHIAZQBzAGUAbgB0AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAeFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0RFTlNFUi0gRGVlcCBFdm9sdXRpb25hcnkgTmV0d29yayBTdHJ1Y3R1cmVkIFJlcHJlc2VudGF0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACLAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAts=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBiLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OL1ZlcnkgZGVlcCBjb252b2x1dGlvbmFsIG5ldHdvcmtzIGZvciBsYXJnZS1zY2FsZSBpbWFnZSByZWNvZ25pdGlvbi5iaWJPEQJOAAAAAAJOAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fVmVyeSBkZWVwIGNvbnZvbHV0I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAA0NOTgAAAgB4LzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkNOTjpWZXJ5IGRlZXAgY29udm9sdXRpb25hbCBuZXR3b3JrcyBmb3IgbGFyZ2Utc2NhbGUgaW1hZ2UgcmVjb2duaXRpb24uYmliAA4AjgBGAFYAZQByAHkAIABkAGUAZQBwACAAYwBvAG4AdgBvAGwAdQB0AGkAbwBuAGEAbAAgAG4AZQB0AHcAbwByAGsAcwAgAGYAbwByACAAbABhAHIAZwBlAC0AcwBjAGEAbABlACAAaQBtAGEAZwBlACAAcgBlAGMAbwBnAG4AaQB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgB2VXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4vVmVyeSBkZWVwIGNvbnZvbHV0aW9uYWwgbmV0d29ya3MgZm9yIGxhcmdlLXNjYWxlIGltYWdlIHJlY29nbml0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACJAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAts=}}

@article{2018arXiv180810631K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180810631K},
	Archiveprefix = {arXiv},
	Author = {{Krestinskaya}, O. and {Salama}, K.~N. and {Pappachen James}, A.},
	Date-Added = {2018-09-30 01:58:11 +0000},
	Date-Modified = {2018-09-30 01:58:18 +0000},
	Eprint = {1808.10631},
	Journal = {ArXiv e-prints},
	Keywords = {BNN;Computer Science - Emerging Technologies, Computer Science - Artificial Intelligence},
	Month = aug,
	Title = {{Learning in Memristive Neural Network Architectures using Analog Backpropagation Circuits}},
	Year = 2018}

@article{2018arXiv180910463B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180910463B},
	Archiveprefix = {arXiv},
	Author = {{Bethge}, J. and {Yang}, H. and {Bartz}, C. and {Meinel}, C.},
	Date-Added = {2018-09-30 01:49:26 +0000},
	Date-Modified = {2018-09-30 01:49:44 +0000},
	Eprint = {1809.10463},
	Journal = {ArXiv e-prints},
	Keywords = {BNN;Computer Science; Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = sep,
	Title = {{Learning to Train a Binary Neural Network}},
	Year = 2018}

@article{2018JInst..13P7027D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018JInst..13P7027D},
	Archiveprefix = {arXiv},
	Author = {{Duarte}, J. and {Han}, S. and {Harris}, P. and {Jindariani}, S. and {Kreinar}, E. and {Kreis}, B. and {Ngadiuba}, J. and {Pierini}, M. and {Rivera}, R. and {Tran}, N. and {Wu}, Z.},
	Date-Added = {2018-09-30 01:22:32 +0000},
	Date-Modified = {2018-10-10 00:48:54 +0000},
	Doi = {10.1088/1748-0221/13/07/P07027},
	Eprint = {1804.06913},
	Journal = {Journal of Instrumentation},
	Keywords = {FPGA-Fin; NN; Physics},
	Month = jul,
	Pages = {P07027},
	Primaryclass = {physics.ins-det},
	Title = {{Fast inference of deep neural networks in FPGAs for particle physics}},
	Volume = 13,
	Year = 2018,
	Bdsk-Url-1 = {https://doi.org/10.1088/1748-0221/13/07/P07027}}

@inproceedings{8052915,
	Abstract = {As a popular deep learning technique, convolutional neural network has been widely used in many tasks such as image classification and object recognition. Convolutional neural network exploits spatial correlations in the images by performing convolution operations in local receptive fields. Convolutional neural networks are preferred over fully connected neural networks because they have fewer weights and are easier to train. Many research works have been conducted to reduce the computational complexity and memory requirements of convolutional neural network, to make it applicable to the low-power embedded applications with limited memories. This paper presents the architecture design of convolutional neural network with binary weights and activations, also known as binary neural network, on an FPGA platform. Weights and input activations are binarized with only two values, +1 and -1. This reduces all the fixed point multiplication operations in convolutional layers and fully connected layers to 1-bit XNOR operations. The proposed design uses only on-chip memories. Furthermore, an efficient implementation of batch normalization operation is introduced. When evaluating the CIFAR-10 benchmark, the proposed FPGA design can achieve a processing rate of 332,158 images per second with with accuracy of 86.06% using 1-bit quantized weights and activations.},
	Author = {Y. Zhou and S. Redkar and X. Huang},
	Booktitle = {2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
	Date-Added = {2018-09-29 09:57:58 +0000},
	Date-Modified = {2018-09-29 09:58:39 +0000},
	Doi = {10.1109/MWSCAS.2017.8052915},
	Keywords = {BNN;field programmable gate arrays;fixed point arithmetic;image classification;learning (artificial intelligence);neural nets;object recognition;quantisation (signal);1-bit XNOR operation;CIFAR-10 benchmark;FPGA platform;batch normalization operation;computational complexity reduction;convolutional neural network;deep learning binary neural network;fixed point multiplication operation;local receptive fields;low-power embedded applications;memory requirement reduction;on-chip memories;spatial correlation;Biological neural networks;Convolution;Field programmable gate arrays;Hardware;Memory management;Training},
	Month = {Aug},
	Pages = {281-284},
	Title = {Deep learning binary neural network on an FPGA},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/MWSCAS.2017.8052915}}

@article{5159360,
	Abstract = {Implementing linearly nonseparable Boolean functions (non-LSBF) has been an important and yet challenging task due to the extremely high complexity of this kind of functions and the exponentially increasing percentage of the number of non-LSBF in the entire set of Boolean functions as the number of input variables increases. In this paper, an algorithm named DNA-like learning and decomposing algorithm (DNA-like LDA) is proposed, which is capable of effectively implementing non-LSBF. The novel algorithm first trains the DNA-like offset sequence and decomposes non-LSBF into logic XOR operations of a sequence of LSBF, and then determines the weight-threshold values of the multilayer perceptron (MLP) that perform both the decompositions of LSBF and the function mapping the hidden neurons to the output neuron. The algorithm is validated by two typical examples about the problem of approximating the circular region and the well-known &lt;i&gt;n&lt;/i&gt; -bit parity Boolean function (PBF).},
	Author = {F. Chen and G. Chen and Q. He and G. He and X. Xu},
	Date-Added = {2018-09-29 09:56:02 +0000},
	Date-Modified = {2018-09-29 09:56:11 +0000},
	Doi = {10.1109/TNN.2009.2023122},
	Issn = {1045-9227},
	Journal = {IEEE Transactions on Neural Networks},
	Keywords = {BNN; Boolean functions;learning (artificial intelligence);multilayer perceptrons;binary neural network;linearly nonseparable Boolean functions;nonLSBF;DNA-like learning and decomposing algorithm;DNA-like LDA;DNA-like offset sequence;logic XOR operation;weight-threshold value;multilayer perceptron;function mapping;parity Boolean function;Neural networks;Boolean functions;Neurons;Multilayer perceptrons;Linear discriminant analysis;Logic;Backpropagation algorithms;Input variables;Mathematics;Binary neural network;DNA-like learning and decomposing algorithm (DNA-like LDA);linearly nonseparable Boolean function (non-LSBF);multilayer perceptron (MLP);parity Boolean function (PBF);Algorithms;Artificial Intelligence;DNA;Linear Models;Neural Networks (Computer)},
	Month = {Aug},
	Number = {8},
	Pages = {1293-1301},
	Title = {Universal Perceptron and DNA-Like Learning Algorithm for Binary Neural Networks: Non-LSBF Implementation},
	Volume = {20},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNN.2009.2023122}}

@inproceedings{7016218,
	Abstract = {This paper presents a new method in forecasting Philippine Peso to US Dollar exchange rate. Compared to the conventional way, in which the Philippine Dealing System (PDS), as monitored by the Central Bank, determines the rate by analysing demand and supply, the use of artificial neural network, having consumer price index, inflation rate, lending interest rate and purchasing power of the peso as the inputs is presented in this paper. Though foreign exchange rates vary on a daily basis, the output of this paper is prediction of the average foreign exchange rate every month. Artificial Neural Network serves as a powerful tool in forecasting Philippine Peso to US Dollar exchange rate not requiring expert knowledge in banking and finance thus letting the public gain access to a helpful beacon which is the foreign exchange rate. However, the accuracy of the forecast using artificial neural network is highly dependent on the volume of the training data, in this paper, an alternative algorithm that will increase the accuracy of the conventional artificial neural network with limited volume of training data is presented and analyze.},
	Author = {M. L. R. Torregoza and E. P. Dadios},
	Booktitle = {2014 International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management (HNICEM)},
	Date-Added = {2018-09-29 02:58:04 +0000},
	Date-Modified = {2018-10-10 06:51:39 +0000},
	Doi = {10.1109/HNICEM.2014.7016218},
	Keywords = {Evolutionary;banking;forecasting theory;genetic algorithms;neural nets;pricing;hybrid genetic algorithm neural network;Philippine peso US dollar exchange rate forecasting;Philippine dealing system;PDS;central bank;demand and supply analysis;artificial neural network;consumer price index;interest rate;inflation rate;lending interest rate;purchasing power;foreign exchange rates;banking;training data;Exchange rates;Genetic algorithms;Artificial neural networks;Forecasting;Conferences;Economic indicators;Artificial Neural Network;forecasting;prediction;exchange rate;evolutionary algorithm},
	Month = {Nov},
	Pages = {1-5},
	Title = {Comparison of neural network and hybrid genetic algorithm-neural network in forecasting of Philippine Peso-US Dollar exchange rate},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/HNICEM.2014.7016218}}

@inproceedings{7838429,
	Abstract = {On-chip implementation of large-scale neural networks with emerging synaptic devices is attractive but challenging, primarily due to the pre-mature analog properties of today's resistive memory technologies. This work aims to realize a large-scale neural network using today's available binary RRAM devices for image recognition. We propose a methodology to binarize the neural network parameters with a goal of reducing the precision of weights and neurons to 1-bit for classification and &lt;;8-bit for online training. We experimentally demonstrate the binary neural network (BNN) on Tsinghua's 16 Mb RRAM macro chip fabricated in 130 nm CMOS process. Even under finite bit yield and endurance cycles, the system performance on MNIST handwritten digit dataset achieves ~96.5% accuracy for both classification and online training, close to ~97% accuracy by the ideal software implementation. This work reports the largest scale of the synaptic arrays and achieved the highest accuracy so far.},
	Author = {S. Yu and Z. Li and P. Chen and H. Wu and B. Gao and D. Wang and W. Wu and H. Qian},
	Booktitle = {2016 IEEE International Electron Devices Meeting (IEDM)},
	Date-Added = {2018-09-28 11:32:29 +0000},
	Date-Modified = {2018-09-28 11:32:37 +0000},
	Doi = {10.1109/IEDM.2016.7838429},
	Issn = {2156-017X},
	Keywords = {BNN; CMOS integrated circuits;electronic engineering computing;image recognition;neural nets;resistive RAM;MNIST handwritten digit dataset;CMOS process;Tsinghua;BNN;image recognition;binary RRAM macrochip device;resistive memory technology;pre-mature analog property;synaptic device;large-scale binary neural network;word length 1 bit;storage capacity 16 Mbit;size 130 nm},
	Month = {Dec},
	Pages = {16.2.1-16.2.4},
	Title = {Binary neural network with 16 Mb RRAM macro chip for classification and online training},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/IEDM.2016.7838429}}

@inproceedings{7033335,
	Author = {O. P. Patel and A. Tiwari},
	Booktitle = {2014 International Conference on Information Technology},
	Date-Added = {2018-09-28 10:26:33 +0000},
	Date-Modified = {2018-09-28 10:26:47 +0000},
	Doi = {10.1109/ICIT.2014.29},
	Keywords = {BNN; generalisation (artificial intelligence);learning (artificial intelligence);neural nets;optimisation;pattern classification;quantum computing;quantum based binary neural network learning algorithm;network structure optimisation;neurons;classification accuracy;hidden layer;training accuracy;generalization accuracy;QANN;Neurons;Accuracy;Training;Biological neural networks;Testing;Diabetes;Binary neural network;Quantum processing;Qubits;Back propagation learning},
	Month = {Dec},
	Pages = {270-274},
	Title = {Quantum Inspired Binary Neural Network Algorithm},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIT.2014.29}}

@inproceedings{1206405,
	Abstract = {This paper describes a 3D VLSI Chip for binary neural network classification applications. The 3D circuit includes three layers of MCM integrating 4 chips each making it a total of 12 chips integrated in a volume of (2 /spl times/ 2 /spl times/ 0.7)cm/sup 3/. The architecture is scalable, and real-time binary neural network classifier systems could be built with one, two or all twelve chip solutions. Each basic chip includes an on-chip control unit for programming options of the neural network topology and precision. The system is modular and presents easy expansibility without requiring extra devices. Experimental test results showed that a full recall operation is obtained in less than 1.2/spl mu/s for any topology with 4-bit or 8-bit precision while it is obtained in less than 2.2/spl mu/s for any 16-bit precision. As a consequence the 3D chip is a very powerful reconfigurable and a multiprecision neural chip exhibiting a significant speed of 1.25 GCPS.},
	Author = {A. Bermak},
	Booktitle = {Proceedings of the 2003 International Symposium on Circuits and Systems, 2003. ISCAS '03.},
	Date-Added = {2018-09-28 10:13:06 +0000},
	Date-Modified = {2018-12-06 14:26:04 +1300},
	Doi = {10.1109/ISCAS.2003.1206405},
	Keywords = {BNN; pattern classification;VLSI;multiprecision neural chip},
	Month = {May},
	Pages = {V-V},
	Title = {A highly scalable 3D chip for binary neural network classification applications},
	Volume = {5},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCAS.2003.1206405}}

@article{2009arXiv0904.4587T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2009arXiv0904.4587T},
	Archiveprefix = {arXiv},
	Author = {{Torres-Moreno}, J.-M. and {Gordon}, M.~B.},
	Date-Added = {2018-09-28 09:40:01 +0000},
	Date-Modified = {2018-09-28 09:40:24 +0000},
	Eprint = {0904.4587},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science; Artificial Intelligence; Neural and Evolutionary Computing},
	Month = apr,
	Primaryclass = {cs.AI},
	Title = {{Adaptive Learning with Binary Neurons}},
	Year = 2009}

@inproceedings{616215,
	Abstract = {An "evolutionary neural network (ENN)" is presented for the max cut problem of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem is to find a partition of V into two disjoint subsets such that the cut size be maximized. The cut size is the sum of weights on edges in E whose endpoints belong to different subsets. The ENN combines the evolutionary initialization scheme of the neural state into the energy minimization criteria of the binary neural network. The performance of ENN is evaluated through simulations in randomly weighted complete graphs and unweighted random graphs with up to 1000 vertices. The results show that the evolutionary initialization scheme drastically improves the solution quality. ENN can always find better solutions than the maximum neural network, the mean field annealing, the simulated annealing, and the greedy algorithm.},
	Author = {N. Funabiki and J. Kitamichi and S. Nishikawa},
	Booktitle = {Proceedings of International Conference on Neural Networks (ICNN'97)},
	Date-Added = {2018-09-28 09:23:32 +0000},
	Date-Modified = {2018-10-10 06:34:05 +0000},
	Doi = {10.1109/ICNN.1997.616215},
	Keywords = {Evolutionary; neural nets;genetic algorithms;set theory;graph theory;minimisation;computational complexity;evolutionary neural network algorithm;ENN;max cut problems;undirected graph;NP-hard problem;partition;disjoint subsets;evolutionary initialization scheme;energy minimization criteria;binary neural network;randomly weighted complete graphs;unweighted random graphs;maximum neural network;mean field annealing;simulated annealing;greedy algorithm;Neural networks;Neurons;Simulated annealing;NP-hard problem;Equations;Multi-layer neural network;Computer networks;Minimization;Greedy algorithms;Approximation algorithms},
	Month = {June},
	Pages = {1260-1265 vol.2},
	Title = {An evolutionary neural network algorithm for max cut problems},
	Volume = {2},
	Year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICNN.1997.616215}}

@inproceedings{4790104,
	Abstract = {Analysis of the state space for the fully-connected binary neural network ("the Hopfield model") remains an important objective in utilizing the network in pattern recognition and associative information retrieval. Most of the research pertaining to the network's state space so far concentrated on stable-state enumeration and often it was assumed that the patterns which are to be stored are random. We discuss the case of deterministic known codewords whose storage is required, and show that for this important case bounds on the retrieval probabilities and convergence rates can be achieved. The main tool which we employ is Birth-and-Death Markov chains, describing the Hamming distance of the network's state from the stored patterns. The results are applicable to both the asynchronous network and to the Boltzmann machine, and can be utilized to compare codeword sets in terms of efficiency of their retrieval, when the neural network is used as a content addressable memory.},
	Author = {M. Kam and R. Cheng and A. Guez},
	Booktitle = {1988 American Control Conference},
	Date-Added = {2018-09-28 09:05:13 +0000},
	Date-Modified = {2018-09-28 09:05:27 +0000},
	Doi = {10.23919/ACC.1988.4790104},
	Keywords = {BNN;State-space methods;Neural networks;Information analysis;Pattern analysis;Hopfield neural networks;Pattern recognition;Information retrieval;Convergence;Hamming distance;Content based retrieval},
	Month = {June},
	Pages = {2276-2281},
	Title = {On the State Space of the Binary Neural Network},
	Year = {1988},
	Bdsk-Url-1 = {https://doi.org/10.23919/ACC.1988.4790104}}

@article{2016arXiv160602580F,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602580F},
	Archiveprefix = {arXiv},
	Author = {{Fernando}, C. and {Banarse}, D. and {Reynolds}, M. and {Besse}, F. and {Pfau}, D. and {Jaderberg}, M. and {Lanctot}, M. and {Wierstra}, D.},
	Date-Added = {2018-09-26 08:54:27 +0000},
	Date-Modified = {2018-10-10 07:00:44 +0000},
	Eprint = {1606.02580},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Computer Science; Neural; Evolutionary Computing; Computer Vision; Pattern Recognition; Machine Learning;},
	Month = jun,
	Title = {{Convolution by Evolution: Differentiable Pattern Producing Networks}},
	Year = 2016}

@article{2017arXiv170303864S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170303864S},
	Archiveprefix = {arXiv},
	Author = {{Salimans}, T. and {Ho}, J. and {Chen}, X. and {Sidor}, S. and {Sutskever}, I.},
	Date-Added = {2018-09-26 08:48:55 +0000},
	Date-Modified = {2018-10-10 00:38:32 +0000},
	Eprint = {1703.03864},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	Month = mar,
	Primaryclass = {stat.ML},
	Title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
	Year = 2017}

@article{2018arXiv180801974T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180801974T},
	Archiveprefix = {arXiv},
	Author = {{Tan}, C. and {Sun}, F. and {Kong}, T. and {Zhang}, W. and {Yang}, C. and {Liu}, C.},
	Date-Added = {2018-09-26 08:24:11 +0000},
	Date-Modified = {2018-10-10 00:49:11 +0000},
	Eprint = {1808.01974},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = aug,
	Title = {{A Survey on Deep Transfer Learning}},
	Year = 2018}

@article{2018arXiv180804752G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180804752G},
	Archiveprefix = {arXiv},
	Author = {{Guo}, Y.},
	Date-Added = {2018-09-26 07:51:55 +0000},
	Date-Modified = {2018-10-14 00:31:41 +0000},
	Eprint = {1808.04752},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning, Neural and Evolutionary Computing, Statistics,Machine Learning},
	Month = aug,
	Title = {{A Survey on Methods and Theories of Quantized Neural Networks}},
	Year = 2018}

@article{2016arXiv161006918A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006918A},
	Archiveprefix = {arXiv},
	Author = {{Abadi}, M. and {Andersen}, D.~G.},
	Date-Added = {2018-09-26 05:39:12 +0000},
	Date-Modified = {2018-10-10 00:36:22 +0000},
	Eprint = {1610.06918},
	Journal = {ArXiv e-prints},
	Keywords = {DCGAN; Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	Month = oct,
	Primaryclass = {cs.CR},
	Title = {{Learning to Protect Communications with Adversarial Neural Cryptography}},
	Year = 2016}

@article{2018arXiv180500728V,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180500728V},
	Archiveprefix = {arXiv},
	Author = {{Volz}, V. and {Schrum}, J. and {Liu}, J. and {Lucas}, S.~M. and {Smith}, A. and {Risi}, S.},
	Date-Added = {2018-09-26 05:35:11 +0000},
	Date-Modified = {2018-10-10 00:33:52 +0000},
	Eprint = {1805.00728},
	Journal = {ArXiv e-prints},
	Keywords = {DCGAN; Evolutionary; Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	Month = may,
	Primaryclass = {cs.AI},
	Title = {{Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network}},
	Year = 2018}

@article{2018arXiv180605695W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180605695W},
	Archiveprefix = {arXiv},
	Author = {{Wilson}, D.~G and {Cussat-Blanc}, S. and {Luga}, H. and {Miller}, J.~F},
	Date-Added = {2018-09-26 05:29:58 +0000},
	Date-Modified = {2018-09-26 05:29:58 +0000},
	Eprint = {1806.05695},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	Month = jun,
	Title = {{Evolving simple programs for playing Atari games}},
	Year = 2018}

@inproceedings{8393327,
	Author = {B. Yang and W. Zhang and L. Gong and H. Ma},
	Booktitle = {2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
	Date-Added = {2018-09-26 01:45:32 +0000},
	Date-Modified = {2018-10-10 07:02:14 +0000},
	Doi = {10.1109/FSKD.2017.8393327},
	Keywords = {Evolutionary; finance; exchange rates;forecasting theory;genetic algorithms;neural nets;stock markets;time series;trees (mathematics);CVFNT model;time series datasets;neural network;finance time series prediction;complex-valued flexible neural tree model;artificial bee colony;forecasting accuracy;genetic algorithm;Shanghai stock index;exchange rates;Neural networks;Time series analysis;Brain modeling;Predictive models;Forecasting;Data models;Indexes;evolutionary method;flexible neural tree;complex-valued;artificial bee colony},
	Month = {July},
	Pages = {54-58},
	Title = {Finance time series prediction using complex-valued flexible neural tree model},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/FSKD.2017.8393327}}

@inproceedings{5432472,
	Author = {L. Liu and M. Deng},
	Booktitle = {2010 Third International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2018-09-26 01:39:28 +0000},
	Date-Modified = {2018-10-10 06:13:15 +0000},
	Doi = {10.1109/WKDD.2010.148},
	Keywords = {Evolutionary; cancer;genetic algorithms;medical image processing;neural nets;artificial neural network;breast cancer diagnosis;women;adaptive genetic algorithm;macro-search capability;global optimization;computational cost;Wisions breast cancer data set;Artificial neural networks;Breast cancer;Genetic algorithms;Genetic mutations;Flowcharts;Economic forecasting;Space technology;Data mining;Conference management;Knowledge management;adaptive genetic algorithm;neural network;weights and thresholds;breast cancer diagnosis},
	Month = {Jan},
	Pages = {593-596},
	Title = {An Evolutionary Artificial Neural Network Approach for Breast Cancer Diagnosis},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/WKDD.2010.148}}

@inproceedings{Krizhevsky2999257,
	Acmid = {2999257},
	Address = {USA},
	Author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	Booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
	Date-Added = {2018-09-25 11:16:05 +0000},
	Date-Modified = {2018-12-01 13:40:11 +1300},
	Keywords = {CNN, ImageNet, ILSVRC, Convolution, AlexNet},
	Location = {Lake Tahoe, Nevada},
	Numpages = {9},
	Pages = {1097--1105},
	Publisher = {Curran Associates Inc.},
	Series = {NIPS'12},
	Title = {ImageNet Classification with Deep Convolutional Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2999134.2999257}}

@webpage{AIWinter-Andrey,
	Author = {Kurenkov, Andrey},
	Date-Added = {2018-09-25 10:57:03 +0000},
	Date-Modified = {2018-10-10 09:00:52 +0000},
	Keywords = {AI, Neural Networks, History},
	Lastchecked = {25-Sep-2018},
	Month = {September},
	Title = {A 'Brief' History of Neural Nets and Deep Learning},
	Url = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning},
	Urldate = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning},
	Year = {2018},
	Bdsk-Url-1 = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning}}

@article{Linnainmaa1976,
	Abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
	Author = {Linnainmaa, Seppo},
	Date-Added = {2018-09-25 08:37:46 +0000},
	Date-Modified = {2018-09-25 08:38:04 +0000},
	Day = {01},
	Doi = {10.1007/BF01931367},
	Issn = {1572-9125},
	Journal = {BIT Numerical Mathematics},
	Keywords = {NN, Neural Networks, Backpropagation},
	Month = {Jun},
	Number = {2},
	Pages = {146--160},
	Title = {Taylor expansion of the accumulated rounding error},
	Url = {https://doi.org/10.1007/BF01931367},
	Volume = {16},
	Year = {1976},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF01931367}}

@book{Minsky:1988:PEE:50066,
	Address = {Cambridge, MA, USA},
	Author = {Minsky, Marvin L. and Papert, Seymour A.},
	Date-Added = {2018-09-25 08:29:03 +0000},
	Date-Modified = {2018-09-25 08:29:27 +0000},
	Isbn = {0-262-63111-3},
	Keywords = {NN, Neural Networks},
	Publisher = {MIT Press},
	Title = {Perceptrons: Expanded Edition},
	Year = {1988},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB+Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgR0EtYmFzZWQgZmxleGlibGUgbGVhcm5pbmcgYWxnb3JpdGhtIHdpdGggZXJyb3IgdG9sZXJhbmNlIGZvciBkaWdpdGFsIGJpbmFyeSBuZXVyYWwgbmV0d29ya3MuYmliTxECvgAAAAACvgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgR0EtYmFzZWQgZmxleGlibCNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANCTk4AAAIAlC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpCTk46QSBHQS1iYXNlZCBmbGV4aWJsZSBsZWFybmluZyBhbGdvcml0aG0gd2l0aCBlcnJvciB0b2xlcmFuY2UgZm9yIGRpZ2l0YWwgYmluYXJ5IG5ldXJhbCBuZXR3b3Jrcy5iaWIADgDGAGIAQQAgAEcAQQAtAGIAYQBzAGUAZAAgAGYAbABlAHgAaQBiAGwAZQAgAGwAZQBhAHIAbgBpAG4AZwAgAGEAbABnAG8AcgBpAHQAaABtACAAdwBpAHQAaAAgAGUAcgByAG8AcgAgAHQAbwBsAGUAcgBhAG4AYwBlACAAZgBvAHIAIABkAGkAZwBpAHQAYQBsACAAYgBpAG4AYQByAHkAIABuAGUAdQByAGEAbAAgAG4AZQB0AHcAbwByAGsAcwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAklVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgR0EtYmFzZWQgZmxleGlibGUgbGVhcm5pbmcgYWxnb3JpdGhtIHdpdGggZXJyb3IgdG9sZXJhbmNlIGZvciBkaWdpdGFsIGJpbmFyeSBuZXVyYWwgbmV0d29ya3MuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAKUAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADZw==}}

@inproceedings{6173439,
	Abstract = {Hardware/software partitioning is a crucial problem in hardware/software co-design. In this paper, we deeply investigate genetic algorithm (GA) for hardware/software partitioning, our co-design targets a heterogeneous multicore system on chip (SoC) which consists of several different types of processing engines(PE), Communicating structure adopts NOC, We use GA for four task graphs to simulate the hardware/software partitioning, experiments show our method is an effective hardware/software partitioning algorithm.},
	Author = {L. Luo and H. He and Q. Dou and W. Xu},
	Booktitle = {2012 Second International Conference on Intelligent System Design and Engineering Application},
	Date-Added = {2018-09-09 11:15:43 +0000},
	Date-Modified = {2018-09-09 11:16:08 +0000},
	Doi = {10.1109/ISdea.2012.501},
	Keywords = {Evolutionary, genetic algorithms;graph theory;hardware-software codesign;multiprocessing systems;system-on-chip;hardware-software partitioning;genetic algorithm;hardware-software codesign;heterogeneous multicore system on chip;processing engines;communicating structure;NOC;task graphs;Software;Hardware;Partitioning algorithms;Software algorithms;Genetic algorithms;Heuristic algorithms;System-on-a-chip;Hardware/Software Partitioning;Genetic Algorithm(GA);Heterogeneous Multicore SOC},
	Month = {Jan},
	Pages = {1267-1270},
	Title = {Hardware/Software Partitioning for Heterogeneous Multicore SoC Using Genetic Algorithm},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISdea.2012.501}}

@inproceedings{Zhang:2016:ECI:2934583.2934644,
	Acmid = {2934644},
	Address = {New York, NY, USA},
	Author = {Zhang, Chen and Wu, Di and Sun, Jiayu and Sun, Guangyu and Luo, Guojie and Cong, Jason},
	Booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
	Date-Added = {2018-08-25 11:20:53 +0000},
	Date-Modified = {2018-09-26 07:26:01 +0000},
	Doi = {10.1145/2934583.2934644},
	Isbn = {978-1-4503-4185-1},
	Keywords = {CNN-FPGA, FPGA, CNN, Pipeline, Cluster},
	Location = {San Francisco Airport, CA, USA},
	Numpages = {6},
	Pages = {326--331},
	Publisher = {ACM},
	Series = {ISLPED '16},
	Title = {Energy-Efficient CNN Implementation on a Deeply Pipelined FPGA Cluster},
	Url = {http://doi.acm.org/10.1145/2934583.2934644},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2934583.2934644},
	Bdsk-Url-2 = {https://doi.org/10.1145/2934583.2934644}}

@article{2017arXiv170402019C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170402019C},
	Archiveprefix = {arXiv},
	Author = {{Chaudhuri}, R. and {Fiete}, I.},
	Date-Added = {2018-07-09 10:12:02 +0000},
	Date-Modified = {2018-07-09 10:12:09 +0000},
	Eprint = {1704.02019},
	Journal = {ArXiv e-prints},
	Keywords = {BNN, Quantitative Biology - Neurons and Cognition, Computer Science - Neural and Evolutionary Computing},
	Month = apr,
	Primaryclass = {q-bio.NC},
	Title = {{Associative content-addressable networks with exponentially many robust stable states}},
	Year = 2017}

@article{32008,
	Author = {M. Verleysen and B. Sirletti and A. M. Vandemeulebroecke and P. G. A. Jespers},
	Date-Added = {2018-07-09 10:11:47 +0000},
	Date-Modified = {2018-07-09 10:11:54 +0000},
	Doi = {10.1109/4.32008},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {BNN, VLSI;computerised pattern recognition;content-addressable storage;integrated memory circuits;learning systems;neural nets;programming;CAM;Hopfield neural network;VLSI circuit;analogue VLSI implementation;content-addressable memory;fully interconnected neural network;learning algorithm;pattern-recognition applications;programming;storage capacity;synapse weights;synaptic cells;Artificial intelligence;Artificial neural networks;Biological neural networks;Biology computing;Circuits;Hopfield neural networks;Neural networks;Neurons;Pattern recognition;Very large scale integration},
	Month = {Jun},
	Number = {3},
	Pages = {562-569},
	Title = {Neural networks for high-storage content-addressable memory: VLSI circuit and learning algorithm},
	Volume = {24},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1109/4.32008}}

@article{Brodsky:93,
	Abstract = {The content-addressable network (CAN) is an efficient, intrinsically discrete training algorithm for binary-valued classification networks. The binary nature of the CAN network permits accelerated learning and significantly reduced hardware-implementation requirements. A multilayer optoelectronic CAN network employing matrix--vector multiplication was constructed. The network learned and correctly classified trained patterns, gaining a measure of fault tolerance by learning associative solutions to optical hardware imperfections. Operation of this system is possible owing to the reduced hardware accuracy requirements of the CAN learning algorithm.},
	Author = {Stephen A. Brodsky and Gary C. Marsden and Clark C. Guest},
	Date-Added = {2018-07-05 08:41:33 +0000},
	Date-Modified = {2018-07-05 08:41:42 +0000},
	Doi = {10.1364/AO.32.001338},
	Journal = {Appl. Opt.},
	Keywords = {BNN, Cylindrical lenses; Light valves; Neural networks; Optical components; Optical neural systems; Parallel processing},
	Month = {Mar},
	Number = {8},
	Pages = {1338--1345},
	Publisher = {OSA},
	Title = {Optical matrix--vector implementation of the content-addressable network},
	Url = {http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338},
	Volume = {32},
	Year = {1993},
	Bdsk-Url-1 = {http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338},
	Bdsk-Url-2 = {https://doi.org/10.1364/AO.32.001338}}

@article{LMNN-UoE-UK,
	Adsurl = {http://homepages.inf.ed.ac.uk/rbf/EUCOGNITION/BRIEFINGS/billings.pdf},
	Archiveprefix = {UoE-UK},
	Author = {{Billings}, Guy},
	Date-Added = {2018-06-22 10:47:53 +0000},
	Date-Modified = {2018-06-22 10:47:53 +0000},
	Eprint = {1502.04390},
	Journal = {Euroinformatics Doctoral Training Centre},
	Keywords = {NN, Learning, Numerical Analysis},
	Primaryclass = {cs.LG},
	Title = {{Learning and Memory in Neural Networks}},
	Year = 2004}

@article{PETERSON1989475,
	Author = {Carsten Peterson and Eric Hartman},
	Date-Added = {2018-06-22 10:47:31 +0000},
	Date-Modified = {2018-12-06 13:49:53 +1300},
	Doi = {https://doi.org/10.1016/0893-6080(89)90045-2},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {NN, Neural network, Bidirectional, Generalization, Content addressable memory, Mean field theory, Learning algorithm},
	Number = {6},
	Pages = {475 - 494},
	Title = {Explorations of the mean field theory learning algorithm},
	Url = {http://www.sciencedirect.com/science/article/pii/0893608089900452},
	Volume = {2},
	Year = {1989},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0893608089900452},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(89)90045-2}}

@article{2018arXiv180305900V,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180305900V},
	Archiveprefix = {arXiv},
	Author = {{Venieris}, S.~I. and {Kouris}, A. and {Bouganis}, C.-S.},
	Date-Added = {2018-06-22 10:47:07 +0000},
	Date-Modified = {2018-06-22 10:47:16 +0000},
	Eprint = {1803.05900},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA-NN, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Learning},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey and Future Directions}},
	Year = 2018}

@inproceedings{Suda:2016:TOF:2847263.2847276,
	Acmid = {2847276},
	Address = {New York, NY, USA},
	Author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
	Booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-06-22 10:45:58 +0000},
	Date-Modified = {2018-06-22 10:46:11 +0000},
	Doi = {10.1145/2847263.2847276},
	Isbn = {978-1-4503-3856-1},
	Keywords = {FPGA-NN, convolutional neural networks, fpga, opencl, optimization},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {16--25},
	Publisher = {ACM},
	Series = {FPGA '16},
	Title = {Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks},
	Url = {http://doi.acm.org/10.1145/2847263.2847276},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2847263.2847276},
	Bdsk-Url-2 = {https://doi.org/10.1145/2847263.2847276}}

@inproceedings{7760779,
	Author = {Wenlai Zhao and Haohuan Fu and W. Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	Booktitle = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Date-Added = {2018-06-22 10:44:51 +0000},
	Date-Modified = {2018-09-26 08:03:08 +0000},
	Doi = {10.1109/ASAP.2016.7760779},
	Keywords = {FPGA-NN, field programmable gate arrays;floating point arithmetic;neural nets;32bit floating-point arithmetic;FPGA-based framework;bandwidth resources;convolutional neural networks;hardware resources;streaming datapath;Bandwidth;Computational modeling;Convolution;Field programmable gate arrays;Neural networks;Runtime;Training},
	Month = {July},
	Pages = {107-114},
	Title = {F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASAP.2016.7760779}}

@inproceedings{374468,
	Author = {A. V. Krishnamoorthy and S. A. Brodsky and C. C. Guest and G. C. Marsden and M. Blume and G. Yayla and J. Merckle and S. C. Esener},
	Booktitle = {Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on},
	Date-Added = {2018-06-22 10:44:13 +0000},
	Date-Modified = {2018-09-26 08:04:11 +0000},
	Doi = {10.1109/ICNN.1994.374468},
	Keywords = {BNN, learning (artificial intelligence);neural chips;neural net architecture;optical neural nets;3D optoelectronic neural system;D-STOP system;Accelerated Learning;connectivity;content addressable network learning algorithm;discrete learning algorithm;dual-scale topology optoelectronic processor neural network;fully-parallel neural networks;generic gradient-descent learning rules;hardware efficient learning;optoelectronic hardware tradeoffs;scalable optically interconnected neural network architecture;Backpropagation algorithms;Hardware;Integrated circuit interconnections;Neural networks;Neurons;Neurotransmitters;Optical interconnections;Optical network units;Optical transmitters;Power system interconnection},
	Month = {Jun},
	Pages = {1998-2003 vol.3},
	Title = {Hardware efficient learning on a 3-D optoelectronic neural system},
	Volume = {3},
	Year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICNN.1994.374468}}

@inproceedings{226962,
	Author = {S. A. Brodsky and C. C. Guest},
	Booktitle = {[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
	Date-Added = {2018-06-22 10:43:49 +0000},
	Date-Modified = {2018-06-22 10:43:58 +0000},
	Doi = {10.1109/IJCNN.1992.226962},
	Keywords = {BNN, backpropagation;content-addressable storage;optical neural nets;VLSI;backpropagation;content addressable networks;discrete mappings;error-free solution;fast convergence rate;initialization;zero error solutions;Backpropagation algorithms;Computer errors;Costs;Error correction;Hardware;Optical computing;Optical devices;Optical fiber networks;Optical network units;Very large scale integration},
	Month = {Jun},
	Pages = {352-357 vol.2},
	Title = {Content addressable networks for initialization of backpropagation with zero error solutions},
	Volume = {2},
	Year = {1992},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBtLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS1OTi9GLUNOTi0gQW4gRlBHQS1iYXNlZCBmcmFtZXdvcmsgZm9yIHRyYWluaW5nIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYk8RAngAAAAAAngAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9GLUNOTi0gQW4gRlBHQS1iYXMjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAHRlBHQS1OTgAAAgCDLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkZQR0EtTk46Ri1DTk4tIEFuIEZQR0EtYmFzZWQgZnJhbWV3b3JrIGZvciB0cmFpbmluZyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4AnABNAEYALQBDAE4ATgAtACAAQQBuACAARgBQAEcAQQAtAGIAYQBzAGUAZAAgAGYAcgBhAG0AZQB3AG8AcgBrACAAZgBvAHIAIAB0AHIAYQBpAG4AaQBuAGcAIABDAG8AbgB2AG8AbAB1AHQAaQBvAG4AYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAIFVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ZQR0EtTk4vRi1DTk4tIEFuIEZQR0EtYmFzZWQgZnJhbWV3b3JrIGZvciB0cmFpbmluZyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAJQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADEA==},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1992.226962}}

@inproceedings{5726804,
	Author = {S. A. Brodsky and C. C. Guest},
	Booktitle = {1990 IJCNN International Joint Conference on Neural Networks},
	Date-Added = {2018-06-22 10:43:21 +0000},
	Date-Modified = {2018-06-22 10:43:33 +0000},
	Doi = {10.1109/IJCNN.1990.137846},
	Keywords = {BNN, content-addressable storage;learning systems;neural nets;arbitrary bit-level significance;binary backpropagation;bit connection weights;content addressable memory;continuous backpropagation network learning model;local computation;pseudoanalog extension},
	Month = {June},
	Pages = {205-210 vol.3},
	Title = {Binary backpropagation in content addressable memory},
	Year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1990.137846}}

@article{31325,
	Author = {M. Verleysen and B. Sirletti and A. Vandemeulebroecke and P. G. A. Jespers},
	Date-Added = {2018-06-22 10:42:54 +0000},
	Date-Modified = {2018-06-22 10:43:03 +0000},
	Doi = {10.1109/31.31325},
	Issn = {0098-4094},
	Journal = {IEEE Transactions on Circuits and Systems},
	Keywords = {BNN, CMOS integrated circuits;VLSI;content-addressable storage;neural nets;Hopfield neural network;VLSI;content-addressable memory;fully interconnected neural network;high-storage capacity;implementation;learning algorithm;neural networks;optimization;pattern recognition;programming algorithm;retrieval capabilities;small area;speed capabilities;CADCAM;Cams;Computer aided manufacturing;Content based retrieval;Hopfield neural networks;Integrated circuit interconnections;Neural networks;Neurons;Pattern recognition;Very large scale integration},
	Month = {May},
	Number = {5},
	Pages = {762-766},
	Title = {A high-storage capacity content-addressable memory and its learning algorithm},
	Volume = {36},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1109/31.31325}}

@article{2014arXiv1409.5185L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.5185L},
	Archiveprefix = {arXiv},
	Author = {{Lee}, C.-Y. and {Xie}, S. and {Gallagher}, P. and {Zhang}, Z. and {Tu}, Z.},
	Date-Added = {2018-06-11 04:57:33 +0000},
	Date-Modified = {2018-06-11 04:57:33 +0000},
	Eprint = {1409.5185},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = sep,
	Primaryclass = {stat.ML},
	Title = {{Deeply-Supervised Nets}},
	Year = 2014}

@article{2013arXiv1306.0239T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1306.0239T},
	Archiveprefix = {arXiv},
	Author = {{Tang}, Y.},
	Date-Added = {2018-06-11 04:57:10 +0000},
	Date-Modified = {2018-06-11 04:57:10 +0000},
	Eprint = {1306.0239},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Deep Learning using Linear Support Vector Machines}},
	Year = 2013}

@article{2016arXiv160504711L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160504711L},
	Archiveprefix = {arXiv},
	Author = {{Li}, F. and {Zhang}, B. and {Liu}, B.},
	Date-Added = {2018-06-11 04:56:39 +0000},
	Date-Modified = {2018-06-11 04:56:39 +0000},
	Eprint = {1605.04711},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{Ternary Weight Networks}},
	Year = 2016}

@article{BALDOMINOS201838,
	Author = {Alejandro Baldominos and Yago Saez and Pedro Isasi},
	Date-Added = {2018-06-02 05:38:07 +0000},
	Date-Modified = {2018-06-02 05:38:25 +0000},
	Doi = {https://doi.org/10.1016/j.neucom.2017.12.049},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {CNN, Neuroevolution, Evolutionary algorithms, Convolutional neural networks, Automatic topology design, Genetic algorithms, Grammatical evolution},
	Pages = {38 - 52},
	Title = {Evolutionary convolutional neural networks: An application to handwriting recognition},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231217319112},
	Volume = {283},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217319112},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.12.049}}

@article{FERREIRA2018205,
	Author = {Martha Dais Ferreira and D{\'e}bora Cristina Corr{\^e}a and Luis Gustavo Nonato and Rodrigo Fernandes de Mello},
	Date-Added = {2018-06-02 05:37:45 +0000},
	Date-Modified = {2018-06-02 05:37:53 +0000},
	Doi = {https://doi.org/10.1016/j.eswa.2017.10.052},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {CNN, Convolutional neural network, Architecture assessment, Dynamical systems, Handwritten digit recognition, Face recognition, Object recognition},
	Pages = {205 - 217},
	Title = {Designing architectures of convolutional neural networks to solve practical problems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417417307340},
	Volume = {94},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417307340},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.10.052}}

@article{LI2018154,
	Author = {Guoqi Li and Lei Deng and Lei Tian and Haotian Cui and Wentao Han and Jing Pei and Luping Shi},
	Date-Added = {2018-06-02 05:36:48 +0000},
	Date-Modified = {2018-06-02 05:37:00 +0000},
	Doi = {https://doi.org/10.1016/j.neucom.2017.06.058},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {BNN, Deep learning, Neural network applications, Discrete state transition, Discrete weight space},
	Pages = {154 - 162},
	Title = {Training deep neural networks with discrete state transition},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231217311864},
	Volume = {272},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217311864},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.06.058}}

@article{LIANG20181072,
	Author = {Shuang Liang and Shouyi Yin and Leibo Liu and Wayne Luk and Shaojun Wei},
	Date-Added = {2018-06-02 05:36:24 +0000},
	Date-Modified = {2018-06-02 05:36:30 +0000},
	Doi = {https://doi.org/10.1016/j.neucom.2017.09.046},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {BNN, Binarized neural network, Hardware accelerator, FPGA},
	Pages = {1072 - 1086},
	Title = {FP-BNN: Binarized neural network on FPGA},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231217315655},
	Volume = {275},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217315655},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.09.046}}

@article{SHOEMAKER1991231,
	Author = {Patrick A. Shoemaker and Michael J. Carlin and Randy L. Shimabukuro},
	Date-Added = {2018-06-02 05:35:50 +0000},
	Date-Modified = {2018-06-02 05:35:58 +0000},
	Doi = {https://doi.org/10.1016/0893-6080(91)90007-R},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {BNN, Neural networks, Learning algorithms, Back propagation, Trinary, VLSI implementations, Nonvolatile weights},
	Number = {2},
	Pages = {231 - 241},
	Title = {Back propagation learning with trinary quantization of weight updates},
	Url = {http://www.sciencedirect.com/science/article/pii/089360809190007R},
	Volume = {4},
	Year = {1991},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/089360809190007R},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(91)90007-R}}

@article{DENG201849,
	Author = {Lei Deng and Peng Jiao and Jing Pei and Zhenzhi Wu and Guoqi Li},
	Date-Added = {2018-06-02 05:35:28 +0000},
	Date-Modified = {2018-06-02 05:35:34 +0000},
	Doi = {https://doi.org/10.1016/j.neunet.2018.01.010},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {BNN, GXNOR-Net, Discrete state transition, Ternary neural networks, Sparse binary networks},
	Pages = {49 - 58},
	Title = {GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework},
	Url = {http://www.sciencedirect.com/science/article/pii/S0893608018300108},
	Volume = {100},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608018300108},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2018.01.010}}

@article{2017arXiv170300810S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170300810S},
	Archiveprefix = {arXiv},
	Author = {{Shwartz-Ziv}, R. and {Tishby}, N.},
	Date-Added = {2018-06-02 03:06:54 +0000},
	Date-Modified = {2018-06-02 03:07:05 +0000},
	Eprint = {1703.00810},
	Journal = {ArXiv e-prints},
	Keywords = {DNN, Computer Science - Learning},
	Month = mar,
	Primaryclass = {cs.LG},
	Title = {{Opening the Black Box of Deep Neural Networks via Information}},
	Year = 2017}

@article{2016arXiv160606160Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160606160Z},
	Archiveprefix = {arXiv},
	Author = {{Zhou}, S. and {Wu}, Y. and {Ni}, Z. and {Zhou}, X. and {Wen}, H. and {Zou}, Y.},
	Date-Added = {2018-05-30 01:32:32 +0000},
	Date-Modified = {2018-08-31 09:56:59 +0000},
	Eprint = {1606.06160},
	Journal = {ArXiv e-prints},
	Keywords = {BNN, Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = jun,
	Title = {{DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients}},
	Year = 2016}

@article{2017arXiv170602379L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602379L},
	Annote = {Same paper as:

Towards a Deeper Understanding of Training Quantized Neural Networks
Published in https://www.padl.ws
Principled Approaches to Deep Learning
ICML 2017, Sydney, Australia
August 10, 2017
https://www.padl.ws/papers/Paper%2016.pdf},
	Archiveprefix = {arXiv},
	Author = {{Li}, H. and {De}, S. and {Xu}, Z. and {Studer}, C. and {Samet}, H. and {Goldstein}, T.},
	Date-Added = {2018-05-30 01:03:06 +0000},
	Date-Modified = {2018-11-28 15:14:52 +1300},
	Eprint = {1706.02379},
	Journal = {ArXiv e-prints},
	Keywords = {DNN, Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Training Quantized Nets: A Deeper Understanding}},
	Year = 2017}

@techreport{abdelouahab:hal-01695375,
	Author = {ABDELOUAHAB, Kamel and Pelcat, Maxime and Berry, Fran{\c c}ois and S{\'e}rot, Jocelyn},
	Date-Added = {2018-05-28 11:51:04 +0000},
	Date-Modified = {2018-05-28 11:51:35 +0000},
	Hal_Id = {hal-01695375},
	Hal_Version = {v2},
	Institution = {{Universit{\'e} Clermont Auvergne ; Institut Pascal, Clermont Ferrand ; IETR/INSA Rennes}},
	Keywords = {FPGA-NN, FPGA, CNN, HDL, Hardware},
	Month = Jan,
	Pdf = {https://hal.archives-ouvertes.fr/hal-01695375/file/hal-accelerating-cnn.pdf},
	Title = {{Accelerating CNN inference on FPGAs: A Survey}},
	Type = {Research Report},
	Url = {https://hal.archives-ouvertes.fr/hal-01695375},
	Year = {2018},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-01695375}}

@article{HASSABIS2017245,
	Author = {Demis Hassabis and Dharshan Kumaran and Christopher Summerfield and Matthew Botvinick},
	Date-Added = {2018-05-25 05:39:29 +0000},
	Date-Modified = {2018-05-25 05:39:29 +0000},
	Doi = {https://doi.org/10.1016/j.neuron.2017.06.011},
	Issn = {0896-6273},
	Journal = {Neuron},
	Keywords = {artificial intelligence, brain, cognition, neural network, learning},
	Number = {2},
	Pages = {245 - 258},
	Title = {Neuroscience-Inspired Artificial Intelligence},
	Url = {http://www.sciencedirect.com/science/article/pii/S0896627317305093},
	Volume = {95},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0896627317305093},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neuron.2017.06.011}}

@article{2016arXiv161200796K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200796K},
	Archiveprefix = {arXiv},
	Author = {{Kirkpatrick}, J. and {Pascanu}, R. and {Rabinowitz}, N. and {Veness}, J. and {Desjardins}, G. and {Rusu}, A.~A. and {Milan}, K. and {Quan}, J. and {Ramalho}, T. and {Grabska-Barwinska}, A. and {Hassabis}, D. and {Clopath}, C. and {Kumaran}, D. and {Hadsell}, R.},
	Date-Added = {2018-05-25 05:39:08 +0000},
	Date-Modified = {2018-05-25 05:39:08 +0000},
	Eprint = {1612.00796},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Overcoming catastrophic forgetting in neural networks}},
	Year = 2016}

@article{2015arXiv151004189N,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151004189N},
	Archiveprefix = {arXiv},
	Author = {{N{\o}kland}, A.},
	Date-Added = {2018-05-21 00:54:01 +0000},
	Date-Modified = {2018-05-21 00:54:01 +0000},
	Eprint = {1510.04189},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = oct,
	Primaryclass = {stat.ML},
	Title = {{Improving Back-Propagation by Adding an Adversarial Gradient}},
	Year = 2015}

@article{2015arXiv150303562C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150303562C},
	Archiveprefix = {arXiv},
	Author = {{Cheng}, Z. and {Soudry}, D. and {Mao}, Z. and {Lan}, Z.},
	Date-Added = {2018-05-19 11:32:04 +0000},
	Date-Modified = {2018-10-09 23:48:40 +0000},
	Eprint = {1503.03562},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	Month = mar,
	Title = {{Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation}},
	Year = 2015}

@article{2016arXiv161103530Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161103530Z},
	Archiveprefix = {arXiv},
	Author = {{Zhang}, C. and {Bengio}, S. and {Hardt}, M. and {Recht}, B. and {Vinyals}, O.},
	Date-Added = {2018-05-19 05:48:30 +0000},
	Date-Modified = {2018-05-19 05:48:30 +0000},
	Eprint = {1611.03530},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Understanding deep learning requires rethinking generalization}},
	Year = 2016}

@article{2017arXiv170107875A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107875A},
	Archiveprefix = {arXiv},
	Author = {{Arjovsky}, M. and {Chintala}, S. and {Bottou}, L.},
	Date-Added = {2018-05-19 04:48:30 +0000},
	Date-Modified = {2018-05-19 04:48:30 +0000},
	Eprint = {1701.07875},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = jan,
	Primaryclass = {stat.ML},
	Title = {{Wasserstein GAN}},
	Year = 2017}

@article{2015arXiv151202479M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151202479M},
	Archiveprefix = {arXiv},
	Author = {{Montavon}, G. and {Bach}, S. and {Binder}, A. and {Samek}, W. and {M{\"u}ller}, K.-R.},
	Date-Added = {2018-05-16 10:38:31 +0000},
	Date-Modified = {2018-05-16 10:38:31 +0000},
	Eprint = {1512.02479},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Explaining NonLinear Classification Decisions with Deep Taylor Decomposition}},
	Year = 2015}

@article{journal.pone.0130140,
	Abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	Author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Gr{\'e}goire AND Klauschen, Frederick AND M{\"u}ller, Klaus-Robert AND Samek, Wojciech},
	Date-Added = {2018-05-16 09:58:57 +0000},
	Date-Modified = {2018-05-16 09:59:52 +0000},
	Doi = {10.1371/journal.pone.0130140},
	Journal = {PLOS ONE},
	Keywords = {DNN, Taylor Series, MNIST, ImageNet, Pixel-wise},
	Month = {07},
	Number = {7},
	Pages = {1-46},
	Publisher = {Public Library of Science},
	Title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
	Url = {https://doi.org/10.1371/journal.pone.0130140},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0130140}}

@article{kim51bitwise,
	Author = {Kim, Minje and Smaragdis, Paris},
	Date-Added = {2018-05-16 00:48:16 +0000},
	Date-Modified = {2018-10-10 08:59:49 +0000},
	Journal = {Urbana},
	Keywords = {BNN, QaD, IBM, Signals, Denoise},
	Pages = {61801},
	Title = {Bitwise Neural Networks for Efficient Single-Channel Source Separation},
	Volume = {51},
	Year = {2018}}

@article{2018arXiv180409154G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180409154G},
	Archiveprefix = {arXiv},
	Author = {{Giacomello}, E. and {Lanzi}, P.~L. and {Loiacono}, D.},
	Date-Added = {2018-05-15 11:53:51 +0000},
	Date-Modified = {2018-05-15 11:53:51 +0000},
	Eprint = {1804.09154},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
	Month = apr,
	Primaryclass = {cs.LG},
	Title = {{DOOM Level Generation using Generative Adversarial Networks}},
	Year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCBLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vT24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliTxECzAAAAAACzAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H09uIFBpeGVsLVdpc2UgRXhwbCNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAJcvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46T24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliAAAOAM4AZgBPAG4AIABQAGkAeABlAGwALQBXAGkAcwBlACAARQB4AHAAbABhAG4AYQB0AGkAbwBuAHMAIABmAG8AcgAgAE4AbwBuAC0ATABpAG4AZQBhAHIAIABDAGwAYQBzAHMAaQBmAGkAZQByACAARABlAGMAaQBzAGkAbwBuAHMAIABiAHkAIABMAGEAeQBlAHIALQBXAGkAcwBlACAAUgBlAGwAZQB2AGEAbgBjAGUAIABQAHIAbwBwAGEAZwBhAHQAaQBvAG4ALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAJVVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL09uIFBpeGVsLVdpc2UgRXhwbGFuYXRpb25zIGZvciBOb24tTGluZWFyIENsYXNzaWZpZXIgRGVjaXNpb25zIGJ5IExheWVyLVdpc2UgUmVsZXZhbmNlIFByb3BhZ2F0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAqAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAN4},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OL05vbi1sb2NhbCBOZXVyYWwgTmV0d29ya3MuYmliTxEBrAAAAAABrAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HU5vbi1sb2NhbCBOZXVyYWwgTmV0d29ya3MuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANDTk4AAAIATy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk46Tm9uLWxvY2FsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4APAAdAE4AbwBuAC0AbABvAGMAYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAE1Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi9Ob24tbG9jYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAYAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIQ}}

@inproceedings{5234726,
	Author = {X. Chen and Q. Ma and T. Alkharobi},
	Booktitle = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
	Date-Added = {2018-05-15 11:27:59 +0000},
	Date-Modified = {2018-05-15 11:27:59 +0000},
	Doi = {10.1109/ICCSIT.2009.5234726},
	Keywords = {Fourier series;data mining;pattern clustering;radial basis function networks;Fourier component neural network;Gauss series clustering neural network;Taylor component neural network;Taylor series;mining;radial basis function neuron;Computer science;Educational institutions;Electronic mail;Gaussian processes;Information science;Input variables;Neural networks;Neurons;Taylor series;Transfer functions;Fourier component neural network;Gauss series Clustering neural network;Taylor component neural network;Taylor series neural network;prediction;stock price},
	Month = {Aug},
	Pages = {291-294},
	Title = {New neural networks based on Taylor series and their research},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCSIT.2009.5234726}}

@inproceedings{li2013,
	Abstract = {This study adopts popular back-propagation neural network to make one-period-ahead prediction of the stock price. A model based on Taylor series by using both fundamental and technical indicators EPS and MACD as input data is built for an empirical study. Leading Taiwanese companies in non-hi-tech industry such as Formosa Plastics, Yieh Phui Steel, Evergreen Marine, and Chang Hwa Bank are picked as targets to analyze their reasonable prices and moving trends. The performance of this model shows remarkable return and high accuracy in making long/short strategies.},
	Author = {Li, Jung Bin and Wu, Chien Ho},
	Booktitle = {Innovation for Applied Science and Technology},
	Date-Added = {2018-05-15 11:27:42 +0000},
	Date-Modified = {2018-05-15 11:27:42 +0000},
	Doi = {10.4028/www.scientific.net/AMM.284-287.3020},
	Keywords = {Neural Network (NN), BPN, Taylor Series, Price Forecast},
	Month = {3},
	Pages = {3020--3024},
	Publisher = {Trans Tech Publications},
	Series = {Applied Mechanics and Materials},
	Title = {An Efficient Neural Network Model with Taylor Series-Based Data Pre-Processing for Stock Price Forecast},
	Volume = {284},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.4028/www.scientific.net/AMM.284-287.3020}}

@article{2016arXiv160205897D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160205897D},
	Archiveprefix = {arXiv},
	Author = {{Daniely}, A. and {Frostig}, R. and {Singer}, Y.},
	Date-Added = {2018-05-15 11:27:25 +0000},
	Date-Modified = {2018-05-15 11:27:25 +0000},
	Eprint = {1602.05897},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Statistics - Machine Learning},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity}},
	Year = 2016}

@article{10.1002-mma.2641,
	Abstract = {This paper focuses on learning algorithms for approximating functional data that are chosen from some Hilbert spaces. An effective algorithm, called Hilbert parallel overrelaxation backpropagation (HPORBP) algorithm, is proposed for training the Hilbert feedforward neural networks that are extensions of feedforward neural networks from Euclidean space to some Hilbert spaces. Furthermore, the convergence of the iterative HPORBP algorithm is analyzed, and a deterministic convergence theorem is proposed for the HPORBP algorithm on the basis of the perturbation results of Mangasarian and Solodov. Some experimental results of learning functional data on some Hilbert spaces illustrate the convergence theorem and show that the proposed HPORBP algorithm has a better accuracy than the Hilbert backpropagation algorithm. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
	Author = {Zhao Jianwei},
	Date-Added = {2018-05-15 11:26:53 +0000},
	Date-Modified = {2018-10-10 08:58:50 +0000},
	Doi = {10.1002/mma.2641},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mma.2641},
	Journal = {Mathematical Methods in the Applied Sciences},
	Keywords = {functional data, feedforward neural network, learning algorithm, convergence},
	Month = {November},
	Number = {17},
	Pages = {2111-2121},
	Title = {Functional data learning by Hilbert feedforward neural networks},
	Url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Volume = {35},
	Year = {2012},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Bdsk-Url-2 = {https://doi.org/10.1002/mma.2641}}

@article{CASTRO2005967,
	Abstract = {The Fuzzy ARTMAP algorithm has been proven to be one of the premier neural network architectures for classification problems. One of the properties of Fuzzy ARTMAP, which can be both an asset and a liability, is its capacity to produce new nodes (templates) on demand to represent classification categories. This property allows Fuzzy ARTMAP to automatically adapt to the database without having to a priori specify its network size. On the other hand, it has the undesirable side effect that large databases might produce a large network size (node proliferation) that can dramatically slow down the training speed of the algorithm. To address the slow convergence speed of Fuzzy ARTMAP for large database problems, we propose the use of space-filling curves, specifically the Hilbert space-filling curves (HSFC). Hilbert space-filling curves allow us to divide the problem into smaller sub-problems, each focusing on a smaller than the original dataset. For learning each partition of data, a different Fuzzy ARTMAP network is used. Through this divide-and-conquer approach we are avoiding the node proliferation problem, and consequently we speedup Fuzzy ARTMAP's training. Results have been produced for a two-class, 16-dimensional Gaussian data, and on the Forest database, available at the UCI repository. Our results indicate that the Hilbert space-filling curve approach reduces the time that it takes to train Fuzzy ARTMAP without affecting the generalization performance attained by Fuzzy ARTMAP trained on the original large dataset. Given that the resulting smaller datasets that the HSFC approach produces can independently be learned by different Fuzzy ARTMAP networks, we have also implemented and tested a parallel implementation of this approach on a Beowulf cluster of workstations that further speeds up Fuzzy ARTMAP's convergence to a solution for large database problems.},
	Author = {Jos{\'e} Castro and Michael Georgiopoulos and Ronald Demara and Avelino Gonzalez},
	Date-Added = {2018-05-15 11:26:41 +0000},
	Date-Modified = {2018-05-15 11:26:41 +0000},
	Doi = {https://doi.org/10.1016/j.neunet.2005.01.007},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Fuzzy-ARTMAP, Hilbert space-filling curve, Data mining, Data-partitioning},
	Number = {7},
	Pages = {967 - 984},
	Title = {Data-partitioning using the Hilbert space filling curves: Effect on the speed of convergence of Fuzzy ARTMAP for large database problems},
	Url = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Volume = {18},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2005.01.007}}

@article{2018arXiv180501934C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180501934C},
	Archiveprefix = {arXiv},
	Author = {{Chen}, C. and {Chen}, Q. and {Xu}, J. and {Koltun}, V.},
	Date-Added = {2018-05-15 11:26:25 +0000},
	Date-Modified = {2018-05-15 11:26:25 +0000},
	Eprint = {1805.01934},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Learning},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{Learning to See in the Dark}},
	Year = 2018}

@inproceedings{2017arXiv171111294L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171111294L},
	Archiveprefix = {arXiv},
	Author = {{Lin}, X. and {Zhao}, C. and {Pan}, W.},
	Booktitle = {31st Conference on Neural Information Processing Systems},
	Date-Added = {2018-05-15 11:26:04 +0000},
	Date-Modified = {2018-11-26 21:20:19 +1300},
	Eprint = {1711.11294},
	Journal = {NIPS 2017},
	Keywords = {BNN; Computer Science; Learning; Statistics; Machine Learning},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Towards Accurate Binary Convolutional Neural Network}},
	Year = 2017}

@article{2017arXiv170906206Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170906206Y},
	Archiveprefix = {arXiv},
	Author = {{Yin}, S. and {Venkataramanaiah}, S.~K. and {Chen}, G.~K. and {Krishnamurthy}, R. and {Cao}, Y. and {Chakrabarti}, C. and {Seo}, J.-s.},
	Date-Added = {2018-05-15 11:24:06 +0000},
	Date-Modified = {2018-10-11 08:40:58 +0000},
	Eprint = {1709.06206},
	Journal = {ArXiv e-prints},
	Keywords = {SNN; Neural and Evolutionary Computing},
	Month = sep,
	Title = {{Algorithm and Hardware Design of Discrete-Time Spiking Neural Networks Based on Back Propagation with Binary Activations}},
	Year = 2017}

@inproceedings{5949465,
	Author = {J. Ranhel and C. V. Lima and J. L. R. Monteiro and J. E. Kogler and M. L. Netto},
	Booktitle = {2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI)},
	Date-Added = {2018-05-15 11:24:06 +0000},
	Date-Modified = {2018-05-15 11:24:06 +0000},
	Doi = {10.1109/FOCI.2011.5949465},
	Keywords = {neural nets;storage management;PNG attributes;binary counters;bistable memory;parallel computing;polychronous group;spiking neural network;Biological system modeling;Computational modeling;Delay;Fires;Firing;Kernel;Neurons;bistable neural memory;neural counters;neural hierarchical organization;neural stack counter;polychronization;spiking neural networks},
	Month = {April},
	Pages = {66-73},
	Title = {Bistable memory and binary counters in spiking neural network},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCI.2011.5949465}}

@conference{2016arXiv161105128Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161105128Y},
	Archiveprefix = {arXiv},
	Author = {{Yang}, T.-J. and {Chen}, Y.-H. and {Sze}, V.},
	Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-05-11 02:51:58 +0000},
	Date-Modified = {2018-11-26 20:59:17 +1300},
	Eprint = {1611.05128},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Computer Vision; Pattern Recognition},
	Month = jul,
	Primaryclass = {cs.CV},
	Title = {{Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning}},
	Year = 2017}

@conference{Chen2018UnderstandingTL,
	Author = {Yu-hsin Chen and Tien-Ju Yang and Joel S. Emer and Vivienne Sze},
	Booktitle = {SysML Conference},
	Date-Added = {2018-05-11 00:57:52 +0000},
	Date-Modified = {2018-11-26 19:52:57 +1300},
	Keywords = {NN, DNN, Hardware, FPGA, ASIC},
	Month = {February},
	Title = {Understanding the Limitations of Existing Energy-Efficient Design Approaches for Deep Neural Networks},
	Year = {2018}}

@inproceedings{2017arXiv170309039S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170309039S},
	Archiveprefix = {arXiv},
	Author = {{Sze}, V. and {Chen}, Y.-H. and {Yang}, T.-J. and {Emer}, J.},
	Booktitle = {Proceedings of the IEEE},
	Date-Added = {2018-05-10 05:28:12 +0000},
	Date-Modified = {2018-11-26 19:56:15 +1300},
	Eprint = {1703.09039},
	Journal = {Proceedings of the IEEE},
	Keywords = {NN, Computer Science; Computer Vision; Pattern Recognition},
	Month = dec,
	Number = {12},
	Primaryclass = {cs.CV},
	Title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
	Volume = {105},
	Year = 2017}

@article{2016arXiv161207625S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161207625S},
	Archiveprefix = {arXiv},
	Author = {{Sze}, V. and {Chen}, Y.-H. and {Emer}, J. and {Suleiman}, A. and {Zhang}, Z.},
	Date-Added = {2018-05-10 05:20:22 +0000},
	Date-Modified = {2018-05-10 05:20:22 +0000},
	Eprint = {1612.07625},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{Hardware for Machine Learning: Challenges and Opportunities}},
	Year = 2016}

@article{2018arXiv180403230Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403230Y},
	Archiveprefix = {arXiv},
	Author = {{Yang}, T.-J. and {Howard}, A. and {Chen}, B. and {Zhang}, X. and {Go}, A. and {Sze}, V. and {Adam}, H.},
	Date-Added = {2018-05-10 04:40:55 +0000},
	Date-Modified = {2018-05-10 04:40:55 +0000},
	Eprint = {1804.03230},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications}},
	Year = 2018}

@article{7738524,
	Author = {Y. H. Chen and T. Krishna and J. S. Emer and V. Sze},
	Date-Added = {2018-05-10 03:54:03 +0000},
	Date-Modified = {2018-10-17 00:13:32 +0000},
	Doi = {10.1109/JSSC.2016.2616357},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {CNN; DRAM chips;data flow computing;energy conservation;feedforward neural nets;learning (artificial intelligence);neural net architecture;power aware computing;reconfigurable architectures;AI systems;AlexNet;CNN shapes;DRAM accesses;Eyeriss;MAC;RS dataflow reconfiguration;;convolutional layers;data movement energy cost;dataflow processing;deep convolutional neural networks;energy efficiency;energy-efficient reconfigurable accelerator;multiply and accumulation;off-chip DRAM;reconfiguring architecture;row stationary;spatial architecture;Clocks;Computer architecture;Hardware;Neural networks;Random access memory;Shape;Throughput;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
	Month = {Jan},
	Number = {1},
	Pages = {127-138},
	Title = {Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
	Volume = {52},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2016.2616357}}

@article{2014arXiv1412.6980K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Ba}, J.},
	Date-Added = {2018-05-09 11:47:33 +0000},
	Date-Modified = {2018-05-09 11:47:33 +0000},
	Eprint = {1412.6980},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Adam: A Method for Stochastic Optimization}},
	Year = 2014}

@inproceedings{8302078,
	Author = {X. Xu and J. Amaro and S. Caulfield and A. Forembski and G. Falcao and D. Moloney},
	Booktitle = {2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
	Date-Added = {2018-05-09 11:20:24 +0000},
	Date-Modified = {2018-09-26 08:02:09 +0000},
	Doi = {10.1109/CISP-BMEI.2017.8302078},
	Keywords = {computer vision;convolution;feedforward neural nets;image classification;image processing;multiprocessing systems;object recognition;stereo image processing;CNN;3D volumetric representation;CNNs;Movidius Neural Compute Stick;USB;VOLA;Volumetric Accelerator;computational requirements;computer vision;dedicated CNN hardware blocks;low-power processing unit;synthetic 3D voxelized point-clouds generation method;trained model;training data;volumetric data;voxelized point-clouds classification;Computational modeling;Graphics processing units;Object recognition;Solid modeling;Task analysis;Three-dimensional displays;Training;Convolutional Neural Networks;Embedded Systems;Point-clouds},
	Month = {Oct},
	Pages = {1-7},
	Title = {Convolutional neural network on neural compute stick for voxelized point-clouds classification},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CISP-BMEI.2017.8302078}}

@article{2015arXiv151003009L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151003009L},
	Archiveprefix = {arXiv},
	Author = {{Lin}, Z. and {Courbariaux}, M. and {Memisevic}, R. and {Bengio}, Y.},
	Date-Added = {2018-05-09 11:05:15 +0000},
	Date-Modified = {2018-05-09 11:05:15 +0000},
	Eprint = {1510.03009},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = oct,
	Primaryclass = {cs.LG},
	Title = {{Neural Networks with Few Multiplications}},
	Year = 2015}

@inproceedings{7929192,
	Author = {E. Nurvitadhi and D. Sheffield and Jaewoong Sim and A. Mishra and G. Venkatesh and D. Marr},
	Booktitle = {2016 International Conference on Field-Programmable Technology (FPT)},
	Date-Added = {2018-05-06 09:18:40 +0000},
	Date-Modified = {2018-10-22 08:34:35 +0000},
	Doi = {10.1109/FPT.2016.7929192},
	Keywords = {BNN, application specific integrated circuits;field programmable gate arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria 10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural networks;deep neural network;hardware acceleration;Biological neural networks;Field programmable gate arrays;Graphics processing units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep learning;FPGA;GPU;binarized neural networks;data analytics;hardware accelerator},
	Month = {Dec},
	Pages = {77-84},
	Title = {Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2016.7929192}}

@inproceedings{Zhao:2017:ABC:3020078.3021741,
	Acmid = {3021741},
	Address = {New York, NY, USA},
	Author = {Zhao, Ritchie and Song, Weinan and Zhang, Wentao and Xing, Tianwei and Lin, Jeng-Hau and Srivastava, Mani and Gupta, Rajesh and Zhang, Zhiru},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-05-06 08:57:34 +0000},
	Date-Modified = {2018-05-06 08:57:34 +0000},
	Doi = {10.1145/3020078.3021741},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGAs, binarized, binarized convolutional networks, deep learning, high-level synthesis, reconfigurable computing},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {15--24},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs},
	Url = {http://doi.acm.org/10.1145/3020078.3021741},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021741},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021741}}

@article{2016arXiv160305279R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160305279R},
	Archiveprefix = {arXiv},
	Author = {{Rastegari}, M. and {Ordonez}, V. and {Redmon}, J. and {Farhadi}, A.},
	Date-Added = {2018-05-06 05:43:43 +0000},
	Date-Modified = {2018-10-25 23:49:22 +0000},
	Eprint = {1603.05279},
	Journal = {European Conference on Computer Vision},
	Keywords = {BNN; Computer Vision; Pattern Recognition},
	Month = October,
	Primaryclass = {cs.CV},
	Title = {{XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks}},
	Year = 2016}

@article{2016arXiv160106071K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160106071K},
	Archiveprefix = {arXiv},
	Author = {{Kim}, M. and {Smaragdis}, P.},
	Date-Added = {2018-05-06 05:27:19 +0000},
	Date-Modified = {2018-05-06 05:27:19 +0000},
	Eprint = {1601.06071},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	Month = jan,
	Primaryclass = {cs.LG},
	Title = {{Bitwise Neural Networks}},
	Year = 2016}

@inbook{Baez2011,
	Abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a ``cobordism'': a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and ``quantum topology''. But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of ``closed symmetric monoidal category''. We assume no prior knowledge of category theory, proof theory or computer science.},
	Address = {Berlin, Heidelberg},
	Author = {Baez, J. and Stay, M.},
	Booktitle = {New Structures for Physics},
	Date-Added = {2018-04-29 11:38:03 +0000},
	Date-Modified = {2018-04-29 11:38:23 +0000},
	Doi = {10.1007/978-3-642-12821-9_2},
	Editor = {Coecke, Bob},
	Isbn = {978-3-642-12821-9},
	Keywords = {Category Theory, Physics, Logic, Computability},
	Pages = {95--172},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Physics, Topology, Logic and Computation: A Rosetta Stone},
	Url = {https://doi.org/10.1007/978-3-642-12821-9_2},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-12821-9_2}}

@article{2017arXiv170404865J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170404865J},
	Archiveprefix = {arXiv},
	Author = {{Juefei-Xu}, F. and {Naresh Boddeti}, V. and {Savvides}, M.},
	Date-Added = {2018-04-28 05:48:26 +0000},
	Date-Modified = {2018-04-28 05:48:26 +0000},
	Eprint = {1704.04865},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking}},
	Year = 2017}

@article{5607329,
	Abstract = {This paper presents the development and implementation of a generalized backpropagation multilayer perceptron (MLP) architecture described in VLSI hardware description language (VHDL). The development of hardware platforms has been complicated by the high hardware cost and quantity of the arithmetic operations required in online artificial neural networks (ANNs), i.e., general purpose ANNs with learning capability. Besides, there remains a dearth of hardware platforms for design space exploration, fast prototyping, and testing of these networks. Our general purpose architecture seeks to fill that gap and at the same time serve as a tool to gain a better understanding of issues unique to ANNs implemented in hardware, particularly using field programmable gate array (FPGA). The challenge is thus to find an architecture that minimizes hardware costs, while maximizing performance, accuracy, and parameterization. This work describes a platform that offers a high degree of parameterization, while maintaining generalized network design with performance comparable to other hardware-based MLP implementations. Application of the hardware implementation of ANN with backpropagation learning algorithm for a realistic application is also presented.},
	Author = {A. Gomperts and A. Ukil and F. Zurfluh},
	Date-Added = {2018-04-28 04:52:24 +0000},
	Date-Modified = {2018-04-28 04:52:24 +0000},
	Doi = {10.1109/TII.2010.2085006},
	Issn = {1551-3203},
	Journal = {IEEE Transactions on Industrial Informatics},
	Keywords = {backpropagation;field programmable gate arrays;hardware description languages;multilayer perceptrons;FPGA;VLSI hardware description language;arithmetic operation;artificial neural network;backpropagation multilayer perceptron;fast prototyping;field programmable gate array;general purpose neural network;hardware-based MLP;learning capability;online application;space exploration;Backpropagation;NIR spectra calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware implementation;multilayer perceptron;neural network;spectroscopy},
	Month = {Feb},
	Number = {1},
	Pages = {78-89},
	Title = {Development and Implementation of Parameterized FPGA-Based General Purpose Neural Networks for Online Applications},
	Volume = {7},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TII.2010.2085006}}

@inproceedings{Sun:2018:FPR:3201607.3201741,
	Acmid = {3201741},
	Address = {Piscataway, NJ, USA},
	Author = {Sun, Xiaoyu and Peng, Xiaochen and Chen, Pai-Yu and Liu, Rui and Seo, Jae-sun and Yu, Shimeng},
	Booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
	Date-Added = {2018-04-28 04:34:43 +0000},
	Date-Modified = {2018-10-15 10:21:08 +0000},
	Keywords = {BNN, P-BNN, CSM, MNIST},
	Location = {Jeju, Republic of Korea},
	Numpages = {6},
	Pages = {574--579},
	Publisher = {IEEE Press},
	Series = {ASPDAC '18},
	Title = {Fully Parallel RRAM Synaptic Array for Implementing Binary Neural Network with (+1, -1) Weights and (+1, 0) Neurons},
	Url = {http://dl.acm.org/citation.cfm?id=3201607.3201741},
	Year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3201607.3201741}}

@article{2018arXiv180200904L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200904L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y. and {Ren}, F.},
	Date-Added = {2018-04-28 04:09:52 +0000},
	Date-Modified = {2018-10-11 08:39:19 +0000},
	Eprint = {1802.00904},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science; Computer Vision; Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning}},
	Year = 2018}

@conference{8541786,
	Adsurl = {http://hdl.handle.net/1854/LU-8541786},
	Archiveprefix = {arXiv},
	Author = {{Leroux}, S. and {Bohez}, S. and {Verbelen}, T. and {Vankeirsbilck}, B. and {Simoens}, P. and {Dhoedt}, B.},
	Booktitle = {31st Conference on Neural Information Processing Systems},
	Date-Added = {2018-04-28 03:56:37 +0000},
	Date-Modified = {2018-12-06 11:17:15 +1300},
	Eprint = {8541786},
	Journal = {NIPS 2017},
	Keywords = {BNN; Computer Science; Neural and Evolutionary Computing; Computer Vision; Pattern Recognition},
	Month = Dec,
	Organization = {NIPS},
	Pages = {1-4},
	Title = {{Transfer Learning with Binary Neural Networks}},
	Year = 2017}

@article{2017arXiv170905306G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170905306G},
	Archiveprefix = {arXiv},
	Author = {{Guan}, T. and {Zeng}, X. and {Seok}, M.},
	Date-Added = {2018-04-28 03:26:42 +0000},
	Date-Modified = {2018-10-11 08:41:32 +0000},
	Eprint = {1709.05306},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Neural and Evolutionary Computing},
	Month = sep,
	Title = {{Recursive Binary Neural Network Learning Model with 2.28b/Weight Storage Requirement}},
	Year = 2017}

@article{2015arXiv150201852H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150201852H},
	Archiveprefix = {arXiv},
	Author = {{He}, K. and {Zhang}, X. and {Ren}, S. and {Sun}, J.},
	Date-Added = {2018-04-28 02:37:55 +0000},
	Date-Modified = {2018-04-28 02:37:55 +0000},
	Eprint = {1502.01852},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Learning},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
	Year = 2015}

@inproceedings{McDanel:2017:EBN:3108009.3108031,
	Acmid = {3108031},
	Address = {USA},
	Author = {McDanel, Bradley and Teerapittayanon, Surat and Kung, H.T.},
	Booktitle = {Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks},
	Date-Added = {2018-04-28 01:31:18 +0000},
	Date-Modified = {2018-04-28 01:34:56 +0000},
	Isbn = {978-0-9949886-1-4},
	Keywords = {BNN, eBNN, CNN, MNIST, CIFAR},
	Location = {Uppsala, Sweden},
	Numpages = {6},
	Pages = {168--173},
	Publisher = {Junction Publishing},
	Series = {EWSN \&\#8217;17},
	Title = {Embedded Binarized Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=3108009.3108031},
	Year = {2017},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3108009.3108031}}

@article{2016arXiv160907061H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160907061H},
	Archiveprefix = {arXiv},
	Author = {{Hubara}, I. and {Courbariaux}, M. and {Soudry}, D. and {El-Yaniv}, R. and {Bengio}, Y.},
	Date-Added = {2018-04-27 00:08:19 +0000},
	Date-Modified = {2018-12-02 19:10:57 +1300},
	Eprint = {1609.07061},
	Journal = {Journal Of Machine Learning Research 18},
	Keywords = {NN, Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = {January},
	Number = {1},
	Pages = {6869-6898},
	Title = {{Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations}},
	Volume = {18},
	Year = 2017}

@inproceedings{Tang2017HowTT,
	Author = {Wei Tang and Gang Hua and Liang Wang},
	Booktitle = {AAAI},
	Date-Added = {2018-04-26 22:22:30 +0000},
	Date-Modified = {2018-04-26 22:22:52 +0000},
	Keywords = {BNN, NN, Training},
	Title = {How to Train a Compact Binary Neural Network with High Accuracy?},
	Year = {2017}}

@inproceedings{Wei:2017:ASA:3061639.3062207,
	Acmid = {3062207},
	Address = {New York, NY, USA},
	Articleno = {29},
	Author = {Wei, Xuechao and Yu, Cody Hao and Zhang, Peng and Chen, Youxiang and Wang, Yuxin and Hu, Han and Liang, Yun and Cong, Jason},
	Booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
	Date-Added = {2018-04-25 23:12:26 +0000},
	Date-Modified = {2018-04-26 22:23:20 +0000},
	Doi = {10.1145/3061639.3062207},
	Isbn = {978-1-4503-4927-7},
	Keywords = {CNN, FPGA, Linear Algebra, Systolic Array},
	Location = {Austin, TX, USA},
	Numpages = {6},
	Pages = {29:1--29:6},
	Publisher = {ACM},
	Series = {DAC '17},
	Title = {Automated Systolic Array Architecture Synthesis for High Throughput CNN Inference on FPGAs},
	Url = {http://doi.acm.org/10.1145/3061639.3062207},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3061639.3062207},
	Bdsk-Url-2 = {https://doi.org/10.1145/3061639.3062207}}

@article{1998adap.org..6001P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/1998adap.org..6001P},
	Author = {{Pang}, X. and {Werbos}, P.},
	Date-Added = {2018-04-24 23:59:42 +0000},
	Date-Modified = {2018-04-24 23:59:42 +0000},
	Journal = {Advances in Astrophysics},
	Keywords = {Adaptation, Noise, and Self-Organizing Systems, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Neurons and Cognition},
	Month = jun,
	Title = {{Neural network design for J function approximation in dynamic programming}},
	Year = 1998}

@article{2018arXiv180200438Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200438Z},
	Archiveprefix = {arXiv},
	Author = {{Zohouri}, H.~R. and {Podobas}, A. and {Matsuoka}, S.},
	Date-Added = {2018-04-24 23:57:57 +0000},
	Date-Modified = {2018-04-24 23:57:57 +0000},
	Eprint = {1802.00438},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{Combined Spatial and Temporal Blocking for High-Performance Stencil Computation on FPGAs Using OpenCL}},
	Year = 2018}

@article{2017arXiv170703049M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170703049M},
	Archiveprefix = {arXiv},
	Author = {{Mostafa}, H. and {Pedroni}, B. and {Sheik}, S. and {Cauwenberghs}, G.},
	Date-Added = {2018-04-24 23:57:22 +0000},
	Date-Modified = {2018-06-22 10:46:48 +0000},
	Eprint = {1707.03049},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA-NN, Computer Science - Neural and Evolutionary Computing},
	Month = jun,
	Title = {{Hardware-efficient on-line learning through pipelined truncated-error backpropagation in binary-state networks}},
	Year = 2017}

@article{2018arXiv180303790S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303790S},
	Archiveprefix = {arXiv},
	Author = {{Shen}, J. and {Qiao}, Y. and {Huang}, Y. and {Wen}, M. and {Zhang}, C.},
	Date-Added = {2018-04-24 23:57:08 +0000},
	Date-Modified = {2018-04-24 23:57:08 +0000},
	Eprint = {1803.03790},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Hardware Architecture},
	Month = mar,
	Title = {{Towards a Multi-array Architecture for Accelerating Large-scale Matrix Multiplication on FPGAs}},
	Year = 2018}

@article{2016arXiv161200694H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200694H},
	Archiveprefix = {arXiv},
	Author = {{Han}, S. and {Kang}, J. and {Mao}, H. and {Hu}, Y. and {Li}, X. and {Li}, Y. and {Xie}, D. and {Luo}, H. and {Yao}, S. and {Wang}, Y. and {Yang}, H. and {Dally}, W.~J.},
	Date-Added = {2018-04-24 10:49:59 +0000},
	Date-Modified = {2018-04-24 10:49:59 +0000},
	Eprint = {1612.00694},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language},
	Month = dec,
	Primaryclass = {cs.CL},
	Title = {{ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA}},
	Year = 2016}

@article{2011arXiv1107.1831H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1107.1831H},
	Archiveprefix = {arXiv},
	Author = {{Homescu}, C.},
	Date-Added = {2018-04-15 11:10:18 +0000},
	Date-Modified = {2018-04-15 11:10:18 +0000},
	Eprint = {1107.1831},
	Journal = {ArXiv e-prints},
	Keywords = {Quantitative Finance - Computational Finance},
	Month = jul,
	Primaryclass = {q-fin.CP},
	Title = {{Adjoints and Automatic (Algorithmic) Differentiation in Computational Finance}},
	Year = 2011}

@inproceedings{1188677,
	Author = {A. A. Gaffar and O. Mencer and W. Luk and P. Y. K. Cheung and N. Shirazi},
	Booktitle = {2002 IEEE International Conference on Field-Programmable Technology, 2002. (FPT). Proceedings.},
	Date-Added = {2018-04-15 10:59:42 +0000},
	Date-Modified = {2018-04-15 10:59:42 +0000},
	Doi = {10.1109/FPT.2002.1188677},
	Keywords = {FIR filters;VLSI;circuit CAD;circuit optimisation;data flow graphs;differentiation;digital filters;digital signal processing chips;discrete Fourier transforms;field programmable gate arrays;floating point arithmetic;high level synthesis;integrated circuit design;sensitivity analysis;DFT implementation;FIR filter implementation;FPGA;VLSI circuits;arithmetic operations;automatic bitwidth analysis;automatic differentiation;dataflow graph representation;discrete Fourier transform implementation;floating-point bitwidth analysis;floating-point designs;high-level programming;high-level synthesis;mathematical method;precision analysis;sensitivity analysis;user-defined numerical constraints;Arithmetic;Automatic programming;Circuits;Data analysis;Discrete Fourier transforms;Field programmable gate arrays;Finite impulse response filter;High level synthesis;Sensitivity analysis;Very large scale integration},
	Month = {Dec},
	Pages = {158-165},
	Title = {Floating-point bitwidth analysis via automatic differentiation},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2002.1188677}}

@inproceedings{Elliott-2009-beautiful-differentiation,
	Author = {Conal Elliott},
	Date-Added = {2018-04-13 05:07:34 +0000},
	Date-Modified = {2018-04-13 05:07:34 +0000},
	Journal = {International Conference on Functional Programming (ICFP)},
	Keywords = {Automatic Differentiation, Vector Spaces, Haskell},
	Month = sep,
	Number = {ICFP},
	Title = {Beautiful Differentiation},
	Url = {http://conal.net/papers/beautiful-differentiation/},
	Year = {2009},
	Bdsk-Url-1 = {http://conal.net/papers/beautiful-differentiation/}}

@article{Elliott-2017-compiling-to-categories,
	Articleno = {48},
	Author = {Conal Elliott},
	Date-Added = {2018-04-07 11:10:44 +0000},
	Date-Modified = {2018-04-07 11:10:44 +0000},
	Doi = {http://dx.doi.org/10.1145/3110271},
	Journal = {Proc. ACM Program. Lang.},
	Keywords = {Automatic Differentiation, Category Theory, Haskell},
	Month = sep,
	Number = {ICFP},
	Numpages = {24},
	Title = {Compiling To Categories},
	Url = {http://conal.net/papers/compiling-to-categories},
	Volume = {1},
	Year = {2017},
	Bdsk-Url-1 = {http://conal.net/papers/compiling-to-categories},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3110271}}

@inproceedings{Elliott-2018-ad-icfp,
	Abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm can be specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.
},
	Author = {Conal Elliott},
	Booktitle = {Proceedings of the ACM on Programming Languages (ICFP)},
	Date-Added = {2018-04-06 11:42:18 +0000},
	Date-Modified = {2018-12-03 19:46:22 +1300},
	Eprint = {1804.00746},
	Keywords = {Haskell, Automatic Differentiation, Math, Category Theory},
	Month = {March},
	Title = {The simple essence of automatic differentiation},
	Url = {http://conal.net/papers/essence-of-ad/},
	Volume = {abs/1804.00746},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.00746},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1804.00746}}

@article{Karczmarczuk2001,
	Abstract = {We present a purely functional implementation of the computational differentiation tools---the well known numeric (i.e., not symbolic) techniques which permit one to compute point-wise derivatives of functions defined by computer programs economically and exactly (with machine precision). We show how the use of lazy evaluation permits a transparent and elegant construction of the entire infinite tower of derivatives of higher order for any expressions present in the program. The formalism may be useful in various problems of scientific computing which often demand a hard and ungracious human preprocessing before writing the final code. Some concrete examples are given.},
	Author = {Karczmarczuk, Jerzy},
	Date-Added = {2018-04-06 04:46:21 +0000},
	Date-Modified = {2018-04-06 04:49:58 +0000},
	Day = {01},
	Doi = {10.1023/A:1011501232197},
	Issn = {1573-0557},
	Journal = {Higher-Order and Symbolic Computation},
	Keywords = {Mathematics, Derivates, Automatic Differentiation, Computer Science},
	Month = {Mar},
	Number = {1},
	Pages = {35--57},
	Title = {Functional Differentiation of Computer Programs},
	Url = {https://doi.org/10.1023/A:1011501232197},
	Volume = {14},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1011501232197}}

@article{2014arXiv1404.7456G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1404.7456G},
	Archiveprefix = {arXiv},
	Author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A.},
	Date-Added = {2018-04-04 09:50:26 +0000},
	Date-Modified = {2018-09-26 08:04:03 +0000},
	Eprint = {1404.7456},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Symbolic Computation, Statistics - Machine Learning, Automatic Differentiation, Automatic Differentiation, Automatic Differentiation, G.1.4, I.2.6},
	Month = apr,
	Primaryclass = {cs.LG},
	Title = {{Automatic Differentiation of Algorithms for Machine Learning}},
	Year = 2014}

@article{Gremse:2016aa,
	Abstract = {Many scientific problems such as classifier training or medical image reconstruction can be expressed as minimization of differentiable real-valued cost functions and solved with iterative gradient-based methods. Adjoint algorithmic differentiation (AAD) enables automated computation of gradients of such cost functions implemented as computer programs. To backpropagate adjoint derivatives, excessive memory is potentially required to store the intermediate partial derivatives on a dedicated data structure, referred to as the ``tape''. Parallelization is difficult because threads need to synchronize their accesses during taping and backpropagation. This situation is aggravated for many-core architectures, such as Graphics Processing Units (GPUs), because of the large number of light-weight threads and the limited memory size in general as well as per thread. We show how these limitations can be mediated if the cost function is expressed using GPU-accelerated vector and matrix operations which are recognized as intrinsic functions by our AAD software. We compare this approach with naive and vectorized implementations for CPUs. We use four increasingly complex cost functions to evaluate the performance with respect to memory consumption and gradient computation times. Using vectorization, CPU and GPU memory consumption could be substantially reduced compared to the naive reference implementation, in some cases even by an order of complexity. The vectorization allowed usage of optimized parallel libraries during forward and reverse passes which resulted in high speedups for the vectorized CPU version compared to the naive reference implementation. The GPU version achieved an additional speedup of 7.5 $\pm$4.4, showing that the processing power of GPUs can be utilized for AAD using this concept. Furthermore, we show how this software can be systematically extended for more complex problems such as nonlinear absorption reconstruction for fluorescence-mediated tomography.},
	An = {PMC4772124},
	Author = {Gremse, Felix and H{\"o}fter, Andreas and Razik, Lukas and Kiessling, Fabian and Naumann, Uwe},
	Date = {2016/03/01},
	Date-Added = {2018-04-04 08:10:03 +0000},
	Date-Modified = {2018-04-04 08:10:23 +0000},
	Db = {PMC},
	Doi = {10.1016/j.cpc.2015.10.027},
	Isbn = {0010-4655},
	J1 = {Comput Phys Commun},
	Journal = {Computer physics communications},
	Keywords = {GPU, Automatic Differentiation, Math, Neural Network},
	Month = {03},
	Pages = {300--311},
	Title = {GPU-Accelerated Adjoint Algorithmic Differentiation},
	Ty = {JOUR},
	U1 = {26941443{$[$}pmid{$]$}},
	Url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Volume = {200},
	Year = {2016},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Bdsk-Url-2 = {https://dx.doi.org/10.1016/j.cpc.2015.10.027}}

@article{6701396,
	Author = {D. Neil and S. C. Liu},
	Date-Added = {2018-03-20 09:15:46 +0000},
	Date-Modified = {2018-03-20 09:15:46 +0000},
	Doi = {10.1109/TVLSI.2013.2294916},
	Issn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {field programmable gate arrays;neural nets;CPU;MNIST handwritten digit classification;Minitaur;event-driven FPGA;event-driven neural network accelerator;field-programmable gate array-based system;neural networks;newsgroups classification data;robotics;spiking deep network;spiking network accelerator;Biological neural networks;Clocks;Computer architecture;Field programmable gate arrays;Mathematical model;Neurons;Performance evaluation;Deep belief networks;field programmable arrays;machine learning;neural networks;restricted Boltzmann machines;spiking neural networks},
	Month = {Dec},
	Number = {12},
	Pages = {2621-2628},
	Title = {Minitaur, an Event-Driven FPGA-Based Spiking Network Accelerator},
	Volume = {22},
	Year = {2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2013.2294916}}

@book{Make-Your-Own-Neural-Network,
	Adsurl = {https://www.amazon.co.uk/Make-Your-Own-Neural-Network/dp/1530826608},
	Author = {{Rashid}, Tariq},
	Date-Added = {2018-03-03 10:33:43 +0000},
	Date-Modified = {2018-03-03 10:33:43 +0000},
	Keywords = {Computer Science, Neural Network, Python},
	Month = March,
	Title = {{Make Your Own Neural Network}},
	Year = 2016}

@article{798320,
	Author = {C. Elliott},
	Date-Added = {2018-02-26 09:50:52 +0000},
	Date-Modified = {2018-02-26 09:50:52 +0000},
	Doi = {10.1109/32.798320},
	Issn = {0098-5589},
	Journal = {IEEE Transactions on Software Engineering},
	Keywords = {computer animation;multimedia computing;simulation languages;Fran;Haskell;declarative host language;embedded domain-specific vocabulary;embedded modeling language approach;growth;interactive 3D animation;interactive multimedia animation;modeled animation;motion;Animation;Automatic programming;Computer graphics;Computer languages;Domain specific languages;Functional programming;Programming profession;Shape;Vocabulary;Writing},
	Month = {May},
	Number = {3},
	Pages = {291-308},
	Title = {An embedded modeling language approach to interactive 3D and multimedia animation},
	Volume = {25},
	Year = {1999},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/32.798320}}

@techreport{Hudak94vs.ada,
	Author = {Paul Hudak and Mark P. Jones},
	Date-Added = {2018-02-26 09:38:35 +0000},
	Date-Modified = {2018-02-26 09:38:35 +0000},
	Keywords = {DARPA, haskell, Cpp, Awk},
	Title = {vs. Ada vs. C++ vs. Awk vs. ... An Experiment in Software Prototyping Productivity Available from http://www.haskell.org/papers/NSWC/jfp.ps},
	Year = {1994}}

@inproceedings{Totoo:2012:HVF:2364474.2364483,
	Acmid = {2364483},
	Address = {New York, NY, USA},
	Author = {Totoo, Prabhat and Deligiannis, Pantazis and Loidl, Hans-Wolfgang},
	Booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2018-02-26 08:51:00 +0000},
	Date-Modified = {2018-02-26 08:51:00 +0000},
	Doi = {10.1145/2364474.2364483},
	Isbn = {978-1-4503-1577-7},
	Keywords = {barnes-hut, f\#, haskell, n-body, parallelism, scala},
	Location = {Copenhagen, Denmark},
	Numpages = {12},
	Pages = {49--60},
	Publisher = {ACM},
	Series = {FHPC '12},
	Title = {Haskell vs. F\# vs. Scala: A High-level Language Features and Parallelism Support Comparison},
	Url = {http://doi.acm.org/10.1145/2364474.2364483},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2364474.2364483},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364483}}

@inproceedings{6209130,
	Author = {M. Fenwick and C. Sesanker and M. R. Schiller and H. J. Ellis and M. L. Hinman and J. Vyas and M. R. Gryk},
	Booktitle = {2012 Ninth International Conference on Information Technology - New Generations},
	Date-Added = {2018-02-26 08:28:41 +0000},
	Date-Modified = {2018-02-26 08:28:41 +0000},
	Doi = {10.1109/ITNG.2012.21},
	Keywords = {Java;LISP;bioinformatics;functional programming;learning (artificial intelligence);public domain software;software engineering;Haskell;Java;LISP;Python;algorithm development;bioinformatics;complex data operations;complex mathematical notions;data processing;functional computing;functional languages;functional programming accessibility;functional programming techniques;learning curve;learning resources;machine learning;multilanguage source-code repository;open-source Sandbox;scientific communities;software integration;Bioinformatics;Data visualization;Functional programming;Nuclear magnetic resonance;Proteins;Schedules;Transient analysis;Clojure;Haskell;Java;LISP;NMR;bioinformatics;functional-programming},
	Month = {April},
	Pages = {89-94},
	Title = {An Open-Source Sandbox for Increasing the Accessibility of Functional Programming to the Bioinformatics and Scientific Communities},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBrLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vR2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWJPEQJyAAAAAAJyAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fR2FuZyBvZiBHQU5zLSBHZW5lI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABURDR0FOAAACAIEvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RENHQU46R2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWIAAA4AnABNAEcAYQBuAGcAIABvAGYAIABHAEEATgBzAC0AIABHAGUAbgBlAHIAYQB0AGkAdgBlACAAQQBkAHYAZQByAHMAYQByAGkAYQBsACAATgBlAHQAdwBvAHIAawBzACAAdwBpAHQAaAAgAE0AYQB4AGkAbQB1AG0AIABNAGEAcgBnAGkAbgAgAFIAYQBuAGsAaQBuAGcALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAH9Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0RDR0FOL0dhbmcgb2YgR0FOcy0gR2VuZXJhdGl2ZSBBZHZlcnNhcmlhbCBOZXR3b3JrcyB3aXRoIE1heGltdW0gTWFyZ2luIFJhbmtpbmcuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACSAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAwg=},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ITNG.2012.21}}

@inproceedings{Pop:2010:ERH:1863543.1863595,
	Acmid = {1863595},
	Address = {New York, NY, USA},
	Author = {Pop, Iustin},
	Booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
	Date-Added = {2018-02-26 08:27:07 +0000},
	Date-Modified = {2018-02-26 08:27:07 +0000},
	Doi = {10.1145/1863543.1863595},
	Isbn = {978-1-60558-794-3},
	Keywords = {ganeti, haskell, python, system administration},
	Location = {Baltimore, Maryland, USA},
	Numpages = {6},
	Pages = {369--374},
	Publisher = {ACM},
	Series = {ICFP '10},
	Title = {Experience Report: Haskell As a Reagent: Results and Observations on the Use of Haskell in a Python Project},
	Url = {http://doi.acm.org/10.1145/1863543.1863595},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1863543.1863595},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1863543.1863595}}

@inproceedings{Nanz:2015:CSP:2818754.2818848,
	Acmid = {2818848},
	Address = {Piscataway, NJ, USA},
	Author = {Nanz, Sebastian and Furia, Carlo A.},
	Booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
	Date-Added = {2018-02-26 07:03:13 +0000},
	Date-Modified = {2018-04-15 11:11:54 +0000},
	Isbn = {978-1-4799-1934-5},
	Keywords = {Languages, Haskell, C++},
	Location = {Florence, Italy},
	Numpages = {11},
	Pages = {778--788},
	Publisher = {IEEE Press},
	Series = {ICSE '15},
	Title = {A Comparative Study of Programming Languages in Rosetta Code},
	Url = {http://dl.acm.org/citation.cfm?id=2818754.2818848},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2818754.2818848}}

@article{2010arXiv1009.0305R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1009.0305R},
	Archiveprefix = {arXiv},
	Author = {{Rabah}, S. and {Li}, J. and {Liu}, M. and {Lai}, Y.},
	Date-Added = {2018-02-22 23:19:52 +0000},
	Date-Modified = {2018-02-22 23:19:52 +0000},
	Eprint = {1009.0305},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Programming Languages, D.3},
	Month = sep,
	Primaryclass = {cs.PL},
	Title = {{Comparative Studies of 10 Programming Languages within 10 Diverse Criteria -- a Team 7 COMP6411-S10 Term Report}},
	Year = 2010}

@article{2017arXiv171109846J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171109846J},
	Archiveprefix = {arXiv},
	Author = {{Jaderberg}, M. and {Dalibard}, V. and {Osindero}, S. and {Czarnecki}, W.~M. and {Donahue}, J. and {Razavi}, A. and {Vinyals}, O. and {Green}, T. and {Dunning}, I. and {Simonyan}, K. and {Fernando}, C. and {Kavukcuoglu}, K.},
	Date-Added = {2018-02-21 06:49:04 +0000},
	Date-Modified = {2018-02-21 06:49:04 +0000},
	Eprint = {1711.09846},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Population Based Training of Neural Networks}},
	Year = 2017}

@article{2014arXiv1410.5401G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1410.5401G},
	Archiveprefix = {arXiv},
	Author = {{Graves}, A. and {Wayne}, G. and {Danihelka}, I.},
	Date-Added = {2018-02-21 06:07:10 +0000},
	Date-Modified = {2018-02-21 06:07:10 +0000},
	Eprint = {1410.5401},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing},
	Month = oct,
	Title = {{Neural Turing Machines}},
	Year = 2014}

@article{2015arXiv150308895S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150308895S},
	Archiveprefix = {arXiv},
	Author = {{Sukhbaatar}, S. and {Szlam}, A. and {Weston}, J. and {Fergus}, R.},
	Date-Added = {2018-02-21 05:45:01 +0000},
	Date-Modified = {2018-02-21 05:45:01 +0000},
	Eprint = {1503.08895},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	Month = mar,
	Title = {{End-To-End Memory Networks}},
	Year = 2015}

@article{2014arXiv1409.0473B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.0473B},
	Archiveprefix = {arXiv},
	Author = {{Bahdanau}, D. and {Cho}, K. and {Bengio}, Y.},
	Date-Added = {2018-02-21 05:29:04 +0000},
	Date-Modified = {2018-02-21 05:29:04 +0000},
	Eprint = {1409.0473},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	Month = sep,
	Primaryclass = {cs.CL},
	Title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
	Year = 2014}

@article{2015arXiv150203044X,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150203044X},
	Archiveprefix = {arXiv},
	Author = {{Xu}, K. and {Ba}, J. and {Kiros}, R. and {Cho}, K. and {Courville}, A. and {Salakhutdinov}, R. and {Zemel}, R. and {Bengio}, Y.},
	Date-Added = {2018-02-21 04:55:23 +0000},
	Date-Modified = {2018-02-21 04:55:23 +0000},
	Eprint = {1502.03044},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
	Year = 2015}

@article{2015arXiv150301007J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150301007J},
	Archiveprefix = {arXiv},
	Author = {{Joulin}, A. and {Mikolov}, T.},
	Date-Added = {2018-02-21 04:03:18 +0000},
	Date-Modified = {2018-02-21 04:03:18 +0000},
	Eprint = {1503.01007},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = mar,
	Title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
	Year = 2015}

@article{2014arXiv1406.6247M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.6247M},
	Archiveprefix = {arXiv},
	Author = {{Mnih}, V. and {Heess}, N. and {Graves}, A. and {Kavukcuoglu}, K.},
	Date-Added = {2018-02-21 04:01:10 +0000},
	Date-Modified = {2018-02-21 04:01:10 +0000},
	Eprint = {1406.6247},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Recurrent Models of Visual Attention}},
	Year = 2014}

@article{2013arXiv1308.0850G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1308.0850G},
	Archiveprefix = {arXiv},
	Author = {{Graves}, A.},
	Date-Added = {2018-02-21 03:44:05 +0000},
	Date-Modified = {2018-02-21 03:44:05 +0000},
	Eprint = {1308.0850},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	Month = aug,
	Title = {{Generating Sequences With Recurrent Neural Networks}},
	Year = 2013}

@article{2015arXiv150204390D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204390D},
	Archiveprefix = {arXiv},
	Author = {{Dauphin}, Y.~N. and {de Vries}, H. and {Bengio}, Y.},
	Date-Added = {2018-02-21 02:45:02 +0000},
	Date-Modified = {2018-02-21 02:45:02 +0000},
	Eprint = {1502.04390},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Numerical Analysis},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Equilibrated adaptive learning rates for non-convex optimization}},
	Year = 2015}

@article{2016arXiv160604934K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604934K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Salimans}, T. and {Jozefowicz}, R. and {Chen}, X. and {Sutskever}, I. and {Welling}, M.},
	Date-Added = {2018-02-20 10:42:29 +0000},
	Date-Modified = {2018-02-20 10:42:29 +0000},
	Eprint = {1606.04934},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
	Year = 2016}

@article{2015arXiv150204623G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204623G},
	Archiveprefix = {arXiv},
	Author = {{Gregor}, K. and {Danihelka}, I. and {Graves}, A. and {Jimenez Rezende}, D. and {Wierstra}, D.},
	Date-Added = {2018-02-20 10:42:05 +0000},
	Date-Modified = {2018-02-20 10:42:05 +0000},
	Eprint = {1502.04623},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{DRAW: A Recurrent Neural Network For Image Generation}},
	Year = 2015}

@article{2016arXiv160603657C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603657C},
	Archiveprefix = {arXiv},
	Author = {{Chen}, X. and {Duan}, Y. and {Houthooft}, R. and {Schulman}, J. and {Sutskever}, I. and {Abbeel}, P.},
	Date-Added = {2018-02-20 00:17:44 +0000},
	Date-Modified = {2018-02-20 00:17:44 +0000},
	Eprint = {1606.03657},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	Year = 2016}

@article{2016arXiv160509674H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160509674H},
	Archiveprefix = {arXiv},
	Author = {{Houthooft}, R. and {Chen}, X. and {Duan}, Y. and {Schulman}, J. and {De Turck}, F. and {Abbeel}, P.},
	Date-Added = {2018-02-19 23:51:31 +0000},
	Date-Modified = {2018-02-19 23:51:31 +0000},
	Eprint = {1605.09674},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	Month = may,
	Primaryclass = {cs.LG},
	Title = {{VIME: Variational Information Maximizing Exploration}},
	Year = 2016}

@article{2016arXiv160603476H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603476H},
	Archiveprefix = {arXiv},
	Author = {{Ho}, J. and {Ermon}, S.},
	Date-Added = {2018-02-19 10:11:33 +0000},
	Date-Modified = {2018-02-19 10:11:33 +0000},
	Eprint = {1606.03476},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Generative Adversarial Imitation Learning}},
	Year = 2016}

@article{SalimansGZCRC16,
	Archiveprefix = {arXiv},
	Author = {Tim Salimans and Ian J. Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/SalimansGZCRC16},
	Date-Added = {2018-02-19 09:53:01 +0000},
	Date-Modified = {2018-10-29 11:30:01 +0000},
	Eprint = {1606.03498},
	Journal = {CoRR},
	Keywords = {DCGAN, MNIST, Semi-Supervised learning, minibatch},
	Timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	Title = {Improved Techniques for Training GANs},
	Url = {http://arxiv.org/abs/1606.03498},
	Volume = {abs/1606.03498},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1606.03498}}

@article{2014arXiv1406.2661G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.2661G},
	Archiveprefix = {arXiv},
	Author = {{Goodfellow}, I.~J. and {Pouget-Abadie}, J. and {Mirza}, M. and {Xu}, B. and {Warde-Farley}, D. and {Ozair}, S. and {Courville}, A. and {Bengio}, Y.},
	Date-Added = {2018-02-19 09:28:01 +0000},
	Date-Modified = {2018-02-19 09:28:01 +0000},
	Eprint = {1406.2661},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = jun,
	Primaryclass = {stat.ML},
	Title = {{Generative Adversarial Networks}},
	Year = 2014}

@article{2013arXiv1312.6114K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P and {Welling}, M.},
	Date-Added = {2018-02-19 09:25:50 +0000},
	Date-Modified = {2018-02-19 09:25:50 +0000},
	Eprint = {1312.6114},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = dec,
	Primaryclass = {stat.ML},
	Title = {{Auto-Encoding Variational Bayes}},
	Year = 2013}

@article{DBLP:journals/corr/OordKK16,
	Archiveprefix = {arXiv},
	Author = {A{\"{a}}ron van den Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/OordKK16},
	Date-Added = {2018-02-19 09:01:51 +0000},
	Date-Modified = {2018-02-19 09:05:45 +0000},
	Eprint = {1601.06759},
	Journal = {CoRR},
	Keywords = {RNN, PixelRNN, BiLSTM, MNIST},
	Timestamp = {Wed, 07 Jun 2017 14:40:22 +0200},
	Title = {Pixel Recurrent Neural Networks},
	Url = {http://arxiv.org/abs/1601.06759},
	Volume = {abs/1601.06759},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1601.06759}}

@article{DBLP:journals/corr/KulkarniWKT15,
	Archiveprefix = {arXiv},
	Author = {Tejas D. Kulkarni and Will Whitney and Pushmeet Kohli and Joshua B. Tenenbaum},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/KulkarniWKT15},
	Date-Added = {2018-02-19 04:44:04 +0000},
	Date-Modified = {2018-02-19 04:44:43 +0000},
	Eprint = {1503.03167},
	Journal = {CoRR},
	Keywords = {CNN, IGN, SGVB, DC-IGN},
	Timestamp = {Wed, 07 Jun 2017 14:40:29 +0200},
	Title = {Deep Convolutional Inverse Graphics Network},
	Url = {http://arxiv.org/abs/1503.03167},
	Volume = {abs/1503.03167},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.03167}}

@article{2017arXiv170100160G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
	Archiveprefix = {arXiv},
	Author = {{Goodfellow}, I.},
	Date-Added = {2018-02-16 06:51:27 +0000},
	Date-Modified = {2018-02-16 06:51:27 +0000},
	Eprint = {1701.00160},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
	Year = 2017}

@article{2017arXiv170900199H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170900199H},
	Archiveprefix = {arXiv},
	Author = {{Hadad}, N. and {Wolf}, L. and {Shahar}, M.},
	Date-Added = {2018-02-07 01:32:29 +0000},
	Date-Modified = {2018-02-07 01:32:29 +0000},
	Eprint = {1709.00199},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = sep,
	Primaryclass = {cs.LG},
	Title = {{Two-Step Disentanglement for Financial Data}},
	Year = 2017}

@inproceedings{10.1007/3-540-28438-9_2,
	Abstract = {Backwards calculation of derivatives -- sometimes called the reverse mode, the full adjoint method, or backpropagation -- has been developed and applied in many fields. This paper reviews several strands of history, advanced capabilities and types of application -- particularly those which are crucial to the development of brain-like capabilities in intelligent control and artificial intelligence.},
	Address = {Berlin, Heidelberg},
	Author = {Werbos, Paul J.},
	Booktitle = {Automatic Differentiation: Applications, Theory, and Implementations},
	Date-Added = {2018-02-07 01:11:34 +0000},
	Date-Modified = {2018-04-15 11:11:31 +0000},
	Editor = {B{\"u}cker, Martin and Corliss, George and Naumann, Uwe and Hovland, Paul and Norris, Boyana},
	Isbn = {978-3-540-28438-3},
	Keywords = {NN, Backpropagation},
	Pages = {15--34},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities},
	Year = {2006}}

@article{2017arXiv170107274L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107274L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y.},
	Date-Added = {2018-02-06 09:19:00 +0000},
	Date-Modified = {2018-02-06 09:19:00 +0000},
	Eprint = {1701.07274},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = jan,
	Primaryclass = {cs.LG},
	Title = {{Deep Reinforcement Learning: An Overview}},
	Year = 2017}

@incollection{NIPS2017_6917,
	Author = {XIAO, SHUAI and Farajtabar, Mehrdad and Ye, Xiaojing and Yan, Junchi and Yang, Xiaokang and Song, Le and Zha, Hongyuan},
	Booktitle = {Advances in Neural Information Processing Systems 30},
	Date-Added = {2018-02-06 09:12:15 +0000},
	Date-Modified = {2018-02-06 09:13:20 +0000},
	Editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	Keywords = {DNN, Generative Adversarial NN, Point processes},
	Pages = {3250--3259},
	Publisher = {Curran Associates, Inc.},
	Title = {Wasserstein Learning of Deep Generative Point Process Models},
	Url = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf}}

@book{TheWayOfTheTurtle,
	Author = {Curtis M. Faith},
	Date-Added = {2018-02-06 08:38:52 +0000},
	Date-Modified = {2018-02-06 08:40:50 +0000},
	Keywords = {Finance, Economics Personal, Professional Development},
	Month = {March},
	Number = {9780071486644},
	Publisher = {McGraw-Hill Osborne Media},
	Title = {Way of the Turtle},
	Year = {2007}}

@article{2015arXiv150205767G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205767G},
	Archiveprefix = {arXiv},
	Author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A. and {Andreyevich Radul}, A. and {Siskind}, J.~M.},
	Date-Added = {2018-02-06 01:01:08 +0000},
	Date-Modified = {2018-09-26 08:04:03 +0000},
	Eprint = {1502.05767},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Automatic Differentiation, Automatic Differentiation, Automatic Differentiation, G.1.4, I.2.6},
	Month = feb,
	Primaryclass = {cs.SC},
	Title = {{Automatic differentiation in machine learning: a survey}},
	Year = 2015}

@misc{2015arXiv151106434R-sc,
	Author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	Date-Added = {2018-02-05 09:50:12 +0000},
	Date-Modified = {2018-02-23 00:38:59 +0000},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	Url = {https://github.com/Newmu/dcgan_code},
	Year = 2015,
	Bdsk-Url-1 = {https://github.com/Newmu/dcgan_code}}

@article{2015arXiv151106434R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
	Archiveprefix = {arXiv},
	Author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	Date-Added = {2018-02-05 09:50:12 +0000},
	Date-Modified = {2018-02-05 09:50:12 +0000},
	Eprint = {1511.06434},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	Year = 2015}

@misc{Grenade-Github-Code,
	Address = {San Francisco, CA, USA},
	Author = {Campbell, Huw},
	Date-Added = {2018-02-05 09:27:10 +0000},
	Date-Modified = {2018-02-23 00:39:26 +0000},
	Day = {24},
	Keywords = {functional programming, DNN, CNN, sourcecode},
	Month = {June},
	Publisher = {Github},
	Title = {Grenade},
	Url = {https://github.com/HuwCampbell/grenade},
	Year = {2016},
	Bdsk-Url-1 = {https://github.com/HuwCampbell/grenade}}

@inproceedings{Zhang:2015:OFA:2684746.2689060,
	Acmid = {2689060},
	Address = {New York, NY, USA},
	Author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	Booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-02-05 09:06:58 +0000},
	Date-Modified = {2018-09-26 08:04:22 +0000},
	Doi = {10.1145/2684746.2689060},
	Isbn = {978-1-4503-3315-3},
	Keywords = {Acceleration, convolutional neural network, fpga, roofline model},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {161--170},
	Publisher = {ACM},
	Series = {FPGA '15},
	Title = {Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks},
	Url = {http://doi.acm.org/10.1145/2684746.2689060},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689060},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2684746.2689060}}

@article{2016arXiv161102450W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161102450W},
	Archiveprefix = {arXiv},
	Author = {{Wang}, D. and {An}, J. and {Xu}, K.},
	Date-Added = {2018-02-05 09:06:58 +0000},
	Date-Modified = {2018-06-22 10:46:32 +0000},
	Eprint = {1611.02450},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA-NN, Computer Science - Hardware Architecture},
	Month = nov,
	Title = {{PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks}},
	Year = 2016}

@article{anonymous2018wavelet,
	Author = {Anonymous},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-12-02 12:41:25 +1300},
	Journal = {International Conference on Learning Representations},
	Keywords = {CNN, Wavelet, MIST, MathConvNet},
	Title = {Wavelet Pooling for Convolutional Neural Networks},
	Url = {https://openreview.net/forum?id=rkhlb8lCZ},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=rkhlb8lCZ}}

@book{Nayak:2017aa,
	Author = {Nayak, Sarat and Bihari Misra, Bijan and Behera, Dr. H.},
	Date = {2017/01/01},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.4018/978-1-5225-0788-8.ch022},
	Journal = {Nature-Inspired Computing: Concepts, Methodologies, Tools, and Applications},
	Keywords = {HONN, Pi-Sigma, Genetic Algorithms},
	Month = {01},
	N2 = {This chapter presents two higher order neural networks (HONN) for efficient prediction of stock market behavior. The models include Pi-Sigma, and Sigma-Pi higher order neural network models. Along with the traditional gradient descent learning, how the evolutionary computation technique such as genetic algorithm (GA) can be used effectively for the learning process is also discussed here. The learning process is made adaptive to handle the noise and uncertainties associated with stock market data. Further, different prediction approaches are discussed here and application of HONN for time series forecasting is illustrated with real life data taken from a number of stock markets across the globe.},
	Title = {Adaptive Hybrid Higher Order Neural Networks for Prediction of Stock Market Behavior},
	Ty = {BOOK},
	Year = {2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.4018/978-1-5225-0788-8.ch022}}

@article{CHONG2017187,
	Abstract = {We offer a systematic analysis of the use of deep learning networks for stock market analysis and prediction. Its ability to extract features from a large set of raw data without relying on prior knowledge of predictors makes deep learning potentially attractive for stock market prediction at high frequencies. Deep learning algorithms vary considerably in the choice of network structure, activation function, and other model parameters, and their performance is known to depend heavily on the method of data representation. Our study attempts to provides a comprehensive and objective assessment of both the advantages and drawbacks of deep learning algorithms for stock market analysis and prediction. Using high-frequency intraday stock returns as input data, we examine the effects of three unsupervised feature extraction methods---principal component analysis, autoencoder, and the restricted Boltzmann machine---on the network's overall ability to predict future market behavior. Empirical results suggest that deep neural networks can extract additional information from the residuals of the autoregressive model and improve prediction performance; the same cannot be said when the autoregressive model is applied to the residuals of the network. Covariance estimation is also noticeably improved when the predictive network is applied to covariance-based market structure analysis. Our study offers practical insights and potentially useful directions for further investigation into how deep learning networks can be effectively used for stock market analysis and prediction.},
	Author = {Eunsuk Chong and Chulwoo Han and Frank C. Park},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.eswa.2017.04.030},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {Stock market prediction, Deep learning, Multilayer neural network, Covariance estimation},
	Pages = {187 - 205},
	Title = {Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Volume = {83},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.04.030}}

@inproceedings{HuangY.2016Etmt,
	Author = {Huang, Y. and Huang, K. and Wang, Y. and Zhang, H. and Guan, J. and Zhou, S.},
	Copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {9783319422961},
	Issn = {03029743},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Keywords = {Convolutional Neural Network ; Deep Neural Network ; Financial Trend Prediction ; Twitter Mood},
	Pages = {449--460},
	Publisher = {Springer Verlag},
	Title = {Exploiting twitter moods to boost financial trend prediction based on deep network models},
	Volume = {9773},
	Year = {2016}}

@article{diartificial,
	Author = {Di Persio, Luca and Honchar, Oleksandr},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Economics and Management Systems},
	Keywords = {CNN, RNN, LSTM},
	Month = {January},
	Pages = {5},
	Title = {Artificial neural networks approach to the forecast of stock market price movements},
	Volume = {1},
	Year = {2016}}

@techreport{ghoshal2017reading,
	Author = {Ghoshal, Sid and Roberts, Stephen},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Institution = {Technical report},
	Keywords = {CNN, RNN, Technical Analysis},
	Title = {Reading the Tea Leaves: A Neural Network Perspective on Technical Trading},
	Year = {2017}}

@article{aggarwal2017deep,
	Author = {Aggarwal, Saurabh and Aggarwal, Somya},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Computer Applications},
	Keywords = {DNN, Finance, Portfolio, LSTM},
	Number = {2},
	Publisher = {Foundation of Computer Science},
	Title = {Deep Investment in Financial Markets using Deep Learning Models},
	Volume = {162},
	Year = {2017}}

@article{di2016artificial,
	Author = {Di Persio, Luca and Honchar, Oleksandr},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Circuits, Systems and Signal Processing},
	Keywords = {CNN, MLP, LSTM, Wavelet},
	Pages = {403--413},
	Title = {Artificial Neural Networks architectures for stock price prediction: comparisons and applications},
	Volume = {10},
	Year = {2016}}

@article{dixon2016classification,
	Author = {Dixon, Matthew Francis and Klabjan, Diego and Bang, Jin Hoon},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {DNN, Futures, Finance},
	Title = {Classification-based Financial Markets Prediction using Deep Neural Networks},
	Year = {2016}}

@misc{essay59381,
	Author = {M. {Kooijman}},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {Haskell, functional programming},
	Month = {December},
	Title = {Haskell as a higher order structural hardware description language},
	Url = {http://essay.utwente.nl/59381/},
	Year = {2009},
	Bdsk-Url-1 = {http://essay.utwente.nl/59381/}}

@conference{L:08,
	Author = {Philip H.W. Leong},
	Booktitle = {Proc. 4th IEEE International Symposium on Electronic Design, Test and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {FPGA, Trends},
	Location = {Hong Kong},
	Note = {\textbf{Invited}},
	Pages = {137--141},
	Title = {Recent Trends in {FPGA} Architectures and Applications},
	Url = {rtfpga_delta08.pdf},
	Year = {2008},
	Bdsk-Url-1 = {rtfpga_delta08.pdf}}

@inproceedings{Pike:2009:RYO:1596638.1596646,
	Acmid = {1596646},
	Address = {New York, NY, USA},
	Author = {Pike, Lee and Brown, Geoffrey and Goodloe, Alwyn},
	Booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1596638.1596646},
	Isbn = {978-1-60558-508-6},
	Keywords = {emulation, functional programming, physical-layer protocol testing},
	Location = {Edinburgh, Scotland},
	Numpages = {8},
	Pages = {61--68},
	Publisher = {ACM},
	Series = {Haskell '09},
	Title = {Roll Your Own Test Bed for Embedded Real-time Protocols: A Haskell Experience},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliTxEBrAAAAAABrAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HkVuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAE8vOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46RW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliAAAOAD4AHgBFAG4AZAAtAFQAbwAtAEUAbgBkACAATQBlAG0AbwByAHkAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAE1Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL0VuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAYAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIQ},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1596638.1596646}}

@inproceedings{Schrage:2005:HRD:1088348.1088351,
	Acmid = {1088351},
	Address = {New York, NY, USA},
	Author = {Schrage, Martijn M. and van IJzendoorn, Arjan and van der Gaag, Linda C.},
	Booktitle = {Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1088348.1088351},
	Isbn = {1-59593-071-X},
	Keywords = {application, bayesian networks, graphical user interface, haskell, wxHaskell},
	Location = {Tallinn, Estonia},
	Numpages = {10},
	Pages = {17--26},
	Publisher = {ACM},
	Series = {Haskell '05},
	Title = {Haskell Ready to Dazzle the Real World},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1088348.1088351}}

@article{Sezer:2017aa,
	Author = {Sezer, Omer Berat and Ozbayoglu, Murat and Dogdu, Erdogan},
	Booktitle = {Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 --November 1, 2017, Chicago, Illinois, USA},
	Da = {2017/01/01/},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.procs.2017.09.031},
	Isbn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Stock Trading; Stock Market; Deep Neural-Network; Evolutionary Algorithms; Technical Analysis},
	Number = {Supplement C},
	Pages = {473--480},
	Title = {A Deep Neural-Network Based Stock Trading System Based on Evolutionary Optimized Technical Analysis Parameters},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Volume = {114},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2017.09.031}}

@inproceedings{Kablan:2009aa,
	Author = {A. Kablan},
	Booktitle = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ADVCOMP.2009.23},
	Journal = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Journal1 = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Keywords = {decision making; expert systems; financial data processing; fuzzy reasoning; neural nets; pattern recognition; stock markets; adaptive neuro fuzzy inference systems; automated trading strategy; decision making; efficient market hypothesis; expert system; financial forecasting; financial markets; financial time series; fuzzy reasoning; high frequency financial trading; neural networks; pattern recognition; Adaptive systems; Economic forecasting; Expert systems; Finance; Frequency; Fuzzy reasoning; Fuzzy systems; Humans; Neural networks; Pattern recognition; efficient market hypothesis; financial prediction; high frequency trading; neuro-fuzzy inference system},
	Pages = {105--110},
	Title = {Adaptive Neuro Fuzzy Inference Systems for High Frequency Financial Trading and Forecasting},
	Ty = {CONF},
	Year = {2009},
	Year1 = {11-16 Oct. 2009},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ADVCOMP.2009.23}}

@article{2003cond.mat..4469K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2003cond.mat..4469K},
	Author = {{Kondratenko}, V.~V. and {Kuperin}, Y.~A},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {cond-mat/0304469},
	Journal = {eprint arXiv:cond-mat/0304469},
	Keywords = {Condensed Matter - Disordered Systems and Neural Networks, Quantitative Finance - Statistical Finance},
	Month = apr,
	Title = {{Using Recurrent Neural Networks To Forecasting of Forex}},
	Year = 2003}

@article{2015arXiv151207108G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151207108G},
	Archiveprefix = {arXiv},
	Author = {{Gu}, J. and {Wang}, Z. and {Kuen}, J. and {Ma}, L. and {Shahroudy}, A. and {Shuai}, B. and {Liu}, T. and {Wang}, X. and {Wang}, L. and {Wang}, G. and {Cai}, J. and {Chen}, T.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1512.07108},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{Recent Advances in Convolutional Neural Networks}},
	Year = 2015}

@article{Niedermeier:2014aa,
	Abstract = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	Author = {Niedermeier,A.},
	Date = {2014/8/29},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.3990/1.9789036537322},
	Isbn = {978-90-365-3732-2},
	Keywords = {IR-91607; EWI-25011; METIS-304761},
	M3 = {PhD Thesis - Research UT, graduation UT},
	Month = {8},
	N2 = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	Title = {A fine-grained parallel dataflow-inspired architecture for streaming applications},
	Ty = {THES},
	U2 = {10.3990/1.9789036537322},
	Year = {2014},
	Year1 = {2014/8/29},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036537322}}

@inbook{Smit:2010aa,
	Abstract = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	Annote = {eemcs-eprint-19169},
	Author = {Smit,Gerardus Johannes Maria and Kuper,Jan and Baaij,C. P. R.},
	Booktitle = {Dagstuhl Seminar on Dynamically Reconfigurable Architectures},
	Date = {2010/12/14},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.4230/OASIcs.WCET.2010.136},
	Keywords = {IR-75334; METIS-275806; Hardware design; EC Grant Agreement nr.: FP7/248465; Streaming Applications; EWI-19169; mathematical specification},
	M3 = {Conference contribution},
	Month = {12},
	N2 = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	Pages = {11},
	Publisher = {Internationales Begegnungs- und Forschungszentrum fur Informatik (IBFI)},
	Title = {A mathematical approach towards hardware design},
	Title1 = {Dagstuhl Seminar Proceedings},
	Ty = {CHAP},
	U2 = {10.4230/OASIcs.WCET.2010.136},
	Year = {2010},
	Year1 = {2010/12/14},
	Bdsk-Url-1 = {https://dx.doi.org/10.4230/OASIcs.WCET.2010.136}}

@article{Wester:2015aa,
	Abstract = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	Author = {Wester,Rinse},
	Date = {2015/7/3},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.3990/1.9789036538879},
	Isbn = {978-90-365-3887-9},
	Keywords = {Higher-order functions; EWI-26125; IR-96278; METIS-310874; transformations; Hardware design},
	M3 = {PhD Thesis - Research UT, graduation UT},
	Month = {7},
	N2 = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	Publisher = {Universiteit Twente},
	Title = {A transformation-based approach to hardware design using higher-order functions},
	Ty = {THES},
	U2 = {10.3990/1.9789036538879},
	Year = {2015},
	Year1 = {2015/7/3},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCRLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0EgRGVlcCBOZXVyYWwtTmV0d29yayBCYXNlZCBTdG9jayBUcmFkaW5nIFN5c3RlbSBCYXNlZCBvbiBFdm9sdXRpb25hcnkgT3B0aW1pemVkIFRlY2huaWNhbCBBbmFseXNpcyBQYXJhbWV0ZXJzLnJpc08RAwgAAAAAAwgAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BIERlZXAgTmV1cmFsLU5ldHcjRkZGRkZGRkYucmlzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGTk4tRmluAAIApy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTi1GaW46QSBEZWVwIE5ldXJhbC1OZXR3b3JrIEJhc2VkIFN0b2NrIFRyYWRpbmcgU3lzdGVtIEJhc2VkIG9uIEV2b2x1dGlvbmFyeSBPcHRpbWl6ZWQgVGVjaG5pY2FsIEFuYWx5c2lzIFBhcmFtZXRlcnMucmlzAAAOAOYAcgBBACAARABlAGUAcAAgAE4AZQB1AHIAYQBsAC0ATgBlAHQAdwBvAHIAawAgAEIAYQBzAGUAZAAgAFMAdABvAGMAawAgAFQAcgBhAGQAaQBuAGcAIABTAHkAcwB0AGUAbQAgAEIAYQBzAGUAZAAgAG8AbgAgAEUAdgBvAGwAdQB0AGkAbwBuAGEAcgB5ACAATwBwAHQAaQBtAGkAegBlAGQAIABUAGUAYwBoAG4AaQBjAGEAbAAgAEEAbgBhAGwAeQBzAGkAcwAgAFAAYQByAGEAbQBlAHQAZQByAHMALgByAGkAcwAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAKVVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9BIERlZXAgTmV1cmFsLU5ldHdvcmsgQmFzZWQgU3RvY2sgVHJhZGluZyBTeXN0ZW0gQmFzZWQgb24gRXZvbHV0aW9uYXJ5IE9wdGltaXplZCBUZWNobmljYWwgQW5hbHlzaXMgUGFyYW1ldGVycy5yaXMAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkALgAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADxA==},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB+Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0FkYXB0aXZlIE5ldXJvIEZ1enp5IEluZmVyZW5jZSBTeXN0ZW1zIGZvciBIaWdoIEZyZXF1ZW5jeSBGaW5hbmNpYWwgVHJhZGluZyBhbmQgRm9yZWNhc3RpbmcucmlzTxECugAAAAACugACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0FkYXB0aXZlIE5ldXJvIEZ1eiNGRkZGRkZGRi5yaXMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgCULzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpBZGFwdGl2ZSBOZXVybyBGdXp6eSBJbmZlcmVuY2UgU3lzdGVtcyBmb3IgSGlnaCBGcmVxdWVuY3kgRmluYW5jaWFsIFRyYWRpbmcgYW5kIEZvcmVjYXN0aW5nLnJpcwAOAMAAXwBBAGQAYQBwAHQAaQB2AGUAIABOAGUAdQByAG8AIABGAHUAegB6AHkAIABJAG4AZgBlAHIAZQBuAGMAZQAgAFMAeQBzAHQAZQBtAHMAIABmAG8AcgAgAEgAaQBnAGgAIABGAHIAZQBxAHUAZQBuAGMAeQAgAEYAaQBuAGEAbgBjAGkAYQBsACAAVAByAGEAZABpAG4AZwAgAGEAbgBkACAARgBvAHIAZQBjAGEAcwB0AGkAbgBnAC4AcgBpAHMADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCSVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi1GaW4vQWRhcHRpdmUgTmV1cm8gRnV6enkgSW5mZXJlbmNlIFN5c3RlbXMgZm9yIEhpZ2ggRnJlcXVlbmN5IEZpbmFuY2lhbCBUcmFkaW5nIGFuZCBGb3JlY2FzdGluZy5yaXMAEwABLwAAFQACAAv//wAAAAgADQAaACQApQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAANj},
	Bdsk-File-3 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBaLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL1VzaW5nIFJlY3VycmVudCBOZXVyYWwgTmV0d29ya3MgVG8gRm9yZWNhc3Rpbmcgb2YgRm9yZXguYmliTxECKgAAAAACKgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1VzaW5nIFJlY3VycmVudCBOZSNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgBwLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpVc2luZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIFRvIEZvcmVjYXN0aW5nIG9mIEZvcmV4LmJpYgAOAHgAOwBVAHMAaQBuAGcAIABSAGUAYwB1AHIAcgBlAG4AdAAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzACAAVABvACAARgBvAHIAZQBjAGEAcwB0AGkAbgBnACAAbwBmACAARgBvAHIAZQB4AC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBuVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi1GaW4vVXNpbmcgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3JrcyBUbyBGb3JlY2FzdGluZyBvZiBGb3JleC5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAgQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKv},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036538879}}

@inproceedings{Wester:2012aa,
	Author = {R. Wester and C. Baaij and J. Kuper},
	Booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2012.6339258},
	Isbn = {1946-147X},
	Journal = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Journal1 = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Keywords = {field programmable gate arrays; functional languages; hardware description languages; logic design; mathematical analysis; particle filtering (numerical methods); program compilers; C\&{\#}x03BB;aSH HDL; DSP application; FPGA; Haskell; adequate abstraction mechanisms; functional hardware description language; higher level abstraction mechanism; higher-order function; mathematical definition; particle filtering; polymorphism; two step hardware design method; Atmospheric measurements; Design methodology; Equations; Hardware; Mathematical model; Particle measurements; Systematics},
	Pages = {181--188},
	Title = {A two step hardware design method using C\&{\#}x03BB;aSH},
	Ty = {CONF},
	Year = {2012},
	Year1 = {29-31 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339258}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5faB,
	Abstract = {Functional hardware description languages are a class of hardware description languages that emphasize on the ability to express higher level structural properties, such a parameterization and regularity. Due to such features as higher-order functions and polymorphism, parameterization in functional hardware description languages is more natural than the parameterization support found in the more traditional hardware description languages, like VHDL and Verilog. We de- velop a new functional hardware description language, CλasH, that borrows both the syntax and semantics from the general-purpose functional programming language Haskell.},
	Author = {C.P.R. Baaij},
	Booktitle = {ClasH - From Haskell To Hardware},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {Haskell, CLaSH},
	Month = {12},
	Pages = {101},
	Publisher = {University of Twente},
	Title = {ClasH - From Haskell To Hardware},
	Year = {2009}}

@misc{essay70777,
	Abstract = {ClaSH is a functional hardware description language (HDL) developed at the CAES
group of the University of Twente. ClaSH borrows both the syntax and semantics from the general-purpose functional programming language Haskell, meaning that circuit designers can define their circuits with regular Haskell syntax.

In this thesis, research is done on the co-simulation of ClaSH and traditional HDLs. The Verilog Procedural Interface (VPI), as defined in the IEEE 1364 standard, is used to set-up the communication and to control a Verilog simulator. An implementation is made, as will be described in this thesis, to show the practical feasibility of co-simulation of ClaSH and Verilog.},
	Author = {J.G.J. {Verheij}},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {CLaSH, Haskell, Simulation, HDL},
	Month = {August},
	Title = {Co-simulation between C$\lambda$aSH and traditional HDLs},
	Url = {http://essay.utwente.nl/70777/},
	Year = {2016},
	Bdsk-Url-1 = {http://essay.utwente.nl/70777/}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5fa,
	Abstract = {As embedded systems are becoming increasingly complex, the design process and verification have become very time-consuming. Additionally, specifying hardware manually in a low-level hardware description language like VHDL is usually an error-prone task. In our group, a tool (the ClaSH compiler) was developed to generate fully synthesisable VHDL code from a specification given in the functional programming language Haskell. In this paper, we present a comparison between two implementations of the same design by using ClaSH and hand-written VHDL. The design is a simple dataflow processor. As measures of interest area, performance, power consumption and source lines of code (SLOC) are used. The obtained results indicate that the ClaSH -generated VHDL code as well as the netlist after synthesis and place and route are functionally correct. The placed and routed hand-written VHDL code has also the correct behaviour. Furthermore, a similar performance is achieved. The power consumption is even lower for the ClaSH implementation. The SLOC for ClaSH is considerably smaller and it is possible to specify the design in a much higher level of abstraction compared to VHDL.},
	Author = {A. Niedermeier and Rinse Wester and Rinse Wester and C.P.R. Baaij and Jan Kuper and Smit, {Gerardus Johannes Maria}},
	Booktitle = {Proceedings of the Workshop on PROGram for Research on Embedded Systems and Software (PROGRESS 2010)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-90-73461-67-3},
	Keywords = {METIS-277454, IR-75095, EWI-18902},
	Month = {11},
	Pages = {216--221},
	Publisher = {Technology Foundation (STW)},
	Title = {Comparing CλaSH and VHDL by implementing a dataflow processor},
	Year = {2010}}

@inproceedings{6339201,
	Author = {B. N. Uchevler and K. Svarstad},
	Booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2012.6339201},
	Issn = {1946-147X},
	Keywords = {circuit complexity;functional languages;hardware description languages;high level synthesis;program compilers;program verification;reconfigurable architectures;CLaSH;RT level VHDL;digital circuit description;digital circuit verification;dynamic reconfigurable system modeling;electronic design complexity;formal verification;functional HDL;high-level Haskell description translation;higher-order functions;parametrization;partial evaluation technique;polymorphism;run-time reconfigurable systems;synthesis tool chain;Communications technology;Consumer electronics;Educational institutions;Field programmable gate arrays;Hardware;Mathematical model;Unified modeling language},
	Month = {Aug},
	Pages = {481-482},
	Title = {Modeling of dynamic reconfigurable systems with Haskell},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBnLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYk8RAmIAAAAAAmIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BIFB5dGhvbmljIEFwcHJvYWMjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAERlBHQQACAH0vOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RlBHQTpBIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYgAADgCWAEoAQQAgAFAAeQB0AGgAbwBuAGkAYwAgAEEAcABwAHIAbwBhAGMAaAAgAGYAbwByACAAUgBhAHAAaQBkACAASABhAHIAZAB3AGEAcgBlACAAUAByAG8AdABvAHQAeQBwAGkAbgBnACAAYQBuAGQAIABJAG4AcwB0AHIAdQBtAGUAbgB0AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAe1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAjgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAL0},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339201}}

@inbook{fff28539e56047c4adca5cc3c9c29cec,
	Abstract = {CλaSH, a functional hardware description language based on Haskell, has several abstraction mechanisms that allow a hardware designer to describe architectures in a short and concise way. In this paper we evaluate CλaSH on a complex DSP application, a Polyphase Filter Bank as it is used in the ASTRON APERTIF project. The Polyphase Filter Bank is implemented in two steps: first in Haskell as being close to a standard mathematical specification, then in CλaSH which is derived from the Haskell formulation by applying only minor changes. We show that the CλaSH formulation can be directly mapped to hardware, thus exploiting the parallelism and concurrency that is present in the original mathematical specification.},
	Author = {Rinse Wester and Dimitrios Sarakiotis and Eric Kooistra and Jan Kuper},
	Booktitle = {Communicating Process Architectures 2012},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-0-9565409-5-9},
	Keywords = {EWI-22586, EC Grant Agreement nr.: FP7/248465, Specification, METIS-289800, APERTIF Project, CλaSH, IR-82307},
	Month = {8},
	Note = {eemcs-eprint-22586},
	Pages = {53--64},
	Publisher = {Open Channel Publishing},
	Title = {Specification of APERTIF Polyphase Filter Bank in CλaSH},
	Year = {2012}}

@inproceedings{6523639,
	Author = {B. N. Uchevler and K. Svarstad and J. Kuper and C. Baaij},
	Booktitle = {International Symposium on Quality Electronic Design (ISQED)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ISQED.2013.6523639},
	Issn = {1948-3287},
	Keywords = {field programmable gate arrays;hardware description languages;logic design;CLaSH;FPGA design;RT level;Suzaku-sz410 board;digital circuit verification;dynamic reconfigurable designs;formal verification;functional HDL;functional programming abstractions;high-level Haskell descriptions;high-level descriptions;high-level structures;higher-order functions;partial evaluation implementation technique;run-time reconfigurable systems;synthesizable VHDL;system-level modelling;Consumer electronics;Digital signal processing;Field programmable gate arrays;Finite impulse response filters;Hardware;Software;Unified modeling language;Functional HDL;Partial Evaluation;Run-Time Reconfiguration;Self-Reconfiguration},
	Month = {March},
	Pages = {379-385},
	Title = {System-level modelling of dynamic reconfigurable designs using functional programming abstractions},
	Year = {2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ISQED.2013.6523639}}

@inproceedings{Oancea:2012:FSG:2364474.2364484,
	Acmid = {2364484},
	Address = {New York, NY, USA},
	Author = {Oancea, Cosmin E. and Andreetta, Christian and Berthold, Jost and Frisch, Alain and Henglein, Fritz},
	Booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/2364474.2364484},
	Isbn = {978-1-4503-1577-7},
	Keywords = {autoparallelization, functional language, memory coalescing, strength reduction, tiling},
	Location = {Copenhagen, Denmark},
	Numpages = {12},
	Pages = {61--72},
	Publisher = {ACM},
	Series = {FHPC '12},
	Title = {Financial Software on GPUs: Between Haskell and Fortran},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364484}}

@inproceedings{Funie:2014aa,
	Author = {A. I. Funie and M. Salmon and W. Luk},
	Booktitle = {2014 13th International Conference on Machine Learning and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ICMLA.2014.11},
	Journal = {2014 13th International Conference on Machine Learning and Applications},
	Journal1 = {2014 13th International Conference on Machine Learning and Applications},
	Keywords = {economics; field programmable gate arrays; foreign exchange trading; genetic algorithms; particle swarm optimisation; economic value; field programmable gate array technology; financial markets; foreign exchange market data; genetic programming; high frequency trading strategies; hybrid evolutionary algorithm; monitor market stability; particle swarm optimisation; Algorithm design and analysis; Genetics; Noise; Prediction algorithms; Sociology; Statistics; Testing},
	Pages = {29--34},
	Title = {A Hybrid Genetic-Programming Swarm-Optimisation Approach for Examining the Nature and Stability of High Frequency Trading Strategies},
	Ty = {CONF},
	Year = {2014},
	Year1 = {3-6 Dec. 2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICMLA.2014.11}}

@inproceedings{Lockwood:2012aa,
	Author = {J. W. Lockwood and A. Gupte and N. Mehta and M. Blott and T. English and K. Vissers},
	Booktitle = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/HOTI.2012.15},
	Isbn = {1550-4794},
	Journal = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Journal1 = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Keywords = {IP networks; electronic engineering computing; electronic trading; field programmable gate arrays; local area networks; memory protocols; microprocessor chips; network interfaces; Ethernet line rate; FPGA IP library; FPGA hardware; HFT platform; I-O interface; alternative hybrid architecture; bit rate 10 Gbit/s; computers software; custom 1U FPGA appliance; electronic trading; financial protocol parser; fixed end-to-end latency; hardware acceleration; high-frequency trading platform; high-performance network adapter; low-latency library; memory interface; pre-built infrastructure; software implementation; time 1 mus; Field programmable gate arrays; Hardware; IP networks; Libraries; Protocols; Registers; Software; Algorithmic; FPGA; HFT; latency; trading},
	Pages = {9--16},
	Title = {A Low-Latency Library in FPGA Hardware for High-Frequency Trading (HFT)},
	Ty = {CONF},
	Year = {2012},
	Year1 = {22-24 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/HOTI.2012.15}}

@inproceedings{Zoican:2016aa,
	Author = {S. Zoican and M. Vochin},
	Booktitle = {2016 International Conference on Communications (COMM)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ICComm.2016.7528289},
	Journal = {2016 International Conference on Communications (COMM)},
	Journal1 = {2016 International Conference on Communications (COMM)},
	Keywords = {financial data processing; computing system; financial market literature; high frequency trading applications; high frequency trading financial applications; high processing speed; high-frequency traders; low latency technology; low network latency; medium cost technology; network architectures; optimal trading speed; Bandwidth; Computer architecture; Computers; Graphics processing units; Instruction sets; Parallel processing; Servers; computer unified device architecture; high frequency trading algorithms; network latency},
	Pages = {139--144},
	Title = {Computing system and network architectures in high frequency trading financial applications},
	Ty = {CONF},
	Year = {2016},
	Year1 = {9-10 June 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICComm.2016.7528289}}

@inproceedings{Litz:2011:DPE:2088256.2088268,
	Acmid = {2088268},
	Address = {New York, NY, USA},
	Author = {Litz, Heiner and Leber, Christian and Geib, Benjamin},
	Booktitle = {Proceedings of the Fourth Workshop on High Performance Computational Finance},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/2088256.2088268},
	Isbn = {978-1-4503-1108-3},
	Keywords = {DSL, FAST, FIX, FPGA, decoder, domain specific language, high throughput, low latency, stock, trading},
	Location = {Seattle, Washington, USA},
	Numpages = {8},
	Pages = {31--38},
	Publisher = {ACM},
	Series = {WHPCF '11},
	Title = {DSL Programmable Engine for High Frequency Trading Acceleration},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2088256.2088268}}

@inproceedings{Woods:2008aa,
	Author = {N. A. Woods and T. VanCourt},
	Booktitle = {2008 International Conference on Field Programmable Logic and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-09-26 08:04:22 +0000},
	Doi = {10.1109/FPL.2008.4629954},
	Isbn = {1946-147X},
	Journal = {2008 International Conference on Field Programmable Logic and Applications},
	Journal1 = {2008 International Conference on Field Programmable Logic and Applications},
	Keywords = {Monte Carlo methods; field programmable gate arrays; financial data processing; FPGA acceleration; finance; multicore processor; pricing simulations; quasiMonte Carlo methods; Acceleration; Computational modeling; Field programmable gate arrays; Finance; Monte Carlo methods; Multicore processing; Pricing; Runtime; Security; Yield estimation},
	Pages = {335--340},
	Title = {FPGA acceleration of quasi-Monte Carlo in finance},
	Ty = {CONF},
	Year = {2008},
	Year1 = {8-10 Sept. 2008},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2008.4629954}}

@article{Tian:2010:HQC:1862648.1862656,
	Acmid = {1862656},
	Address = {New York, NY, USA},
	Articleno = {26},
	Author = {Tian, Xiang and Benkrid, Khaled},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1862648.1862656},
	Issn = {1936-7406},
	Issue_Date = {November 2010},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {CPU, FPGA, GPU, Maxwell, Quasi-Monte Carlo simulations, option pricing},
	Month = nov,
	Number = {4},
	Numpages = {22},
	Pages = {26:1--26:22},
	Publisher = {ACM},
	Title = {High-Performance Quasi-Monte Carlo Financial Simulation: FPGA vs. GPP vs. GPU},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Volume = {3},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1862648.1862656}}

@inproceedings{Dvorak:2014aa,
	Author = {M. Dvo{\v r}{\'a}k and J. Ko{\v r}enek},
	Booktitle = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/DDECS.2014.6868785},
	Journal = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Journal1 = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Keywords = {electronic trading; field programmable gate arrays; logic design; FPGA; QDR SRAM; algorithmic trading; best bid price; best offer price; financial instrument; hardware architecture; hardware market state; high frequency trading; lookup latency-memory utilization trade-off; low latency book handling; low latency trading system; market data processing; storage capacity 144 Mbit; Algorithm design and analysis; Feeds; Field programmable gate arrays; Hardware; Instruments; Memory management},
	Pages = {175--178},
	Title = {Low latency book handling in FPGA for high frequency trading},
	Ty = {CONF},
	Year = {2014},
	Year1 = {23-25 April 2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBfLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRXF1aWxpYnJhdGVkIGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzIGZvciBub24tY29udmV4IG9wdGltaXphdGlvbi5iaWJPEQJEAAAAAAJEAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRXF1aWxpYnJhdGVkIGFkYXB0I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAAk5OAAIAdS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTjpFcXVpbGlicmF0ZWQgYWRhcHRpdmUgbGVhcm5pbmcgcmF0ZXMgZm9yIG5vbi1jb252ZXggb3B0aW1pemF0aW9uLmJpYgAADgCKAEQARQBxAHUAaQBsAGkAYgByAGEAdABlAGQAIABhAGQAYQBwAHQAaQB2AGUAIABsAGUAYQByAG4AaQBuAGcAIAByAGEAdABlAHMAIABmAG8AcgAgAG4AbwBuAC0AYwBvAG4AdgBlAHgAIABvAHAAdABpAG0AaQB6AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAc1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRXF1aWxpYnJhdGVkIGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzIGZvciBub24tY29udmV4IG9wdGltaXphdGlvbi5iaWIAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAIYAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACzg==},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/DDECS.2014.6868785}}

@article{Thomas:2013aa,
	Author = {D. B. Thomas and W. Luk},
	Booktitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/TVLSI.2012.2228017},
	Isbn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Journal1 = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {Gaussian distribution; adders; field programmable gate arrays; graphics processing units; random number generation; shift registers; table lookup; Virtex-5 FPGA; adders; block-memory resources; field-programmable gate array; frequency 1.2 GHz; frequency 400 MHz; graphics processing unit; independent Gaussian samples; logic resources; lookup-tables; multiplierless algorithm; multivariate Gaussian distribution; multivariate Gaussian vectors; multivariate generator; numerical simulation; pair-wise correlations; random number generation; read-only memories; registers; scalar Gaussian generator; uniform distribution; Covariance matrix; Field programmable gate arrays; Generators; Matrix decomposition; Standards; Table lookup; Vectors; Field-programmable gate array (FPGA); Monte Carlo simulation; multivariate samples; random number generation},
	Number = {12},
	Pages = {2193--2205},
	Title = {Multiplierless Algorithm for Multivariate Gaussian Random Number Generation in FPGAs},
	Ty = {JOUR},
	Vo = {21},
	Volume = {21},
	Year = {2013},
	Year1 = {Dec. 2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2012.2228017}}

@article{2017arXiv171105860H-2,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105860H},
	Archiveprefix = {arXiv},
	Author = {{Hao}, Y.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1711.05860},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.CV},
	Title = {{A General Neural Network Hardware Architecture on FPGA}},
	Year = 2017}

@article{2017arXiv170206392L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170206392L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y. and {Liu}, Z. and {Xu}, K. and {Yu}, H. and {Ren}, F.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1702.06392},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, C.3},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks}},
	Year = 2017}

@article{2017arXiv170808917D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170808917D},
	Archiveprefix = {arXiv},
	Author = {{Ding}, C. and {Liao}, S. and {Wang}, Y. and {Li}, Z. and {Liu}, N. and {Zhuo}, Y. and {Wang}, C. and {Qian}, X. and {Bai}, Y. and {Yuan}, G. and {Ma}, X. and {Zhang}, Y. and {Tang}, J. and {Qiu}, Q. and {Lin}, X. and {Yuan}, B.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1708.08917},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	Month = aug,
	Primaryclass = {cs.CV},
	Title = {{CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices}},
	Year = 2017}

@inproceedings{Zhao:2016aa,
	Author = {Wenlai Zhao and Haohuan Fu and W. Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	Booktitle = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-09-26 08:03:08 +0000},
	Doi = {10.1109/ASAP.2016.7760779},
	Journal = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Journal1 = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Keywords = {field programmable gate arrays; floating point arithmetic; neural nets; 32bit floating-point arithmetic; FPGA-based framework; bandwidth resources; convolutional neural networks; hardware resources; streaming datapath; Bandwidth; Computational modeling; Convolution; Field programmable gate arrays; Neural networks; Runtime; Training},
	Pages = {107--114},
	Title = {F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	Ty = {CONF},
	Year = {2016},
	Year1 = {6-8 July 2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBWLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXNPEQIcAAAAAAIcAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fQWNjZWxlcmF0aW5nIExhcmdlI0ZGRkZGRkZGLnJpcwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABEZQR0EAAgBsLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkZQR0E6QWNjZWxlcmF0aW5nIExhcmdlLVNjYWxlIEhQQyBBcHBsaWNhdGlvbnMgVXNpbmcgRlBHQXMucmlzAA4AdAA5AEEAYwBjAGUAbABlAHIAYQB0AGkAbgBnACAATABhAHIAZwBlAC0AUwBjAGEAbABlACAASABQAEMAIABBAHAAcABsAGkAYwBhAHQAaQBvAG4AcwAgAFUAcwBpAG4AZwAgAEYAUABHAEEAcwAuAHIAaQBzAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAalVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXMAEwABLwAAFQACAAv//wAAAAgADQAaACQAfQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKd},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAxLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vV2Fzc2Vyc3RlaW4gR0FOLmJpYk8RAYoAAAAAAYoAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xNXYXNzZXJzdGVpbiBHQU4uYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAFRENHQU4AAAIARy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpEQ0dBTjpXYXNzZXJzdGVpbiBHQU4uYmliAAAOACgAEwBXAGEAcwBzAGUAcgBzAHQAZQBpAG4AIABHAEEATgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIARVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vV2Fzc2Vyc3RlaW4gR0FOLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAWAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAHm},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ASAP.2016.7760779}}

@inproceedings{Abdelouahab:2017:WTH:3131885.3131937,
	Acmid = {3131937},
	Address = {New York, NY, USA},
	Author = {Abdelouahab, Kamel and Pelcat, Maxime and Berry, Francois},
	Booktitle = {Proceedings of the 11th International Conference on Distributed Smart Cameras},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-09-26 08:04:22 +0000},
	Doi = {10.1145/3131885.3131937},
	Isbn = {978-1-4503-5487-5},
	Keywords = {FPGA, TanH, Harware, Acceleration},
	Location = {Stanford, CA, USA},
	Numpages = {3},
	Pages = {199--201},
	Publisher = {ACM},
	Series = {ICDSC 2017},
	Title = {Why TanH is a Hardware Friendly Activation Function for CNNs},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/3131885.3131937}}

@article{2017arXiv170304691B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170304691B},
	Archiveprefix = {arXiv},
	Author = {{Borovykh}, A. and {Bohte}, S. and {Oosterlee}, C.~W.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-10-24 07:44:45 +0000},
	Eprint = {1703.04691},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Statistics; Machine Learning},
	Month = mar,
	Primaryclass = {stat.ML},
	Title = {{Conditional Time Series Forecasting with Convolutional Neural Networks}},
	Year = 2017,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB0Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OLUZQR0EvT3B0aW1pemluZyBGUEdBLWJhc2VkIEFjY2VsZXJhdG9yIERlc2lnbiBmb3IgRGVlcCBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWJPEQKQAAAAAAKQAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fT3B0aW1pemluZyBGUEdBLWJhI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAACENOTi1GUEdBAAIAii86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk4tRlBHQTpPcHRpbWl6aW5nIEZQR0EtYmFzZWQgQWNjZWxlcmF0b3IgRGVzaWduIGZvciBEZWVwIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgAOAKgAUwBPAHAAdABpAG0AaQB6AGkAbgBnACAARgBQAEcAQQAtAGIAYQBzAGUAZAAgAEEAYwBjAGUAbABlAHIAYQB0AG8AcgAgAEQAZQBzAGkAZwBuACAAZgBvAHIAIABEAGUAZQBwACAAQwBvAG4AdgBvAGwAdQB0AGkAbwBuAGEAbAAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCIVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4tRlBHQS9PcHRpbWl6aW5nIEZQR0EtYmFzZWQgQWNjZWxlcmF0b3IgRGVzaWduIGZvciBEZWVwIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACbAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAy8=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB6Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OLUZQR0EvUGlwZUNOTi0gQW4gT3BlbkNMLUJhc2VkIE9wZW4tU291cmNlIEZQR0EgQWNjZWxlcmF0b3IgZm9yIENvbnZvbHV0aW9uIE5ldXJhbCBOZXR3b3Jrcy5iaWJPEQKoAAAAAAKoAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fUGlwZUNOTi0gQW4gT3BlbkNMI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAACENOTi1GUEdBAAIAkC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk4tRlBHQTpQaXBlQ05OLSBBbiBPcGVuQ0wtQmFzZWQgT3Blbi1Tb3VyY2UgRlBHQSBBY2NlbGVyYXRvciBmb3IgQ29udm9sdXRpb24gTmV1cmFsIE5ldHdvcmtzLmJpYgAOALQAWQBQAGkAcABlAEMATgBOAC0AIABBAG4AIABPAHAAZQBuAEMATAAtAEIAYQBzAGUAZAAgAE8AcABlAG4ALQBTAG8AdQByAGMAZQAgAEYAUABHAEEAIABBAGMAYwBlAGwAZQByAGEAdABvAHIAIABmAG8AcgAgAEMAbwBuAHYAbwBsAHUAdABpAG8AbgAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCOVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4tRlBHQS9QaXBlQ05OLSBBbiBPcGVuQ0wtQmFzZWQgT3Blbi1Tb3VyY2UgRlBHQSBBY2NlbGVyYXRvciBmb3IgQ29udm9sdXRpb24gTmV1cmFsIE5ldHdvcmtzLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAChAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA00=}}

@inproceedings{Ding:2015:DLE:2832415.2832572,
	Acmid = {2832572},
	Author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	Booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-1-57735-738-4},
	Keywords = {Deep learning, CNN, NN, S&P},
	Location = {Buenos Aires, Argentina},
	Numpages = {7},
	Pages = {2327--2333},
	Publisher = {AAAI Press},
	Series = {IJCAI'15},
	Title = {Deep Learning for Event-driven Stock Prediction},
	Url = {http://dl.acm.org/citation.cfm?id=2832415.2832572},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2832415.2832572}}

@inproceedings{Chen:2016aa,
	Author = {J. F. Chen and W. L. Chen and C. P. Huang and S. H. Huang and A. P. Chen},
	Booktitle = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-10-10 00:34:11 +0000},
	Doi = {10.1109/CCBD.2016.027},
	Journal = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Journal1 = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Keywords = {CNN-FIN; decision support systems; feature extraction; feedforward neural nets; financial data processing; learning (artificial intelligence); stock markets; time series; FinTech; Taiwan Stock Index Futures; artificial intelligence; deep convolutional neural networks; deep learning; feature extraction; financial markets; financial time-series data analysis; historical datasets; intelligent trading decision support system; multimedia fields; next financial technology generation; numerical features; planar feature representation; time-series data prediction; time-series data processing; trading simulation application; Data models; Feature extraction; Machine learning; Market research; Neural networks; Time series analysis; Training; Deep learning; convolutional neural networks; data visualization; machine learning; trend prediction},
	Pages = {87--92},
	Title = {Financial Time-Series Data Analysis Using Deep Convolutional Neural Networks},
	Ty = {CONF},
	Year = {2016},
	Year1 = {16-18 Nov. 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CCBD.2016.027}}

@inproceedings{Tsantekidis:2017aa,
	Abstract = {In today's financial markets, where most trades are performed in their entirety by electronic means and the largest fraction of them is completely automated, an opportunity has risen from analyzing this vast amount of transactions. Since all the transactions are recorded in great detail, investors can analyze all the generated data and detect repeated patterns of the price movements. Being able to detect them in advance, allows them to take profitable positions or avoid anomalous events in the financial markets. In this work we proposed a deep learning methodology, based on Convolutional Neural Networks (CNNs), that predicts the price movements of stocks, using as input large-scale, high-frequency time-series derived from the order book of financial exchanges. The dataset that we use contains more than 4 million limit order events and our comparison with other methods, like Multilayer Neural Networks and Support Vector Machines, shows that CNNs are better suited for this kind of task.},
	Author = {A. Tsantekidis and N. Passalis and A. Tefas and J. Kanniainen and M. Gabbouj and A. Iosifidis},
	Booktitle = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/CBI.2017.23},
	Journal = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Journal1 = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Keywords = {economic forecasting; feedforward neural nets; learning (artificial intelligence); pricing; stock markets; time series; CNN; convolutional neural networks; deep learning methodology; financial exchanges; financial markets; input large-scale high-frequency time-series; limit order book; price movements; stock price forecasting; stock price movement prediction; transaction analysis; Convolution; Data models; Machine learning; Market research; Mathematical model; Neural networks; Support vector machines; Convolutional Neural Networks; Large scale financial data; Limit Orderbook},
	Pages = {7--12},
	Title = {Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks},
	Ty = {CONF},
	Vo = {01},
	Volume = {01},
	Year = {2017},
	Year1 = {24-27 July 2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CBI.2017.23}}

@article{Gunduz:2017aa,
	Author = {Gunduz, Hakan and Yaslan, Yusuf and Cataltepe, Zehra},
	Da = {2017/12/01/},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.knosys.2017.09.023},
	Isbn = {0950-7051},
	Journal = {Knowledge-Based Systems},
	Keywords = {Stock market prediction; Deep learning; Borsa Istanbul; Convolutional neural networks; CNN; Feature selection; Feature correlations},
	Number = {Supplement C},
	Pages = {138--148},
	Title = {Intraday prediction of Borsa Istanbul using convolutional neural networks and feature correlations},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Volume = {137},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.knosys.2017.09.023}}

@article{2016arXiv160306995C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160306995C},
	Archiveprefix = {arXiv},
	Author = {{Cui}, Z. and {Chen}, W. and {Chen}, Y.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1603.06995},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{Multi-Scale Convolutional Neural Networks for Time Series Classification}},
	Year = 2016}

@article{2017arXiv171105860H,
	Adsnote = {International Journal of Computer Science and Information Technologies (IJCSIT{\textregistered}) is published using an open access publishing model, which makes the full-text of all peer-reviewed papers freely available online with no subscription or registration barriers.},
	Adsurl = {http://ijcsit.com/docs/Volume%207/vol7issue5/ijcsit20160705014.pdf},
	Archiveprefix = {pdf},
	Author = {{Bhandare}, Ashwin, {Bhide}, Maithili, {Gokhale}, Pranav, {Chandavarkar}, Rohan,},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1711.05860},
	Journal = {International Journal of Computer Science and Information Technologies},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	Month = 5,
	Primaryclass = {cs.CV},
	Title = {{Applications of Convolutional Neural Networks}},
	Year = 2016,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBiLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0RlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWJPEQJKAAAAAAJKAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRGVlcCBJbnZlc3RtZW50IGluI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABk5OLUZpbgACAHgvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk4tRmluOkRlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWIADgCIAEMARABlAGUAcAAgAEkAbgB2AGUAcwB0AG0AZQBuAHQAIABpAG4AIABGAGkAbgBhAG4AYwBpAGEAbAAgAE0AYQByAGsAZQB0AHMAIAB1AHMAaQBuAGcAIABEAGUAZQBwACAATABlAGEAcgBuAGkAbgBnACAATQBvAGQAZQBsAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAHZVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9EZWVwIEludmVzdG1lbnQgaW4gRmluYW5jaWFsIE1hcmtldHMgdXNpbmcgRGVlcCBMZWFybmluZyBNb2RlbHMuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAIkAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAC1w==}}
