%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for RHVT at 2020-01-20 23:26:16 +1300 


%% Saved with string encoding Unicode (UTF-8) 



@article{MEURICEDEDORMALE200772,
	Abstract = {For the last decade, Elliptic Curve Cryptography (ECC) has gained increasing acceptance in the industry and the academic community and has been the subject of several standards. This interest is mainly due to the high level of security with relatively small keys provided by ECC. To sustain the high throughput required by applications like network servers, high-speed implementations of public-key cryptosystems are needed. For that purpose, hardware-based accelerators are often the only solution reaching an acceptable performance-cost ratio. The fundamental question that arises is how to choose the appropriate efficiency--flexibility tradeoff. In this survey, techniques for implementing Elliptic Curve Cryptography at a high-speed are explored. A classification of the work available in the open literature in function of the level of efficiency and flexibility is also proposed. In particular, the subjects of reconfigurable, dedicated, generator, versatile and general purpose scalar multipliers are addressed. Finally, some words about future work that should be tackled are provided.},
	Author = {Guerric Meurice de Dormale and Jean-Jacques Quisquater},
	Date-Added = {2020-01-20 23:26:10 +1300},
	Date-Modified = {2020-01-20 23:26:10 +1300},
	Doi = {https://doi.org/10.1016/j.sysarc.2006.09.002},
	Issn = {1383-7621},
	Journal = {Journal of Systems Architecture},
	Keywords = {Public-key cryptography, Elliptic Curve Cryptography, High-speed hardware implementation, Efficiency--flexibility tradeoffs, Network applications},
	Note = {Embedded Hardware for Cryptosystems},
	Number = {2},
	Pages = {72 - 84},
	Title = {High-speed hardware implementations of Elliptic Curve Cryptography: A survey},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762106001044},
	Volume = {53},
	Year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106001044},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.09.002}}

@article{ISMAYILOV2020307,
	Abstract = {Workflow scheduling is a largely studied research topic in cloud computing, which targets to utilize cloud resources for workflow tasks by considering the objectives specified in QoS. In this paper, we model dynamic workflow scheduling problem as a dynamic multi-objective optimization problem (DMOP) where the source of dynamism is based on both resource failures and the number of objectives which may change over time. Software faults and/or hardware faults may cause the first type of dynamism. On the other hand, confronting real-life scenarios in cloud computing may change number of objectives at runtime during the execution of a workflow. In this study, we propose a prediction-based dynamic multi-objective evolutionary algorithm, called NN-DNSGA-II algorithm, by incorporating artificial neural network with the NSGA-II algorithm. Additionally, five leading non-prediction based dynamic algorithms from the literature are adapted for the dynamic workflow scheduling problem. Scheduling solutions are found by the consideration of six objectives: minimization of makespan, cost, energy and degree of imbalance; and maximization of reliability and utilization. The empirical study based on real-world applications from Pegasus workflow management system reveals that our NN-DNSGA-II algorithm significantly outperforms the other alternatives in most cases with respect to metrics used for DMOPs with unknown true Pareto-optimal front, including the number of non-dominated solutions, Schott's spacing and Hypervolume indicator.},
	Author = {Goshgar Ismayilov and Haluk Rahmi Topcuoglu},
	Date-Added = {2020-01-20 23:15:20 +1300},
	Date-Modified = {2020-01-20 23:15:28 +1300},
	Doi = {https://doi.org/10.1016/j.future.2019.08.012},
	Issn = {0167-739X},
	Journal = {Future Generation Computer Systems},
	Keywords = {NN; Workflow scheduling, Resource failures, Changing number of objectives, Dynamic multi-objective evolutionary algorithms, Neural networks},
	Pages = {307 - 322},
	Title = {Neural network based multi-objective evolutionary algorithm for dynamic workflow scheduling in cloud computing},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X19306983},
	Volume = {102},
	Year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167739X19306983},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.future.2019.08.012}}

@article{NALEPA2020102994,
	Abstract = {Hyperspectral image analysis has been gaining research attention thanks to the current advances in sensor design which have made acquiring such imagery much more affordable. Although there exist various approaches for segmenting hyperspectral images, deep learning has become the mainstream. However, such large-capacity learners are characterized by significant memory footprints. This is a serious obstacle in employing deep neural networks on board a satellite for Earth observation. In this paper, we introduce resource-frugal quantized convolutional neural networks, and greatly reduce their size without adversely affecting the classification capability. Our experiments performed over two hyperspectral benchmarks showed that the quantization process can be seamlessly applied during the training, and it leads to much smaller and still well-generalizing deep models.},
	Author = {Jakub Nalepa and Marek Antoniak and Michal Myller and Pablo Ribalta Lorenzo and Michal Marcinkiewicz},
	Date-Added = {2020-01-20 23:08:37 +1300},
	Date-Modified = {2020-01-20 23:08:46 +1300},
	Doi = {https://doi.org/10.1016/j.micpro.2020.102994},
	Issn = {0141-9331},
	Journal = {Microprocessors and Microsystems},
	Keywords = {NN, Hyperspectral imaging, Deep neural network, Convolutional neural network, Quantization, Segmentation, Classification},
	Pages = {102994},
	Title = {Towards resource-frugal deep convolutional neural networks for hyperspectral image segmentation},
	Url = {http://www.sciencedirect.com/science/article/pii/S0141933119302844},
	Volume = {73},
	Year = {2020},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0141933119302844},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.micpro.2020.102994}}

@inproceedings{1590168,
	Abstract = {The goal of any cryptographic system is the exchange of information among the intended users without any leakage of information to others who may have unauthorized access to it. In 1976, Diffie & Hellmann found that a common secret key could be created over a public channel accessible to any opponent. Since then many public key cryptography have been presented which are based on number theory and they demand large computational power. Moreover the process involved in generating public key is very complex and time consuming. To overcome these disadvantages, the neural networks can be used to generate common secret key. This is the motivation for this present work on interacting neural networks and cryptography[1].In the case of neural cryptography, both the communicating networks receive an identical input vector, generate an output bit and are trained based on the output bit. The dynamics of the two networks and their weight vectors is found to exhibit a novel phenomenon, where the networks synchronize to a state with identical time-dependent weights. This concept of synchronization by mutual learning can be applied to a secret key exchange protocol over a public channel. The generation of secret key over a public channel has been studied and the generated key is used for encrypting and decrypting the given message using DES algorithm which is simulated and synthesized using VHDL},
	Author = {T. {Godhavari} and N. R. {Alamelu} and R. {Soundararajan}},
	Booktitle = {2005 Annual IEEE India Conference - Indicon},
	Date-Added = {2020-01-20 19:52:11 +1300},
	Date-Modified = {2020-01-20 19:52:19 +1300},
	Doi = {10.1109/INDCON.2005.1590168},
	Issn = {2325-9418},
	Keywords = {Crypto; neural cryptography;mutual learning;time-dependent weights;Neural networks;Public key cryptography;Biological neural networks;Public key;Computer networks;Concurrent computing;Artificial neural networks;Cryptographic protocols;Computational modeling;Network synthesis;neural cryptography;mutual learning;time-dependent weights},
	Month = {Dec},
	Pages = {258-261},
	Title = {Cryptography Using Neural Network},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/INDCON.2005.1590168}}

@inproceedings{7583925,
	Abstract = {In cryptography secret information is made unreadable for an unauthorized user. There are many cryptographic algorithms are available, but they are more complex techniques and requires more computational power. This paper gives a review of how Neural Networks contributes a help in cryptography and how neural network and cryptography together can be used for security.},
	Author = {P. P. {Hadke} and S. G. {Kale}},
	Booktitle = {2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare (Startup Conclave)},
	Date-Added = {2020-01-20 19:46:34 +1300},
	Date-Modified = {2020-01-20 19:46:42 +1300},
	Doi = {10.1109/STARTUP.2016.7583925},
	Issn = {null},
	Keywords = {Crypto; authorisation;cryptography;neural nets;neural networks;cryptography secret information;user unauthorization;complex techniques;computational power;Biological neural networks;Encryption;Watermarking;Neurons;cryptography;cryptosystem;Neural Network;key generation and management},
	Month = {Feb},
	Pages = {1-4},
	Title = {Use of Neural Networks in cryptography: A review},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/STARTUP.2016.7583925}}

@inproceedings{861518,
	Abstract = {Visual cryptography finds many applications in the cryptographic field such as key management, message concealment, authorization, authentication, identification, and entertainment. The authors propose a novel approach for visual cryptography using neural networks (NNs). To perform encrypting, the input to the NN is a set of gray level images, and the output is a set of binary images (shares) that fulfils the desirable access scheme. This approach is considerably different from the traditional one, and can be applied to cope with very complex access schemes.},
	Author = {{Tai-Wen Yue} and {Suchen Chiang}},
	Booktitle = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
	Date-Added = {2020-01-20 19:45:33 +1300},
	Date-Modified = {2020-01-20 19:45:41 +1300},
	Doi = {10.1109/IJCNN.2000.861518},
	Issn = {1098-7576},
	Keywords = {Crypto; cryptography;neural nets;image processing;quantum computing;neural network approach;visual cryptography;cryptographic field;key management;message concealment;authorization;authentication;identification;entertainment;encrypting;NN;gray level images;binary images;access scheme;Neural networks;Cryptography;Books;Computer science;Application software;Engineering management;Authorization;Authentication;Image recognition;Target recognition},
	Month = {July},
	Pages = {494-499 vol.5},
	Title = {A neural network approach for visual cryptography},
	Volume = {5},
	Year = {2000},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2000.861518}}

@inproceedings{8953134,
	Abstract = {With the explosive interest in the utilization of Neural Networks, several approaches have taken place to make them faster, more accurate or power efficient; one technique used to simplify inference models is the utilization of binary representations for weights, activations, inputs and outputs. This paper presents a novel approach to train from scratch Binary Neural Networks using neuroevolution as its base technique (gradient descent free), to then apply such results to standard Reinforcement Learning environments tested in the OpenAI Gym. The results and code can be found in https://github.com/rval735/BiSUNA.},
	Author = {R. {Valencia} and C. {Sham} and O. {Sinnen}},
	Booktitle = {2019 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
	Date-Added = {2020-01-20 16:32:29 +1300},
	Date-Modified = {2020-01-20 16:32:40 +1300},
	Doi = {10.1109/APCCAS47518.2019.8953134},
	Issn = {null},
	Keywords = {BNN; Neuroevolution;binary neural networks;BiSUNA;discrete optimization},
	Month = {Nov},
	Pages = {301-304},
	Title = {Using Neuroevolved Binary Neural Networks to solve reinforcement learning environments},
	Year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/APCCAS47518.2019.8953134}}

@article{SEC-019,
	Author = {David Evans and Vladimir Kolesnikov and Mike Rosulek},
	Date-Added = {2019-11-06 23:12:31 +1300},
	Date-Modified = {2019-11-06 23:13:08 +1300},
	Doi = {10.1561/3300000019},
	Issn = {2474-1558},
	Journal = {Foundations and Trends{\textregistered} in Privacy and Security},
	Keywords = {Crypto, Multi-party computations, Books},
	Number = {2-3},
	Pages = {70-246},
	Title = {A Pragmatic Introduction to Secure Multi-Party Computation},
	Url = {http://dx.doi.org/10.1561/3300000019},
	Volume = {2},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1561/3300000019}}

@inproceedings{ijcai2018-0547,
	Author = {Qiao Zhang and Cong Wang and Hongyi Wu and Chunsheng Xin and Tran V. Phuong},
	Booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
	Date-Added = {2019-11-06 19:56:30 +1300},
	Date-Modified = {2019-11-06 19:57:39 +1300},
	Doi = {10.24963/ijcai.2018/547},
	Keywords = {Crypto, NN, Stegranography, Homomorphic encryption},
	Month = {7},
	Pages = {3933--3939},
	Publisher = {International Joint Conferences on Artificial Intelligence Organization},
	Title = {GELU-Net: A Globally Encrypted, Locally Unencrypted Deep Neural Network for Privacy-Preserved Learning},
	Url = {https://doi.org/10.24963/ijcai.2018/547},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2018/547}}

@electronic{BNN-FPT-Code,
	Author = {Raul Valencia},
	Date-Added = {2019-10-25 07:32:09 +1300},
	Date-Modified = {2019-10-25 07:35:38 +1300},
	Keywords = {Paper, Github, FPT, Data},
	Title = {BNN-FPT Source code},
	Url = {https://github.com/rval735/bisunaocl},
	Urldate = {Oct 25},
	Bdsk-Url-1 = {github.com/rval735/BNN-PhD/tree/master/Presentations/Paper-BNNFPGA}}

@inproceedings{10.1007/978-3-030-30048-7_24,
	Abstract = {Binarized Neural Networks (BNNs) are an important class of neural network characterized by weights and activations restricted to the set {\$}{\$}{\backslash}{\{}-1,+1{\backslash}{\}}{\$}{\$}. BNNs provide simple compact descriptions and as such have a wide range of applications in low-power devices. In this paper, we investigate a model-based approach to training BNNs using constraint programming (CP), mixed-integer programming (MIP), and CP/MIP hybrids. We formulate the training problem as finding a set of weights that correctly classify the training set instances while optimizing objective functions that have been proposed in the literature as proxies for generalizability. Our experimental results on the MNIST digit recognition dataset suggest that---when training data is limited---the BNNs found by our hybrid approach generalize better than those obtained from a state-of-the-art gradient descent method. More broadly, this work enables the analysis of neural network performance based on the availability of optimal solutions and optimality bounds.},
	Address = {Cham},
	Author = {Toro Icarte, Rodrigo and Illanes, Le{\'o}n and Castro, Margarita P. and Cire, Andre A. and McIlraith, Sheila A. and Beck, J. Christopher},
	Booktitle = {Principles and Practice of Constraint Programming},
	Date-Added = {2019-10-09 12:46:42 +1300},
	Date-Modified = {2019-10-09 12:47:13 +1300},
	Editor = {Schiex, Thomas and de Givry, Simon},
	Isbn = {978-3-030-30048-7},
	Keywords = {BNN, Mixed-Integer Programming, Constraint Programming},
	Pages = {401--417},
	Publisher = {Springer International Publishing},
	Title = {Training Binarized Neural Networks Using MIP and CP},
	Year = {2019}}

@inproceedings{Ma_2019_CVPR_Workshops,
	Author = {Ma, Yinglan and Xiong, Hongyu and Hu, Zhe and Ma, Lizhuang},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	Date-Added = {2019-10-09 12:38:51 +1300},
	Date-Modified = {2019-10-09 12:39:21 +1300},
	Keywords = {BNN, image processing, CNN},
	Month = {June},
	Title = {Efficient Super Resolution Using Binarized Neural Network},
	Year = {2019}}

@article{Su:2017:NNB:3039902.3039915,
	Acmid = {3039915},
	Address = {New York, NY, USA},
	Author = {Su, Jiang and Liu, Jianxiong and Thomas, David B. and Cheung, Peter Y.K.},
	Date-Added = {2019-10-08 23:15:02 +1300},
	Date-Modified = {2019-10-08 23:15:25 +1300},
	Doi = {10.1145/3039902.3039915},
	Issn = {0163-5964},
	Issue_Date = {September 2016},
	Journal = {SIGARCH Comput. Archit. News},
	Keywords = {FPGA-NN, DQN, Reinforcement Learning},
	Month = jan,
	Number = {4},
	Numpages = {6},
	Pages = {68--73},
	Publisher = {ACM},
	Title = {Neural Network Based Reinforcement Learning Acceleration on FPGA Platforms},
	Url = {http://doi.acm.org/10.1145/3039902.3039915},
	Volume = {44},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3039902.3039915},
	Bdsk-Url-2 = {https://doi.org/10.1145/3039902.3039915}}

@article{electronics8060661,
	Abstract = {In this work, we review Binarized Neural Networks (BNNs). BNNs are deep neural networks that use binary values for activations and weights, instead of full precision values. With binary values, BNNs can execute computations using bitwise operations, which reduces execution time. Model sizes of BNNs are much smaller than their full precision counterparts. While the accuracy of a BNN model is generally less than full precision models, BNNs have been closing accuracy gap and are becoming more accurate on larger datasets like ImageNet. BNNs are also good candidates for deep learning implementations on FPGAs and ASICs due to their bitwise efficiency. We give a tutorial of the general BNN methodology and review various contributions, implementations and applications of BNNs.},
	Article-Number = {661},
	Author = {Simons, Taylor and Lee, Dah-Jye},
	Date-Added = {2019-10-08 15:58:12 +1300},
	Date-Modified = {2019-10-08 15:58:34 +1300},
	Doi = {10.3390/electronics8060661},
	Issn = {2079-9292},
	Journal = {Electronics},
	Keywords = {BNN, Survey, backpropagation},
	Number = {6},
	Title = {A Review of Binarized Neural Networks},
	Url = {https://www.mdpi.com/2079-9292/8/6/661},
	Volume = {8},
	Year = {2019},
	Bdsk-Url-1 = {https://www.mdpi.com/2079-9292/8/6/661},
	Bdsk-Url-2 = {https://doi.org/10.3390/electronics8060661}}

@electronic{BNN-FPT-Paper-Data,
	Author = {Raul Valencia},
	Date-Added = {2019-09-15 23:25:18 +1200},
	Date-Modified = {2019-09-15 23:27:03 +1200},
	Keywords = {Paper, Github, FPT, Data},
	Title = {BNN-FPT Raw Data},
	Url = {github.com/rval735/BNN-PhD/tree/master/Presentations/Paper-BNNFPGA},
	Urldate = {Sep 15},
	Bdsk-Url-1 = {github.com/rval735/BNN-PhD/tree/master/Presentations/Paper-BNNFPGA}}

@electronic{BiSUNAGithub,
	Author = {Raul Valencia},
	Date-Added = {2019-09-14 21:28:31 +1200},
	Date-Modified = {2019-09-14 21:30:01 +1200},
	Keywords = {BiSUNA,},
	Month = {Sep},
	Title = {Binary Spectrum-diverse Unified Neuroevolution Architecture},
	Url = {https://github.com/rval735/BiSUNA},
	Urldate = {Sep 14, 2019},
	Year = {2019},
	Bdsk-Url-1 = {https://github.com/rval735/BiSUNA}}

@article{8594633,
	Abstract = {Due to recent advances in digital technologies, and availability of credible data, an area of artificial intelligence, deep learning, has emerged and has demonstrated its ability and effectiveness in solving complex learning problems not possible before. In particular, convolutional neural networks (CNNs) have demonstrated their effectiveness in the image detection and recognition applications. However, they require intensive CPU operations and memory bandwidth that make general CPUs fail to achieve the desired performance levels. Consequently, hardware accelerators that use application-specific integrated circuits, field-programmable gate arrays (FPGAs), and graphic processing units have been employed to improve the throughput of CNNs. More precisely, FPGAs have been recently adopted for accelerating the implementation of deep learning networks due to their ability to maximize parallelism and their energy efficiency. In this paper, we review the recent existing techniques for accelerating deep learning networks on FPGAs. We highlight the key features employed by the various techniques for improving the acceleration performance. In addition, we provide recommendations for enhancing the utilization of FPGAs for CNNs acceleration. The techniques investigated in this paper represent the recent trends in the FPGA-based accelerators of deep learning networks. Thus, this paper is expected to direct the future advances on efficient hardware accelerators and to be useful for deep learning researchers.},
	Author = {A. {Shawahna} and S. M. {Sait} and A. {El-Maleh}},
	Date-Added = {2019-09-14 16:22:32 +1200},
	Date-Modified = {2019-09-14 16:23:27 +1200},
	Doi = {10.1109/ACCESS.2018.2890150},
	Journal = {IEEE Access},
	Keywords = {FPGA-NN, Deep learning; FPGA; Neural networks; Hardware; Acceleration;Convolution; Adaptable architectures; CNN; deep learning; dynamic reconfiguration; energy-efficient architecture; hardware accelerator; machine learning; neural networks; optimization;parallel computer architecture;reconfigurable computing},
	Pages = {7823-7859},
	Title = {FPGA-Based Accelerators of Deep Learning Networks for Learning and Classification: A Review},
	Volume = {7},
	Year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACCESS.2018.2890150}}

@article{2019arXiv190407852B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190407852B},
	Archiveprefix = {arXiv},
	Author = {{Bulat}, A. and {Kossaifi}, J. and {Tzimiropoulos}, G. and {Pantic}, M.},
	Date-Added = {2019-07-28 17:05:45 +1200},
	Date-Modified = {2019-07-28 17:06:01 +1200},
	Eprint = {1904.07852},
	Journal = {arXiv e-prints},
	Keywords = {BNN, Computer Vision and Pattern Recognition, Artificial Intelligence, Machine Learning},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{Matrix and tensor decompositions for training binary neural networks}},
	Year = 2019}

@article{2019arXiv190608637B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190608637B},
	Archiveprefix = {arXiv},
	Author = {{Bethge}, J. and {Yang}, H. and {Bornstein}, M. and {Meinel}, C.},
	Date-Added = {2019-07-28 17:03:09 +1200},
	Date-Modified = {2019-07-28 17:03:31 +1200},
	Eprint = {1906.08637},
	Journal = {arXiv e-prints},
	Keywords = {BNN, Machine Learning, Computer Vision and Pattern Recognition, Machine Learning},
	Month = jun,
	Title = {{Back to Simplicity: How to Train Accurate BNNs from Scratch?}},
	Year = 2019,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBkLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL01hdHJpeCBhbmQgdGVuc29yIGRlY29tcG9zaXRpb25zIGZvciB0cmFpbmluZyBiaW5hcnkgbmV1cmFsIG5ldHdvcmtzLmJpYk8RAlYAAAAAAlYAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9NYXRyaXggYW5kIHRlbnNvciAjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADQk5OAAACAHovOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Qk5OOk1hdHJpeCBhbmQgdGVuc29yIGRlY29tcG9zaXRpb25zIGZvciB0cmFpbmluZyBiaW5hcnkgbmV1cmFsIG5ldHdvcmtzLmJpYgAOAJIASABNAGEAdAByAGkAeAAgAGEAbgBkACAAdABlAG4AcwBvAHIAIABkAGUAYwBvAG0AcABvAHMAaQB0AGkAbwBuAHMAIABmAG8AcgAgAHQAcgBhAGkAbgBpAG4AZwAgAGIAaQBuAGEAcgB5ACAAbgBlAHUAcgBhAGwAIABuAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAHhVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0JOTi9NYXRyaXggYW5kIHRlbnNvciBkZWNvbXBvc2l0aW9ucyBmb3IgdHJhaW5pbmcgYmluYXJ5IG5ldXJhbCBuZXR3b3Jrcy5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAiwAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAALl}}

@webpage{NectarCloud,
	Author = {National Research Infraestructure for Australia},
	Date-Added = {2019-07-28 12:42:59 +1200},
	Date-Modified = {2019-07-28 12:47:32 +1200},
	Keywords = {Nectar, Cloud, New Zealand, Research},
	Lastchecked = {Jul, 2019},
	Month = {Nov},
	Title = {New Zealand Nectar Cloud},
	Url = {https://nectar.org.au/new-zealand-researchers-join-nectar-cloud/},
	Urldate = {Nov, 2017},
	Year = {2017},
	Bdsk-Url-1 = {https://nectar.org.au/new-zealand-researchers-join-nectar-cloud/}}

@inproceedings{10.1007/978-3-319-78890-6_44,
	Abstract = {OpenCL has been proposed as a means of accelerating functional computation using FPGA and GPU accelerators. Although it provides ease of programmability and code portability, questions remain about the performance portability and underlying vendor's compiler capabilities to generate efficient implementations without user-defined, platform specific optimizations. In this work, we systematically evaluate this by formalizing a design space exploration strategy using platform-independent micro-architectural and application-specific optimizations only. The optimizations are then applied across Altera FPGA, NVIDIA GPU and ARM Mali GPU platforms for three computing examples, namely matrix-matrix multiplication, binomial-tree option pricing and 3-dimensional finite difference time domain. Our strategy enables a fair comparison across platforms in terms of throughput and energy efficiency by using the same design effort. Our results indicate that FPGA provides better performance portability in terms of achieved percentage of device's peak performance (68{\%}) compared to NVIDIA GPU (20{\%}) and also achieves better energy efficiency (up to 1.4{\$}{\$}{\backslash}times {\$}{\$}) for some of the considered cases without requiring in-depth hardware design expertise.},
	Address = {Cham},
	Annote = {Inside book "Applied Reconfigurable Computing. Architectures, Tools, and Applications"},
	Author = {Minhas, Umar Ibrahim and Woods, Roger and Karakonstantis, Georgios},
	Booktitle = {Applied Reconfigurable Computing. Architectures, Tools, and Applications},
	Date-Added = {2019-06-14 08:22:58 +1200},
	Date-Modified = {2019-06-14 08:23:35 +1200},
	Editor = {Voros, Nikolaos and Huebner, Michael and Keramidas, Georgios and Goehringer, Diana and Antonopoulos, Christos and Diniz, Pedro C.},
	Isbn = {978-3-319-78890-6},
	Keywords = {Books, FPGA, OpenCL, Odroid},
	Pages = {551--563},
	Publisher = {Springer International Publishing},
	Title = {Exploring Functional Acceleration of OpenCL on FPGAs and GPUs Through Platform-Independent Optimizations},
	Year = {2018}}

@misc{cryptoeprint:2018:1056,
	Author = {Ahmad Al Badawi and Jin Chao and Jie Lin and Chan Fook Mun and Jun Jie Sim and Benjamin Hong Meng Tan and Xiao Nan and Khin Mi Mi Aung and Vijay Ramaseshan Chandrasekhar},
	Date-Added = {2019-06-06 21:06:38 +1200},
	Date-Modified = {2019-06-06 21:06:38 +1200},
	Howpublished = {Cryptology ePrint Archive, Report 2018/1056},
	Note = {\url{https://eprint.iacr.org/2018/1056}},
	Title = {The AlexNet Moment for Homomorphic Encryption: HCNN, the First Homomorphic CNN on Encrypted Data with GPUs},
	Year = {2018}}

@inproceedings{10.1007/978-3-642-22792-9_28,
	Abstract = {At Eurocrypt 2010 van Dijk et al. described a fully homomorphic encryption scheme over the integers. The main appeal of this scheme (compared to Gentry's) is its conceptual simplicity. This simplicity comes at the expense of a public key size in {\$}{\{}{\backslash}cal {\backslash}tilde O{\}}({\backslash}lambda^{\{}10{\}}){\$}which is too large for any practical system. In this paper we reduce the public key size to {\$}{\{}{\backslash}cal {\backslash}tilde O{\}}({\backslash}lambda^{\{}7{\}}){\$}by encrypting with a quadratic form in the public key elements, instead of a linear form. We prove that the scheme remains semantically secure, based on a stronger variant of the approximate-GCD problem, already considered by van Dijk et al.},
	Address = {Berlin, Heidelberg},
	Author = {Coron, Jean-S{\'e}bastien and Mandal, Avradip and Naccache, David and Tibouchi, Mehdi},
	Booktitle = {Advances in Cryptology -- CRYPTO 2011},
	Date-Added = {2019-06-06 21:04:34 +1200},
	Date-Modified = {2019-06-06 21:05:00 +1200},
	Editor = {Rogaway, Phillip},
	Isbn = {978-3-642-22792-9},
	Keywords = {Crypto, Fully Homomorphic Encryption, Public key},
	Pages = {487--504},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Fully Homomorphic Encryption over the Integers with Shorter Public Keys},
	Year = {2011}}

@article{10.1371/journal.pone.0122236,
	Abstract = {Secure multiparty computation allows for a set of users to evaluate a particular function over their inputs without revealing the information they possess to each other. Theoretically, this can be achieved using fully homomorphic encryption systems, but so far they remain in the realm of computational impracticability. An alternative is to consider secure function evaluation using homomorphic public-key cryptosystems or Garbled Circuits, the latter being a popular trend in recent times due to important breakthroughs. We propose a technique for computing the logsum operation using Garbled Circuits. This technique relies on replacing the logsum operation with an equivalent piecewise linear approximation, taking advantage of recent advances in efficient methods for both designing and implementing Garbled Circuits. We elaborate on how all the required blocks should be assembled in order to obtain small errors regarding the original logsum operation and very fast execution times.},
	Author = {Port{\^e}lo, Jos{\'e} AND Raj, Bhiksha AND Trancoso, Isabel},
	Date-Added = {2019-06-06 20:35:53 +1200},
	Date-Modified = {2019-06-06 20:37:16 +1200},
	Doi = {10.1371/journal.pone.0122236},
	Journal = {PLOS ONE},
	Keywords = {Crypto, garbled-circuit, Linear approximation},
	Month = {03},
	Number = {3},
	Pages = {1-16},
	Publisher = {Public Library of Science},
	Title = {Logsum Using Garbled Circuits},
	Url = {https://doi.org/10.1371/journal.pone.0122236},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0122236}}

@article{Rouhani:2018:RRP:3299999.3242899,
	Acmid = {3242899},
	Address = {New York, NY, USA},
	Articleno = {21},
	Author = {Rouhani, Bita Darvish and Hussain, Siam Umar and Lauter, Kristin and Koushanfar, Farinaz},
	Date-Added = {2019-06-06 14:16:45 +1200},
	Date-Modified = {2019-06-06 14:16:55 +1200},
	Doi = {10.1145/3242899},
	Issn = {1936-7406},
	Issue_Date = {December 2018},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {Crypto, Secure machine learning, data mining, deep learning, garbled Circuit, privacy-preserving computation},
	Month = dec,
	Number = {3},
	Numpages = {21},
	Pages = {21:1--21:21},
	Publisher = {ACM},
	Title = {ReDCrypt: Real-Time Privacy-Preserving Deep Learning Inference in Clouds Using FPGAs},
	Url = {http://doi.acm.org/10.1145/3242899},
	Volume = {11},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3242899},
	Bdsk-Url-2 = {https://doi.org/10.1145/3242899}}

@inproceedings{1133,
	Attachments = {http://aceslab.org/sites/default/files/XONN.pdf},
	Author = {M. Sadegh Riazi and Mohammad Samragh and Hao Chen and Kim Laine and Kristin Lauter and Farinaz Koushanfar},
	Booktitle = {USENIX Security},
	Date-Added = {2019-06-06 14:06:32 +1200},
	Date-Modified = {2019-06-06 14:07:02 +1200},
	Keywords = {Crypto, Homomorphic encryption, BNN, Cryptography and Security},
	Month = {05/2019},
	Organization = {USENIX},
	Publisher = {USENIX},
	Title = {XONN: XNOR-based Oblivious Deep Neural Network Inference},
	Url = {https://arxiv.org/pdf/1902.07342.pdf},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/pdf/1902.07342.pdf}}

@inproceedings{1136,
	Abstract = {<p>We present FASE, an FPGA accelerator for Secure Function Evaluation (SFE) by employing the well-known cryptographic protocol named Yao\&rsquo;s Garbled Circuit (GC). SFE allows two parties to jointly compute a function on their private data and learn the output without revealing their inputs to each other. FASE is designed to allow cloud servers to provide secure services to a large number of clients in parallel while preserving the privacy of the data from both sides. Current SFE accelerators either target specific applications, and therefore are not amenable to generic use, or have low throughput due to inefficient management of resources. In this work, we present a pipelined architecture along with an efficient scheduling scheme to ensure optimal usage of the available resources. The scheme is built around a simulator of the hardware design that schedules the workload and assigns the most suitable task to the encryption cores at each cycle. This, coupled with optimal management of the read and write cycles of the Block RAM on FPGA, results in a minimum 2 orders of magnitude improvement in terms of throughput per core for the reported benchmarks compared to the most recent generic GC accelerator. Moreover, our encryption core requires 17\% less resource compared to the most recent secure GC realization.\&nbsp;</p>
},
	Address = {San Diego},
	Attachments = {http://aceslab.org/sites/default/files/FASE.pdf},
	Author = {Siam U. Hussain and Farinaz Koushanfar},
	Booktitle = {Field-Programmable Custom Computing Machines (FCCM)},
	Date-Added = {2019-06-06 14:03:09 +1200},
	Date-Modified = {2019-06-06 14:03:30 +1200},
	Keywords = {Crypto, garbled-circuit, Secure computation, FPGA},
	Month = {04/2019},
	Title = {FASE: FPGA Acceleration of Secure Function Evaluation},
	Year = {2019}}

@inproceedings{10.1007/978-3-319-96878-0_17,
	Abstract = {The rise of machine learning as a service multiplies scenarios where one faces a privacy dilemma: either sensitive user data must be revealed to the entity that evaluates the cognitive model (e.g., in the Cloud), or the model itself must be revealed to the user so that the evaluation can take place locally. Fully Homomorphic Encryption (FHE) offers an elegant way to reconcile these conflicting interests in the Cloud-based scenario and also preserve non-interactivity. However, due to the inefficiency of existing FHE schemes, most applications prefer to use Somewhat Homomorphic Encryption (SHE), where the complexity of the computation to be performed has to be known in advance, and the efficiency of the scheme depends on this global complexity.},
	Address = {Cham},
	Annote = {Found in book "Advances in Cryptology -- CRYPTO 2018"},
	Author = {Bourse, Florian and Minelli, Michele and Minihold, Matthias and Paillier, Pascal},
	Booktitle = {Advances in Cryptology -- CRYPTO 2018},
	Date-Added = {2019-06-05 22:22:42 +1200},
	Date-Modified = {2019-06-05 22:32:53 +1200},
	Editor = {Shacham, Hovav and Boldyreva, Alexandra},
	Isbn = {978-3-319-96878-0},
	Keywords = {Books, Homomorphic encryption, MNIST, Discrete NN},
	Pages = {483--512},
	Publisher = {Springer International Publishing},
	Title = {Fast Homomorphic Evaluation of Deep Discretized Neural Networks},
	Year = {2018}}

@inproceedings{10.1007/978-3-319-98530-5_66,
	Abstract = {Machine learning servers with mass storage and computing power is an ideal platform to store, manage, and analyze data and support decision-making. However, the main issue is providing security and privacy to the data, as the data is stored in a public way. Recently, homomorphic data encryption has been proposed as a solution due to its capabilities in performing computations over encrypted data. In this paper, we proposed an encrypted all convolutional net that transformed traditional all convolutional net into a net based on homomorphic encryption. This scheme allows different data holders to send their encrypted data to cloud service, complete predictions, and return them in encrypted form as the cloud service provider does not have a secret key. Therefore, the cloud service provider and others cannot get unencrypted raw data. When applied to the MNIST database, privacy-preserving all convolutional based on homomorphic encryption predict efficiently, accurately and with privacy protection.},
	Address = {Cham},
	Annote = {Inside book Advances in Network-Based Information Systems},
	Author = {Liu, Wenchao and Pan, Feng and Wang, Xu An and Cao, Yunfei and Tang, Dianhua},
	Booktitle = {Advances in Network-Based Information Systems},
	Date-Added = {2019-06-05 21:55:08 +1200},
	Date-Modified = {2019-06-05 22:16:49 +1200},
	Editor = {Barolli, Leonard and Kryvinska, Natalia and Enokido, Tomoya and Takizawa, Makoto},
	Isbn = {978-3-319-98530-5},
	Keywords = {Books, Homomorphic encryption, CNN, MNIST},
	Pages = {752--762},
	Publisher = {Springer International Publishing},
	Title = {Privacy-Preserving All Convolutional Net Based on Homomorphic Encryption},
	Year = {2019}}

@inproceedings{10.1007/978-3-319-61982-8_9,
	Abstract = {In this paper, the suitability of implementing parallel homomorphic word searching on Intel Xeon Phi coprocessors is evaluated for the first time. Homomorphic encryption allows to produce a cryptogram that encrypts the result of applying some values to any function, even when the input values are encrypted and without access to the private-key. For example, it is possible to search if any word of a set of encrypted words matches a plaintext reference word and generate a new cryptogram that encrypts the amount of matches. In this paper it is shown that this operation is about 834 times faster by using a system with 4 Intel Xeon Phi coprocessors 5110P attached to an Intel Xeon CPU E5-2630 v2, when compared with an implementation on a single core of the Xeon CPU.},
	Address = {Cham},
	Author = {Martins, Paulo and Sousa, Leonel},
	Booktitle = {High Performance Computing for Computational Science -- VECPAR 2016},
	Date-Added = {2019-06-05 21:38:48 +1200},
	Date-Modified = {2019-06-05 21:39:17 +1200},
	Editor = {Dutra, In{\^e}s and Camacho, Rui and Barbosa, Jorge and Marques, Osni},
	Isbn = {978-3-319-61982-8},
	Keywords = {Books, Crypto, homomorphic encryption, Word search, Xeon Phi},
	Pages = {75--88},
	Publisher = {Springer International Publishing},
	Title = {HPC on the Intel Xeon Phi: Homomorphic Word Searching},
	Year = {2017}}

@inproceedings{10.1007/978-3-662-48324-4_8,
	Abstract = {Homomorphic encryption allows computation on encrypted data and makes it possible to securely outsource computational tasks to untrusted environments. However, all proposed schemes are quite inefficient and homomorphic evaluation of ciphertexts usually takes several seconds on high-end CPUs, even for evaluating simple functions. In this work we investigate the potential of FPGAs for speeding up those evaluation operations. We propose an architecture to accelerate schemes based on the ring learning with errors (RLWE) problem and specifically implemented the somewhat homomorphic encryption scheme YASHE, which was proposed by Bos, Lauter, Loftus, and Naehrig in 2013. Due to the large size of ciphertexts and evaluation keys, on-chip storage of all data is not possible and external memory is required. For efficient utilization of the external memory we propose an efficient double-buffered memory access scheme and a polynomial multiplier based on the number theoretic transform (NTT). For the parameter set ({\$}{\$}n=16384,{\backslash}lceil {\backslash}log {\_}2 q {\backslash}rceil ={\{}512{\}}{\$}{\$}n=16384,⌈log2q⌉=512) capable of evaluating 9 levels of multiplications, we can perform a homomorphic addition in 0.94 ms and a homomorphic multiplication in 48.67 ms.},
	Address = {Berlin, Heidelberg},
	Author = {P{\"o}ppelmann, Thomas and Naehrig, Michael and Putnam, Andrew and Macias, Adrian},
	Booktitle = {Cryptographic Hardware and Embedded Systems -- CHES 2015},
	Date-Added = {2019-06-05 21:18:08 +1200},
	Date-Modified = {2019-06-05 21:19:33 +1200},
	Editor = {G{\"u}neysu, Tim and Handschuh, Helena},
	Isbn = {978-3-662-48324-4},
	Keywords = {Books, Crypto, Homomorphic encryption, FPGA, YASHE, Stratix},
	Pages = {143--163},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Accelerating Homomorphic Evaluation on Reconfigurable Hardware},
	Year = {2015}}

@inproceedings{7561676,
	Abstract = {Neural networks (NNs) have been widely used in microwave device modeling. One of the greatest challenges is how to speed up the model training process and reduce the development cost. To address the issue, this paper exploits FPGAs to accelerate NN training. Experimental results demonstrate that the model training time can be reduced by up to 99.1%, compared to the traditional software implementation.},
	Author = {R. {Sang} and Q. {Liu} and Q. {Zhang}},
	Booktitle = {2016 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization (NEMO)},
	Date-Added = {2019-05-30 17:33:25 +1200},
	Date-Modified = {2019-05-30 17:33:34 +1200},
	Doi = {10.1109/NEMO.2016.7561676},
	Keywords = {FPGA-NN, cost reduction;field programmable gate arrays;learning (artificial intelligence);microwave devices;FPGA-based acceleration;neural network training;microwave device modeling;model training process;development cost reduction;Training;Field programmable gate arrays;Artificial neural networks;Software;Random access memory;Hardware;neural network;quasi-Newton method;FPGA;hardware acceleration},
	Month = {July},
	Pages = {1-2},
	Title = {FPGA-based acceleration of neural network training},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/NEMO.2016.7561676}}

@inproceedings{Conti:2018:IEE:3327345.3327410,
	Acmid = {3327410},
	Address = {USA},
	Author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	Booktitle = {Proceedings of the 32Nd International Conference on Neural Information Processing Systems},
	Date-Added = {2019-05-30 13:56:09 +1200},
	Date-Modified = {2019-05-30 13:57:58 +1200},
	Keywords = {Evolutionary, evolution strategies, Uber, Mujoco, Atari},
	Location = {Montr\&\#233;al, Canada},
	Numpages = {12},
	Pages = {5032--5043},
	Publisher = {Curran Associates Inc.},
	Series = {NIPS'18},
	Title = {Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-seeking Agents},
	Url = {http://dl.acm.org/citation.cfm?id=3327345.3327410},
	Year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3327345.3327410}}

@inproceedings{Cong:2018:UPD:3174243.3174970,
	Acmid = {3174970},
	Address = {New York, NY, USA},
	Author = {Cong, Jason and Fang, Zhenman and Lo, Michael and Wang, Hanrui and Xu, Jingxian and Zhang, Shaochong},
	Booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2019-05-30 13:29:20 +1200},
	Date-Modified = {2019-05-30 13:30:40 +1200},
	Doi = {10.1145/3174243.3174970},
	Isbn = {978-1-4503-5614-5},
	Keywords = {FPGA, accelerator, gpu, rodinia},
	Location = {Monterey, CALIFORNIA, USA},
	Numpages = {1},
	Pages = {288--288},
	Publisher = {ACM},
	Series = {FPGA '18},
	Title = {Understanding Performance Differences of FPGAs and GPUs: (Abtract Only)},
	Url = {http://doi.acm.org/10.1145/3174243.3174970},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3174243.3174970},
	Bdsk-Url-2 = {https://doi.org/10.1145/3174243.3174970}}

@inproceedings{10.1007/3-540-44614-1_27,
	Abstract = {In this paper, we show that a co-processor system with a Virtex FPGA can achieve high performance in evolutionary computations by utilizing the two features of the FPGA. First, agents in evolutionary computation models which are usually expressed using short bit-strings can be stored in distributed select RAMs of Virtex FPGAs very efficiently. Second, the partial reconfiguration and readback functions of the FPGAs make it possible to exploit more parallelism without thinking about circuits for data I/O. The preliminary results of a model base on Iterated Prisoner's Dilemma showed that the system can achieve high performance because of the two features.},
	Address = {Berlin, Heidelberg},
	Author = {Yamaguchi, Yoshiki and Miyashita, Akira and Maruyama, Tsutomu and Hoshino, Tsutomu},
	Booktitle = {Field-Programmable Logic and Applications: The Roadmap to Reconfigurable Computing},
	Date-Added = {2019-05-29 11:22:10 +1200},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {Hartenstein, Reiner W. and Gr{\"u}nbacher, Herbert},
	Isbn = {978-3-540-44614-9},
	Keywords = {Books, Evolutionary, FPGA, Co-design},
	Pages = {240--249},
	Publisher = {Springer Berlin Heidelberg},
	Title = {A Co-processor System with a Virtex FPGA for Evolutionary Computation},
	Year = {2000}}

@article{Gomez-Pulido2011,
	Abstract = {Many large combinatorial optimization problems tackled with evolutionary algorithms often require very high computational times, usually due to the fitness evaluation. This fact forces programmers to use clusters of computers, a computational solution very useful for running applications of intensive calculus but having a high acquisition price and operation cost, mainly due to the Central Processing Unit (CPU) power consumption and refrigeration devices. A low-cost and high-performance alternative comes from reconfigurable computing, a hardware technology based on Field Programmable Gate Array devices (FPGAs). The main objective of the work presented in this paper is to compare implementations on FPGAs and CPUs of different fitness functions in evolutionary algorithms in order to study the performance of the floating-point arithmetic in FPGAs and CPUs that is often present in the optimization problems tackled by these algorithms. We have taken advantage of the parallelism at chip-level of FPGAs pursuing the acceleration of the fitness functions (and consequently, of the evolutionary algorithms) and showing the parallel scalability to reach low cost, low power and high performance computational solutions based on FPGA. Finally, the recent popularity of GPUs as computational units has moved us to introduce these devices in our performance comparisons. We analyze performance in terms of computation times and economic cost.},
	Author = {Gomez-Pulido, Juan A. and Vega-Rodriguez, Miguel A. and Sanchez-Perez, Juan M. and Priem-Mendes, Silvio and Carreira, Vitor},
	Date-Added = {2019-05-29 11:20:45 +1200},
	Date-Modified = {2019-05-29 11:21:02 +1200},
	Day = {01},
	Doi = {10.1007/s10710-011-9137-2},
	Issn = {1573-7632},
	Journal = {Genetic Programming and Evolvable Machines},
	Keywords = {Evolutionary, FPGA, GPU, floating-point},
	Month = {Dec},
	Number = {4},
	Pages = {403--427},
	Title = {Accelerating floating-point fitness functions in evolutionary algorithms: a FPGA-CPU-GPU performance comparison},
	Url = {https://doi.org/10.1007/s10710-011-9137-2},
	Volume = {12},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-011-9137-2}}

@article{EmpiricalAssetML,
	Author = {Gu, Shihao and T. Kelly, Bryan and Xiu, Dacheng},
	Date-Added = {2019-05-22 13:04:51 +1200},
	Date-Modified = {2019-05-22 13:05:46 +1200},
	Doi = {10.2139/ssrn.3159577},
	Journal = {SSRN Electronic Journal},
	Keywords = {NN-Fin, Machine Learning, Return Prediction, Cross-Section of Returns, Ridge Regression, (Group) Lasso, Elastic Net, Random Forest, Gradient Boosting, (Deep) Neural Networks, Fintech},
	Month = {01},
	Title = {Empirical Asset Pricing Via Machine Learning},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.2139/ssrn.3159577}}

@inproceedings{10.1007/978-3-642-04843-2_4,
	Abstract = {Genetic Algorithm (GA) is a powerful tool for science computing, while Parallel Genetic Algorithm (PGA) further promotes the performance of computing. However, the traditional parallel computing environment is very difficult to set up, much less the price. This gives rise to the appearance of moving dense computing to graphics hardware, which is inexpensive and more powerful. The paper presents a hierarchical parallel genetic algorithm, implemented by NVIDIA's Compute Unified Device Architecture (CUDA). Mixed with master-slave parallelization method and multiple-demes parallelization method, this algorithm has contributed to better utilization of threads and high-speed shared memory in CUDA.},
	Address = {Berlin, Heidelberg},
	Annote = {Inside book Advances In Computation And Intelligence 2009},
	Author = {Zhang, Sifa and He, Zhenming},
	Booktitle = {Advances in Computation and Intelligence},
	Date-Added = {2019-05-14 18:33:01 +1200},
	Date-Modified = {2019-05-14 18:35:20 +1200},
	Editor = {Cai, Zhihua and Li, Zhenhua and Kang, Zhuo and Liu, Yong},
	Isbn = {978-3-642-04843-2},
	Keywords = {Books, Evolutionary, Advances In Computation And Intelligence 2009, Genetic Algorithm, CUDA},
	Pages = {24--30},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Implementation of Parallel Genetic Algorithm Based on CUDA},
	Year = {2009}}

@article{Tsang:2004aa,
	Abstract = {Evolutionary Dynamic Data Investment Evaluator (EDDIE) is a genetic programming (GP)-based decision support tool for financial forecasting. EDDIE itself does not replace forecasting experts. It serves to improve the productivity of experts in searching the space of decision trees, with the aim to improve the odds in its user's favour. The efficacy of EDDIE has been reported in the literature. However, discovering patterns in historical data is only the first step towards building a practical financial forecasting tool. Data preparation, rules organization and application are all important issues. This paper describes an architecture that embeds EDDIE for learning from and monitoring the stock market.},
	Author = {Tsang, Edward and Yung, Paul and Li, Jin},
	Booktitle = {Data mining for financial decision making},
	Da = {2004/09/01/},
	Date-Added = {2019-05-13 14:05:58 +1200},
	Date-Modified = {2019-05-13 14:06:27 +1200},
	Doi = {https://doi.org/10.1016/S0167-9236(03)00087-3},
	Isbn = {0167-9236},
	Journal = {Decision Support Systems},
	Keywords = {NN-Fin, Financial forecasting tools; Genetic programming},
	Number = {4},
	Pages = {559--565},
	Title = {EDDIE-Automation, a decision support tool for financial forecasting},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923603000873},
	Volume = {37},
	Year = {2004},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167923603000873},
	Bdsk-Url-2 = {https://doi.org/10.1016/S0167-9236(03)00087-3}}

@inproceedings{Brookhouse:2014:WOS:2598394.2605689,
	Acmid = {2605689},
	Address = {New York, NY, USA},
	Author = {Brookhouse, James and Otero, Fernando E.B. and Kampouridis, Michael},
	Booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
	Date-Added = {2019-05-13 13:51:50 +1200},
	Date-Modified = {2019-05-13 13:53:15 +1200},
	Doi = {10.1145/2598394.2605689},
	Isbn = {978-1-4503-2881-4},
	Keywords = {NN-Fin, financial forecasting, genetic programming, gpu, opencl},
	Location = {Vancouver, BC, Canada},
	Numpages = {8},
	Pages = {1117--1124},
	Publisher = {ACM},
	Series = {GECCO Comp '14},
	Title = {Working with OpenCL to Speed Up a Genetic Programming Financial Forecasting Algorithm: Initial Results},
	Url = {http://doi.acm.org/10.1145/2598394.2605689},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2598394.2605689},
	Bdsk-Url-2 = {https://doi.org/10.1145/2598394.2605689}}

@inproceedings{GECCO06-trading,
	Author = {Harish Subramanian and Subramanian Ramamoorthy and Peter Stone and Benjamin Kuipers},
	Booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	Date-Added = {2019-05-13 10:10:07 +1200},
	Date-Modified = {2019-05-13 10:10:32 +1200},
	Keywords = {NN-Fin, Evolutionary Algorithms, Finance, Trading},
	Month = {July},
	Title = {Designing Safe, Profitable Automated Stock Trading Agents Using Evolutionary Algorithms},
	Url = {http://www.cs.utexas.edu/users/ai-lab/?GECCO06-trading},
	Year = {2006},
	Bdsk-Url-1 = {http://www.cs.utexas.edu/users/ai-lab/?GECCO06-trading}}

@inproceedings{Camargo-Bareno:2011:GECCOcomp,
	Abstract = {This paper presents a novel a parallel genetic programming (PGP) Boolean synthesis implementation on a low cost cluster of an embedded open platform called SIE. Some tasks of the PGP have been accelerated through a hardware coprocessor called FCU, that allows to evaluate individuals onchip as intrinsic evolution. Results have been compared with GPU and HPC implementations, resulting in speedup values up to approximately 2 and 180 respectively.},
	Address = {Dublin, Ireland},
	Author = {Carlos Ivan {Camargo Bareno} and Cesar Augusto {Pedraza Bonilla} and Luis Fernado Nino and Jose Ignacio {Martinez Torre}},
	Booktitle = {GECCO '11: Proceedings of the 13th annual conference companion on Genetic and evolutionary computation},
	Date-Added = {2019-05-13 09:53:07 +1200},
	Date-Modified = {2019-05-13 09:53:30 +1200},
	Doi = {doi:10.1145/2001858.2001964},
	Editor = {Natalio Krasnogor and Pier Luca Lanzi and Andries Engelbrecht and David Pelta and Carlos Gershenson and Giovanni Squillero and Alex Freitas and Marylyn Ritchie and Mike Preuss and Christian Gagne and Yew Soon Ong and Guenther Raidl and Marcus Gallager and Jose Lozano and Carlos Coello-Coello and Dario Landa Silva and Nikolaus Hansen and Silja Meyer-Nieberg and Jim Smith and Gus Eiben and Ester Bernado-Mansilla and Will Browne and Lee Spector and Tina Yu and Jeff Clune and Greg Hornby and Man-Leung Wong and Pierre Collet and Steve Gustafson and Jean-Paul Watson and Moshe Sipper and Simon Poulding and Gabriela Ochoa and Marc Schoenauer and Carsten Witt and Anne Auger},
	Isbn13 = {978-1-4503-0690-4},
	Keywords = {Evolutionary, FPGA, genetic algorithms, genetic programming, GPU: Poster},
	Month = {12-16 } # jul,
	Notes = {Also known as \cite{2001964} Distributed on CD-ROM at GECCO-2011. ACM Order Number 910112.},
	Organisation = {SIGEVO},
	Pages = {189--190},
	Publisher = {ACM},
	Publisher_Address = {New York, NY, USA},
	Title = {Intrinsic evolvable hardware for combinatorial synthesis based on SoC+FPGA and GPU platforms},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1145/2001858.2001964}}

@inproceedings{10.1007/978-3-642-24013-3_11,
	Abstract = {We discuss the parallel implementation of Genetic Algorithms and Evolution Strategy on General-Purpose Graphical Units, using the OpenCL framework. Multiple evolutionary operators are tested (tournament, roulette wheel selection, uniform and Gaussian mutation, crossover, recombination), as well as different approaches for parallelism, for small and large problem sizes. We use the Island Model of Parallel GA, with random migration. Performance is measured using two graphic cards: NVidia GeForce GTX 560Ti and AMD Radeon 6950. Tests are performed in a distributed grid, using the Java Parallel Processing Framework.},
	Address = {Berlin, Heidelberg},
	Annote = {Inside book "Intelligent Distributed Computing V"},
	Author = {L{\H{o}}rentz, Istv{\'a}n and Andonie, R{\u{a}}zvan and Mali{\c{T}}a, Mihaela},
	Booktitle = {Intelligent Distributed Computing V},
	Date-Added = {2019-05-12 22:07:17 +1200},
	Date-Modified = {2019-05-12 22:08:18 +1200},
	Editor = {Brazier, F. M. T. and Nieuwenhuis, Kees and Pavlin, Gregor and Warnier, Martijn and Badica, Costin},
	Isbn = {978-3-642-24013-3},
	Keywords = {Books, Evolutionary, Intelligent Distributed Computing V, OpenCL, Evolutionary algorithms, Genetic Algorithm},
	Pages = {103--113},
	Publisher = {Springer Berlin Heidelberg},
	Title = {An Implementation of Evolutionary Computation Operators in OpenCL},
	Year = {2012}}

@article{7604133,
	Abstract = {In this paper we report on our advances designing and implementing an FPGA-based computation accelerator as part of a Homomorphic Encryption Processing Unit (HEPU) co-processor. This hardware accelerator technology improves the practicality of computing on encrypted data by reducing the computational bottlenecks of lattice encryption primitives that support homomorphic encryption schemes. We focus on accelerating the Chinese Remainder Transform (CRT) and inverse Chinese Remainder Transform (iCRT) for power-of-2 cyclotomic rings, but also accelerate other basic ring arithmetic such as Ring Addition, Ring Subtraction and Ring Multiplication. We instantiate this capability in a Xilinx Virtex-7 FPGA that can attach to a host computer through either a PCI-Express port or Ethernet. We focus our experimental performance analysis on the NTRU-based LTV Homomorphic Encryption scheme. This is a leveled homomorphic encryption scheme, but our accelerator is compatible with other lattice-based schemes and recent improved bootstrapping designs to support arbitrary depth computation. We experimentally compare performance with a reference software implementations of the CRT and iCRT bottlenecks and when used in a practical application of encrypted string comparison.},
	Author = {D. B. {Cousins} and K. {Rohloff} and D. {Sumorok}},
	Date-Added = {2019-05-12 00:57:09 +1200},
	Date-Modified = {2019-05-12 00:57:19 +1200},
	Doi = {10.1109/TETC.2016.2619669},
	Issn = {2168-6750},
	Journal = {IEEE Transactions on Emerging Topics in Computing},
	Keywords = {Crypto; coprocessors;cryptography;field programmable gate arrays;local area networks;logic design;peripheral interfaces;transforms;FPGA designing;homomorphic encryption processing unit coprocessor;HEPU coprocessor;hardware accelerator;inverse Chinese remainder transform;iCRT;ring arithmetic;Xilinx Virtex-7 FPGA;PCI-Express port;Ethernet;Encryption;Cathode ray tubes;Acceleration;Hardware;Field programmable gate arrays;Applied cryptography;hardware acceleration;homomorphic encryption},
	Month = {April},
	Number = {2},
	Pages = {193-206},
	Title = {Designing an FPGA-Accelerated Homomorphic Encryption Co-Processor},
	Volume = {5},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TETC.2016.2619669}}

@article{8318681,
	Abstract = {In this paper, we present an FPGA based hardware accelerator `$\mathsf{HEPCloud}$' for homomorphic evaluations of medium depth functions which has applications in cloud computing. Our$\mathsf{HEPCloud}$architecture supports the polynomial ring based homomorphic encryption scheme FV for a ring-LWE parameter set of dimension$2^{15}$, modulus size 1,228-bit, and a standard deviation 50. This parameter-set offers a multiplicative depth 36 and at least 85 bit security. The processor of$\mathsf{HEPCloud}$is composed of multiple parallel cores. To achieve fast computation time for such a large parameter-set, various optimizations in both algorithm and architecture levels are performed. For fast polynomial multiplications, we use CRT with NTT and achieve two dimensional parallelism in$\mathsf{HEPCloud}$. We optimize the BRAM access, use a fast Barrett like polynomial reduction method, optimize the cost of CRT, and design a fast divide-and-round unit. Beside parallel processing, we apply pipelining strategy in several of the sequential building blocks to reduce the impact of sequential computations. Finally, we implement$\mathsf{HEPCloud}$on a medium-size Xilinx Virtex 6 FPGA board ML605 board and measure its on-board performance. To store the ciphertexts during a homomorphic function evaluation, we use the large DDR3 memory of the ML605 board. Our FPGA-based implementation of$\mathsf{HEPCloud}$computes a homomorphic multiplication in 26.67 s, of which the actual computation takes only 3.36 s and the rest is spent for off-chip memory access. It requires about 37,551 s to evaluate the SIMON-64/128 block cipher, but the per-block timing is only about 18 s because$\mathsf{HEPCloud}$processes 2,048 blocks simultaneously. The results show that FPGA-based acceleration of homomorphic function evaluations is feasible, but fast memory interface is crucial for the performance.},
	Author = {S. {Sinha Roy} and K. {J{\"a}rvinen} and J. {Vliegen} and F. {Vercauteren} and I. {Verbauwhede}},
	Date-Added = {2019-05-12 00:55:19 +1200},
	Date-Modified = {2019-05-12 00:55:33 +1200},
	Doi = {10.1109/TC.2018.2816640},
	Issn = {0018-9340},
	Journal = {IEEE Transactions on Computers},
	Keywords = {Crypto; Hardware;Encryption;Computer architecture;Cloud computing;Acceleration;Homomorphic encryption;FV;lattice-based cryptography;ring-LWE;polynomial multiplication;number theoretic transform;hardware implementation},
	Month = {Nov},
	Number = {11},
	Pages = {1637-1650},
	Title = {HEPCloud: An FPGA-Based Multicore Processor for FV Somewhat Homomorphic Function Evaluation},
	Volume = {67},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TC.2018.2816640}}

@misc{TFHE,
	Author = {Ilaria Chillotti and Nicolas Gama and Mariya Georgieva and Malika Izabach{\`e}ne},
	Date-Added = {2019-05-12 00:39:10 +1200},
	Date-Modified = {2019-05-12 00:39:22 +1200},
	Keywords = {Crypto, Library, Homomorphic encryption},
	Note = {https://tfhe.github.io/tfhe/},
	Title = {{TFHE}: Fast Fully Homomorphic Encryption Library},
	Year = {August 2016}}

@article{Roy2019FPGABasedHP,
	Author = {Sujoy Sinha Roy and Furkan Turan and Kimmo J{\"a}rvinen and Frederik Vercauteren and Ingrid Verbauwhede},
	Date-Added = {2019-05-12 00:31:44 +1200},
	Date-Modified = {2019-05-12 00:31:58 +1200},
	Journal = {2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
	Keywords = {Crypto, Homomorphic encryption, FPGA},
	Pages = {387-398},
	Title = {FPGA-Based High-Performance Parallel Architecture for Homomorphic Computing on Encrypted Data},
	Year = {2019}}

@article{Barcau2018BoundedFH,
	Author = {Mugurel Barcau and Vicentiu Pasol},
	Date-Added = {2019-05-12 00:14:22 +1200},
	Date-Modified = {2019-05-12 00:14:36 +1200},
	Journal = {IACR Cryptology ePrint Archive},
	Keywords = {Crypto, Patented, Monoid},
	Pages = {584},
	Title = {Bounded Fully Homomorphic Encryption from Monoid Algebras},
	Volume = {2018},
	Year = {2018}}

@article{journals/iacr/BouraGG18,
	Added-At = {2018-09-12T00:00:00.000+0200},
	Author = {Boura, Christina and Gama, Nicolas and Georgieva, Mariya},
	Biburl = {https://www.bibsonomy.org/bibtex/23a26ad6c2b19b760421d9d209b35db7b/dblp},
	Date-Added = {2019-05-12 00:02:23 +1200},
	Date-Modified = {2019-05-12 00:13:27 +1200},
	Ee = {https://eprint.iacr.org/2018/758},
	Interhash = {6fdde7eaad29a2d2e98bf0a44d60cea2},
	Intrahash = {3a26ad6c2b19b760421d9d209b35db7b},
	Journal = {IACR Cryptology ePrint Archive},
	Keywords = {Crypto, Homomorphic encryption},
	Pages = 758,
	Timestamp = {2018-09-13T11:37:07.000+0200},
	Title = {Chimera: a unified framework for B/FV, TFHE and HEAAN fully homomorphic encryption and predictions for deep learning.},
	Url = {http://dblp.uni-trier.de/db/journals/iacr/iacr2018.html#BouraGG18},
	Volume = 2018,
	Year = 2018,
	Bdsk-Url-1 = {http://dblp.uni-trier.de/db/journals/iacr/iacr2018.html#BouraGG18}}

@inproceedings{pmlr-v80-sanyal18a,
	Abstract = {Machine learning methods are widely used for a variety of prediction problems. Prediction as a service is a paradigm in which service providers with technological expertise and computational resources may perform predictions for clients. However, data privacy severely restricts the applicability of such services, unless measures to keep client data private (even from the service provider) are designed. Equally important is to minimize the nature of computation and amount of communication required between client and server. Fully homomorphic encryption offers a way out, whereby clients may encrypt their data, and on which the server may perform arithmetic computations. The one drawback of using fully homomorphic encryption is the amount of time required to evaluate large machine learning models on encrypted data. We combine several ideas from the machine learning literature, particularly work on quantization and sparsification of neural networks, together with algorithmic tools to speed-up and parallelize computation using encrypted data.},
	Address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	Author = {Sanyal, Amartya and Kusner, Matt and Gascon, Adria and Kanade, Varun},
	Booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	Date-Added = {2019-05-11 23:51:50 +1200},
	Date-Modified = {2019-05-11 23:52:20 +1200},
	Editor = {Dy, Jennifer and Krause, Andreas},
	Keywords = {Crypto, Homomorphic encryption, PaaS},
	Month = {10--15 Jul},
	Pages = {4490--4499},
	Pdf = {http://proceedings.mlr.press/v80/sanyal18a/sanyal18a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{TAPAS}: Tricks to Accelerate (encrypted) Prediction As a Service},
	Url = {http://proceedings.mlr.press/v80/sanyal18a.html},
	Volume = {80},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v80/sanyal18a.html}}

@article{2018arXiv181000845D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181000845D},
	Archiveprefix = {arXiv},
	Author = {{Dathathri}, R. and {Saarikivi}, O. and {Chen}, H. and {Laine}, K. and {Lauter}, K. and {Maleki}, S. and {Musuvathi}, M. and {Mytkowicz}, T.},
	Date-Added = {2019-05-11 23:38:53 +1200},
	Date-Modified = {2019-05-11 23:40:32 +1200},
	Eprint = {1810.00845},
	Journal = {Privacy Preserving Machine Learning},
	Keywords = {Crypto, Machine Learning, Computer Science - Cryptography and Security, Computer Science - Programming Languages, Statistics - Machine Learning},
	Month = Dec,
	Title = {{CHET: Compiler and Runtime for Homomorphic Evaluation of Tensor Programs}},
	Year = 2018}

@inproceedings{crypto-2018-28796,
	Author = {Florian Bourse and Michele Minelli and Matthias Minihold and Pascal Paillier},
	Booktitle = {Advances in Cryptology -- CRYPTO 2018},
	Date-Added = {2019-05-11 22:29:00 +1200},
	Date-Modified = {2019-05-11 23:16:46 +1200},
	Doi = {10.1007/978-3-319-96878-0_17},
	Keywords = {Crypto, Homomorphic encryption, Git, MNIST},
	Pages = {483-512},
	Publisher = {Springer},
	Series = {Lecture Notes in Computer Science},
	Title = {Fast Homomorphic Evaluation of Deep Discretized Neural Networks},
	Volume = {10993},
	Year = 2018,
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-319-96878-0_17}}

@inproceedings{10.1007/978-3-540-68083-3_25,
	Abstract = {Scientific computing applications with highly demanding data capacity and computation power drive a computing platform migration from shared memory machines to multi-core/multiprocessor computer clusters. However, overheads in coordinating operations across computing nodes could counteract the benefit of having extra machines. Furthermore, the hidden dependency in applications slows down the simulation over non-shared memory machines. This paper proposed a framework to utilize multi-core/multiprocessor clusters for distributed simulation. Among several coordination schemes, decentralized control approach has demonstrated its effectiveness in reducing the communication overheads. A speculative execution strategy is applied to exploit parallelism thoroughly and overcome strong data dependency. Performance analysis and experiments are provided to demonstrate the performance gains.},
	Address = {Berlin, Heidelberg},
	Annote = {Inside book "Advances in Grid and Pervasive Computing"},
	Author = {Li, Ruipeng and Jiang, Hai and Su, Hung-Chi and Zhang, Bin and Jenness, Jeff},
	Booktitle = {Advances in Grid and Pervasive Computing},
	Date-Added = {2019-05-10 19:51:09 +1200},
	Date-Modified = {2019-05-10 19:51:40 +1200},
	Editor = {Wu, Song and Yang, Laurence T. and Xu, Tony Li},
	Isbn = {978-3-540-68083-3},
	Keywords = {Books, Decentralized, Advances in Grid and Pervasive Computing},
	Pages = {244--255},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Parallel and Distributed Particle Collision Simulation with Decentralized Control},
	Year = {2008}}

@inproceedings{10.1007/978-3-319-19282-6_4,
	Abstract = {Comingle is a logic programming framework aimed at simplifying the development of applications distributed over multiple mobile devices. Applications are written as a single declarative program (in a system-centric way) rather than in the traditional node-centric manner, where separate communicating code is written for each participating node. Comingle is based on committed-choice multiset rewriting and is founded on linear logic. We describe a prototype targeting the Android operating system and illustrate how Comingle is used to program distributed mobile applications. As a proof of concept, we discuss several such applications orchestrated using Comingle.},
	Address = {Cham},
	Annote = {Inside book "Coordination Models and Languages"},
	Author = {Lam, Edmund Soon Lee and Cervesato, Iliano and Fatima, Nabeeha},
	Booktitle = {Coordination Models and Languages},
	Date-Added = {2019-05-10 19:48:30 +1200},
	Date-Modified = {2019-05-10 19:48:55 +1200},
	Editor = {Holvoet, Tom and Viroli, Mirko},
	Isbn = {978-3-319-19282-6},
	Keywords = {Books, Decentralized, Coordination Models and Languages},
	Pages = {51--66},
	Publisher = {Springer International Publishing},
	Title = {Comingle: Distributed Logic Programming for Decentralized Mobile Ensembles},
	Year = {2015}}

@inbook{Tantitharanukul2015,
	Abstract = {Decentralized distributed systems, such as grids, clouds or networks of sensors, have been widely investigated recently. An important nature of such systems is the heterogeneity of their resources; in order to archive the availability, scalability and flexibility. As a consequence, managing the systems to meet requirements is obviously a nontrivial work. The issue is even more challenging in term of job scheduling when the task dependency within each job exists. In this paper, we address such problem of job scheduling, so called workflow-based job scheduling, in the decentralized distributed systems with heterogeneous resources. As such problem is proven to be an NP-complete problem, an efficient heuristic algorithm to address this problem is proposed. The algorithm is based on an observation that the heterogeneity of the resources can affect the execution time of the scheduling. We compare the effectiveness and efficiency of the proposed algorithm with a baseline algorithm. The result shows that our algorithm is highly effective and efficient for the scheduling problem in the decentralized distributed system with heterogeneous resources environment both in terms of the solution quality and the execution time respectively.},
	Address = {Cham},
	Author = {Tantitharanukul, Nasi and Natwichai, Juggapong and Boonma, Pruet},
	Booktitle = {Computer and Information Science},
	Date-Added = {2019-05-10 19:37:11 +1200},
	Date-Modified = {2019-05-10 19:37:31 +1200},
	Doi = {10.1007/978-3-319-10509-3_8},
	Editor = {Lee, Roger},
	Isbn = {978-3-319-10509-3},
	Keywords = {Books, Decentralized, Computer and Information Science},
	Pages = {101--114},
	Publisher = {Springer International Publishing},
	Title = {A Heuristic Algorithm for Workflow-Based Job Scheduling in Decentralized Distributed Systems with Heterogeneous Resources},
	Url = {https://doi.org/10.1007/978-3-319-10509-3_8},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-319-10509-3_8}}

@inproceedings{10.1007/978-3-642-22110-1_41,
	Abstract = {In distributed systems, local controllers often need to impose global guarantees. A solution that will not impose additional synchronization may not be feasible due to the lack of ability of one process to know the current situation at another. On the other hand, a completely centralized solution will eliminate all concurrency. A good solution is usually a compromise between these extremes, where synchronization is allowed for in principle, but avoided whenever possible. In a quest for practicable solutions to the distributed control problem, one can constrain the executions of a system based on the pre-calculation of knowledge properties and allow for temporary interprocess synchronization in order to combine the knowledge needed to control the system. This type of control, however, may incur a heavy communication overhead. We introduce the use of simple supervisor processes that accumulate information about processes until sufficient knowledge is collected to allow for safe progression. We combine the knowledge approach with a game theoretic search that prevents progressing to states from which there is no way to guarantee the imposed constraints.},
	Address = {Berlin, Heidelberg},
	Annote = {Inside book "Computer Aided Verification"},
	Author = {Katz, Gal and Peled, Doron and Schewe, Sven},
	Booktitle = {Computer Aided Verification},
	Date-Added = {2019-05-10 19:25:17 +1200},
	Date-Modified = {2019-05-10 19:25:47 +1200},
	Editor = {Gopalakrishnan, Ganesh and Qadeer, Shaz},
	Isbn = {978-3-642-22110-1},
	Keywords = {Books, Decentralized, Computer Aided Verification},
	Pages = {510--525},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Synthesis of Distributed Control through Knowledge Accumulation},
	Year = {2011}}

@inbook{Kushner1997,
	Abstract = {This chapter is concerned with decentralized and asynchronous forms of the stochastic approximation algorithms, a relatively new area of research. Compared with the rapid progress and extensive literature in stochastic approximation methods, the study of parallel stochastic approximations is still in its infancy. Perhaps the first work on the subject was [14], which dealt with a very particular class of algorithms, where similar computations were done by several processors and convex combinations were taken. The general ideas of weak convergence theory were applied to a fairly broad class of realistic algorithms in [111, 112]. The general ideas presented there and in [105, 171] form the basis of this chapter. Analogously to the problems in Chapter 8, those methods can handle correlated and state dependent noise, delays in communication, and asynchronous and distributed network forms. Various examples are given in Section 1. In the basic model, there are several processors; each one is responsible for the updating of only a part of the parameter vector. There might be overlaps in that several processors contribute to the updating of the same component of the parameter. Such models were treated in [105, 171, 172]. For a similar model, the problem of finding zeros of a nonlinear function with noisy observations via parallel processing methods and with random truncation bounds was treated in [201]. An attempt to get real-time implementable procedures via pipelining (see Section 1.2) of communication and computation for algorithms with ``delayed'' observations was in [203].},
	Address = {New York, NY},
	Annote = {Part of book "Stochastic Approximation Algorithms and Applications"},
	Author = {Kushner, Harold J. and Yin, G. George},
	Booktitle = {Stochastic Approximation Algorithms and Applications},
	Date-Added = {2019-05-10 19:14:26 +1200},
	Date-Modified = {2019-05-10 19:15:05 +1200},
	Doi = {10.1007/978-1-4899-2696-8_12},
	Isbn = {978-1-4899-2696-8},
	Keywords = {Books, Decentralized, Stochastic Approximation Algorithms and Applications},
	Pages = {347--391},
	Publisher = {Springer New York},
	Title = {Distributed/Decentralized and Asynchronous Algorithms},
	Url = {https://doi.org/10.1007/978-1-4899-2696-8_12},
	Year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4899-2696-8_12}}

@inproceedings{10.1007/978-3-319-22572-2_11,
	Abstract = {In this paper we present BitWorker, a platform for community distributed computing based on BitTorrent. Any splittable task can be easily specified by a user in a meta-information task file, such that it can be downloaded and performed by other volunteers. Peers find each other using Distributed Hash Tables, download existing results, and compute missing ones. Unlike existing distributed computing schemes relying on centralized coordination point(s), our scheme is totally distributed, therefore, highly robust. We evaluate the performance of BitWorker using mathematical models and real tests, showing processing and robustness gains. BitWorker is available for download [1] and use by the community.},
	Address = {Cham},
	Annote = {Part of Book "Wired/Wireless Internet Communications"},
	Author = {Durand, Arnaud and Gasparian, Mikael and Rouvinez, Thomas and Aad, Imad and Braun, Torsten and Trinh, Tuan Anh},
	Booktitle = {Wired/Wireless Internet Communications},
	Date-Added = {2019-05-10 19:10:21 +1200},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {Aguayo-Torres, Mari Carmen and G{\'o}mez, Gerardo and Poncela, Javier},
	Isbn = {978-3-319-22572-2},
	Keywords = {Books, Decentralized},
	Pages = {151--164},
	Publisher = {Springer International Publishing},
	Title = {BitWorker, a Decentralized Distributed Computing System Based on BitTorrent},
	Year = {2015}}

@article{article,
	Author = {Alesawy, Othman and Muniyandi, Ravie},
	Date-Added = {2019-05-09 11:30:09 +1200},
	Date-Modified = {2019-05-09 11:31:27 +1200},
	Doi = {10.3923/itj.2016.77.83},
	Journal = {Information Technology Journal},
	Keywords = {Evolutionary; Cryptography; Diffie-Hellman; Neural Network, Genetic Algorithm},
	Month = {06},
	Pages = {77-83},
	Title = {Elliptic Curve Diffie-Hellman Random Keys Using Artificial Neural Network and Genetic Algorithm for Secure Data over Private Cloud},
	Volume = {15},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.3923/itj.2016.77.83}}

@article{CryptoNNSurvey,
	Author = {A El-Zoghabi, Adel and Yassin, Amr and Hamdy, Hany},
	Date-Added = {2019-05-08 21:34:28 +1200},
	Date-Modified = {2019-05-08 21:36:49 +1200},
	Journal = {International Journal of Emerging Technology and Advanced Engineering},
	Keywords = {NN; Cryptography, Survey},
	Month = {12},
	Pages = {449-455},
	Title = {Survey Report on Cryptography Based on Neural Network},
	Volume = {9001},
	Year = {2013}}

@inproceedings{6612177,
	Abstract = {By making use of Artificial Intelligence (AI), Human Intelligence can be simulated by a machine, Neural Networks is one such sub field of AI. Artificial Neural Networks (ANN) consists of neurons and weights assigned to inter neuron connections helps in storing the acquired knowledge. This paper makes use of Hebbian learning rule to train the ANN of both sender and receiver machines. In the field of Public Key Cryptography (PKC), Pseudo Random Number Generator (PRNG) are widely used to generate unique keys and random numbers used in ANN which are found to possess many types of possible attacks. It is essential for a key to possess randomness for key strength and security. This paper proposes key generation for PKC by application of ANN using Genetic Algorithm (GA). It was noticed that use of ANN along with GA has not as yet been explored. GA approach is often applied for obtaining optimization and solutions in search problems. GA correlates to the nature to a large extent producing population of numbers where number possessing higher fitness value is replicated more. Thus, making GA a very good contender for PRNGs. Good Fitness function helps in exploring search space of random numbers in more efficient manner. GA PRNGs result samples satisfies frequency test and gap test. Thus the numbers generated after each iteration by GA PRNG are statistically verified to be random and nonrepeating, having no prior relation of next number from the previous ones, acting as an essential initialization parameter for neural algorithm overcomes the problem of acknowledging the random number generated by traditional PRNG. For generating public and private keys, different number of rounds of mixing is used. This ensures that the private key generated cannot be derived from public key. Our algorithm was observed to give fast and improved performance results having practical and feasible implementation.},
	Author = {S. {Jhajharia} and S. {Mishra} and S. {Bali}},
	Booktitle = {2013 Sixth International Conference on Contemporary Computing (IC3)},
	Date-Added = {2019-05-08 21:12:10 +1200},
	Date-Modified = {2019-05-08 21:12:18 +1200},
	Doi = {10.1109/IC3.2013.6612177},
	Keywords = {NN; genetic algorithms;neural nets;public key cryptography;search problems;public key cryptography;genetic algorithm;artificial intelligence;human intelligence;artificial neural networks;neurons;interneuron connection;acquired knowledge storage;Hebbian learning rule;ANN training;sender machine;receiver machine;PKC;pseudo random number generator;PRNG;unique key generation;key strength;security;optimization;search problem;fitness value;search space exploration;random numbers;frequency test;gap test;initialization parameter;neural algorithm;public key generation;private key generation;Genetic algorithms;Artificial neural networks;Biological cells;Sociology;Statistics;Neurons;Biological neural networks;artificial neural networks;neural cryptography;hebbian theory;genetic algorithm;public key cryptography;random number},
	Month = {Aug},
	Pages = {137-142},
	Title = {Public key cryptography using neural networks and genetic algorithms},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/IC3.2013.6612177}}

@article{2018arXiv181108162G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181108162G},
	Archiveprefix = {arXiv},
	Author = {{Goyal}, M. and {Tatwawadi}, K. and {Chandak}, S. and {Ochoa}, I.},
	Date-Added = {2019-05-08 16:38:43 +1200},
	Date-Modified = {2019-05-08 16:38:51 +1200},
	Eprint = {1811.08162},
	Journal = {arXiv e-prints},
	Keywords = {NN, Computation and Language, Electrical Engineering and Systems Science - Signal Processing, Quantitative Biology - Genomics},
	Month = nov,
	Primaryclass = {cs.CL},
	Title = {{DeepZip: Lossless Data Compression using Recurrent Neural Networks}},
	Year = 2018}

@article{2017arXiv171111279K,
	Archiveprefix = {arXiv},
	Author = {{Kim}, B. and Wattenberg M. and {Gilmer}, J. and {Cai} C. and {Wexler} J. and and {Viegas}, F. and {Sayres}, R.},
	Date-Added = {2019-05-08 13:01:02 +1200},
	Date-Modified = {2019-05-08 13:02:11 +1200},
	Eprint = {1711.11279},
	Journal = {ICML},
	Keywords = {NN, Explainable AI, Concept Activation Vectors},
	Title = {{ Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV) }},
	Year = 2018}

@inproceedings{pmlr-v80-srouji18a,
	Abstract = {In recent years, Deep Reinforcement Learning has made impressive advances in solving several important benchmark problems for sequential decision making. Many control applications use a generic multilayer perceptron (MLP) for non-vision parts of the policy network. In this work, we propose a new neural network architecture for the policy network representation that is simple yet effective. The proposed Structured Control Net (SCN) splits the generic MLP into two separate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hypothesize that this will bring together the benefits of both linear and nonlinear policies: improve training sample efficiency, final episodic reward, and generalization of learned policy, while requiring a smaller network and being generally applicable to different training methods. We validated our hypothesis with competitive results on simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom urban driving environment, with various ablation and generalization tests, trained with multiple black-box and policy gradient training methods. The proposed architecture has the potential to improve upon broader control tasks by incorporating problem specific priors into the architecture. As a case study, we demonstrate much improved performance for locomotion tasks by emulating the biological central pattern generators (CPGs) as the nonlinear part of the architecture.},
	Address = {Stockholmsm{\"a}ssan, Stockholm Sweden},
	Author = {Srouji, Mario and Zhang, Jian and Salakhutdinov, Ruslan},
	Booktitle = {Proceedings of the 35th International Conference on Machine Learning},
	Date-Added = {2019-05-07 22:07:24 +1200},
	Date-Modified = {2019-05-07 22:07:49 +1200},
	Editor = {Dy, Jennifer and Krause, Andreas},
	Keywords = {Other, Reinforcement Learning, OpenAI Gym},
	Month = {10--15 Jul},
	Pages = {4742--4751},
	Pdf = {http://proceedings.mlr.press/v80/srouji18a/srouji18a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Structured Control Nets for Deep Reinforcement Learning},
	Url = {http://proceedings.mlr.press/v80/srouji18a.html},
	Volume = {80},
	Year = {2018},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v80/srouji18a.html}}

@inproceedings{Duan:2016:BDR:3045390.3045531,
	Acmid = {3045531},
	Author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	Booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	Date-Added = {2019-05-07 19:16:39 +1200},
	Date-Modified = {2019-05-07 19:17:04 +1200},
	Keywords = {Other, Reinforcement Learning, benchmark data},
	Location = {New York, NY, USA},
	Numpages = {10},
	Pages = {1329--1338},
	Publisher = {JMLR.org},
	Series = {ICML'16},
	Title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	Url = {http://dl.acm.org/citation.cfm?id=3045390.3045531},
	Year = {2016},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3045390.3045531}}

@electronic{nchainv0OpenAI,
	Author = {blole},
	Date-Added = {2019-05-02 22:52:37 +1200},
	Date-Modified = {2019-05-02 22:54:27 +1200},
	Keywords = {WebPage, OpenAI Gym, SUNA},
	Lastchecked = {2/May/2019},
	Title = {Algorithm on NChain-v0},
	Url = {https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/},
	Year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/}}

@electronic{duplicatedOpenAIGym,
	Author = {colinmorris},
	Date-Added = {2019-04-30 21:35:34 +1200},
	Date-Modified = {2019-04-30 21:37:44 +1200},
	Keywords = {WebPage, OpenAI Gym, Reinforcement Learning, BiSUNA},
	Lastchecked = {30 Apr 2019},
	Title = {Algorithm on DuplicatedInput-v0},
	Url = {https://gym.openai.com/evaluations/eval_5PlrBv8wRNGI2J1lp3otUA/},
	Year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_5PlrBv8wRNGI2J1lp3otUA/}}

@electronic{rouletteV0OpenAI,
	Author = {tanemaki},
	Date-Added = {2019-04-30 21:30:55 +1200},
	Date-Modified = {2019-04-30 21:38:56 +1200},
	Keywords = {WebPage, OpenAI Gym, Reinforcement Learning, BiSUNA},
	Lastchecked = {30 Apr 2019},
	Title = {Algorithm on Roulette-v0},
	Url = {https://gym.openai.com/evaluations/eval_DFWdtrdSCikuZWwf8HN8A/},
	Year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_DFWdtrdSCikuZWwf8HN8A/}}

@phdthesis{Moore-1991-13223,
	Address = {Pittsburgh, PA},
	Author = {Andrew Moore},
	Date-Added = {2019-04-30 21:14:43 +1200},
	Date-Modified = {2019-04-30 21:15:10 +1200},
	Keywords = {NN, SUNA, Reinforcement Learning, Mountain Car},
	Month = {March},
	School = {Carnegie Mellon University},
	Title = {Efficient Memory-based Learning for Robot Control},
	Year = {1991}}

@inproceedings{8398183,
	Abstract = {Stock price volatility is a highly complex nonlinear dynamic system. The stock's trading volume affects the stock's self correlation, self correlation and inertial effect, and the adjustment of the stock is not to advance with a homogeneous time process, which has its own independent time to promote the process. LSTM (Term Memory Long-Short) is a kind of time recurrent neural network, which is suitable for processing and predicting the important events of interval and long delay in time series. Based on temporal characteristics of stock and LSTM neural network algorithm, this paper uses the LSTM recurrent neural networks to filter, extract feature value and analyze the stock data, and set up the the prediction model of the corresponding stock transaction.},
	Author = {S. {Liu} and G. {Liao} and Y. {Ding}},
	Booktitle = {2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA)},
	Date-Added = {2019-04-29 12:00:47 +1200},
	Date-Modified = {2019-04-29 12:01:04 +1200},
	Doi = {10.1109/ICIEA.2018.8398183},
	Issn = {2158-2297},
	Keywords = {NN-Fin; SUNA; nonlinear dynamical systems;pricing;recurrent neural nets;stock markets;time series;homogeneous time process;time recurrent neural network;time series;LSTM neural network algorithm;LSTM recurrent neural networks;stock price volatility;nonlinear dynamic system;stock transaction prediction modeling;stock trading volume;stock self correlation;term memory long short;feature value extraction;stock data analysis;Predictive models;Logic gates;Computational modeling;Indexes;Neural networks;Time series analysis;Feature extraction;machine learning;neural network;stock transaction prediction;LSTM},
	Month = {May},
	Pages = {2787-2790},
	Title = {Stock transaction prediction modeling and analysis based on LSTM},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIEA.2018.8398183}}

@inproceedings{6215974,
	Abstract = {Flip-flops are critical timing elements in digital circuits which have a large impact on circuit speed and power consumption. The performance of the Flip-Flop is an important element to determine the performance of the whole synchronous circuit. The pulse generator can be shared among many flip-flops to reduce the power dissipation. Firstly, in the Dual edge static pulsed flip-flop suffers from high leakage current leads to more power consumption. Secondly, Dual edge trigger sense amplifier flip-flop having unnecessary transitions which causes power consumption. Thirdly, Dual edge trigger NAND keeper flip-flop keeper technique is used to pull up the voltage to VDD having full swing and this keeper transistor width is high and which consumes more power. The power consumption of the Dual edge nand keeper flip-flop is 347uW. Lastly, Dual edge trigger pulsed flip-flop is introduced by employing a technique called conditional switching for further power reduction. The circuits are designed in a 0.18-um standard CMOS process with a 1.8V power supply voltage.},
	Author = {D. {Bhargavaram} and M. G. K. {Pillai}},
	Booktitle = {IEEE-International Conference On Advances In Engineering, Science And Management (ICAESM -2012)},
	Date-Added = {2019-04-29 11:48:45 +1200},
	Date-Modified = {2019-04-29 11:48:58 +1200},
	Keywords = {Other, SUNA, circuit switching;CMOS logic circuits;flip-flops;logic design;low-power electronics;NAND circuits;fow power dual edge triggered flip-flop;digital circuit;power consumption;synchronous circuit;pulse generator;power dissipation reduction;dual edge static pulsed flip-flop;dual edge trigger sense amplifier flip-flop;dual edge trigger NAND keeper flip-flop keeper technique;keeper transistor width;conditional switching;circuit design;CMOS process;power 347 muW;voltage 1.8 V;size 0.18 mum;Switches;Flip-flops;Clocks;MOS devices;Topology;Dual pulse generator;sense amplifier flip-flop;Static pulsed flip-flop;NAND keeper flip-flop;Pulsed flip-flop},
	Month = {March},
	Pages = {63-67},
	Title = {Low power dual edge triggered flip-flop},
	Year = {2012}}

@article{6804688,
	Abstract = {The ability to carry out signal processing, classification, recognition, and computation in artificial spiking neural networks (SNNs) is mediated by their synapses. In particular, through activity-dependent alteration of their efficacies, synapses play a fundamental role in learning. The mathematical prescriptions under which synapses modify their weights are termed synaptic plasticity rules. These learning rules can be based on abstract computational neuroscience models or on detailed biophysical ones. As these rules are being proposed and developed by experimental and computational neuroscientists, engineers strive to design and implement them in silicon and en masse in order to employ them in complex real-world applications. In this paper, we describe analog very large-scale integration (VLSI) circuit implementations of multiple synaptic plasticity rules, ranging from phenomenological ones (e.g., based on spike timing, mean firing rates, or both) to biophysically realistic ones (e.g., calcium-dependent models). We discuss the application domains, weaknesses, and strengths of various representative approaches proposed in the literature, and provide insight into the challenges that engineers face when designing and implementing synaptic plasticity rules in VLSI technology for utilizing them in real-world applications.},
	Author = {M. {Rahimi Azghadi} and N. {Iannella} and S. F. {Al-Sarawi} and G. {Indiveri} and D. {Abbott}},
	Date-Added = {2019-04-28 22:43:25 +1200},
	Date-Modified = {2019-04-28 22:43:25 +1200},
	Doi = {10.1109/JPROC.2014.2314454},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Keywords = {analogue integrated circuits;learning (artificial intelligence);neural chips;VLSI;spike based synaptic plasticity;artificial spiking neural networks;mathematical prescriptions;learning rules;abstract computational neuroscience models;analog very large scale integration circuit;VLSI;neural chips;Transistors;Logic gates;Silicon;Neuromorphics;Neurons;Neuroscience;Learning systems;Plastics;Analog/digital synapse;Bienenstock--Cooper--Munro (BCM);calcium-based plasticity;learning;local correlation plasticity (LCP);neuromorphic engineering;rate-based plasticity;spike-timing-dependent plasticity (STDP);spike-based plasticity;spiking neural networks;synaptic plasticity;triplet STDP;very large-scale integration (VLSI);voltage-based STDP;Analog/digital synapse;Bienenstock¿Cooper¿Munro (BCM);calcium-based plasticity;learning;local correlation plasticity (LCP);neuromorphic engineering;rate-based plasticity;spike-timing-dependent plasticity (STDP);spike-based plasticity;spiking neural networks;synaptic plasticity;triplet STDP;very large-scale integration (VLSI);voltage-based STDP},
	Month = {May},
	Number = {5},
	Pages = {717-737},
	Title = {Spike-Based Synaptic Plasticity in Silicon: Design, Implementation, Application, and Challenges},
	Volume = {102},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/JPROC.2014.2314454}}

@article{5678632,
	Abstract = {Neurological disorders are becoming increasingly common in developed countries as a result of the aging population. In spite of medications, these disorders can result in progressive loss of function as well as chronic physical, cognitive, and emotional disability that ultimately places enormous emotional and economic on the patient, caretakers, and the society in general. Neuromodulation is emerging as a therapeutic option in these patients. Neuromodulation is a field, which involves implantable devices that allow for the reversible adjustable application of electrical, chemical, or biological agents to the central or peripheral nervous system with the objective of altering its functioning with the objective of achieving a therapeutic or clinically beneficial effect. It is a rapidly evolving field that brings together many different specialties in the fields of medicine, materials science, computer science and technology, biomedical, and neural engineering as well as the surgical or interventional specialties. It has multiple current and emerging indications, and an enormous potential for growth. The main challenges before it are in the need for effective collaboration between engineers, basic scientists, and clinicians to develop innovations that address specific problems resulting in new devices and clinical applications.},
	Author = {C. O. {Oluigbo} and A. R. {Rezai}},
	Date-Added = {2019-04-28 20:57:10 +1200},
	Date-Modified = {2019-04-28 20:57:24 +1200},
	Doi = {10.1109/TBME.2010.2102758},
	Issn = {0018-9294},
	Journal = {IEEE Transactions on Biomedical Engineering},
	Keywords = {Other; SUNA; biochemistry;bioelectric phenomena;brain;cognition;electrochemistry;macromolecules;medical disorders;molecular biophysics;neuromuscular stimulation;prosthetics;neurological disorders;neuromodulation;emotional disability;aging population;chronic physical disability;patient therapy;implantable devices;reversible adjustable application;electrical agents;chemical agents;biological agents;central nervous system;peripheral nervous system;clinical applications;electrical stimulation;cognitive disability;Neuromuscular;Aging;Medical services;Neuromodulation;Gerontology;Neuromodulation;neurological disorders;Adult;Biomedical Engineering;Deep Brain Stimulation;Electric Stimulation Therapy;Electric Stimulation Therapy;Electrodes, Implanted;Female;Humans;Infusion Pumps, Implantable;Male;Nerve Net;Nervous System Diseases;Neurosciences;Vagus Nerve Stimulation},
	Month = {July},
	Number = {7},
	Pages = {1907-1917},
	Title = {Addressing Neurological Disorders With Neuromodulation},
	Volume = {58},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TBME.2010.2102758}}

@phdthesis{stanley:phd04,
	Author = {Kenneth O. Stanley},
	Date-Added = {2019-04-28 16:54:44 +1200},
	Date-Modified = {2019-04-28 16:54:59 +1200},
	Keywords = {Books, NeuroEvolution, NEAT},
	School = {Department of Computer Sciences, The University of Texas at Austin},
	Title = {Efficient Evolution of Neural Networks Through Complexification},
	Url = {http://nn.cs.utexas.edu/?stanley:phd2004},
	Year = {2004},
	Bdsk-Url-1 = {http://nn.cs.utexas.edu/?stanley:phd2004}}

@article{Siebel:2007:ERL:1367012.1367016,
	Acmid = {1367016},
	Address = {Amsterdam, The Netherlands, The Netherlands},
	Author = {Siebel, Nils T. and Sommer, Gerald},
	Date-Added = {2019-04-28 16:41:08 +1200},
	Date-Modified = {2019-04-28 16:41:33 +1200},
	Issn = {1448-5869},
	Issue_Date = {August 2007},
	Journal = {Int. J. Hybrid Intell. Syst.},
	Keywords = {Evolutionary, Reinforcement Learning,},
	Month = aug,
	Number = {3},
	Numpages = {13},
	Pages = {171--183},
	Publisher = {IOS Press},
	Title = {Evolutionary Reinforcement Learning of Artificial Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=1367012.1367016},
	Volume = {4},
	Year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1367012.1367016}}

@inproceedings{Gruau:1996:CCE:1595536.1595547,
	Acmid = {1595547},
	Address = {Cambridge, MA, USA},
	Author = {Gruau, Fr{\'e}d{\'e}ric and Whitley, Darrell and Pyeatt, Larry},
	Booktitle = {Proceedings of the 1st Annual Conference on Genetic Programming},
	Date-Added = {2019-04-28 16:27:02 +1200},
	Date-Modified = {2019-04-28 16:27:30 +1200},
	Isbn = {0-262-61127-9},
	Keywords = {Evolutionary, Cellular Encoding, Genetic Algorithm},
	Location = {Stanford, California},
	Numpages = {9},
	Pages = {81--89},
	Publisher = {MIT Press},
	Title = {A Comparison Between Cellular Encoding and Direct Encoding for Genetic Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=1595536.1595547},
	Year = {1996},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1595536.1595547}}

@article{Kung2018,
	Abstract = {Memory performance is a key bottleneck for deep learning systems. Binarization of both activations and weights is one promising approach that can best scale to realize the highest energy efficient system using the lowest possible precision. In this paper, we utilize and analyze the binarized neural network in doing human detection on infrared images. Our results show comparable algorithmic performance of binarized versus 32bit floating-point networks, with the added benefit of greatly simplified computation and reduced memory overhead. In addition, we present a system architecture designed specifically for computation using binary representation that achieves at least 4{\texttimes} speedup and the energy is improved by three orders of magnitude over GPU.},
	Author = {Kung, Jaeha and Zhang, David and van der Wal, Gooitzen and Chai, Sek and Mukhopadhyay, Saibal},
	Date-Added = {2019-04-28 14:28:44 +1200},
	Date-Modified = {2019-04-28 14:32:52 +1200},
	Day = {01},
	Doi = {10.1007/s11265-017-1255-5},
	Issn = {1939-8115},
	Journal = {Journal of Signal Processing Systems},
	Keywords = {BNN, object detection, GPU, Infrared, FPGA},
	Month = {Jun},
	Number = {6},
	Pages = {877--890},
	Title = {Efficient Object Detection Using Embedded Binarized Neural Networks},
	Url = {https://doi.org/10.1007/s11265-017-1255-5},
	Volume = {90},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11265-017-1255-5}}

@inproceedings{Yaman:2018:LEC:3205455.3205555,
	Acmid = {3205555},
	Address = {New York, NY, USA},
	Author = {Yaman, Anil and Mocanu, Decebal Constantin and Iacca, Giovanni and Fletcher, George and Pechenizkiy, Mykola},
	Booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	Date-Added = {2019-04-27 14:01:29 +1200},
	Date-Modified = {2019-04-27 14:03:02 +1200},
	Doi = {10.1145/3205455.3205555},
	Isbn = {978-1-4503-5618-3},
	Keywords = {Evolutionary, cooperative co-evolution, differential evolution, direct encoding, neuroevolution, LECCDE},
	Location = {Kyoto, Japan},
	Numpages = {8},
	Pages = {569--576},
	Publisher = {ACM},
	Series = {GECCO '18},
	Title = {Limited Evaluation Cooperative Co-evolutionary Differential Evolution for Large-scale Neuroevolution},
	Url = {http://doi.acm.org/10.1145/3205455.3205555},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3205455.3205555},
	Bdsk-Url-2 = {https://doi.org/10.1145/3205455.3205555}}

@inproceedings{10.1007/978-3-030-00111-7_23,
	Abstract = {Stochastic gradient descent is the most prevalent algorithm to train neural networks. However, other approaches such as evolutionary algorithms are also applicable to this task. Evolutionary algorithms bring unique trade-offs that are worth exploring, but computational demands have so far restricted exploration to small networks with few parameters. We implement an evolutionary algorithm that executes entirely on the GPU, which allows to efficiently batch-evaluate a whole population of networks. Within this framework, we explore the limited evaluation evolutionary algorithm for neural network training and find that its batch evaluation idea comes with a large accuracy trade-off. In further experiments, we explore crossover operators and find that unprincipled random uniform crossover performs extremely well. Finally, we train a network with 92k parameters on MNIST using an EA and achieve 97.6{\%} test accuracy compared to 98{\%} test accuracy on the same network trained with Adam. Code is available at https://github.com/jprellberg/gpuea.},
	Address = {Cham},
	Author = {Prellberg, Jonas and Kramer, Oliver},
	Booktitle = {KI 2018: Advances in Artificial Intelligence},
	Date-Added = {2019-04-27 12:26:17 +1200},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {Trollmann, Frank and Turhan, Anni-Yasmin},
	Isbn = {978-3-030-00111-7},
	Keywords = {Books, LEEA, MNIST, GPU},
	Pages = {270--283},
	Publisher = {Springer International Publishing},
	Title = {Limited Evaluation Evolutionary Optimization of Large Neural Networks},
	Year = {2018}}

@article{Dietterich:2000:HRL:1622262.1622268,
	Acmid = {1622268},
	Address = {USA},
	Author = {Dietterich, Thomas G.},
	Date-Added = {2019-04-18 23:38:36 +1200},
	Date-Modified = {2019-04-18 23:40:14 +1200},
	Issn = {1076-9757},
	Issue_Date = {August 2000},
	Journal = {J. Artif. Int. Res.},
	Keywords = {NN, OpenAI Gym, Reinforcement Learning},
	Month = nov,
	Number = {1},
	Numpages = {77},
	Pages = {227--303},
	Publisher = {AI Access Foundation},
	Title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	Url = {http://dl.acm.org/citation.cfm?id=1622262.1622268},
	Volume = {13},
	Year = {2000},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1622262.1622268}}

@inproceedings{pmlr-v48-zaremba16,
	Abstract = {We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using Q-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by Q-learning.},
	Address = {New York, New York, USA},
	Author = {Wojciech Zaremba and Tomas Mikolov and Armand Joulin and Rob Fergus},
	Booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	Date-Added = {2019-04-14 21:01:09 +1200},
	Date-Modified = {2019-04-14 21:01:31 +1200},
	Editor = {Maria Florina Balcan and Kilian Q. Weinberger},
	Keywords = {NN, Reinforcement Learning, OpenAI Gym, Q-Learning},
	Month = {20--22 Jun},
	Pages = {421--429},
	Pdf = {http://proceedings.mlr.press/v48/zaremba16.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Learning Simple Algorithms from Examples},
	Url = {http://proceedings.mlr.press/v48/zaremba16.html},
	Volume = {48},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/zaremba16.html}}

@inproceedings{Strens:2000:BFR:645529.658114,
	Acmid = {658114},
	Address = {San Francisco, CA, USA},
	Author = {Strens, Malcolm J. A.},
	Booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	Date-Added = {2019-04-14 16:42:28 +1200},
	Date-Modified = {2019-04-14 16:43:57 +1200},
	Isbn = {1-55860-707-2},
	Keywords = {NN, Reinforcement Learning, OpenAI Gym},
	Numpages = {8},
	Pages = {943--950},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Series = {ICML '00},
	Title = {A Bayesian Framework for Reinforcement Learning},
	Url = {http://dl.acm.org/citation.cfm?id=645529.658114},
	Year = {2000},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=645529.658114}}

@article{Krotov201820458,
	Abstract = {Despite great success of deep learning a question remains to what extent the computational properties of deep neural networks are similar to those of the human brain. The particularly nonbiological aspect of deep learning is the supervised training process with the backpropagation algorithm, which requires massive amounts of labeled data, and a nonlocal learning rule for changing the synapse strengths. This paper describes a learning algorithm that does not suffer from these two problems. It learns the weights of the lower layer of neural networks in a completely unsupervised fashion. The entire algorithm utilizes local learning rules which have conceptual biological plausibility.It is widely believed that end-to-end training with the backpropagation algorithm is essential for learning good feature detectors in early layers of artificial neural networks, so that these detectors are useful for the task performed by the higher layers of that neural network. At the same time, the traditional form of backpropagation is biologically implausible. In the present paper we propose an unusual learning rule, which has a degree of biological plausibility and which is motivated by Hebb{\textquoteright}s idea that change of the synapse strength should be local{\textemdash}i.e., should depend only on the activities of the pre- and postsynaptic neurons. We design a learning algorithm that utilizes global inhibition in the hidden layer and is capable of learning early feature detectors in a completely unsupervised way. These learned lower-layer feature detectors can be used to train higher-layer weights in a usual supervised way so that the performance of the full network is comparable to the performance of standard feedforward networks trained end-to-end with a backpropagation algorithm on simple tasks.},
	Author = {Krotov, Dmitry and J. Hopfield, John},
	Date-Added = {2019-04-11 22:23:26 +1200},
	Date-Modified = {2019-04-11 22:24:04 +1200},
	Doi = {10.1073/pnas.1820458116},
	Elocation-Id = {201820458},
	Eprint = {https://www.pnas.org/content/early/2019/03/27/1820458116.full.pdf},
	Issn = {0027-8424},
	Journal = {Proceedings of the National Academy of Sciences},
	Keywords = {DNN, Biological neural networks, MNIST, CIFAR, Hebb's rule, SGD},
	Publisher = {National Academy of Sciences},
	Title = {Unsupervised learning by competing hidden units},
	Url = {https://www.pnas.org/content/early/2019/03/27/1820458116},
	Year = {2019},
	Bdsk-Url-1 = {https://www.pnas.org/content/early/2019/03/27/1820458116},
	Bdsk-Url-2 = {https://doi.org/10.1073/pnas.1820458116}}

@article{2018arXiv181112028K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181112028K},
	Archiveprefix = {arXiv},
	Author = {{Kitai}, H. and {Cruz}, J.~P. and {Yanai}, N. and {Nishida}, N. and {Oba}, T. and {Unagami}, Y. and {Teruya}, T. and {Attrapadung}, N. and {Matsuda}, T. and {Hanaoka}, G.},
	Date-Added = {2019-04-05 19:42:55 +1300},
	Date-Modified = {2019-04-05 19:43:07 +1300},
	Eprint = {1811.12028},
	Journal = {arXiv e-prints},
	Keywords = {BNN; Cryptography and Security, Artificial Intelligence},
	Month = nov,
	Primaryclass = {cs.CR},
	Title = {{MOBIUS: Model-Oblivious Binarized Neural Networks}},
	Year = 2018}

@inproceedings{Juvekar:2018:GLL:3277203.3277326,
	Acmid = {3277326},
	Address = {Berkeley, CA, USA},
	Author = {Juvekar, Chiraag and Vaikuntanathan, Vinod and Chandrakasan, Anantha},
	Booktitle = {Proceedings of the 27th USENIX Conference on Security Symposium},
	Date-Added = {2019-04-05 19:00:22 +1300},
	Date-Modified = {2019-04-05 19:25:39 +1300},
	Isbn = {978-1-931971-46-1},
	Keywords = {DNN, Homomorphic encryption, packed additive homomorphic encryption, Security, garbled-circuit},
	Location = {Baltimore, MD, USA},
	Numpages = {18},
	Pages = {1651--1668},
	Publisher = {USENIX Association},
	Series = {SEC'18},
	Title = {GAZELLE: A Low Latency Framework for Secure Neural Network Inference},
	Url = {http://dl.acm.org/citation.cfm?id=3277203.3277326},
	Year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3277203.3277326}}

@inproceedings{10.1007/978-3-319-78890-6_3,
	Abstract = {Modern Convolutional Neural Networks (CNNs) are typically based on floating point linear algebra based implementations. Recently, reduced precision Neural Networks (NNs) have been gaining popularity as they require significantly less memory and computational resources compared to floating point. This is particularly important in power constrained compute environments. However, in many cases a reduction in precision comes at a small cost to the accuracy of the resultant network. In this work, we investigate the accuracy-throughput trade-off for various parameter precision applied to different types of NN models. We firstly propose a quantization training strategy that allows reduced precision NN inference with a lower memory footprint and competitive model accuracy. Then, we quantitatively formulate the relationship between data representation and hardware efficiency. Our experiments finally provide insightful observation. For example, one of our tests show 32-bit floating point is more hardware efficient than 1-bit parameters to achieve 99{\%} MNIST accuracy. In general, 2-bit and 4-bit fixed point parameters show better hardware trade-off on small-scale datasets like MNIST and CIFAR-10 while 4-bit provide the best trade-off in large-scale tasks like AlexNet on ImageNet dataset within our tested problem domain.},
	Address = {Cham},
	Author = {Su, Jiang and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Durelli, Gianluca and Thomas, David B. and Leong, Philip H. W. and Cheung, Peter Y. K.},
	Booktitle = {Applied Reconfigurable Computing. Architectures, Tools, and Applications},
	Date-Added = {2019-04-04 23:58:00 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {Voros, Nikolaos and Huebner, Michael and Keramidas, Georgios and Goehringer, Diana and Antonopoulos, Christos and Diniz, Pedro C.},
	Isbn = {978-3-319-78890-6},
	Keywords = {Books, BNN, MNIST, CIFAR},
	Pages = {29--42},
	Publisher = {Springer International Publishing},
	Title = {Accuracy to Throughput Trade-Offs for Reduced Precision Neural Networks on Reconfigurable Logic},
	Year = {2018}}

@article{2018arXiv181211800D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181211800D},
	Archiveprefix = {arXiv},
	Author = {{Darabi}, S. and {Belbahri}, M. and {Courbariaux}, M. and {Partovi Nia}, V.},
	Date-Added = {2019-04-04 23:44:44 +1300},
	Date-Modified = {2019-04-04 23:44:56 +1300},
	Eprint = {1812.11800},
	Journal = {arXiv e-prints},
	Keywords = {BNN; Machine Learning, Computer Vision and Pattern Recognition},
	Month = dec,
	Title = {{BNN+: Improved Binary Network Training}},
	Year = 2018}

@inproceedings{10.1007/978-3-319-46493-0_32,
	Abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}faster convolutional operations (in terms of number of the high precision operations) and 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than {\$}{\$}16{\backslash},{\backslash}{\%}{\$}{\$}16{\%}in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.},
	Address = {Cham},
	Author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	Booktitle = {Computer Vision -- ECCV 2016},
	Date-Added = {2019-04-04 23:37:48 +1300},
	Date-Modified = {2019-04-04 23:38:31 +1300},
	Editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	Isbn = {978-3-319-46493-0},
	Keywords = {BNN, XNOR, ImageNet, CNN, AlexNet},
	Pages = {525--542},
	Publisher = {Springer International Publishing},
	Title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
	Year = {2016}}

@article{2018arXiv181201965B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181201965B},
	Archiveprefix = {arXiv},
	Author = {{Bethge}, J. and {Bornstein}, M. and {Loy}, A. and {Yang}, H. and {Meinel}, C.},
	Date-Added = {2019-04-04 23:09:42 +1300},
	Date-Modified = {2019-04-04 23:10:00 +1300},
	Eprint = {1812.01965},
	Journal = {arXiv e-prints},
	Keywords = {BNN; Machine Learning, Computer Vision and Pattern Recognition, Machine Learning},
	Month = dec,
	Title = {{Training Competitive Binary Neural Networks from Scratch}},
	Year = 2018}

@article{2019arXiv190309807K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2019arXiv190309807K},
	Archiveprefix = {arXiv},
	Author = {{Kim}, H. and {Kim}, Y. and {Ryu}, S. and {Kim}, J.-J.},
	Date-Added = {2019-04-04 22:42:22 +1300},
	Date-Modified = {2019-04-04 22:42:40 +1300},
	Eprint = {1903.09807},
	Journal = {arXiv e-prints},
	Keywords = {BNN; Neural and Evolutionary Computing, Computer Vision and Pattern Recognition, Machine Learning},
	Month = mar,
	Title = {{BitSplit-Net: Multi-bit Deep Neural Network with Bitwise Activation Function}},
	Year = 2019}

@inproceedings{Cai_2017_CVPR,
	Author = {Cai, Zhaowei and He, Xiaodong and Sun, Jian and Vasconcelos, Nuno},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2019-04-04 22:25:05 +1300},
	Date-Modified = {2019-04-04 22:25:44 +1300},
	Keywords = {BNN, Guassian, HWGQ, VGGNet, GoogLeNet},
	Month = {July},
	Title = {Deep Learning With Low Precision by Half-Wave Gaussian Quantization},
	Year = {2017}}

@inproceedings{10.1007/978-3-030-01237-3_23,
	Abstract = {Although weight and activation quantization is an effective approach for Deep Neural Network (DNN) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized model and the full-precision model. To address this gap, we propose to jointly train a quantized, bit-operation-compatible DNN and its associated quantizers, as opposed to using fixed, handcrafted quantization schemes such as uniform or logarithmic quantization. Our method for learning the quantizers applies to both network weights and activations with arbitrary-bit precision, and our quantizers are easy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets show that our method works consistently well for various network structures such as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous quantization methods in terms of accuracy by an appreciable margin. Code available at https://github.com/Microsoft/LQ-Nets.},
	Address = {Cham},
	Author = {Zhang, Dongqing and Yang, Jiaolong and Ye, Dongqiangzi and Hua, Gang},
	Booktitle = {Computer Vision -- ECCV 2018},
	Date-Added = {2019-04-04 19:51:38 +1300},
	Date-Modified = {2019-04-04 19:52:17 +1300},
	Editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	Isbn = {978-3-030-01237-3},
	Keywords = {BNN, LQ-Net, Quantization, CIFAR, ImageNet},
	Pages = {373--390},
	Publisher = {Springer International Publishing},
	Title = {LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks},
	Year = {2018}}

@inproceedings{Liang:2018:EAS:3205455.3205489,
	Acmid = {3205489},
	Address = {New York, NY, USA},
	Author = {Liang, Jason and Meyerson, Elliot and Miikkulainen, Risto},
	Booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
	Date-Added = {2019-03-18 22:58:02 +1300},
	Date-Modified = {2019-03-18 22:58:12 +1300},
	Doi = {10.1145/3205455.3205489},
	Isbn = {978-1-4503-5618-3},
	Keywords = {Evolutionary, artificial intelligence, neural networks/deep learning},
	Location = {Kyoto, Japan},
	Numpages = {8},
	Pages = {466--473},
	Publisher = {ACM},
	Series = {GECCO '18},
	Title = {Evolutionary Architecture Search for Deep Multitask Networks},
	Url = {http://doi.acm.org/10.1145/3205455.3205489},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3205455.3205489},
	Bdsk-Url-2 = {https://doi.org/10.1145/3205455.3205489}}

@article{Stanley:2019aa,
	Abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field's contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
	Author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
	Da = {2019/01/01},
	Date-Added = {2019-03-18 21:31:28 +1300},
	Date-Modified = {2019-03-18 21:31:49 +1300},
	Doi = {10.1038/s42256-018-0006-z},
	Id = {Stanley2019},
	Isbn = {2522-5839},
	Journal = {Nature Machine Intelligence},
	Keywords = {Evolutionary, NeuroEvolution, Nature, Survey},
	Number = {1},
	Pages = {24--35},
	Title = {Designing neural networks through neuroevolution},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s42256-018-0006-z},
	Volume = {1},
	Year = {2019},
	Bdsk-Url-1 = {https://doi.org/10.1038/s42256-018-0006-z}}

@inproceedings{7257254,
	Abstract = {In the real world, the environment is constantly changing with the input variables under the effect of noise. However, few algorithms were shown to be able to work under those circumstances. Here, Novelty-Organizing Team of Classifiers (NOTC) is applied to the continuous action mountain car as well as two variations of it: a noisy mountain car and an unstable weather mountain car. These problems take respectively noise and change of problem dynamics into account. Moreover, NOTC is compared with NeuroEvolution of Augmenting Topologies (NEAT) in these problems, revealing a trade-off between the approaches. While NOTC achieves the best performance in all of the problems, NEAT needs less trials to converge. It is demonstrated that NOTC achieves better performance because of its division of the input space (creating easier problems). Unfortunately, this division of input space also requires a bit of time to bootstrap.},
	Author = {D. V. {Vargas} and H. {Takano} and J. {Murata}},
	Booktitle = {2015 IEEE Congress on Evolutionary Computation (CEC)},
	Date-Added = {2019-03-18 20:50:14 +1300},
	Date-Modified = {2019-03-18 20:50:24 +1300},
	Doi = {10.1109/CEC.2015.7257254},
	Issn = {1089-778X},
	Keywords = {Evolutionary; neural nets;pattern classification;statistical analysis;dynamic environments;noisy environments;novelty-organizing team of classifiers;NOTC;noisy mountain car;continuous action mountain car;unstable weather mountain car;neuroevolution of augmenting topologies;NEAT;bootstrap;Sociology;Statistics;Arrays;Noise;Meteorology;Noise measurement},
	Month = {May},
	Pages = {2937-2944},
	Title = {Novelty-organizing team of classifiers in noisy and dynamic environments},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2015.7257254}}

@article{REHMAN2014239,
	Abstract = {Feedback in Neuro-Evolution is explored and evaluated for its application in devising prediction models for foreign currency exchange rates. A novel approach to foreign currency exchange rates forecasting based on Recurrent Neuro-Evolution is introduced. Cartesian Genetic Programming (CGP) is the algorithm deployed for the forecasting model. Recurrent Cartesian Genetic Programming evolved Artificial Neural Network (RCGPANN) is demonstrated to produce computationally efficient and accurate model for forex prediction with an accuracy of as high as 98.872% for a period of 1000 days. The approach utilizes the trends that are being followed in historical data to predict five currency rates against Australian dollar. The model is evaluated using statistical metrics and compared. The computational method outperforms the other methods particularly due to its capability to select the best possible feature in real time and the flexibility that the system provides in feature selection, connectivity pattern and network.},
	Author = {Mehreen Rehman and Gul Muhammad Khan and Sahibzada Ali Mahmud},
	Date-Added = {2019-03-14 15:59:59 +1300},
	Date-Modified = {2019-03-14 16:03:00 +1300},
	Doi = {https://doi.org/10.1016/j.ieri.2014.09.083},
	Issn = {2212-6678},
	Journal = {IERI Procedia},
	Keywords = {Evolutionary, Finance, Foreign exchange rate forecasting, Neural Networks, Cartesian Genetic Programming, Neuro-evolution, Recurrent Networks, Time Series Prediction},
	Note = {International Conference on Future Information Engineering (FIE 2014)},
	Pages = {239 - 244},
	Title = {Foreign Currency Exchange Rates Prediction Using CGP and Recurrent Neural Network},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212667814001312},
	Volume = {10},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212667814001312},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ieri.2014.09.083}}

@article{2016arXiv160601540B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160601540B},
	Archiveprefix = {arXiv},
	Author = {{Brockman}, G. and {Cheung}, V. and {Pettersson}, L. and {Schneider}, J. and {Schulman}, J. and {Tang}, J. and {Zaremba}, W.},
	Date-Added = {2019-03-13 17:26:41 +1300},
	Date-Modified = {2019-03-13 17:26:58 +1300},
	Eprint = {1606.01540},
	Journal = {arXiv e-prints},
	Keywords = {Other, OpenAI, Machine Learning, Artificial Intelligence},
	Month = jun,
	Title = {{OpenAI Gym}},
	Year = 2016}

@article{Turner2017,
	Abstract = {Cartesian Genetic Programming of Artificial Neural Networks is a NeuroEvolutionary method based on Cartesian Genetic Programming. Cartesian Genetic Programming has recently been extended to allow recurrent connections. This work investigates applying the same recurrent extension to Cartesian Genetic Programming of Artificial Neural Networks in order to allow the evolution of recurrent neural networks. The new Recurrent Cartesian Genetic Programming of Artificial Neural Networks method is applied to the domain of series forecasting where it is shown to significantly outperform all standard forecasting techniques used for comparison including autoregressive integrated moving average and multilayer perceptrons. An ablation study is also performed isolating which specific aspects of Recurrent Cartesian Genetic Programming of Artificial Neural Networks contribute to it's effectiveness for series forecasting.},
	Author = {Turner, Andrew James and Miller, Julian Francis},
	Date-Added = {2019-03-13 14:15:39 +1300},
	Date-Modified = {2019-03-13 14:16:03 +1300},
	Day = {01},
	Doi = {10.1007/s10710-016-9276-6},
	Issn = {1573-7632},
	Journal = {Genetic Programming and Evolvable Machines},
	Keywords = {Evolutionary, CGP, RCGP, Cartesian Genetic Programming, Time Series},
	Month = {Jun},
	Number = {2},
	Pages = {185--212},
	Title = {Recurrent Cartesian Genetic Programming of Artificial Neural Networks},
	Url = {https://doi.org/10.1007/s10710-016-9276-6},
	Volume = {18},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-016-9276-6}}

@article{2019arXiv190200730L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2019arXiv190200730L},
	Archiveprefix = {arXiv},
	Author = {{Lahoud}, F. and {Achanta}, R. and {M{\'a}rquez-Neila}, P. and {S{\"u}sstrunk}, S.},
	Date-Added = {2019-03-08 13:24:44 +1300},
	Date-Modified = {2019-03-08 13:24:57 +1300},
	Eprint = {1902.00730},
	Journal = {arXiv e-prints},
	Keywords = {BNN, Computer Vision and Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{Self-Binarizing Networks}},
	Year = 2019}

@inproceedings{Shabash:2018:ECE:3205651.3208282,
	Acmid = {3208282},
	Address = {New York, NY, USA},
	Author = {Shabash, Boris and Wiese, Kay C.},
	Booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
	Date-Added = {2019-03-06 19:11:06 +1300},
	Date-Modified = {2019-03-06 19:11:14 +1300},
	Doi = {10.1145/3205651.3208282},
	Isbn = {978-1-4503-5764-7},
	Keywords = {Evolutionary, artificial neural networks, evolutionary computation, evolving neural network activation functions, fitness functions},
	Location = {Kyoto, Japan},
	Numpages = {8},
	Pages = {1449--1456},
	Publisher = {ACM},
	Series = {GECCO '18},
	Title = {EvoNN: A Customizable Evolutionary Neural Network with Heterogenous Activation Functions},
	Url = {http://doi.acm.org/10.1145/3205651.3208282},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3205651.3208282},
	Bdsk-Url-2 = {https://doi.org/10.1145/3205651.3208282}}

@article{8651459,
	Abstract = {Privacy in internet of things is a fundamental challenge for Ubiquitous healthcare systems that depends on data aggregated and collaborative deep learning among different parties. This paper proposes MSCryptoNet, a novel framework that enables scalable execution and the conversion of state-of-the-art learned neural network to MSCryptoNet models in the privacy-preservation setting.We also design a method for approximation of the activation function basically used in convolutional neural network (CNN) (i.e. Sigmoid, Rectified linear unit (ReLU) etc.) with low degree polynomials which is vital for computations in homomorphic encryption schemes. Our model seems to target the following scenarios: (1) The practical way to enforce the evaluation of classifier whose inputs are encrypted with possibly different encryption schemes or even different keys whiles securing all operations including intermediate results. (2) The minimization of communication and computational cost of Data Providers. MSCryptoNet is based on multi-scheme fully homomorphic encryption (MS-FHE). We also prove that MSCryptoNet as a privacy-preserving deep learning scheme over the aggregated encrypted data are secured.},
	Author = {O. {Kwabena} and Z. {Qin} and T. {Zhuang} and Z. {Qin}},
	Date-Added = {2019-02-26 18:48:08 +1300},
	Date-Modified = {2019-02-26 18:48:15 +1300},
	Doi = {10.1109/ACCESS.2019.2901219},
	Issn = {2169-3536},
	Journal = {IEEE Access},
	Keywords = {CNN; Cryptography;Neural networks;Training;Cloud computing;Deep learning;Computational modeling;Data models;Internet of Things;Privacy-preserving;Fully Homomorphic Encryption},
	Pages = {1-1},
	Title = {MSCryptoNet: Multi-Scheme privacy-preserving deep learning in cloud computing},
	Year = {2019},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBFLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgUmV2aWV3IG9mIEJpbmFyaXplZCBOZXVyYWwgTmV0d29ya3MuYmliTxEB3AAAAAAB3AACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgUmV2aWV3IG9mIEJpbmFyaSNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANCTk4AAAIAWy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpCTk46QSBSZXZpZXcgb2YgQmluYXJpemVkIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4AVAApAEEAIABSAGUAdgBpAGUAdwAgAG8AZgAgAEIAaQBuAGEAcgBpAHoAZQBkACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAFlVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0JOTi9BIFJldmlldyBvZiBCaW5hcml6ZWQgTmV1cmFsIE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAbAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAJM},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACCESS.2019.2901219}}

@article{7460958,
	Abstract = {Learning algorithms are being increasingly adopted in various applications. However, further expansion will require methods that work more automatically. To enable this level of automation, a more powerful solution representation is needed. However, by increasing the representation complexity, a second problem arises. The search space becomes huge, and therefore, an associated scalable and efficient searching algorithm is also required. To solve both the problems, first a powerful representation is proposed that unifies most of the neural networks features from the literature into one representation. Second, a new diversity preserving method called spectrum diversity is created based on the new concept of chromosome spectrum that creates a spectrum out of the characteristics and frequency of alleles in a chromosome. The combination of spectrum diversity with a unified neuron representation enables the algorithm to either surpass or equal NeuroEvolution of Augmenting Topologies on all of the five classes of problems tested. Ablation tests justify the good results, showing the importance of added new features in the unified neuron representation. Part of the success is attributed to the novelty-focused evolution and good scalability with a chromosome size provided by spectrum diversity. Thus, this paper sheds light on a new representation and diversity preserving mechanism that should impact algorithms and applications to come.},
	Author = {Danilo {Vargas}, Junichi {Murata}},
	Date-Added = {2019-02-25 23:58:32 +1300},
	Date-Modified = {2019-04-17 21:06:05 +1200},
	Doi = {10.1109/TNNLS.2016.2551748},
	Issn = {2162-237X},
	Journal = {IEEE Transactions on Neural Networks and Learning Systems},
	Keywords = {Evolutionary; computational complexity;evolutionary computation;learning (artificial intelligence);neural nets;search problems;spectrum-diverse neuroevolution;unified neural models;learning algorithms;solution representation;representation complexity;search space;efficient searching algorithm;neural networks features;diversity preserving method;chromosome spectrum;NeuroEvolution;augmenting topologies;unified neuron representation;chromosome size;Neurons;Biological neural networks;Topology;Network topology;Biological cells;Encoding;Technological innovation;General artificial intelligence;neuroevolution;neuroEvolution of Augmenting Topology (NEAT);reinforcement learning;spectrum diversity;topology and weight evolving artificial neural network (TWEANN);unified neuron model; SUNA},
	Month = {Aug},
	Number = {8},
	Pages = {1759-1773},
	Title = {Spectrum-Diverse Neuroevolution With Unified Neural Models},
	Volume = {28},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2016.2551748}}

@inproceedings{8590945,
	Abstract = {Modern cryptographic schemes is developed based on the mathematical theory. Recently works show a new direction about cryptography based on the neural networks. Instead of learning a specific algorithm, a cryptographic scheme is generated automatically. While one kind of neural network is used to achieve the scheme, the idea of the neural cryptography can be realized by other neural network architecture is unknown. In this paper, we make use of this property to create neural cryptography scheme on a new topology evolving neural network architecture called Spectrum-diverse unified neuroevolution architecture. First, experiments are conducted to verify that Spectrum-diverse unified neuroevolution architecture is able to achieve automatic encryption and decryption. Subsequently, we do experiments to achieve the neural symmetric cryptosystem by using adversarial training.},
	Author = {Y. {Zhu} and D. V. {Vargas} and K. {Sakurai}},
	Booktitle = {2018 Sixth International Symposium on Computing and Networking Workshops (CANDARW)},
	Date-Added = {2019-02-25 23:04:26 +1300},
	Date-Modified = {2019-04-28 20:13:28 +1200},
	Doi = {10.1109/CANDARW.2018.00091},
	Keywords = {Evolutionary; SUNA; cryptography;learning (artificial intelligence);neural nets;topology evolving neural networks;neural network architecture;neural cryptography scheme;neural symmetric cryptosystem;mathematical theory;spectrum-diverse unified neuroevolution architecture;automatic encryption;automatic decryption;adversarial training;Neurons;Encryption;Biological neural networks;Network topology;Topology;Neural cryptography, Symmetric cryptosystem, Spectrum-diverse unified neuroevolution architecture, Topology evolving neural networks},
	Month = {Nov},
	Pages = {472-478},
	Title = {Neural Cryptography Based on the Topology Evolving Neural Networks},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CANDARW.2018.00091}}

@inproceedings{8588788,
	Abstract = {It has been proven that Recurrent Neural Networks (RNNs) are Turing Complete, i.e. for any given computable function there exists a finite RNN to compute it. Consequently, researchers have trained Recurrent Neural Networks to learn simple functions like sorting, addition, compression and more recently, even classical cryptographic ciphers such as the Enigma. In this paper, we try to identify the characteristics of functions that make them easy or difficult for the RNN to learn. We look at functions from a cryptographic point of view by studying the ways in which the output depends on the input. We use cryptographic parameters (confusion and diffusion) for determining the strength of a cipher and quantify this dependence to show that a strong correlation exists between the learning capability of an RNN and the function's cryptographic parameters.},
	Author = {S. {Srivastava} and A. {Bhatia}},
	Booktitle = {2018 IEEE International Conference on Big Knowledge (ICBK)},
	Date-Added = {2019-02-25 22:52:19 +1300},
	Date-Modified = {2019-02-25 22:52:38 +1300},
	Doi = {10.1109/ICBK.2018.00029},
	Keywords = {DNN; RNN, cryptography;learning (artificial intelligence);recurrent neural nets;learning capability;recurrent neural networks;cryptographic perspective;simple functions;classical cryptographic ciphers;RNN;computable function;Ciphers;Task analysis;Recurrent neural networks;Computer architecture;Training;Recurrent Neural Networks;Cryptographic Ciphers;Confusion Parameter;Diffusion Parameter},
	Month = {Nov},
	Pages = {162-167},
	Title = {On the Learning Capabilities of Recurrent Neural Networks: A Cryptographic Perspective},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICBK.2018.00029}}

@inproceedings{8622812,
	Abstract = {With the rapidly emerging encryption techniques for network traffic, the classification of encrypted traffic has increasingly become significantly important in network management and security. In this paper, we propose a novel deep neural network that combines both the convolutional network and the recurrent network to improve the accuracy of the classification results. The convolutional network is used to extract the packet features for a single packet. The recurrent network is trained to pick out the flow features based on the inputs of the packet features of any three consecutive packets in a flow. The proposed model surpasses the existing studies which ask for the first packets of a flow, and it provides more flexibility in real practice. We compare our model with the existing work under deep learning for encrypted traffic classification, based on the public dataset. The experimental results show that our model outperforms the state-of-the-art work in terms of both higher efficiency and effectiveness.},
	Author = {Z. {Zou} and J. {Ge} and H. {Zheng} and Y. {Wu} and C. {Han} and Z. {Yao}},
	Booktitle = {2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)},
	Date-Added = {2019-02-25 22:35:03 +1300},
	Date-Modified = {2019-02-25 22:35:32 +1300},
	Doi = {10.1109/HPCC/SmartCity/DSS.2018.00074},
	Keywords = {CNN; communications, computer network management;computer network security;cryptography;learning (artificial intelligence);recurrent neural nets;telecommunication traffic;encryption techniques;public dataset;consecutive packets;flow features;packet features;recurrent network;deep neural network;security;network management;network traffic;convolutional long short-term memory neural network;encrypted traffic classification;Feature extraction;Encryption;Virtual private networks;Neural networks;Machine learning;Payloads;Encrypted Traffic Classification, Deep Learning},
	Month = {June},
	Pages = {329-334},
	Title = {Encrypted Traffic Classification with a Convolutional Long Short-Term Memory Neural Network},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/HPCC/SmartCity/DSS.2018.00074}}

@article{Alaghi:2013:SSC:2465787.2465794,
	Acmid = {2465794},
	Address = {New York, NY, USA},
	Articleno = {92},
	Author = {Alaghi, Armin and Hayes, John P.},
	Date-Added = {2019-02-25 16:29:35 +1300},
	Date-Modified = {2019-02-25 16:29:43 +1300},
	Doi = {10.1145/2465787.2465794},
	Issn = {1539-9087},
	Issue_Date = {May 2013},
	Journal = {ACM Trans. Embed. Comput. Syst.},
	Keywords = {Other, Probabilistic computation, stochastic computing, stochastic logic},
	Month = may,
	Number = {2s},
	Numpages = {19},
	Pages = {92:1--92:19},
	Publisher = {ACM},
	Title = {Survey of Stochastic Computing},
	Url = {http://doi.acm.org/10.1145/2465787.2465794},
	Volume = {12},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2465787.2465794},
	Bdsk-Url-2 = {https://doi.org/10.1145/2465787.2465794}}

@inproceedings{7100476,
	Abstract = {In conventional security mechanism, cryptography is a process of information and data hiding from unauthorized access. It offers the unique possibility of certifiably secure data transmission among users at different remote locations. Cryptography is used to achieve availability, privacy and integrity over different networks. Usually, there are two categories of cryptography i.e. symmetric and asymmetric. In this paper, we have proposed a new symmetric key algorithm based on genetic algorithm (GA) and error back propagation neural network (EBP-NN). Genetic algorithm has been used for encryption and neural network has been used for decryption process. Consequently, this paper proposes an easy cryptographic secure algorithm for communication over the public computer networks.},
	Author = {V. {Sagar} and K. {Kumar}},
	Booktitle = {2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)},
	Date-Added = {2019-02-25 16:07:03 +1300},
	Date-Modified = {2019-02-25 16:07:27 +1300},
	Keywords = {Evolutionary; backpropagation;computer network security;cryptography;genetic algorithms;neural network;symmetric key cryptography;data hiding;information hiding;unauthorized access;certifiably secure data transmission;remote locations;data integrity;data privacy;genetic algorithm;error back propagation neural network;EBP-NN;GA;decryption process;cryptographic secure algorithm;public computer networks;Neurons;Genetic algorithms;Encryption;Artificial neural networks;Receivers;cryptography;error back propagation neural network;genetic algorithm;symmetric key},
	Month = {March},
	Pages = {1386-1391},
	Title = {A symmetric key cryptography using genetic algorithm and error back propagation neural network},
	Year = {2015}}

@article{lmcs:1132,
	Author = {Hirschowitz, Tom},
	Date-Added = {2019-02-01 09:35:36 +1300},
	Date-Modified = {2019-02-01 09:36:13 +1300},
	Doi = {10.2168/LMCS-9(3:10)2013},
	Journal = {Logical Methods in Computer Science},
	Keywords = {Haskell, Logic in Computer Science ; Programming Languages ; Mathematics; Category Theory},
	Month = Sep,
	Title = {Cartesian closed 2-categories and permutation equivalence in higher-order rewriting},
	Url = {https://lmcs.episciences.org/1132},
	Volume = {Volume 9, Issue 3},
	Year = {2013},
	Bdsk-Url-1 = {https://lmcs.episciences.org/1132},
	Bdsk-Url-2 = {https://doi.org/10.2168/LMCS-9(3:10)2013}}

@inbook{Cotta2002,
	Abstract = {Training artificial neural networks is a complex task of great practical importance. Besides classical ad-hoc algorithms such as backpropagation, this task can be approached by using Evolutionary Computation, a highly configurable and effective optimization paradigm. This chapter provides a brief overview of these techniques, and shows how they can be readily applied to the resolution of this problem. Three popular variants of Evolutionary Algorithms ---Genetic Algorithms, Evolution Strategies and Estimation of Distribution Algorithms--- are described and compared. This comparison is done on the basis of a benchmark comprising several standard classification problems of interest for neural networks. The experimental results confirm the general appropriateness of Evolutionary Computation for this problem. Evolution Strategies seem particularly proficient techniques in this optimization domain, and Estimation of Distribution Algorithms are also a competitive approach.},
	Address = {Boston, MA},
	Author = {Cotta, C. and Alba, E. and Sagarna, R. and Larra{\~{n}}aga, P.},
	Booktitle = {Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation},
	Date-Added = {2019-01-29 20:09:59 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Doi = {10.1007/978-1-4615-1539-5_18},
	Editor = {Larra{\~{n}}aga, Pedro and Lozano, Jose A.},
	Isbn = {978-1-4615-1539-5},
	Keywords = {Books, Evolutionary, NN, GA},
	Pages = {361--377},
	Publisher = {Springer US},
	Title = {Adjusting Weights in Artificial Neural Networks using Evolutionary Algorithms},
	Url = {https://doi.org/10.1007/978-1-4615-1539-5_18},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4615-1539-5_18}}

@inproceedings{Modesitt2018NeuralC,
	Author = {Dylan Modesitt and Tim D Henry and Jon Coden and Rachel Lathe},
	Booktitle = {MIT Coursework},
	Date-Added = {2019-01-14 19:46:16 +1300},
	Date-Modified = {2019-01-14 19:49:12 +1300},
	Keywords = {NN; Cryptonet, Stegranography, Adversarial Neural Cryptography},
	Title = {Neural Cryptography: From Symmetric Encryption to Adversarial Steganography},
	Year = {2018}}

@phdthesis{Homomorphic-Gentry,
	Author = {Craig Gentry},
	Date-Added = {2019-01-13 00:28:06 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Keywords = {Books, Homomorphic encryption, Thesis, Encryption},
	Month = {September},
	School = {Stanford University},
	Title = {A Fully Homomorphic Encryption Scheme},
	Type = {Dissertation},
	Year = {2009}}

@article{6591252,
	Abstract = {Artificial intelligence can be approached through the fast-time evolution of finite-state machines. Random mutation of an arbitrary machine yields an ``offspring.'' Both machines are driven by the available history and evaluated in terms of the given goal, and the machine having the higher score is selected to serve as the new parent. Such fast-time mutation and selection is continued with real-time decisions being based on the logic of the surviving machine. Saving the best few machines increases the security against gross nonstationarity of the environment. The efficiency of the evolutionary program is improved by introducing a cost-for-complexity weighting on each machine. An ability to predict one's environment is prerequisite to purposeful behavior. With this in mind, IBM7094 experiments were conducted to examine evolutionary prediction. As expected, cyclic signals in various degrees of noise were soon characterized by the predictor-machines. The transition probabilities within the sequence of predictions of low-order Markov processes were in close correspondence with those of the environment. The evolutionary program was also required to predict the (4-symbol) output sequence of an arbitrary machine that was driven by random binary noise. After 160 predictions the percent correct reached 51.5. When the evolutionary program was also given, the input binary variable this score reached 80 percent, showing a rapid approach toward the 100 percent asymptote. In contrast, providing an uncorrelated binary variable degraded the performance to 40.5 percent by requiring an attempt to extract nonexistent information. A formal technique was devised which translates a predictor machine into a set of hypotheses concerning the logic of the environment.},
	Author = {L. J. Fogel and A. J. Owens and M. J. Walsh},
	Date-Added = {2019-01-12 18:38:20 +1300},
	Date-Modified = {2019-01-12 18:38:32 +1300},
	Doi = {10.1109/THFE.1965.6591252},
	Issn = {0096-249X},
	Journal = {IEEE Transactions on Human Factors in Electronics},
	Keywords = {Evolutionary; Noise;Automata;Decision making;Markov processes;Prediction algorithms;Shape;Springs},
	Month = {Sep.},
	Number = {1},
	Pages = {13-23},
	Title = {Intelligent decision-making through a simulation of evolution},
	Volume = {HFE-6},
	Year = {1965},
	Bdsk-Url-1 = {https://doi.org/10.1109/THFE.1965.6591252}}

@incollection{NIPS2017_6768,
	Author = {Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	Booktitle = {Advances in Neural Information Processing Systems 30},
	Date-Added = {2019-01-12 14:34:44 +1300},
	Date-Modified = {2019-01-12 14:56:53 +1300},
	Editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	Keywords = {NN; SGD; Parallel; communicating structure},
	Pages = {1709--1720},
	Publisher = {Curran Associates, Inc.},
	Title = {QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},
	Url = {http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf}}

@phdthesis{Brodsky-CAN,
	Author = {Stephen A. Brodsky},
	Date-Added = {2019-01-11 21:58:01 +1300},
	Date-Modified = {2019-01-11 22:01:45 +1300},
	Keywords = {Thesis, BNN, CAN},
	School = {University of California, San Diego},
	Title = {Content Addressable Networks},
	Year = {1995}}

@inproceedings{Liu_2018_ECCV,
	Author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
	Booktitle = {The European Conference on Computer Vision (ECCV)},
	Date-Added = {2019-01-11 21:15:50 +1300},
	Date-Modified = {2019-01-11 21:26:47 +1300},
	Keywords = {CNN; search problem, Reinforcement learning,},
	Month = {September},
	Title = {Progressive Neural Architecture Search},
	Year = {2018}}

@url{EvolStraOAI,
	Author = {Open AI},
	Date-Added = {2019-01-11 17:13:38 +1300},
	Date-Modified = {2019-01-11 17:15:07 +1300},
	Keywords = {evolution strategies, OpenAI, Atari, Videogames, Reinforcement learning},
	Lastchecked = {11},
	Month = {Jan},
	Title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
	Url = {https://blog.openai.com/evolution-strategies/},
	Year = {2018},
	Bdsk-Url-1 = {https://blog.openai.com/evolution-strategies/}}

@webpage{DeepNeuroEvol,
	Author = {Uber},
	Date-Added = {2019-01-11 17:11:10 +1300},
	Date-Modified = {2019-01-11 17:13:02 +1300},
	Keywords = {Neuroevolution; Atari, Videogames; evolution strategies; uber},
	Lastchecked = {11/Jan},
	Month = {January},
	Title = {Accelerating Deep Neuroevolution: Train Atari in Hours on a Single Personal Computer},
	Url = {https://eng.uber.com/accelerated-neuroevolution/},
	Year = {2019},
	Bdsk-Url-1 = {https://eng.uber.com/accelerated-neuroevolution/}}

@inproceedings{4670938,
	Abstract = {This paper introduces a chaotic encryption system using a principal component analysis (PCA) neural network. The PCA neural network can produce the chaotic behaviors under certain conditions so that it serves as a pseudo-random number generator to generate random private keys. In this encryption system, the one-time pad encryption method is used, which is regarded as the most secure encryption method. The proposed system can encrypt any kind of data. The security and high performance of encryption are illustrated via some simulations.},
	Author = {Xiao Fei and Guisong Liu and Bochuan Zheng},
	Booktitle = {2008 IEEE Conference on Cybernetics and Intelligent Systems},
	Date-Added = {2019-01-11 16:41:09 +1300},
	Date-Modified = {2019-01-11 16:41:25 +1300},
	Doi = {10.1109/ICCIS.2008.4670938},
	Issn = {2326-8123},
	Keywords = {NN; chaotic communication;neural nets;principal component analysis;private key cryptography;random number generation;chaotic encryption system;PCA neural networks;principal component analysis;pseudo-random number generator;random private keys;one-time pad encryption method;Chaos;Cryptography;Principal component analysis;Neural networks;Information security;Random number generation;Computational intelligence;Laboratories;Computer science;Paper technology;Principal Component Analysis;Neural Networks;Chaotic Encryption;Information Security; Cryptonet},
	Month = {Sep.},
	Pages = {465-469},
	Title = {A chaotic encryption system using PCA neural networks},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCIS.2008.4670938}}

@inproceedings{4582655,
	Abstract = {Two constructive generalized synchronization (GS) theorems for a kind of neural network are introduced, which are described by discrete-time array equation systems (DTAEs). Based on the theorems, one can design a GS driven DTAE via a driving chaotic DTAE and an inverse function of H. As an application, a generalized Henon cellular neural network (CNN) with three state variables is introduced. Using the GS theorems and the generalized Henon CNN constructs a coupled GS DTAE with 2646 cells. The hyper chaotic GS phenomena of the GS DTAE have been simulated. The numerical simulation results display complex behaviors of the GS DTAE. Using the DTAE designs a encryption scheme with ldquoone-time padrdquo function. This scheme is able successfully to encrypt and decrypt original information without any loss. The scheme is sensitive to the perturbations of the initial conditions and some system parameters of the DTAE. The key space is huge.},
	Author = {Hongyan Zang and Lequan Min},
	Booktitle = {2008 3rd IEEE Conference on Industrial Electronics and Applications},
	Date-Added = {2019-01-11 16:38:55 +1300},
	Date-Modified = {2019-01-11 16:39:11 +1300},
	Doi = {10.1109/ICIEA.2008.4582655},
	Issn = {2156-2318},
	Keywords = {NN; cellular neural nets;chaotic communication;cryptography;discrete time systems;Henon mapping;synchronisation;generalized synchronization theorems;neural network;data encryption;discrete-time array equation systems;inverse function;generalized Henon cellular neural network;three state variables;hyper chaotic phenomena;one-time pad function;key space; Cryptonet},
	Month = {June},
	Pages = {948-953},
	Title = {Generalized synchronization theorems for a kind of Neural Network with application in data encryption},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIEA.2008.4582655}}

@inproceedings{7965079,
	Abstract = {Scaling up Artificial Intelligence (AI) algorithms for massive datasets to improve their performance is becoming crucial. In Machine Translation (MT), one of most important research fields of AI, models based on Recurrent Neural Net- works (RNN) show state-of-the-art performance in recent years, and many researchers keep working on improving RNN-based models to achieve better accuracy in translation tasks. Most implementations of Neural Machine Translation (NMT) models employ a padding strategy when processing a mini-batch to make all sentences in a mini-batch have the same length. This enables an efficient utilization of caches and GPU/SIMD parallelism but leads to a waste of computation time. In this paper, we implement and parallelize batch learning for a Sequence-to- Sequence (Seq2Seq) model, which is the most basic model of NMT, without using a padding strategy. More specifically, our approach forms vectors which represent the input words as well as the neural network's states at different time steps into matrices when it processes one sentence, and as a result, the approach makes a better use of cache and optimizes the process that adjusts weights and biases during the back-propagation phase. Our experimental evaluation shows that our implementation achieves better scalability on multi-core CPUs. We also discuss our approach's potential to be used in other implementations of RNN-based models.},
	Author = {Y. Qiao and K. Hashimoto and A. Eriguchi and H. Wang and D. Wang and Y. Tsuruoka and K. Taura},
	Booktitle = {2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
	Date-Added = {2019-01-11 16:36:22 +1300},
	Date-Modified = {2019-01-11 16:36:34 +1300},
	Doi = {10.1109/IPDPSW.2017.165},
	Keywords = {NN; backpropagation;cache storage;graphics processing units;language translation;matrix algebra;multiprocessing systems;recurrent neural nets;vectors;cache friendly parallelization;neural encoder-decoder models;multicore architecture;scaling up artificial intelligence algorithms;neural machine translation models;NMT models;recurrent neural networks;RNN-based models;translation tasks;GPU-SIMD parallelism;batch learning parallelization;sequence-to-sequence model;Seq2Seq model;back-propagation phase;multicore CPUs;Computational modeling;Instruction sets;Scalability;Electronic mail;Recurrent neural networks;Training;Libraries;Neural Machine Translation;Cache Optimization;Parallel Programming; Cryptonet},
	Month = {May},
	Pages = {437-440},
	Title = {Cache Friendly Parallelization of Neural Encoder-Decoder Models Without Padding on Multi-core Architecture},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPDPSW.2017.165}}

@inproceedings{5209334,
	Abstract = {Based on best square approximation theory, new feed-forward neural networks are introduced where hidden units activation functions employ Laguerre orthogonal polynomials. Use these neural networks as the identifier model of the chaotic time series. Then, by varying the chaotic initial value and inputting to the networks, can produce new chaotic series, which are close to the theoretical values. We extract a subsequence as same length as the plaintext from the chaotic series and sort it. At last, by permuting the plaintext according to the sorted results of the subsequence, we can achieve the ciphertext. In the encryption system, the security of it depends completely on the complexity and unpredictability of the chaos. Especially, by varying the chaotic initial value, we can implement asynchronous "one-time pad cipher" encryption. The theoretical analysis and encryption instances proved that our arithmetic is useful, simple and high security, and it also has many advantages that a synchronous system can never achieve.},
	Author = {A. Zou and X. Xiao},
	Booktitle = {2009 WRI Global Congress on Intelligent Systems},
	Date-Added = {2019-01-11 16:33:09 +1300},
	Date-Modified = {2019-01-11 16:33:58 +1300},
	Doi = {10.1109/GCIS.2009.82},
	Issn = {2155-6083},
	Keywords = {NN; computational complexity;cryptography;feedforward neural nets;time series;asynchronous encryption arithmetic;Laguerre chaotic neural networks;feedforward neural networks;Laguerre orthogonal polynomials;chaotic time series;chaotic initial value;ciphertext;one-time pad cipher;Cryptography;Arithmetic;Chaos;Neural networks;Chaotic communication;Polynomials;Approximation methods;Intelligent systems;Intelligent networks;Educational institutions;Neural Networks;Chaos;Permute;Laguerre Polynomial;Asynchronous Encryption; Cryptonet},
	Month = {May},
	Pages = {36-39},
	Title = {An Asynchronous Encryption Arithmetic Based on Laguerre Chaotic Neural Networks},
	Volume = {4},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/GCIS.2009.82}}

@article{2018arXiv180201548R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180201548R},
	Archiveprefix = {arXiv},
	Author = {{Real}, E. and {Aggarwal}, A. and {Huang}, Y. and {Le}, Q.~V},
	Date-Added = {2019-01-11 15:55:33 +1300},
	Date-Modified = {2019-01-11 15:56:13 +1300},
	Eprint = {1802.01548},
	Journal = {arXiv e-prints},
	Keywords = {Evolutionarym Neural Computing, Artificial Intelligence, Computer Vision and Pattern Recognition, Distributed, Parallel, and Cluster Computing, Google},
	Month = feb,
	Title = {{Regularized Evolution for Image Classifier Architecture Search}},
	Year = 2018}

@inproceedings{pmlr-v70-real17a,
	Abstract = {Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6\% (95.6\% for ensemble) and 77.0\%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Esteban Real and Sherry Moore and Andrew Selle and Saurabh Saxena and Yutaka Leon Suematsu and Jie Tan and Quoc V. Le and Alexey Kurakin},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Date-Added = {2019-01-11 13:48:56 +1300},
	Date-Modified = {2019-01-11 15:28:49 +1300},
	Editor = {Doina Precup and Yee Whye Teh},
	Keywords = {Evolutionary; Neural and Evolutionary Computing; Artificial Intelligence; Computer Vision; Pattern Recognition; Distributed, Parallel, Cluster Computing, image classification},
	Month = {06--11 Aug},
	Pages = {2902--2911},
	Pdf = {http://proceedings.mlr.press/v70/real17a/real17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Large-Scale Evolution of Image Classifiers},
	Url = {http://proceedings.mlr.press/v70/real17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/real17a.html}}

@article{s18051306,
	Abstract = {Researches in Artificial Intelligence (AI) have achieved many important breakthroughs, especially in recent years. In some cases, AI learns alone from scratch and performs human tasks faster and better than humans. With the recent advances in AI, it is natural to wonder whether Artificial Neural Networks will be used to successfully create or break cryptographic algorithms. Bibliographic review shows the main approach to this problem have been addressed throughout complex Neural Networks, but without understanding or proving the security of the generated model. This paper presents an analysis of the security of cryptographic algorithms generated by a new technique called Adversarial Neural Cryptography (ANC). Using the proposed network, we show limitations and directions to improve the current approach of ANC. Training the proposed Artificial Neural Network with the improved model of ANC, we show that artificially intelligent agents can learn the unbreakable One-Time Pad (OTP) algorithm, without human knowledge, to communicate securely through an insecure communication channel. This paper shows in which conditions an AI agent can learn a secure encryption scheme. However, it also shows that, without a stronger adversary, it is more likely to obtain an insecure one.},
	Article-Number = {1306},
	Author = {Coutinho, Murilo and de Oliveira Albuquerque, Robson and Borges, F{\'a}bio and Garc{\'\i}a Villalba, Luis Javier and Kim, Tai-Hoon},
	Date-Added = {2019-01-10 20:35:57 +1300},
	Date-Modified = {2019-01-11 15:27:46 +1300},
	Doi = {10.3390/s18051306},
	Issn = {1424-8220},
	Journal = {Sensors},
	Keywords = {DCGAN, Cryptography, Adversarial Neural Cryptography, Cryptonet},
	Number = {5},
	Title = {Learning Perfectly Secure Cryptography to Protect Communications with Adversarial Neural Cryptography},
	Url = {http://www.mdpi.com/1424-8220/18/5/1306},
	Volume = {18},
	Year = {2018},
	Bdsk-Url-1 = {http://www.mdpi.com/1424-8220/18/5/1306},
	Bdsk-Url-2 = {https://doi.org/10.3390/s18051306}}

@article{2018arXiv181004714D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181004714D},
	Archiveprefix = {arXiv},
	Author = {{Dong}, H.-W. and {Yang}, Y.-H.},
	Date-Added = {2019-01-10 16:41:50 +1300},
	Date-Modified = {2019-01-10 16:42:08 +1300},
	Eprint = {1810.04714},
	Journal = {arXiv e-prints},
	Keywords = {BNN; Machine Learning, Statistics, Machine Learning},
	Month = oct,
	Title = {{Training Generative Adversarial Networks with Binary Neurons by End-to-end Backpropagation}},
	Year = 2018}

@paper{AAAI1817150,
	Abstract = {The most striking successes in image retrieval using deep hashing have mostly involved discriminative models, which require labels. In this paper, we use binary generative adversarial networks (BGAN) to embed images to binary codes in an unsupervised way. By restricting the input noise variable of generative adversarial networks (GAN) to be binary and conditioned on the features of each input image, BGAN can simultaneously learn a binary representation per image, and generate an image plausibly similar to the original one. In the proposed framework, we address two main problems: 1) how to directly generate binary codes without relaxation? 2) how to equip the binary representation with the ability of accurate image retrieval? We resolve these problems by proposing new sign-activation strategy and a loss function steering the learning process, which consists of new models for adversarial loss, a content loss, and a neighborhood structure loss. Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly outperforms existing hashing methods by up to 107% in terms of mAP (See Table 2).},
	Author = {Jingkuan Song and Tao He and Lianli Gao and Xing Xu and Alan Hanjalic and Heng Tao Shen},
	Conference = {AAAI Conference on Artificial Intelligence},
	Date-Added = {2019-01-10 16:21:03 +1300},
	Date-Modified = {2019-01-10 16:33:15 +1300},
	Keywords = {DCGAN; hashing; GAN; image retrieval},
	Title = {Binary Generative Adversarial Networks for Image Retrieval},
	Url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17150},
	Year = {2018},
	Bdsk-Url-1 = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17150}}

@electronic{titanRTXSpecs,
	Author = {Tech Power Up},
	Date-Added = {2019-01-09 17:13:52 +1300},
	Date-Modified = {2019-01-09 17:16:02 +1300},
	Keywords = {NVidia, GPU, URL},
	Lastchecked = {9/Jan/2018},
	Title = {NVidia Titan RTX},
	Url = {https://www.techpowerup.com/gpu-specs/titan-rtx.c3311},
	Bdsk-Url-1 = {https://www.techpowerup.com/gpu-specs/titan-rtx.c3311}}

@inbook{Tettamanzi2001,
	Abstract = {WE saw in Chapter 2 that artificial neural networks are biologically-inspired computational models that have the capability of somehow ``learning'' or ``self-organizing'' to accomplish a given task. They are particularly efficient when the nature of the task is ill-defined and the input/output mapping largely unknown. However, many aspects may affect the performance of an ANN on a given problem. Among them, the most important is the structure of the neuron connections i.e., the topology of the net, the connection weights, the details of the learning rules and of the neural activation function, and the data sets to be used for learning. There are guidelines for picking or finding reasonable values for all of these network parameters but most are rules of thumb with little theoretical background and without any relationship with each other.},
	Address = {Berlin, Heidelberg},
	Author = {Tettamanzi, Andrea and Tomassini, Marco},
	Booktitle = {Soft Computing: Integrating Evolutionary, Neural, and Fuzzy Systems},
	Date-Added = {2018-12-18 15:56:40 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Doi = {10.1007/978-3-662-04335-6_4},
	Isbn = {978-3-662-04335-6},
	Keywords = {Evolutionary, Neural Network, Books},
	Pages = {123--159},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Evolutionary Design of Artificial Neural Networks},
	Url = {https://doi.org/10.1007/978-3-662-04335-6_4},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-662-04335-6_4}}

@article{Jha2018,
	Abstract = {This paper proposes a novel approach for the evolution of artificial creatures which moves in a 3D virtual environment based on the neuroevolution of augmenting topologies (NEAT) algorithm. The NEAT algorithm is used to evolve neural networks that observe the virtual environment and respond to it, by controlling the muscle force of the creature. The genetic algorithm is used to emerge the architecture of creature based on the distance metrics for fitness evaluation. The damaged morphologies of creature are elaborated, and a crossover algorithm is used to control it. Creatures with similar morphological traits are grouped into the same species to limit the complexity of the search space. The motion of virtual creature having 2--3 limbs is recorded at three different angles to check their performance in different types of viscous mediums. The qualitative demonstration of motion of virtual creature represents that improved swimming of virtual creatures is achieved in simulating mediums with viscous drag 1--10 arbitrary unit.},
	Author = {Jha, Sunil Kr. and Josheski, Filip},
	Date-Added = {2018-12-17 20:49:27 +1300},
	Date-Modified = {2018-12-17 20:49:47 +1300},
	Day = {01},
	Doi = {10.1007/s00521-016-2664-2},
	Issn = {1433-3058},
	Journal = {Neural Computing and Applications},
	Keywords = {Evolutionary, NEAT, Neuroevolution,},
	Month = {Jun},
	Number = {12},
	Pages = {1337--1347},
	Title = {Artificial evolution using neuroevolution of augmenting topologies (NEAT) for kinetics study in diverse viscous mediums},
	Url = {https://doi.org/10.1007/s00521-016-2664-2},
	Volume = {29},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00521-016-2664-2}}

@inproceedings{10.1007/978-3-642-01129-0_27,
	Abstract = {Neuro-evolution of augmenting topologies (NEAT) is a recently developed neuro-evolutionary algorithm. This study uses NEAT to evolve dynamic trading agents for the German Bond Futures Market. High frequency data for three German Bond Futures is used to train and test the agents. Four fitness functions are tested and their out of sample performance is presented. The results suggest the methodology can outperform a random agent. However, while some structure was found in the data, the agents fail to yield positive returns when realistic transaction costs are included. A number of avenues of future work are indicated.},
	Address = {Berlin, Heidelberg},
	Author = {Bradley, Robert and Brabazon, Anthony and O'Neill, Michael},
	Booktitle = {Applications of Evolutionary Computing},
	Date-Added = {2018-12-17 19:33:30 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {Giacobini, Mario and Brabazon, Anthony and Cagnoni, Stefano and Di Caro, Gianni A. and Ek{\'a}rt, Anik{\'o} and Esparcia-Alc{\'a}zar, Anna Isabel and Farooq, Muddassar and Fink, Andreas and Machado, Penousal},
	Isbn = {978-3-642-01129-0},
	Keywords = {Evolutionary, Neuroevolution, Finance, HFT, Books},
	Pages = {233--242},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Dynamic High Frequency Trading: A Neuro-Evolutionary Approach},
	Year = {2009}}

@inbook{Castellano2007,
	Abstract = {In recent years, the use of hybrid Soft Computing methods has shown that in various applications the synergism of several techniques is superior to a single technique. For example, the use of a neural fuzzy system and an evolutionary fuzzy system hybridises the approximate reasoning mechanism of fuzzy systems with the learning capabilities of neural networks and evolutionary algorithms. Evolutionary neural systems hybridise the neurocomputing approach with the solution-searching ability of evolutionary computing. Such hybrid methodologies retain limitations that can be overcome with full integration of the three basic Soft Computing paradigms, and this leads to evolutionary neural fuzzy systems. The objective of this chapter is to provide an account of hybrid Soft Computing systems, with special attention to the combined use of evolutionary algorithms and neural networks in order to endow fuzzy systems with learning and adaptive capabilities. After an introduction to basic Soft Computing paradigms, the various forms of hybridisation are considered, which results in evolutionary neural fuzzy systems. The chapter also describes a particular approach that jointly uses neural learning and genetic optimisation to learn a fuzzy model from the given data and to optimise it for accuracy and interpretability.},
	Address = {Berlin, Heidelberg},
	Author = {Castellano, G. and Castiello, C. and Fanelli, A. M. and Jain, L.},
	Booktitle = {Advances in Evolutionary Computing for System Design},
	Date-Added = {2018-12-17 18:43:55 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Doi = {10.1007/978-3-540-72377-6_2},
	Editor = {Jain, Lakhmi C. and Palade, Vasile and Srinivasan, Dipti},
	Isbn = {978-3-540-72377-6},
	Keywords = {Evolutionary, Neuroevolution, Fuzzy systems, Books},
	Pages = {11--45},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Evolutionary Neuro-Fuzzy Systems and Applications},
	Url = {https://doi.org/10.1007/978-3-540-72377-6_2},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-540-72377-6_2}}

@article{Yalcin2018,
	Abstract = {This study introduces a derivative of the well-known optimization algorithm, Big Bang--Big Crunch (BB--BC), named Nuclear Fission--Nuclear Fusion-based BB--BC, simply referred to as N2F. Broadly preferred in the engineering optimization community, BB--BC provides accurate solutions with reasonably fast convergence rates for many engineering problems. Regardless, the algorithm often suffers from stagnation issues. More specifically, for some problems, BB--BC either converges prematurely or exploits the promising regions inefficiently, both of which prevent obtaining the optimal solution. To overcome such problems, N2F algorithm is proposed, inspired by two major phenomena of nuclear physics: fission and fusion reactions. In N2F, two concepts named ``Nuclear Fission'' and ``Nuclear Fusion'' are introduced, replacing the ``Big Bang'' and ``Big Crunch'' phases of BB--BC, respectively. With the ``Nuclear Fission'' phase represented through a parameter named amplification factor, premature convergence issues are eliminated to a great extent. Meanwhile, convergence rate and exploitation capability of the algorithm are enhanced largely through a precision control parameter named magnification factor, in the ``Nuclear Fusion'' phase. The performance of N2F algorithm is investigated through unconstrained test functions and compared with the conventional BB--BC and other metaheuristics including genetic algorithm, Particle Swarm Optimization (PSO), Artificial Bee Colony Optimization (ABC), Drone Squadron Optimization (DSO) and Salp Swarm Algorithm (SSA). Then, further analyses are performed with constrained design benchmarks, validating the applicability of N2F to engineering problems. With superior statistical performance compared to BB--BC, GA, PSO, ABC, DSO and SSA in unconstrained problems and improved results with respect to the literature studies, N2F is proven to be an efficient and robust optimization algorithm.},
	Author = {Yalcin, Yagizer and Pekcan, Onur},
	Date-Added = {2018-12-17 11:38:17 +1300},
	Date-Modified = {2018-12-17 11:38:34 +1300},
	Day = {15},
	Doi = {10.1007/s00521-018-3907-1},
	Issn = {1433-3058},
	Journal = {Neural Computing and Applications},
	Keywords = {Other, N2F, global optimization},
	Month = {Dec},
	Title = {Nuclear Fission--Nuclear Fusion algorithm for global optimization: a modified Big Bang--Big Crunch algorithm},
	Url = {https://doi.org/10.1007/s00521-018-3907-1},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBPLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRE5OL1Vuc3VwZXJ2aXNlZCBsZWFybmluZyBieSBjb21wZXRpbmcgaGlkZGVuIHVuaXRzLmJpYk8RAgQAAAAAAgQAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9VbnN1cGVydmlzZWQgbGVhcm4jRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADRE5OAAACAGUvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RE5OOlVuc3VwZXJ2aXNlZCBsZWFybmluZyBieSBjb21wZXRpbmcgaGlkZGVuIHVuaXRzLmJpYgAADgBoADMAVQBuAHMAdQBwAGUAcgB2AGkAcwBlAGQAIABsAGUAYQByAG4AaQBuAGcAIABiAHkAIABjAG8AbQBwAGUAdABpAG4AZwAgAGgAaQBkAGQAZQBuACAAdQBuAGkAdABzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBjVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9ETk4vVW5zdXBlcnZpc2VkIGxlYXJuaW5nIGJ5IGNvbXBldGluZyBoaWRkZW4gdW5pdHMuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAB2AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAn4=},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00521-018-3907-1}}

@article{Mittal2018,
	Abstract = {Deep convolutional neural networks (CNNs) have recently shown very high accuracy in a wide range of cognitive tasks, and due to this, they have received significant interest from the researchers. Given the high computational demands of CNNs, custom hardware accelerators are vital for boosting their performance. The high energy efficiency, computing capabilities and reconfigurability of FPGA make it a promising platform for hardware acceleration of CNNs. In this paper, we present a survey of techniques for implementing and optimizing CNN algorithms on FPGA. We organize the works in several categories to bring out their similarities and differences. This paper is expected to be useful for researchers in the area of artificial intelligence, hardware architecture and system design.},
	Author = {Mittal, Sparsh},
	Date-Added = {2018-12-17 00:07:23 +1300},
	Date-Modified = {2018-12-17 00:07:52 +1300},
	Day = {06},
	Doi = {10.1007/s00521-018-3761-1},
	Issn = {1433-3058},
	Journal = {Neural Computing and Applications},
	Keywords = {FPGA-NN, Binary neural network, Batch processing},
	Month = {Oct},
	Title = {A survey of FPGA-based accelerators for convolutional neural networks},
	Url = {https://doi.org/10.1007/s00521-018-3761-1},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s00521-018-3761-1}}

@inproceedings{8056820,
	Abstract = {Convolutional Neural Networks (CNNs) can achieve high classification accuracy while they require complex computation. Binarized Neural Networks (BNNs) with binarized weights and activations can simplify computation but suffer from obvious accuracy loss. In this paper, low bit-width CNNs, BNNs and standard CNNs are compared to show that low bit-width CNNs is better suited for embedded systems. An architecture based on the two-stage arithmetic unit (TSAU) as the basic processing element is proposed to process each layer iteratively for low bit-width CNN accelerators. Then the DoReFa-Net which is trained with weights and activations represented in 1 bit and 2 bits respectively is implemented on Zynq XC7Z020 FPGA with a 410.2 GOPS performance. The accelerator can meet the real-time requirement of embedded applications with a 106 FPS throughput and a 73.1% top-5 accuracy on the ImageNet dataset. The accelerator outperforms existing FPGA-based CNN accelerators in the tradeoff among accuracy, energy and resource efficiency.},
	Author = {L. Jiao and C. Luo and W. Cao and X. Zhou and L. Wang},
	Booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-12-16 23:41:56 +1300},
	Date-Modified = {2018-12-16 23:42:05 +1300},
	Doi = {10.23919/FPL.2017.8056820},
	Issn = {1946-1488},
	Keywords = {BNN; digital arithmetic;embedded systems;field programmable gate arrays;neural nets;low bit-width CNN accelerators;Zynq XC7Z020 FPGA;DoReFa-Net neural nets;two-stage arithmetic unit;embedded systems;embedded systems;binarized activations;binarized weights;Binarized Neural Networks;complex computation;high classification accuracy;embedded FPGA;low bit-width convolutional neural networks;Kernel;Field programmable gate arrays;Table lookup;Neural networks;Computational modeling;Quantization (signal);Convolution},
	Month = {Sept},
	Pages = {1-4},
	Title = {Accelerating low bit-width convolutional neural networks with embedded FPGA},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/FPL.2017.8056820}}

@inproceedings{8297303,
	Abstract = {Deep convolutional neural networks (CNNs) are widely used in many computer vision tasks. Since CNNs involve billions of computations, it is critical to reduce the resource /power consumption and improve parallelism. Compared with extensive researches on fixed point conversion for cost reduction, floating point customization has not been paid enough attention due to its higher cost than fixed point. This paper explores the customized floating point for both the training and inference of CNNs. 9-bit customized floating point is found sufficient for the training of ResNet-20 on CIFAR-10 dataset with less than 1% accuracy loss, which can also be applied to the inference of CNNs. With reduced bit-width, a computational unit (CU) based on Quad-Multiplier Packing is proposed to improve the resource efficiency of CNNs on FPGA. This design can save 87.5% DSP slices and 62.5% LUTs on Xilinx Kintex-7 platform compared to CU using 32-bit floating point. More CUs can be arranged on FPGA and higher throughput can be expected accordingly.},
	Author = {Z. Zhang and D. Zhou and S. Wang and S. Kimura},
	Booktitle = {2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)},
	Date-Added = {2018-12-16 22:57:17 +1300},
	Date-Modified = {2018-12-16 22:57:28 +1300},
	Doi = {10.1109/ASPDAC.2018.8297303},
	Issn = {2153-697X},
	Keywords = {CNN-FPGA, computer vision;field programmable gate arrays;neural nets;fixed point conversion;point customization;reduced bit-width;computational unit;FPGA;32-bit floating point;Quad-multiplier packing;computer vision tasks;resource /power consumption;deep convolutional neural networks;CNNs;9-bit customized floating point;ResNet-20;Xilinx Kintex-7 platform;Training;Field programmable gate arrays;Kernel;Hardware;Convolutional neural networks;Computer vision},
	Month = {Jan},
	Pages = {184-189},
	Title = {Quad-multiplier packing based on customized floating point for convolutional neural networks on FPGA},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASPDAC.2018.8297303}}

@inproceedings{Zhao:2018:HRA:3220162.3220178,
	Acmid = {3220178},
	Address = {New York, NY, USA},
	Author = {Zhao, Boya and Li, Jingqun and Pan, Hongli and Wang, Mingjiang},
	Booktitle = {Proceedings of the 3rd International Conference on Multimedia Systems and Signal Processing},
	Date-Added = {2018-12-16 22:51:49 +1300},
	Date-Modified = {2018-12-16 22:51:59 +1300},
	Doi = {10.1145/3220162.3220178},
	Isbn = {978-1-4503-6457-7},
	Keywords = {FPGA-NN; ASIC, AlexNet, CNN, GoogleNet, ResNet, accelerator, convolutional neural network},
	Location = {Shenzhen, China},
	Numpages = {6},
	Pages = {150--155},
	Publisher = {ACM},
	Series = {ICMSSP '18},
	Title = {A High-Performance Reconfigurable Accelerator for Convolutional Neural Networks},
	Url = {http://doi.acm.org/10.1145/3220162.3220178},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3220162.3220178},
	Bdsk-Url-2 = {https://doi.org/10.1145/3220162.3220178}}

@article{Blott:2018:FRE:3299999.3242897,
	Acmid = {3242897},
	Address = {New York, NY, USA},
	Articleno = {16},
	Author = {Blott, Michaela and Preusser, Thomas B. and Fraser, Nicholas J. and Gambardella, Giulio and O'brien, Kenneth and Umuroglu, Yaman and Leeser, Miriam and Vissers, Kees},
	Date-Added = {2018-12-16 22:41:43 +1300},
	Date-Modified = {2019-04-28 15:11:33 +1200},
	Doi = {10.1145/3242897},
	Issn = {1936-7406},
	Issue_Date = {December 2018},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {BNN; FINN, FPGA, Neural network, artificial intelligence, convolutional neural networks, hardware accellerator, inference, quantized neural networks},
	Month = dec,
	Number = {3},
	Numpages = {23},
	Pages = {16:1--16:23},
	Publisher = {ACM},
	Title = {FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks},
	Url = {http://doi.acm.org/10.1145/3242897},
	Volume = {11},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3242897},
	Bdsk-Url-2 = {https://doi.org/10.1145/3242897}}

@article{MapFPGADSP2015,
	Author = {Bajaj, Ronak and Fahmy, Suhaib},
	Date-Added = {2018-12-16 15:13:50 +1300},
	Date-Modified = {2018-12-16 15:14:20 +1300},
	Doi = {10.1109/TCAD.2015.2474363},
	Journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	Keywords = {FPGA, DSP},
	Month = {01},
	Pages = {1-1},
	Title = {Mapping for maximum performance on FPGA DSP blocks},
	Volume = {35},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCAD.2015.2474363}}

@article{Esser11441,
	Abstract = {Brain-inspired computing seeks to develop new technologies that solve real-world problems while remaining grounded in the physical requirements of energy, speed, and size. Meeting these challenges requires high-performing algorithms that are capable of running on efficient hardware. Here, we adapt deep convolutional neural networks, which are today{\textquoteright}s state-of-the-art approach for machine perception in many domains, to perform classification tasks on neuromorphic hardware, which is today{\textquoteright}s most efficient platform for running neural networks. Using our approach, we demonstrate near state-of-the-art accuracy on eight datasets, while running at between 1,200 and 2,600 frames/s and using between 25 and 275 mW.Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-efficiency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that (i) approach state-of-the-art classification accuracy across eight standard datasets encompassing vision and speech, (ii) perform inference while preserving the hardware{\textquoteright}s underlying energy-efficiency and high throughput, running on the aforementioned datasets at between 1,200 and 2,600 frames/s and using between 25 and 275 mW (effectively \&gt;6,000 frames/s per Watt), and (iii) can be specified and trained using backpropagation with the same ease-of-use as contemporary deep learning. This approach allows the algorithmic power of deep learning to be merged with the efficiency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
	Author = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Andreopoulos, Alexander and Berg, David J. and McKinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and di Nolfo, Carmelo and Datta, Pallab and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
	Date-Added = {2018-12-16 12:32:34 +1300},
	Date-Modified = {2018-12-16 12:32:55 +1300},
	Doi = {10.1073/pnas.1604850113},
	Eprint = {https://www.pnas.org/content/113/41/11441.full.pdf},
	Issn = {0027-8424},
	Journal = {Proceedings of the National Academy of Sciences},
	Keywords = {CNN; ASIC, TrueNorth, Neuromorphic},
	Number = {41},
	Pages = {11441--11446},
	Publisher = {National Academy of Sciences},
	Title = {Convolutional networks for fast, energy-efficient neuromorphic computing},
	Url = {https://www.pnas.org/content/113/41/11441},
	Volume = {113},
	Year = {2016},
	Bdsk-Url-1 = {https://www.pnas.org/content/113/41/11441},
	Bdsk-Url-2 = {https://doi.org/10.1073/pnas.1604850113}}

@inproceedings{8436912,
	Abstract = {Scalability, distributivity, interoperability and modularity introduced in cloud computing have deeply changed the legacy data center's architecture, implementation and processing capabilities. The atomic network services offered by cloud architectures are called microservices. Unlike virtual machines, microservices can be implemented in the form of low resources footprint applications as containers (Docker, LXC etc.) or even smaller as unikernels (IncludeOS, ClickOS, Rumprun, HermitOS etc.). The need to efficiently offload the processing of computation-intensive applications has motivated the introduction of Field Programmable Gate Arrays (FPGA) boards in servers. FPGAs can nowadays be considered as cloud-standard processing resources. However, in today's cloud data centers, FPGAs cannot be accessed to run concurrent microservices. This severely limits the efficient deployment of microservices. This paper aims at introducing an FPGA-based system for the concurrent acceleration of cloud-native microservices onto FPGAs.},
	Author = {J. Lallet and A. Enrici and A. Saffar},
	Booktitle = {2018 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)},
	Date-Added = {2018-12-16 11:58:55 +1300},
	Date-Modified = {2018-12-16 11:59:07 +1300},
	Doi = {10.1109/BMSB.2018.8436912},
	Issn = {2155-5052},
	Keywords = {FPGA, cloud computing;computer centres;field programmable gate arrays;open systems;FPGA-based system;concurrent acceleration;cloud-native microservices;cloud microservices;interoperability;modularity;cloud computing;legacy data center;atomic network services;cloud architectures;virtual machines;low resources footprint applications;computation-intensive applications;Field Programmable Gate Arrays boards;cloud-standard processing resources;cloud data centers;concurrent microservices;Field programmable gate arrays;Acceleration;Cloud computing;Servers;Containers;Computer architecture;FPGA;Cloud;Microservice;Acceleration},
	Month = {June},
	Pages = {1-5},
	Title = {FPGA-Based System for the Acceleration of Cloud Microservices},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/BMSB.2018.8436912}}

@inproceedings{8443248,
	Abstract = {Network Functions Virtualization paradigm has emerged as a new concept in networking which aims at cost reduction and ease of network scalability by leveraging on virtualization technologies and commercial-off-the-shelf hardware to disintegrate the software implementation of network functions from the underlying hardware. Recently, lightweight virtualization techniques have emerged as efficient alternatives to traditional Virtual Network Functions (VNFs) developed as VMs. At the same time ARMv8 servers are gaining traction in the server world, mostly because of their interesting performance per watt characteristics. In this paper, the CPU, memory and Input/Output (I/O) performance of such lightweight techniques are compared with that of classic virtual machines on both x86 and ARMv8 platforms. More in particular, we selected KVM as hypervisor solution, Docker and rkt as container engines and finally Rumprun and OSv as unikernels. On x86, our results for CPU and memory related workloads highlight a slightly better performance for containers and unikernels whereas both of them perform almost twice as better as KVM for network I/O operations. This highlights performance issues of the Linux tap bridge with KVM but that can easily be overcome by using a user space virtual switch such as VOSYSwitch and OVS/DPDK. On ARM, both KVM and containers produce similar results for CPU and memory workloads, but have an exception for network I/O operations where KVM proves to be the fastest. We also showcase the several shortcomings of unikernels on ARM which account for their lack of stable support for this architecture.},
	Author = {A. Acharya and J. Fangu{\`e}de and M. Paolino and D. Raho},
	Booktitle = {2018 European Conference on Networks and Communications (EuCNC)},
	Date-Added = {2018-12-16 11:52:27 +1300},
	Date-Modified = {2018-12-16 11:52:36 +1300},
	Doi = {10.1109/EuCNC.2018.8443248},
	Issn = {2575-4912},
	Keywords = {Other; Containers;Virtualization;Virtual machine monitors;Benchmark testing;Hardware;Kernel;Linux},
	Month = {June},
	Pages = {282-9},
	Title = {A Performance Benchmarking Analysis of Hypervisors Containers and Unikernels on ARMv8 and x86 CPUs},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/EuCNC.2018.8443248}}

@inproceedings{8567366,
	Abstract = {Unikernels are a relatively recent way to create and quickly deploy extremely small virtual machines that do not require as much functional and operational software overhead as containers or virtual machines by leaving out unnecessary parts. This paradigm aims to replace bulky virtual machines on one hand, and to open up new classes of hardware for virtualization and networking applications on the other. In recent years, the tool chains used to create unikernels have grown from proof of concept to platforms that can run both new and existing software written in various programming languages. This paper studies the performance (both execution time and memory footprint) of unikernels versus Docker containers in the context of REST services and heavy processing workloads, written in Java, Go, and Python. With the results of the performance evaluations, predictions can be made about which cases could benefit from the use of unikernels over containers.},
	Author = {T. Goethals and M. Sebrechts and A. Atrey and B. Volckaert and F. De Turck},
	Booktitle = {2018 IEEE 8th International Symposium on Cloud and Service Computing (SC2)},
	Date-Added = {2018-12-16 11:51:43 +1300},
	Date-Modified = {2018-12-16 11:52:05 +1300},
	Doi = {10.1109/SC2.2018.00008},
	Keywords = {Other, Containers;Virtual machining;Unikernel;Java;Python;Virtual machine monitors;containers;microservices;virtualization;IoT},
	Month = {Nov},
	Pages = {1-8},
	Title = {Unikernels vs Containers: An In-Depth Benchmarking Study in the Context of Microservice Applications},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/SC2.2018.00008}}

@inproceedings{Ioffe:2015:BNA:3045118.3045167,
	Acmid = {3045167},
	Author = {Ioffe, Sergey and Szegedy, Christian},
	Booktitle = {Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37},
	Date-Added = {2018-12-16 10:36:12 +1300},
	Date-Modified = {2018-12-16 19:09:08 +1300},
	Keywords = {NN, batch normalization operation, Google},
	Location = {Lille, France},
	Numpages = {9},
	Pages = {448--456},
	Publisher = {JMLR.org},
	Series = {ICML'15},
	Title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
	Url = {http://dl.acm.org/citation.cfm?id=3045118.3045167},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3045118.3045167}}

@inproceedings{He:2015:DDR:2919332.2919814,
	Acmid = {2919814},
	Address = {Washington, DC, USA},
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	Booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
	Date-Added = {2018-12-16 10:32:16 +1300},
	Date-Modified = {2018-12-16 10:32:45 +1300},
	Doi = {10.1109/ICCV.2015.123},
	Isbn = {978-1-4673-8391-2},
	Keywords = {NN; ReLu, ImageNet, Microsoft},
	Numpages = {9},
	Pages = {1026--1034},
	Publisher = {IEEE Computer Society},
	Series = {ICCV '15},
	Title = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
	Url = {http://dx.doi.org/10.1109/ICCV.2015.123},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCV.2015.123}}

@inproceedings{7528127,
	Abstract = {The Fully Homomorphic Encryption (FHE) is an encryption technique for processing encrypted data without the need of decrypting them. This method is suitable for use in untrusted environments, such as cloud computing platforms. Various methods have been proposed to implement this technique. But the greatest problem of these methods is that for its operation, there is the need to generate public keys with large sizes, whose immediate consequence is to cause such encryption schemes not reach the desired runtime performance. This article aims to optimize public keys compression techniques using Genetic Algorithms (GA) for parameter calibration of Coron test variants primitives in order to speed up the execution time of each of these primitives.},
	Author = {J. Gavinho Filho and G. P. Silva and C. Miceli},
	Booktitle = {2016 19th International Conference on Information Fusion (FUSION)},
	Date-Added = {2018-12-16 10:20:53 +1300},
	Date-Modified = {2018-12-16 10:21:04 +1300},
	Keywords = {Evolutionary, genetic algorithms;public key cryptography;fully homomorphic encryption;genetic algorithms;FHE;encrypted data;cloud computing platforms;public keys compression;GA;parameter calibration;Coron test variants primitives;Encryption;Public key;Genetic algorithms;Proposals;Sociology;Fully Homomorphic Encryption;Compression;Security;Genetic Algorithm},
	Month = {July},
	Pages = {1991-1998},
	Title = {A public key compression method for Fully Homomorphic Encryption using Genetic Algorithms},
	Year = {2016}}

@article{10.3389/fnins.2017.00496,
	Abstract = {Artificial neural networks (ANNs) trained using backpropagation are powerful learning architectures that have achieved state-of-the-art performance in various benchmarks. Significant effort has been devoted to developing custom silicon devices to accelerate inference in ANNs. Accelerating the training phase, however, has attracted relatively little attention. In this paper, we describe a hardware-efficient on-line learning technique for feedforward multi-layer ANNs that is based on pipelined backpropagation. Learning is performed in parallel with inference in the forward pass, removing the need for an explicit backward pass and requiring no extra weight lookup. By using binary state variables in the feedforward network and ternary errors in truncated-error backpropagation, the need for any multiplications in the forward and backward passes is removed, and memory requirements for the pipelining are drastically reduced.  Further reduction in addition operations owing to the sparsity in the forward neural and backpropagating error signal paths contributes to highly efficient hardware implementation.  For proof-of-concept validation, we demonstrate on-line learning of MNIST handwritten digit classification on a Spartan 6 FPGA interfacing with an external 1Gb DDR2 DRAM, that shows small degradation in test error performance compared to an equivalently sized binary ANN trained off-line using standard back-propagation and exact errors. Our results highlight an attractive synergy between pipelined backpropagation and binary-state networks in substantially reducing computation and memory requirements, making pipelined on-line learning practical in deep networks.},
	Author = {Mostafa, Hesham and Pedroni, Bruno and Sheik, Sadique and Cauwenberghs, Gert},
	Date-Added = {2018-12-16 00:39:15 +1300},
	Date-Modified = {2018-12-16 00:39:33 +1300},
	Doi = {10.3389/fnins.2017.00496},
	Issn = {1662-453X},
	Journal = {Frontiers in Neuroscience},
	Keywords = {BNN, FPGA, Back propagation, Quantization},
	Pages = {496},
	Title = {Hardware-Efficient On-line Learning through Pipelined Truncated-Error Backpropagation in Binary-State Networks},
	Url = {https://www.frontiersin.org/article/10.3389/fnins.2017.00496},
	Volume = {11},
	Year = {2017},
	Bdsk-Url-1 = {https://www.frontiersin.org/article/10.3389/fnins.2017.00496},
	Bdsk-Url-2 = {https://doi.org/10.3389/fnins.2017.00496}}

@article{Shackleford2001,
	Abstract = {Accelerating a genetic algorithm (GA) by implementing it in a reconfigurable field programmable gate array (FPGA) is described. The implemented GA features: random parent selection, which conserves selection circuitry; a steady-state memory model, which conserves chip area; survival of fitter child chromosomes over their less-fit parent chromosomes, which promotes evolution. A net child chromosome generation rate of one per clock cycle is obtained by pipelining the parent selection, crossover, mutation, and fitness evaluation functions. Complex fitness functions can be further pipelined to maintain a high-speed clock cycle. Fitness functions with a pipeline initiation interval of greater than one can be plurally implemented to maintain a net evaluated-chromosome throughput of one per clock cycle. Two prototypes are described: The first prototype (c. 1996 technology) is a multiple-FPGA chip implementation, running at a 1 MHz clock rate, that solves a 94-row {\texttimes} 520-column set covering problem 2,200{\texttimes} faster than a 100 MHz workstation running the same algorithm in C. The second prototype (Xilinx XVC300) is a single-FPGA chip implementation, running at a 66 MHZ clock rate, that solves a 36-residue protein folding problem in a 2-d lattice 320{\texttimes} faster than a 366 MHz Pentium II. The current largest FPGA (Xilinx XCV3200E) has circuitry available for the implementation of 30 fitness function units which would yield an acceleration of 9,600{\texttimes} for the 36-residue protein folding problem.},
	Author = {Shackleford, Barry and Snider, Greg and Carter, Richard J. and Okushi, Etsuko and Yasuda, Mitsuhiro and Seo, Katsuhiko and Yasuura, Hiroto},
	Date-Added = {2018-12-16 00:23:25 +1300},
	Date-Modified = {2018-12-16 00:23:40 +1300},
	Day = {01},
	Doi = {10.1023/A:1010018632078},
	Issn = {1573-7632},
	Journal = {Genetic Programming and Evolvable Machines},
	Keywords = {FPGA, GA, Pipeline, HPC},
	Month = {Mar},
	Number = {1},
	Pages = {33--60},
	Title = {A High-Performance, Pipelined, FPGA-Based Genetic Algorithm Machine},
	Url = {https://doi.org/10.1023/A:1010018632078},
	Volume = {2},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1010018632078}}

@article{Guo:2016:PGA:2927964.2927980,
	Acmid = {2927980},
	Address = {New York, NY, USA},
	Author = {Guo, Liucheng and Funie, Andreea Ingrid and Thomas, David B. and Fu, Haohuan and Luk, Wayne},
	Date-Added = {2018-12-16 00:15:15 +1300},
	Date-Modified = {2018-12-16 00:15:36 +1300},
	Doi = {10.1145/2927964.2927980},
	Issn = {0163-5964},
	Issue_Date = {September 2015},
	Journal = {SIGARCH Comput. Archit. News},
	Keywords = {FPGA, Parallel, Traveling Salesman Problem},
	Month = apr,
	Number = {4},
	Numpages = {8},
	Pages = {86--93},
	Publisher = {ACM},
	Title = {Parallel Genetic Algorithms on Multiple FPGAs},
	Url = {http://doi.acm.org/10.1145/2927964.2927980},
	Volume = {43},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2927964.2927980},
	Bdsk-Url-2 = {https://doi.org/10.1145/2927964.2927980}}

@inproceedings{1321812,
	Abstract = {This paper presents the research work directed regards the synthesis and implementation of a parallel-pipelined hardware genetic algorithm (PPHGA) utilizing very high speed integrated circuit hardware description language (VHDL) for programming field programmable gate arrays (FPGAs). The main design is divided into several modules. The modules are autonomous in operation once the system starts to run. They communicate with each other using a handshaking protocol. Three applications are then experimented using the PPHGA to test its optimization power. These are linear interpolation, thermistor data processing, and vehicle acceleration computation.},
	Author = {H. E. Mostafa and A. I. Khadragi and Y. Y. Hanafi},
	Booktitle = {Proceedings of the Twenty-First National Radio Science Conference, 2004. NRSC 2004.},
	Date-Added = {2018-12-16 00:13:44 +1300},
	Date-Modified = {2018-12-16 00:13:58 +1300},
	Doi = {10.1109/NRSC.2004.240504},
	Keywords = {FPGA; parallel programming;pipeline processing;program testing;hardware description languages;logic testing;genetic algorithms;logic programming;logic design;interpolation;thermistors;protocols;parallel-pipelined hardware genetic algorithm;very high speed integrated circuit hardware description language;VHDL;field programmable gate array;gate arrays programming;FPGA design;handshaking protocol;program testing;optimization power;linear interpolation;thermistor data processing;vehicle acceleration computation;lateral acceleration;Genetic algorithms;Field programmable gate arrays;Integrated circuit synthesis;Very high speed integrated circuits;Hardware design languages;Parallel programming;Protocols;Circuit testing;Interpolation;Thermistors},
	Month = {March},
	Pages = {C9-1},
	Title = {Hardware implementation of genetic algorithm on FPGA},
	Year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/NRSC.2004.240504}}

@inproceedings{inproceedings,
	Author = {Deliparaschos, Kyriakos and Tzafestas, Spyros},
	Booktitle = {Panhellenic Conference on Electronics and Telecommunications},
	Date-Added = {2018-12-15 23:47:47 +1300},
	Date-Modified = {2018-12-15 23:53:06 +1300},
	Doi = {10.13140/2.1.1543.1689},
	Keywords = {FPGA, GA, Fuzzy logic, ASIC, Traveling Salesman Problem},
	Month = {September},
	Title = {Design Paradigms of Intelligent Control Systems on a Chip},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.13140/2.1.1543.1689}}

@article{Torquato2018HighPerformancePI,
	Author = {Matheus F. Torquato and Marcelo A. C. Fernandes},
	Date-Added = {2018-12-15 23:21:11 +1300},
	Date-Modified = {2018-12-15 23:21:32 +1300},
	Journal = {CoRR},
	Keywords = {FPGA, GA, parallel computing, HPC},
	Title = {High-Performance Parallel Implementation of Genetic Algorithm on FPGA},
	Volume = {abs/1806.11555},
	Year = {2018}}

@article{PhysRevLett.119.208301,
	Author = {Farkhooi, Farzad and Stannat, Wilhelm},
	Date-Added = {2018-12-15 22:47:16 +1300},
	Date-Modified = {2018-12-15 22:47:38 +1300},
	Doi = {10.1103/PhysRevLett.119.208301},
	Issue = {20},
	Journal = {Phys. Rev. Lett.},
	Keywords = {BNN, Mean field theory, Recurrent NN},
	Month = {Nov},
	Numpages = {5},
	Pages = {208301},
	Publisher = {American Physical Society},
	Title = {Complete Mean-Field Theory for Dynamics of Binary Recurrent Networks},
	Url = {https://link.aps.org/doi/10.1103/PhysRevLett.119.208301},
	Volume = {119},
	Year = {2017},
	Bdsk-Url-1 = {https://link.aps.org/doi/10.1103/PhysRevLett.119.208301},
	Bdsk-Url-2 = {https://doi.org/10.1103/PhysRevLett.119.208301}}

@article{2015arXiv150602078K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://beta.openreview.net/forum?id=71BmK0m6qfAE8VvKUQWB},
	Author = {{Karpathy}, A. and {Johnson}, J. and {Fei-Fei}, L.},
	Date-Added = {2018-12-15 22:23:49 +1300},
	Date-Modified = {2018-12-15 22:28:23 +1300},
	Eprint = {1506.02078},
	Journal = {International Conference on Learning Representations},
	Keywords = {CNN; Machine Learning, Computation and Language, Neural and Evolutionary Computing},
	Month = May,
	Title = {{Visualizing and Understanding Recurrent Networks}},
	Year = 2016}

@inproceedings{1809.11086,
	Author = {Arash Ardakani, Sean C. Smithson, Warren J. Gross, Zhengyun Ji, Brett H. Meyer},
	Booktitle = {International Conference on Learning Representations},
	Date-Added = {2018-12-15 18:36:35 +1300},
	Date-Modified = {2018-12-15 18:59:39 +1300},
	Keywords = {BNN, Recurrent NN, Text analysis,},
	Note = {under review},
	Organization = {McGill University},
	Title = {Learning Recurrent Binary/Ternary Weights},
	Url = {https://openreview.net/forum?id=HkNGYjR9FX},
	Year = {2019},
	Bdsk-Url-1 = {https://openreview.net/forum?id=HkNGYjR9FX}}

@article{Hesamifard2017CryptoDLDN,
	Author = {Ehsan Hesamifard and Hassan Takabi and Mehdi Ghasemi},
	Date-Added = {2018-12-15 16:57:47 +1300},
	Date-Modified = {2018-12-15 16:58:11 +1300},
	Journal = {CoRR},
	Keywords = {NN, Cryptography, Homomorphic encryption, MNIST},
	Title = {CryptoDL: Deep Neural Networks over Encrypted Data},
	Volume = {abs/1711.05189},
	Year = {2017}}

@inproceedings{pmlr-v48-gilad-bachrach16,
	Abstract = {Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.},
	Address = {New York, New York, USA},
	Author = {Ran Gilad-Bachrach and Nathan Dowlin and Kim Laine and Kristin Lauter and Michael Naehrig and John Wernsing},
	Booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
	Date-Added = {2018-12-15 16:14:29 +1300},
	Date-Modified = {2019-01-11 15:27:42 +1300},
	Editor = {Maria Florina Balcan and Kilian Q. Weinberger},
	Keywords = {NN; Cryptography, Neural Network, Homomorphic encryption, Cryptonet},
	Month = {20--22 Jun},
	Pages = {201--210},
	Pdf = {http://proceedings.mlr.press/v48/gilad-bachrach16.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy},
	Url = {http://proceedings.mlr.press/v48/gilad-bachrach16.html},
	Volume = {48},
	Year = {2016},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v48/gilad-bachrach16.html}}

@article{PractBNN,
	Author = {Austin, Jim},
	Date-Added = {2018-12-14 20:34:29 +1300},
	Date-Modified = {2018-12-16 15:14:48 +1300},
	Journal = {Adaptive computing and Information processing conference},
	Keywords = {BNN, ADAM, Associative Memory, Image Segmentation},
	Month = {07},
	Title = {The Practical Application of Binary Neural Networks.},
	Year = {1994}}

@article{CUCS-017-15,
	Author = {Townsend, Richard Morse; Kim, Martha Allen; Edwards, Stephen A.},
	Date-Added = {2018-12-14 12:56:57 +1300},
	Date-Modified = {2018-12-14 12:58:42 +1300},
	Journal = {Columbia University Computer Science Technical Reports},
	Keywords = {Haskell-FPGA, Memory, Computer programs},
	Title = {Hardware in Haskell: Implementing Memories in a Stream-Based World},
	Year = {2015}}

@phdthesis{AronssonHWSWCoDesignThesis,
	Abstract = {Developing software for embedded systems presents quite the challenge---not only do these systems demand good knowledge of the hardware they run on, but their limited resources also make it difficult to achieve efficiency. For embedded systems with different kinds of processing elements, the challenge is even greater; the presence of heterogeneous elements both raises all of the issues associated with homogeneous systems, and may also cause non-uniform system development and capability.

In this thesis we explore a functional approach to heterogeneous system development, with a staged hardware software co-design language embedded in Haskell, to address many of the modularity problems typically found in such systems. This staged approach enables designers to build their applications from reusable components and skeletons, while retaining control over much of the generated source code. Design exploration also benefits from the functional approach, since Haskell's type classes can be used to ensure that certain operations will be available. As a result, a developer can not only write for hardware and software in the co-design language, but she can also write generic programs that are suitable for both.

Internally, the co-design language is based on a monadic representation of imperative programs that abstracts away from its underlying statement, expression, and predicate types by establishing an interface to their respective interpreters. Programs are thus loosely coupled to their underlying types, giving a clear separation of concerns. The compilation process is expressed as a series of translations between progressively smaller typed languages, which safeguards against many common errors.

In addition to the hardware software co-design language, this thesis also introduces a language for expressing digital signal processing algorithms, using a model of synchronous data-flow that is embedded in Haskell. The language supports definitions in a functional style, reducing the gap between an algorithm's mathematical specification and its implementation. A vector language is also presented, which builds on a functional representation that guarantees fusion for arrays. Both of these languages are intended to be extensions of the co-design language, but neither one is dependent on it and can thus be used to extend other languages as well.},
	Author = {Markus Aronsson},
	Date-Added = {2018-12-14 12:38:48 +1300},
	Date-Modified = {2018-12-14 12:41:47 +1300},
	Keywords = {Haskell-FPGA, Thesis, Hardware, Software, Co-design},
	Month = {October},
	School = {Chalmers, Computer Science and Engineering, Functional Programming},
	Title = {A Functional Approach to Hardware Software Co-Design},
	Type = {Licentiate thesis},
	Year = {2018}}

@inproceedings{10.1007/978-3-540-85373-2_8,
	Abstract = {For the memory intensive task of graph reduction, modern PCs are limited not by processor speed, but by the rate that data can travel between processor and memory. This limitation is known as the von Neumann bottleneck. We explore the effect of widening this bottleneck using a special-purpose graph reduction machine with wide, parallel memories. Our prototype machine -- the Reduceron -- is implemented using an FPGA, and is based on a simple template-instantiation evaluator. Running at only 91.5MHz on an FPGA, the Reduceron is faster than mature bytecode implementations of Haskell running on a 2.8GHz PC.},
	Address = {Berlin, Heidelberg},
	Author = {Naylor, Matthew and Runciman, Colin},
	Booktitle = {Implementation and Application of Functional Languages},
	Date-Added = {2018-12-14 11:56:31 +1300},
	Date-Modified = {2018-12-14 11:56:55 +1300},
	Editor = {Chitil, Olaf and Horv{\'a}th, Zolt{\'a}n and Zs{\'o}k, Vikt{\'o}ria},
	Isbn = {978-3-540-85373-2},
	Keywords = {Haskel-FPGA, Graph reduction, Parallel memory},
	Pages = {129--146},
	Publisher = {Springer Berlin Heidelberg},
	Title = {The Reduceron: Widening the von Neumann Bottleneck for Graph Reduction Using an FPGA},
	Year = {2008}}

@inproceedings{Aronsson:2017:HSC:3122955.3122970,
	Acmid = {3122970},
	Address = {New York, NY, USA},
	Author = {Aronsson, Markus and Sheeran, Mary},
	Booktitle = {Proceedings of the 10th ACM SIGPLAN International Symposium on Haskell},
	Date-Added = {2018-12-14 11:42:12 +1300},
	Date-Modified = {2018-12-14 11:42:22 +1300},
	Doi = {10.1145/3122955.3122970},
	Isbn = {978-1-4503-5182-9},
	Keywords = {Haskel-FPGA, domain specific language, hardware software co-design},
	Location = {Oxford, UK},
	Numpages = {12},
	Pages = {162--173},
	Publisher = {ACM},
	Series = {Haskell 2017},
	Title = {Hardware Software Co-design in Haskell},
	Url = {http://doi.acm.org/10.1145/3122955.3122970},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3122955.3122970},
	Bdsk-Url-2 = {https://doi.org/10.1145/3122955.3122970}}

@inproceedings{Zhai:2015:HSR:2830840.2830850,
	Acmid = {2830850},
	Address = {Piscataway, NJ, USA},
	Author = {Zhai, Kuangya and Townsend, Richard and Lairmore, Lianne and Kim, Martha A. and Edwards, Stephen A.},
	Booktitle = {Proceedings of the 10th International Conference on Hardware/Software Codesign and System Synthesis},
	Date-Added = {2018-12-14 11:23:43 +1300},
	Date-Modified = {2018-12-14 11:23:52 +1300},
	Isbn = {978-1-4673-8321-9},
	Keywords = {Haskell-FPGA, functional hardware, high-level synthesis, recursion},
	Location = {Amsterdam, The Netherlands},
	Numpages = {11},
	Pages = {83--93},
	Publisher = {IEEE Press},
	Series = {CODES '15},
	Title = {Hardware Synthesis from a Recursive Functional Language},
	Url = {http://dl.acm.org/citation.cfm?id=2830840.2830850},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2830840.2830850}}

@book{Yu2008,
	Abstract = {Deploying Evolutionary Computation (EC) solutions to real-world problems involves a wide spectrum of activities, ranging from framing the business problems and implementing the solutions to the final deployment of the solutions to the field. However, issues related to these activities are not commonly discussed in a typical EC course curriculum. Meanwhile, although the values of applied research are acknowledged by most EC technologists, the perception seems to be very narrow: success stories boost morale and high profile applications can help to secure funding for future research and can help to attract high caliber students.},
	Address = {Berlin, Heidelberg},
	Author = {Yu, Tina and Davis, Lawrence},
	Booktitle = {Evolutionary Computation in Practice},
	Date-Added = {2018-12-14 11:01:07 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Doi = {10.1007/978-3-540-75771-9_1},
	Editor = {Yu, Tina and Davis, Lawrence and Baydar, Cem and Roy, Rajkumar},
	Isbn = {978-3-540-75771-9},
	Keywords = {Evolutionary, Books},
	Pages = {1--8},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Evolutionary Computation in Practice},
	Url = {https://doi.org/10.1007/978-3-540-75771-9_1},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-540-75771-9_1}}

@inproceedings{Brown00agenetic,
	Author = {Deryck Brown and A. Beatriz and Beatriz Garmendia-Doval and John A.W. McCall and The Robert},
	Booktitle = {In Proceedings of 2nd Asia-Pacific Workshop on Genetic Algorithms and their Applications (APGA 2000},
	Date-Added = {2018-12-14 10:40:55 +1300},
	Date-Modified = {2018-12-14 10:42:03 +1300},
	Keywords = {Haskell, Genetic Algorithm,},
	Pages = {152--163},
	Publisher = {Global-Link Publishing Company},
	Title = {A genetic algorithm framework using Haskell},
	Year = {2000}}

@inproceedings{Wang:2016:AUG:2976002.2976009,
	Acmid = {2976009},
	Address = {New York, NY, USA},
	Author = {Wang, Yisu Remy and Nunez, Diogenes and Fisher, Kathleen},
	Booktitle = {Proceedings of the 9th International Symposium on Haskell},
	Date-Added = {2018-12-14 10:34:30 +1300},
	Date-Modified = {2018-12-14 10:34:30 +1300},
	Doi = {10.1145/2976002.2976009},
	Isbn = {978-1-4503-4434-0},
	Keywords = {Haskell, genetic algorithms, laziness, strictness annotations},
	Location = {Nara, Japan},
	Numpages = {13},
	Pages = {114--126},
	Publisher = {ACM},
	Series = {Haskell 2016},
	Title = {Autobahn: Using Genetic Algorithms to Infer Strictness Annotations},
	Url = {http://doi.acm.org/10.1145/2976002.2976009},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2976002.2976009},
	Bdsk-Url-2 = {https://doi.org/10.1145/2976002.2976009}}

@article{2018arXiv181012894B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181012894B},
	Archiveprefix = {arXiv},
	Author = {{Burda}, Y. and {Edwards}, H. and {Storkey}, A. and {Klimov}, O.},
	Date-Added = {2018-12-13 22:12:36 +1300},
	Date-Modified = {2018-12-13 22:12:46 +1300},
	Eprint = {1810.12894},
	Journal = {arXiv e-prints},
	Keywords = {DCGAN, Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Uber},
	Month = oct,
	Title = {{Exploration by Random Network Distillation}},
	Year = 2018}

@article{2018arXiv180600175K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180600175K},
	Archiveprefix = {arXiv},
	Author = {{Keramati}, R. and {Whang}, J. and {Cho}, P. and {Brunskill}, E.},
	Date-Added = {2018-12-13 22:10:44 +1300},
	Date-Modified = {2018-12-13 22:11:14 +1300},
	Eprint = {1806.00175},
	Journal = {arXiv e-prints},
	Keywords = {DCGAN, Artificial Intelligence, Uber},
	Month = may,
	Primaryclass = {cs.AI},
	Title = {{Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning}},
	Year = 2018}

@article{2018arXiv181111357T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181111357T},
	Archiveprefix = {arXiv},
	Author = {{Turner}, R. and {Hung}, J. and {Saatci}, Y. and {Yosinski}, J.},
	Date-Added = {2018-12-13 21:54:57 +1300},
	Date-Modified = {2018-12-13 21:55:25 +1300},
	Eprint = {1811.11357},
	Journal = {arXiv e-prints},
	Keywords = {DCGAN, Statistics, Machine Learning, Machine Learning, Uber},
	Month = nov,
	Primaryclass = {stat.ML},
	Title = {{Metropolis-Hastings Generative Adversarial Networks}},
	Year = 2018}

@book{Salvaris2018Book,
	Abstract = {This chapter discusses some of the trends in deep learning and related fields. We cover specifically which trends might be useful for what tasks as well as discuss some of the methods and ideas that could have far-reaching implications but have yet to be applied to many real-world problems. We finish by covering briefly some of the current limitations of deep learning as well as some other areas of AI that seem to hold promise for future AI applications, and discuss briefly some of the ethical and legal implications of deep learning applications.},
	Address = {Berkeley, CA},
	Author = {Salvaris, Mathew and Dean, Danielle and Tok, Wee Hyong},
	Booktitle = {Deep Learning with Azure: Building and Deploying Artificial Intelligence Solutions on the Microsoft AI Platform},
	Date-Added = {2018-12-12 22:25:37 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Doi = {10.1007/978-1-4842-3679-6_3},
	Isbn = {978-1-4842-3679-6},
	Keywords = {Evolutionary, Books, Neuroevolution},
	Publisher = {Apress},
	Title = {Trends in Deep Learning},
	Url = {https://doi.org/10.1007/978-1-4842-3679-6_3},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4842-3679-6_3}}

@book{Sher2013NeuroEvol,
	Abstract = {This chapter discusses the numerous reasons for why one might wish to study the subject of neuroevolution. I cover a number of different applications of such a system, giving examples and scenarios of a neuroevolutionary system being applied within a variety of different fields. A discussion then follows on where all of this research is heading, and what the next step within this field might be. Finally, a whirlwind introduction of the book is given, with a short summary of what is covered in every chapter.},
	Address = {New York, NY},
	Author = {Sher, Gene I.},
	Booktitle = {Handbook of Neuroevolution Through Erlang},
	Date-Added = {2018-12-12 19:57:45 +1300},
	Date-Modified = {2018-12-12 19:59:29 +1300},
	Doi = {10.1007/978-1-4614-4463-3_1},
	Isbn = {978-1-4614-4463-3},
	Keywords = {Evolutionary, Neuroevolution, Erlang, Finance},
	Pages = {1--39},
	Publisher = {Springer New York},
	Title = {Handbook of Neuroevolution Through Erlang},
	Url = {https://doi.org/10.1007/978-1-4614-4463-3_1},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4614-4463-3_1}}

@inproceedings{10.1007/978-3-642-33860-1_3,
	Abstract = {The idea of using evolutionary computation to train artificial neural networks, or neuroevolution (NE), has now been around for over 20 years. The main appeal of this approach is that, because it does not rely on gradient information (e.g. backpropagation), it can potentially harness the universal function approximation capability of neural networks to solve reinforcement learning (RL) tasks, where there is no ``teacher'' (i.e. no targets or examples of correct behavior). Instead of incrementally adjusting the synaptic weights of a single network, the space of network parameters is searched directly according to principles inspired by natural selection: (1) encode a population of networks as strings, or genomes, (2) transform them into networks, (3) evaluate them on the task, (4) generate new, hopefully better, nets by recombining those that are most ``fit'', (5) goto step 2 until a solution is found. By evolving neural networks, NE can cope naturally with tasks that have continuous inputs and outputs, and, by evolving networks with feedback connections (recurrent networks), it can tackle more general tasks that require memory.},
	Address = {Berlin, Heidelberg},
	Author = {Gomez, Faustino},
	Booktitle = {Theory and Practice of Natural Computing},
	Date-Added = {2018-12-12 18:32:49 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {Dediu, Adrian-Horia and Mart{\'\i}n-Vide, Carlos and Truthe, Bianca},
	Isbn = {978-3-642-33860-1},
	Keywords = {Evolutionary, Books, Scalable, Reinforcement Learning},
	Pages = {27--29},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Scalable Neuroevolution for Reinforcement Learning},
	Year = {2012}}

@inproceedings{10.1007/11893295_130,
	Abstract = {This paper addresses the problem of accelerating large artificial neural networks (ANN), whose topology and weights can evolve via the use of a genetic algorithm. The proposed digital hardware architecture is capable of processing any evolved network topology, whilst at the same time providing a good trade off between throughput, area and power consumption. The latter is vital for a longer battery life on mobile devices. The architecture uses multiple parallel arithmetic units in each processing element (PE). Memory partitioning and data caching are used to minimise the effects of PE pipeline stalling. A first order minimax polynomial approximation scheme, tuned via a genetic algorithm, is used for the activation function generator. Efficient arithmetic circuitry, which leverages modified Booth recoding, column compressors and carry save adders, is adopted throughout the design.},
	Address = {Berlin, Heidelberg},
	Author = {Larkin, Daniel and Kinane, Andrew and O'Connor, Noel},
	Booktitle = {Neural Information Processing},
	Date-Added = {2018-12-12 18:30:15 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Editor = {King, Irwin and Wang, Jun and Chan, Lai-Wan and Wang, DeLiang},
	Isbn = {978-3-540-46485-3},
	Keywords = {Evolutionary, Books, Neuroevolution, Processing Element},
	Pages = {1178--1188},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Towards Hardware Acceleration of Neuroevolution for Multimedia Processing Applications on Mobile Devices},
	Year = {2006}}

@article{7307180,
	Abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyze the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The paper also highlights important open research challenges in the field.},
	Author = {S. Risi and J. Togelius},
	Date-Added = {2018-12-12 16:15:08 +1300},
	Date-Modified = {2018-12-12 16:15:18 +1300},
	Doi = {10.1109/TCIAIG.2015.2494596},
	Issn = {1943-068X},
	Journal = {IEEE Transactions on Computational Intelligence and AI in Games},
	Keywords = {evolutionary; computer games;learning (artificial intelligence);neural nets;neuroevolution;NE;computer game;artificial neural network training;evolutionary algorithm;Games;Artificial intelligence;Evolutionary computation;Genetic algorithms;Biological neural networks;Network topology;Evolutionary algorithms;neural networks;neuroevolution},
	Month = {March},
	Number = {1},
	Pages = {25-41},
	Title = {Neuroevolution in Games: State of the Art and Open Challenges},
	Volume = {9},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCIAIG.2015.2494596}}

@inproceedings{4250190,
	Abstract = {Appropriate topology and connection weight are two very important properties a neural network must have in order to successfully perform data classification. In this paper, we propose a hybrid training scheme Learning-NEAT (L-NEAT) for data classification problem. L-NEAT simplifies evolution by dividing the complete problem domain into sub tasks and learn the sub tasks by incorporating back propagation rule into the NeuroEvolution of Augmenting Topologies (NEAT) algorithm. The new algorithm combines the strength of searching for topology and weights from NEAT and back propagation respectively while overcoming problems associated with direct use of NEAT. We claim that L-NEAT can produce neural network for classification problem effectively and efficiently. Empirical evaluation shows that L-NEAT evolves classifying neural network with good generalization ability. Its accuracy outperforms original NEAT.},
	Author = {L. Chen and D. Alahakoon},
	Booktitle = {2006 International Conference on Information and Automation},
	Date-Added = {2018-12-12 16:06:11 +1300},
	Date-Modified = {2018-12-12 16:06:25 +1300},
	Doi = {10.1109/ICINFA.2006.374100},
	Issn = {2151-1802},
	Keywords = {Evolutionary; backpropagation;neural nets;pattern classification;search problems;topology;neuroevolution of augmenting topologies;augmenting topology;data classification learning;neural network;learning-NEAT training scheme;backpropagation;search problem;Network topology;Artificial neural networks;Neural networks;Supervised learning;Biological cells;Information technology;Evolutionary computation;Unsupervised learning;Technological innovation},
	Month = {Dec},
	Pages = {367-371},
	Title = {NeuroEvolution of Augmenting Topologies with Learning for Data Classification},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICINFA.2006.374100}}

@inproceedings{7991745,
	Abstract = {The article describes the problematic issues of neuroevolution, i.e. a promising approach for solving complex problems of machine learning, neural networks, adaptive management and multi-agent systems, evolutionary robotics, gaming strategies, and computer art. The authors have suggested neuro evolutional algorithm and presented experiment results on a standard task: the balancing trolley with two flagpoles of different lengths.},
	Author = {S. Rodzin and O. Rodzina and L. Rodzina},
	Booktitle = {2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)},
	Date-Added = {2018-12-12 13:20:17 +1300},
	Date-Modified = {2018-12-12 13:20:28 +1300},
	Doi = {10.1109/ICAICT.2016.7991745},
	Issn = {2472-8586},
	Keywords = {Evolutionary;learning (artificial intelligence);multi-agent systems;neural nets;machine learning;neural networks;adaptive management;multiagent systems;evolutionary robotics;gaming strategies;computer art;neuro evolutional algorithm;balancing trolley;Biological neural networks;Neurons;Encoding;Sociology;Statistics;Topology;Head;Neuroevolution;reinforcement machine learning;optimization;evolutionary computation;fitness function},
	Month = {Oct},
	Pages = {1-4},
	Title = {Neuroevolution: Problems, algorithms, and experiments},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICAICT.2016.7991745}}

@inproceedings{8489184,
	Abstract = {Memetic algorithms have been a promising strategy to enhance neuroevolution in the past. Cooperative coevolution has been combined as memetic cooperative neuroevolution with application to chaotic time series prediction. Although the method has shown promising performance, there are limitations in the balance between global and local search. The previous study used a specific local search strategy for intensification that affected the diversity of solutions. In this study, we address this limitation by information (meme) collection strategies that maintains and refines a pool of memes during global search. We present two strategies where one is sequential and the other is concurrent meme collection implemented at different stages of evolution. In the majority of the given problems, the proposed strategies showed improvement in prediction accuracy over the related methods.},
	Author = {G. Wong and A. Sharma and R. Chandra},
	Booktitle = {2018 International Joint Conference on Neural Networks (IJCNN)},
	Date-Added = {2018-12-12 13:05:44 +1300},
	Date-Modified = {2018-12-12 13:05:58 +1300},
	Doi = {10.1109/IJCNN.2018.8489184},
	Issn = {2161-4407},
	Keywords = {Evolutionary;search problems;social sciences;time series;memetic algorithms;neuroevolution;coevolution;chaotic time series prediction;global search;specific local search strategy;information collection strategies;concurrent meme collection;prediction accuracy;Memetics;Time series analysis;Neurons;Prediction algorithms;Sociology;Cooperative Coevolution;Memetic Algorithms;Time Series Prediction Global Search;Local Search;Neuroevolution},
	Month = {July},
	Pages = {1-6},
	Title = {Information Collection Strategies In Memetic Cooperative Neuroevolution For Time Series Prediction},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2018.8489184}}

@inproceedings{Han:2016:EEI:3001136.3001163,
	Acmid = {3001163},
	Address = {Piscataway, NJ, USA},
	Author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
	Booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
	Date-Added = {2018-12-11 11:22:33 +1300},
	Date-Modified = {2018-12-11 11:22:42 +1300},
	Doi = {10.1109/ISCA.2016.30},
	Isbn = {978-1-4673-8947-1},
	Keywords = {FPGA-NN, ASIC, algorithm-hardware co-design, deep learning, hardware acceleration, model compression},
	Location = {Seoul, Republic of Korea},
	Numpages = {12},
	Pages = {243--254},
	Publisher = {IEEE Press},
	Series = {ISCA '16},
	Title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
	Url = {https://doi.org/10.1109/ISCA.2016.30},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCA.2016.30}}

@inproceedings{shayer2018learning,
	Author = {Oran Shayer and Dan Levi and Ethan Fetaya},
	Booktitle = {International Conference on Learning Representations},
	Date-Added = {2018-12-09 16:39:21 +1300},
	Date-Modified = {2018-12-09 16:39:50 +1300},
	Keywords = {BNN, Discrete, MNIST, CIFAR, ImageNet},
	Title = {Learning Discrete Weights Using the Local Reparameterization Trick},
	Url = {https://openreview.net/forum?id=BySRH6CpW},
	Year = {2018},
	Bdsk-Url-1 = {https://openreview.net/forum?id=BySRH6CpW}}

@inproceedings{wu2018training,
	Author = {Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},
	Booktitle = {International Conference on Learning Representations},
	Date-Added = {2018-12-09 16:29:45 +1300},
	Date-Modified = {2018-12-09 17:10:02 +1300},
	Keywords = {BNN; Binary, MNIST, CIFAR, ImageNet},
	Month = {Feb},
	Title = {Training and Inference with Integers in Deep Neural Networks},
	Url = {https://openreview.net/forum?id=HJGXzmspb},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCNLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0xpbWl0ZWQgZXZhbHVhdGlvbiBjb29wZXJhdGl2ZSBjby1ldm9sdXRpb25hcnkgZGlmZmVyZW50aWFsIGV2b2x1dGlvbiBmb3IgbGFyZ2Utc2NhbGUgbmV1cm9ldm9sdXRpb24uYmliTxEC8gAAAAAC8gACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0xpbWl0ZWQgZXZhbHVhdGlvbiNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAxFdm9sdXRpb25hcnkAAgCjLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkV2b2x1dGlvbmFyeTpMaW1pdGVkIGV2YWx1YXRpb24gY29vcGVyYXRpdmUgY28tZXZvbHV0aW9uYXJ5IGRpZmZlcmVudGlhbCBldm9sdXRpb24gZm9yIGxhcmdlLXNjYWxlIG5ldXJvZXZvbHV0aW9uLmJpYgAADgDSAGgATABpAG0AaQB0AGUAZAAgAGUAdgBhAGwAdQBhAHQAaQBvAG4AIABjAG8AbwBwAGUAcgBhAHQAaQB2AGUAIABjAG8ALQBlAHYAbwBsAHUAdABpAG8AbgBhAHIAeQAgAGQAaQBmAGYAZQByAGUAbgB0AGkAYQBsACAAZQB2AG8AbAB1AHQAaQBvAG4AIABmAG8AcgAgAGwAYQByAGcAZQAtAHMAYwBhAGwAZQAgAG4AZQB1AHIAbwBlAHYAbwBsAHUAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAoVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0xpbWl0ZWQgZXZhbHVhdGlvbiBjb29wZXJhdGl2ZSBjby1ldm9sdXRpb25hcnkgZGlmZmVyZW50aWFsIGV2b2x1dGlvbiBmb3IgbGFyZ2Utc2NhbGUgbmV1cm9ldm9sdXRpb24uYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAC0AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA6o=},
	Bdsk-Url-1 = {https://openreview.net/forum?id=HJGXzmspb}}

@inbook{Moussa2006,
	Abstract = {Artificial Neural Networks (ANNs) are inherently parallel architectures which represent a natural fit for custom implementation on FPGAs. One important implementation issue is to determine the numerical precision format that allows an optimum tradeoff between precision and implementation areas. Standard single or double precision floating-point representations minimize quantization errors while requiring significant hardware resources. Less precise fixed-point representation may require less hardware resources but add quantization errors that may prevent learning from taking place, especially in regression problems. This chapter examines this issue and reports on a recent experiment where we implemented a Multi-layer perceptron (MLP) on an FPGA using both fixed and floating point precision. Results show that the fixed-point MLP implementation was over 12x greater in speed, over 13x smaller in area, and achieves far greater processing density compared to the floating-point FPGA-based MLP.},
	Address = {Boston, MA},
	Author = {Moussa, Medhat and Areibi, Shawki and Nichols, Kristian},
	Booktitle = {FPGA Implementations of Neural Networks},
	Date-Added = {2018-12-06 14:20:11 +1300},
	Date-Modified = {2018-12-06 14:20:23 +1300},
	Doi = {10.1007/0-387-28487-7_2},
	Editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Isbn = {978-0-387-28487-3},
	Keywords = {FPGA-NN, Book Chapter},
	Pages = {37--61},
	Publisher = {Springer US},
	Title = {On the Arithmetic Precision for Implementing Back-Propagation Networks on FPGA: A Case Study},
	Url = {https://doi.org/10.1007/0-387-28487-7_2},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_2}}

@inbook{Paul2006,
	Abstract = {Back propagation is a well known technique used in the implementation of artificial neural networks. The algorithm can be described essentially as a sequence of matrix vector multiplications and outer product operations interspersed with the application of a point wise non linear function. The algorithm is compute intensive and lends itself to a high degree of parallelism. These features motivate a systolic design of hardware to implement the Back Propagation algorithm. We present in this chapter a new systolic architecture for the complete back propagation algorithm. For a neural network with N input neurons, P hidden layer neurons and M output neurons, the proposed architecture with P processors, has a running time of (2N + 2M + P + max(M,P)) for each training set vector. This is the first such implementation of the back propagation algorithm which completely parallelizes the entire computation of learning phase. The array has been implemented on an Annapolis FPGA based coprocessor and it achieves very favorable performance with range of 5 GOPS. The proposed new design targets Virtex boards.},
	Address = {Boston, MA},
	Author = {Paul, Kolin and Rajopadhye, Sanjay},
	Booktitle = {FPGA Implementations of Neural Networks},
	Date-Added = {2018-12-06 14:17:29 +1300},
	Date-Modified = {2018-12-06 14:17:42 +1300},
	Doi = {10.1007/0-387-28487-7_5},
	Editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Isbn = {978-0-387-28487-3},
	Keywords = {FPGA-NN, Book Chapter},
	Pages = {137--165},
	Publisher = {Springer US},
	Title = {Back-Propagation Algorithm Achieving 5 Gops on the Virtex-E},
	Url = {https://doi.org/10.1007/0-387-28487-7_5},
	Year = {2006},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBYLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYk8RAiIAAAAAAiIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IUEMgb24gdGhlIEludGVsIFgjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGQ3J5cHRvAAIAbi86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDcnlwdG86SFBDIG9uIHRoZSBJbnRlbCBYZW9uIFBoaS0gSG9tb21vcnBoaWMgV29yZCBTZWFyY2hpbmcuYmliAA4AdAA5AEgAUABDACAAbwBuACAAdABoAGUAIABJAG4AdABlAGwAIABYAGUAbwBuACAAUABoAGkALQAgAEgAbwBtAG8AbQBvAHIAcABoAGkAYwAgAFcAbwByAGQAIABTAGUAYQByAGMAaABpAG4AZwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAbFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAB/AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAqU=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBYLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYk8RAiIAAAAAAiIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IUEMgb24gdGhlIEludGVsIFgjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGQ3J5cHRvAAIAbi86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDcnlwdG86SFBDIG9uIHRoZSBJbnRlbCBYZW9uIFBoaS0gSG9tb21vcnBoaWMgV29yZCBTZWFyY2hpbmcuYmliAA4AdAA5AEgAUABDACAAbwBuACAAdABoAGUAIABJAG4AdABlAGwAIABYAGUAbwBuACAAUABoAGkALQAgAEgAbwBtAG8AbQBvAHIAcABoAGkAYwAgAFcAbwByAGQAIABTAGUAYQByAGMAaABpAG4AZwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAbFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0hQQyBvbiB0aGUgSW50ZWwgWGVvbiBQaGktIEhvbW9tb3JwaGljIFdvcmQgU2VhcmNoaW5nLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAB/AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAqU=},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_5}}

@inbook{Omondi2006,
	Abstract = {This introductory chapter reviews the basics of artificial-neural-network theory, discusses various aspects of the hardware implementation of neural networks (in both ASIC and FPGA technologies, with a focus on special features of artificial neural networks), and concludes with a brief note on performance-evaluation. Special points are the exploitation of the parallelism inherent in neural networks and the appropriate implementation of arithmetic functions, especially the sigmoid function. With respect to the sigmoid function, the chapter includes a significant contribution.},
	Address = {Boston, MA},
	Author = {Omondi, Amos R. and Rajapakse, Jagath C. and Bajger, Mariusz},
	Booktitle = {FPGA Implementations of Neural Networks},
	Date-Added = {2018-12-06 14:08:39 +1300},
	Date-Modified = {2018-12-06 14:17:53 +1300},
	Doi = {10.1007/0-387-28487-7_1},
	Editor = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Isbn = {978-0-387-28487-3},
	Keywords = {FPGA-NN, NN, Neurocomputers, Book Chapter},
	Pages = {1--36},
	Publisher = {Springer US},
	Title = {FPGA Neurocomputers},
	Url = {https://doi.org/10.1007/0-387-28487-7_1},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1007/0-387-28487-7_1}}

@inproceedings{758889,
	Abstract = {A comparison between a bit-level and a conventional VLSI implementation of a binary neural network is presented. This network is based on Correlation Matrix Memory (CMM) that stores relationships between pairs of binary vectors. The bit-level architecture consists of an n/spl times/m array of bit-level processors holding the storage and computation elements. The conventional CMM architecture consists of a RAM memory holding the CMM storage and an array of counters. Since we are interested in the VLSI implementation of such networks, hardware complexities and speeds of both bit-level and conventional architecture were compared by using VLSI tools. It is shown that a significant speedup is achieved by using the bit-level architecture since the speed of this last configuration is not limited by the memory addressing delay. Moreover, the bit-level architecture is very simple and reduces the bus/routing, making the architecture suitable for VLSI implementation. The main drawback of such an approach compared to the conventional one is the demand for a high number of adders for dealing with a large number of inputs.},
	Author = {A. Bermak and J. Austin},
	Booktitle = {Proceedings of the Seventh International Conference on Microelectronics for Neural, Fuzzy and Bio-Inspired Systems},
	Date-Added = {2018-12-06 10:54:22 +1300},
	Date-Modified = {2018-12-06 10:54:32 +1300},
	Doi = {10.1109/MN.1999.758889},
	Keywords = {BNN; VLSI;neural chips;neural net architecture;CMOS digital integrated circuits;VLSI implementation;binary neural network;bit-level architecture;conventional architecture;correlation matrix memory;bit-level processors;RAM memory;counter array;hardware complexities;speed comparison;memory addressing delay;adders;VLSI digital design;Very large scale integration;Neural networks;Coordinate measuring machines;Computer architecture;Random access memory;Read-write memory;Counting circuits;Hardware;Added delay;Routing},
	Month = {April},
	Pages = {374-379},
	Title = {VLSI implementation of a binary neural network-two case studies},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1109/MN.1999.758889}}

@electronic{xnor.ai-webpage,
	Author = {Ali Farhadi; Mohammad Rastegari},
	Date-Added = {2018-12-06 10:26:37 +1300},
	Date-Modified = {2018-12-06 11:06:21 +1300},
	Keywords = {BNN, XNOR, Yolo},
	Lastchecked = {6/Dec/2018},
	Month = {Dec},
	Title = {xnor.ai Company},
	Url = {https://www.xnor.ai/},
	Urldate = {2018},
	Year = {2016},
	Bdsk-Url-1 = {https://www.xnor.ai/}}

@inproceedings{8416941,
	Author = {H. Yonekawa and S. Sato and H. Nakahara},
	Booktitle = {2018 IEEE 48th International Symposium on Multiple-Valued Logic (ISMVL)},
	Date-Added = {2018-12-05 21:41:24 +1300},
	Date-Modified = {2018-12-05 22:14:52 +1300},
	Doi = {10.1109/ISMVL.2018.00038},
	Issn = {2378-2226},
	Keywords = {BNN; Neurons;Training;Two dimensional displays;Embedded systems;Character recognition;Computational modeling;Convolutional neural networks},
	Month = {May},
	Pages = {174-179},
	Title = {A Ternary Weight Binary Input Convolutional Neural Network: Realization on the Embedded Processor},
	Url = {doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038},
	Volume = {00},
	Year = {2018},
	Bdsk-Url-1 = {doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038},
	Bdsk-Url-2 = {https://doi.org/10.1109/ISMVL.2018.00038}}

@inproceedings{Faraone_2018_CVPR,
	Author = {Faraone, Julian and Fraser, Nicholas and Blott, Michaela and Leong, Philip H.W.},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-04 23:43:14 +1300},
	Date-Modified = {2018-12-05 16:06:33 +1300},
	Keywords = {BNN; Quantization, FPGA, LUT},
	Month = {June},
	Title = {SYQ: Learning Symmetric Quantization for Efficient Deep Neural Networks},
	Year = {2018}}

@inproceedings{Zhou_2018_CVPR,
	Author = {Zhou, Aojun and Yao, Anbang and Wang, Kuan and Chen, Yurong},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-04 23:14:06 +1300},
	Date-Modified = {2018-12-04 23:15:40 +1300},
	Keywords = {BNN; Quantization, Regularization},
	Month = {June},
	Title = {Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks},
	Year = {2018}}

@inproceedings{Wang_2018_CVPR,
	Author = {Wang, Peisong and Hu, Qinghao and Zhang, Yifan and Zhang, Chunjie and Liu, Yang and Cheng, Jian},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-03 22:57:17 +1300},
	Date-Modified = {2018-12-03 22:57:41 +1300},
	Keywords = {BNN; Quantization, Low-bit},
	Month = {June},
	Title = {Two-Step Quantization for Low-Bit Neural Networks},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB2Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRE5OL09uIHRoZSBMZWFybmluZyBDYXBhYmlsaXRpZXMgb2YgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3Jrcy0gQSBDcnlwdG9ncmFwaGljIFBlcnNwZWN0aXZlLmJpYk8RAp4AAAAAAp4AAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9PbiB0aGUgTGVhcm5pbmcgQ2EjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADRE5OAAACAIwvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RE5OOk9uIHRoZSBMZWFybmluZyBDYXBhYmlsaXRpZXMgb2YgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3Jrcy0gQSBDcnlwdG9ncmFwaGljIFBlcnNwZWN0aXZlLmJpYgAOALYAWgBPAG4AIAB0AGgAZQAgAEwAZQBhAHIAbgBpAG4AZwAgAEMAYQBwAGEAYgBpAGwAaQB0AGkAZQBzACAAbwBmACAAUgBlAGMAdQByAHIAZQBuAHQAIABOAGUAdQByAGEAbAAgAE4AZQB0AHcAbwByAGsAcwAtACAAQQAgAEMAcgB5AHAAdABvAGcAcgBhAHAAaABpAGMAIABQAGUAcgBzAHAAZQBjAHQAaQB2AGUALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAIpVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ROTi9PbiB0aGUgTGVhcm5pbmcgQ2FwYWJpbGl0aWVzIG9mIFJlY3VycmVudCBOZXVyYWwgTmV0d29ya3MtIEEgQ3J5cHRvZ3JhcGhpYyBQZXJzcGVjdGl2ZS5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAnQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAM/}}

@inproceedings{Xie_2017_ICCV,
	Author = {Xie, Lingxi and Yuille, Alan},
	Booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
	Date-Added = {2018-12-03 22:12:21 +1300},
	Date-Modified = {2018-12-03 22:12:58 +1300},
	Keywords = {CNN, Genetic Algorithm, VGGNet, ResNet, DenseNet},
	Month = {Oct},
	Title = {Genetic CNN},
	Year = {2017}}

@inproceedings{Sholomon_2013_ICCV_Workshops,
	Author = {Sholomon, Dror and David, Omid and Netanyahu, Nathan S.},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-03 22:02:22 +1300},
	Date-Modified = {2018-12-03 22:03:08 +1300},
	Keywords = {Evolutionary, Genetic Algorithm, Jigzaw, Puzzle},
	Month = {June},
	Title = {A Genetic Algorithm-Based Solver for Very Large Jigsaw Puzzles},
	Year = {2013}}

@article{2017arXiv171100205Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171100205Z},
	Archiveprefix = {arXiv},
	Author = {{Zhuang}, B. and {Shen}, C. and {Tan}, M. and {Liu}, L. and {Reid}, I.},
	Date-Added = {2018-12-03 21:17:35 +1300},
	Date-Modified = {2018-12-03 21:21:48 +1300},
	Eprint = {1711.00205},
	Journal = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Keywords = {BNN; Computer Vision; Pattern Recognition},
	Month = Jun,
	Primaryclass = {cs.CV},
	Title = {{Towards Effective Low-bitwidth Convolutional Neural Networks}},
	Year = 2017}

@article{6134678,
	Abstract = {This paper proposes a set of adaptive learning rules for binary feedforward neural networks (BFNNs) by means of the sensitivity measure that is established to investigate the effect of a BFNN's weight variation on its output. The rules are based on three basic adaptive learning principles: the benefit principle, the minimal disturbance principle, and the burden-sharing principle. In order to follow the benefit principle and the minimal disturbance principle, a neuron selection rule and a weight adaptation rule are developed. Besides, a learning control rule is developed to follow the burden-sharing principle. The advantage of the rules is that they can effectively guide the BFNN's learning to conduct constructive adaptations and avoid destructive ones. With these rules, a sensitivity-based adaptive learning (SBALR) algorithm for BFNNs is presented. Experimental results on a number of benchmark data demonstrate that the SBALR algorithm has better learning performance than the Madaline rule II and backpropagation algorithms.},
	Author = {S. Zhong and X. Zeng and S. Wu and L. Han},
	Date-Added = {2018-12-02 16:32:30 +1300},
	Date-Modified = {2018-12-02 16:32:45 +1300},
	Doi = {10.1109/TNNLS.2011.2177860},
	Issn = {2162-237X},
	Journal = {IEEE Transactions on Neural Networks and Learning Systems},
	Keywords = {BNN; feedforward neural nets;learning (artificial intelligence);sensitivity analysis;sensitivity-based adaptive learning rules;binary feedforward neural network;sensitivity measurement;BFNN weight variation;adaptive learning principles;benefit principle;minimal disturbance principle;burden-sharing principle;neuron selection rule;weight adaptation rule;learning control rule;BFNN learning;benchmark data;SBALR algorithm;Madaline rule II;backpropagation algorithm;Neurons;Sensitivity;Training;Learning systems;Weight measurement;Biological neural networks;Feedforward neural networks;Adaptive learning algorithm;binary feedforward neural networks;learning rule;sensitivity;Artificial Intelligence;Databases, Factual;Neural Networks (Computer)},
	Month = {March},
	Number = {3},
	Pages = {480-491},
	Title = {Sensitivity-Based Adaptive Learning Rules for Binary Feedforward Neural Networks},
	Volume = {23},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2011.2177860}}

@article{7878541,
	Abstract = {Convolutional neural networks (CNNs) have revolutionized the world of computer vision over the last few years, pushing image classification beyond human accuracy. The computational effort of today's CNNs requires power-hungry parallel processors or GP-GPUs. Recent developments in CNN accelerators for system-on-chip integration have reduced energy consumption significantly. Unfortunately, even these highly optimized devices are above the power envelope imposed by mobile and deeply embedded applications and face hard limitations caused by CNN weight I/O and storage. This prevents the adoption of CNNs in future ultralow power Internet of Things end-nodes for near-sensor analytics. Recent algorithmic and theoretical advancements enable competitive classification accuracy even when limiting CNNs to binary (+1/-1) weights during training. These new findings bring major optimization opportunities in the arithmetic core by removing the need for expensive multiplications, as well as reducing I/O bandwidth and storage. In this paper, we present an accelerator optimized for binary-weight CNNs that achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate equivalent (MGE) or 1.9 mm<sup>2</sup>and with a power dissipation of 895 μW in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms the state-of-the-art in terms of energy and area efficiency achieving 61.2 TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.},
	Author = {R. Andri and L. Cavigelli and D. Rossi and L. Benini},
	Date-Added = {2018-12-02 15:33:17 +1300},
	Date-Modified = {2018-12-02 15:33:26 +1300},
	Doi = {10.1109/TCAD.2017.2682138},
	Issn = {0278-0070},
	Journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	Keywords = {BNN; CMOS logic circuits;computer vision;convolution;coprocessors;embedded systems;image classification;integrated circuit design;low-power electronics;neural nets;optimisation;power aware computing;system-on-chip;I/O storage;I/O bandwidth;algorithmic advancements;binary weights;competitive classification accuracy;hard limitations;deeply embedded applications;mobile embedded applications;power envelope;energy consumption;system-on-chip integration;CNN accelerators;GP-GPUs;power-hungry parallel processors;computational effort;human accuracy;image classification;computer vision;convolutional neural networks;ultralow power binary-weight CNN acceleration;power dissipation;binary-weight CNNs;accelerator;optimization opportunities;ASIC;binary weights;convolutional neural networks (CNNs);hardware accelerator;Internet of Things (IoT)},
	Month = {Jan},
	Number = {1},
	Pages = {48-60},
	Title = {YodaNN: An Architecture for Ultralow Power Binary-Weight CNN Acceleration},
	Volume = {37},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TCAD.2017.2682138}}

@article{POSEWSKY2018151,
	Abstract = {Deep neural networks are an extremely successful and widely used technique for various pattern recognition and machine learning tasks. Due to power and resource constraints, these computationally intensive networks are difficult to implement in embedded systems. Yet, the number of applications that can benefit from the mentioned possibilities is rapidly rising. In this paper, we propose novel architectures for the inference of previously learned and arbitrary deep neural networks on FPGA-based SoCs that are able to overcome these limitations. Our key contributions include the reuse of previously transferred weight matrices across multiple input samples, which we refer to as batch processing, and the usage of compressed weight matrices, also known as pruning. An extensive evaluation of these optimizations is presented. Both techniques allow a significant mitigation of data transfers and speed-up the network inference by one order of magnitude. At the same time, we surpass the data throughput of fully-featured x86-based systems while only using a fraction of their energy consumption.},
	Author = {Thorbj{\"o}rn Posewsky and Daniel Ziener},
	Date-Added = {2018-12-02 13:07:55 +1300},
	Date-Modified = {2018-12-02 13:08:06 +1300},
	Doi = {https://doi.org/10.1016/j.micpro.2018.04.004},
	Issn = {0141-9331},
	Journal = {Microprocessors and Microsystems},
	Keywords = {FPGA-NN; Deep neural networks, Batch processing, Pruning, Compression, FPGA, Inference, Throughput optimizations, Fully-connected},
	Pages = {151 - 161},
	Title = {Throughput optimizations for FPGA-based deep neural network inference},
	Url = {http://www.sciencedirect.com/science/article/pii/S014193311730296X},
	Volume = {60},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S014193311730296X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.micpro.2018.04.004}}

@article{HAJDUK2018227,
	Abstract = {This brief paper presents two implementations of feed-forward artificial neural networks in FPGAs. The implementations differ in the FPGA resources requirement and calculations speed. Both implementations exercise floating point arithmetic, apply very high accuracy activation function realization, and enable easy alteration of the neural network's structure without the need of a re-implementation of the entire FPGA project.},
	Author = {Zbigniew Hajduk},
	Date-Added = {2018-12-02 12:40:03 +1300},
	Date-Modified = {2018-12-02 12:40:15 +1300},
	Doi = {https://doi.org/10.1016/j.neucom.2018.04.077},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {FPGA-NN, FPGA, Neural networks},
	Pages = {227 - 234},
	Title = {Reconfigurable FPGA implementation of neural networks},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231218305393},
	Volume = {308},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBrLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0NIRVQtIENvbXBpbGVyIGFuZCBSdW50aW1lIGZvciBIb21vbW9ycGhpYyBFdmFsdWF0aW9uIG9mIFRlbnNvciBQcm9ncmFtcy5iaWJPEQJwAAAAAAJwAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fQ0hFVC0gQ29tcGlsZXIgYW5kI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABkNyeXB0bwACAIEvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Q3J5cHRvOkNIRVQtIENvbXBpbGVyIGFuZCBSdW50aW1lIGZvciBIb21vbW9ycGhpYyBFdmFsdWF0aW9uIG9mIFRlbnNvciBQcm9ncmFtcy5iaWIAAA4AmgBMAEMASABFAFQALQAgAEMAbwBtAHAAaQBsAGUAcgAgAGEAbgBkACAAUgB1AG4AdABpAG0AZQAgAGYAbwByACAASABvAG0AbwBtAG8AcgBwAGgAaQBjACAARQB2AGEAbAB1AGEAdABpAG8AbgAgAG8AZgAgAFQAZQBuAHMAbwByACAAUAByAG8AZwByAGEAbQBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgB/VXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DcnlwdG8vQ0hFVC0gQ29tcGlsZXIgYW5kIFJ1bnRpbWUgZm9yIEhvbW9tb3JwaGljIEV2YWx1YXRpb24gb2YgVGVuc29yIFByb2dyYW1zLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAkgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAMG},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231218305393},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2018.04.077}}

@book{Omondi:2010:FIN:1941654,
	Author = {Omondi, Amos R. and Rajapakse, Jagath C.},
	Date-Added = {2018-12-02 11:27:41 +1300},
	Date-Modified = {2018-12-02 11:28:42 +1300},
	Edition = {1st},
	Isbn = {1441939423, 9781441939425},
	Keywords = {FPGA-NN, ASIC, Book by Publications},
	Publisher = {Springer Publishing Company, Incorporated},
	Title = {FPGA Implementations of Neural Networks},
	Year = {2010}}

@inproceedings{Raina:2009:LDU:1553374.1553486,
	Acmid = {1553486},
	Address = {New York, NY, USA},
	Author = {Raina, Rajat and Madhavan, Anand and Ng, Andrew Y.},
	Booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	Date-Added = {2018-12-01 22:23:56 +1300},
	Date-Modified = {2018-12-01 22:24:24 +1300},
	Doi = {10.1145/1553374.1553486},
	Isbn = {978-1-60558-516-1},
	Keywords = {NN; GPU, Neural Network},
	Location = {Montreal, Quebec, Canada},
	Numpages = {8},
	Pages = {873--880},
	Publisher = {ACM},
	Series = {ICML '09},
	Title = {Large-scale Deep Unsupervised Learning Using Graphics Processors},
	Url = {http://doi.acm.org/10.1145/1553374.1553486},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1553374.1553486},
	Bdsk-Url-2 = {https://doi.org/10.1145/1553374.1553486}}

@proceedings{2016arXiv160207261S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160207261S},
	Archiveprefix = {arXiv},
	Author = {{Szegedy}, C. and {Ioffe}, S. and {Vanhoucke}, V. and {Alemi}, A.},
	Date-Added = {2018-12-01 20:45:24 +1300},
	Date-Modified = {2018-12-01 20:47:18 +1300},
	Eprint = {1602.07261},
	Journal = {Prceedings of the 31st AAAI Conference on Artificial Intelligence},
	Keywords = {CNN; Computer Vision; Pattern Recognition; Inception},
	Month = feb,
	Number = {31},
	Organization = {AAAI},
	Primaryclass = {cs.CV},
	Publisher = {AAAI},
	Title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
	Volume = {31},
	Year = 2017}

@article{2016arXiv160507678C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160507678C},
	Annote = {https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
https://towardsdatascience.com/neural-network-architectures-156e5bad51ba
},
	Archiveprefix = {arXiv},
	Author = {{Canziani}, A. and {Paszke}, A. and {Culurciello}, E.},
	Date-Added = {2018-12-01 20:05:07 +1300},
	Date-Modified = {2018-12-01 23:10:43 +1300},
	Eprint = {1605.07678},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{An Analysis of Deep Neural Network Models for Practical Applications}},
	Year = 2016}

@inproceedings{8099726,
	Abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
	Author = {G. Huang and Z. Liu and L. v. d. Maaten and K. Q. Weinberger},
	Booktitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-12-01 19:33:27 +1300},
	Date-Modified = {2018-12-01 19:33:50 +1300},
	Doi = {10.1109/CVPR.2017.243},
	Issn = {1063-6919},
	Keywords = {CNN; convolution;feedforward neural nets;learning (artificial intelligence);DenseNet;traditional convolutional networks;feature propagation;feature reuse;object recognition benchmark tasks;dense convolutional network;vanishing-gradient problem;Training;Convolution;Network architecture;Convolutional codes;Neural networks;Road transportation; DenseNet},
	Month = {July},
	Pages = {2261-2269},
	Title = {Densely Connected Convolutional Networks},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2017.243}}

@article{Simonyan14c,
	Author = {Simonyan, K. and Zisserman, A.},
	Date-Added = {2018-12-01 13:32:36 +1300},
	Date-Modified = {2018-12-01 13:33:51 +1300},
	Journal = {ILSVRC - CoRR},
	Keywords = {CNN; image classification, VGG network},
	Title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	Volume = {abs/1409.1556},
	Year = {2014}}

@electronic{AppleFaceDetectionURL,
	Author = {Computer Vision Machine Learning Team},
	Date-Added = {2018-11-29 21:01:15 +1300},
	Date-Modified = {2018-11-29 21:07:30 +1300},
	Keywords = {Neural Network, Vision Framework, Face recognition},
	Lastchecked = {29/Nov/2018},
	Month = {11},
	Title = {An On-device Deep Neural Network for Face Detection},
	Url = {https://machinelearning.apple.com/2017/11/16/face-detection.html},
	Urldate = {November 2017},
	Year = {2017},
	Bdsk-Url-1 = {https://machinelearning.apple.com/2017/11/16/face-detection.html}}

@inproceedings{5524599,
	Abstract = {Artificial neural networks play an important role in robot programming by demonstration. In this paper we present a method for artificial neural network training. The main idea of this method is to train the artificial neural network with all of the data, before the current training step, and at a certain step the network is already trained a huge number of times. Some features of the quality of neural network training, using this method, were presented in. Because the method uses all of the data before the current training step, in this paper, we are concerned about training time and computing time comportment of the neural network. A software application for obtaining training time based on the number of training steps was designed. This software application implements the training method on an unidirectional multi-layer neural network and prints into a graph the training time and computing time. The results obtained using the software application and important conclusions towards the training and computing time comportment are also presented.},
	Author = {M. Stoica and G. A. Calangiu and F. Sisak},
	Booktitle = {19th International Workshop on Robotics in Alpe-Adria-Danube Region (RAAD 2010)},
	Date-Added = {2018-11-29 19:30:41 +1300},
	Date-Modified = {2018-11-29 19:30:48 +1300},
	Doi = {10.1109/RAAD.2010.5524599},
	Keywords = {NN; graph theory;learning (artificial intelligence);neural nets;robot programming;neural network training;training steps;artificial neural networks;robot programming;unidirectional multi-layer neural network;graph;Time measurement;Neural networks;Robot kinematics;Service robots;Sliding mode control;Control systems;Biological system modeling;Humans;Biological neural networks;Artificial neural networks},
	Month = {June},
	Pages = {109-113},
	Title = {Measuring the time needed for training a neural network based on the number of training steps},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/RAAD.2010.5524599}}

@inproceedings{ijcai2017-316,
	Author = {Tao Lin and Tian Guo and Karl Aberer},
	Booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {IJCAI-17}},
	Date-Added = {2018-11-29 19:19:54 +1300},
	Date-Modified = {2018-11-29 19:20:41 +1300},
	Doi = {10.24963/ijcai.2017/316},
	Keywords = {NN; Neural Network, LSTM, Convolutional Neural Network, Hybrid},
	Pages = {2273--2279},
	Title = {Hybrid Neural Networks for Learning the Trend in Time Series},
	Url = {https://doi.org/10.24963/ijcai.2017/316},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.24963/ijcai.2017/316}}

@article{SCHMIDHUBER201585,
	Abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	Author = {J{\"u}rgen Schmidhuber},
	Date-Added = {2018-11-28 23:30:32 +1300},
	Date-Modified = {2018-11-28 23:31:02 +1300},
	Doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {DNN; Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
	Pages = {85 - 117},
	Title = {Deep learning in neural networks: An overview},
	Url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	Volume = {61},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2014.09.003}}

@inproceedings{5981829,
	Abstract = {In this paper we present a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms - neuFlow - and a dataflow compiler - luaFlow - that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications. We present an application of the system on street scene analysis, segmenting 20 categories on 500 × 375 frames at 12 frames per second on our custom hardware neuFlow.},
	Author = {C. Farabet and B. Martini and B. Corda and P. Akselrod and E. Culurciello and Y. LeCun},
	Booktitle = {CVPR 2011 WORKSHOPS},
	Date-Added = {2018-11-28 17:19:47 +1300},
	Date-Modified = {2018-11-28 17:19:58 +1300},
	Doi = {10.1109/CVPRW.2011.5981829},
	Issn = {2160-7508},
	Keywords = {CNN-FPGA; computer vision;field programmable gate arrays;flow graphs;NeuFlow;runtime reconfigurable dataflow processor;scalable dataflow hardware architecture;dataflow compiler;luaFlow;flow graph representations;machine code;Xilinx Virtex 6 FPGA platform;laptop computer;Tiles;Computer architecture;Runtime;Field programmable gate arrays;Hardware;Convolvers;Feature extraction},
	Month = {June},
	Pages = {109-116},
	Title = {NeuFlow: A runtime reconfigurable dataflow processor for vision},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPRW.2011.5981829}}

@book{Hertz:1991:ITN:104000,
	Address = {Boston, MA, USA},
	Author = {Hertz, John and Krogh, Anders and Palmer, Richard G.},
	Date-Added = {2018-11-26 21:51:03 +1300},
	Date-Modified = {2019-06-05 21:20:16 +1200},
	Isbn = {0-201-50395-6},
	Keywords = {NN; Books, Neural Computation},
	Publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	Source = {ISBN 0-201-51560-1},
	Title = {Introduction to the Theory of Neural Computation},
	Year = {1991}}

@article{Silver:2017aa,
	Author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	Date = {2017/10/18/online},
	Date-Added = {2018-11-26 19:46:00 +1300},
	Date-Modified = {2018-11-26 19:46:20 +1300},
	Day = {18},
	Journal = {Nature},
	Keywords = {NN, Go, Google, AlphaGo},
	L3 = {10.1038/nature24270; https://www.nature.com/articles/nature24270#supplementary-information},
	M3 = {Article},
	Month = {10},
	Pages = {354 EP -},
	Publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved. SN -},
	Title = {Mastering the game of Go without human knowledge},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature24270},
	Volume = {550},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature24270}}

@article{doi:10.1177/1555343417695197,
	Abstract = { Autonomous and semiautonomous vehicles are currently being developed by over14 companies. These vehicles may improve driving safety and convenience, or they may create new challenges for drivers, particularly with regard to situation awareness (SA) and autonomy interaction. I conducted a naturalistic driving study on the autonomy features in the Tesla Model S, recording my experiences over a 6-month period, including assessments of SA and problems with the autonomy. This preliminary analysis provides insights into the challenges that drivers may face in dealing with new autonomous automobiles in realistic driving conditions, and it extends previous research on human-autonomy interaction to the driving domain. Issues were found with driver training, mental model development, mode confusion, unexpected mode interactions, SA, and susceptibility to distraction. New insights into challenges with semiautonomous driving systems include increased variability in SA, the replacement of continuous control with serial discrete control, and the need for more complex decisions. Issues that deserve consideration in future research and a set of guidelines for driver interfaces of autonomous systems are presented and used to create recommendations for improving driver SA when interacting with autonomous vehicles. },
	Author = {Mica R. Endsley},
	Date-Added = {2018-11-26 19:43:09 +1300},
	Date-Modified = {2018-11-26 19:43:24 +1300},
	Doi = {10.1177/1555343417695197},
	Eprint = {https://doi.org/10.1177/1555343417695197},
	Journal = {Journal of Cognitive Engineering and Decision Making},
	Keywords = {NN, autopilot, tesla},
	Number = {3},
	Pages = {225-238},
	Title = {Autonomous Driving Systems: A Preliminary Naturalistic Study of the Tesla Model S},
	Url = {https://doi.org/10.1177/1555343417695197},
	Volume = {11},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1177/1555343417695197}}

@inproceedings{nirkin2018_faceswap,
	Author = {Nirkin, Yuval and Masi, Iacopo and Tran, Anh Tuan and Hassner, Tal and Medioni, G\'{e}rard},
	Booktitle = {IEEE Conference on Automatic Face and Gesture Recognition},
	Date-Added = {2018-11-26 19:30:06 +1300},
	Date-Modified = {2018-11-26 19:35:44 +1300},
	Keywords = {CNN, Faceswap, DeepFakes, CNN},
	Title = {On Face Segmentation, Face Swapping, and Face Perception},
	Year = {2018}}

@article{McCulloch1943,
	Abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	Author = {McCulloch, Warren S. and Pitts, Walter},
	Date-Added = {2018-11-25 21:59:10 +1300},
	Date-Modified = {2018-11-25 21:59:28 +1300},
	Day = {01},
	Doi = {10.1007/BF02478259},
	Issn = {1522-9602},
	Journal = {The bulletin of mathematical biophysics},
	Keywords = {NN, Neural network, mathematical definition},
	Month = {Dec},
	Number = {4},
	Pages = {115--133},
	Title = {A logical calculus of the ideas immanent in nervous activity},
	Url = {https://doi.org/10.1007/BF02478259},
	Volume = {5},
	Year = {1943},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02478259}}

@inproceedings{Qiu:2016:GDE:2847263.2847265,
	Acmid = {2847265},
	Address = {New York, NY, USA},
	Author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen and Wang, Yu and Yang, Huazhong},
	Booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 19:49:32 +1300},
	Date-Modified = {2018-11-25 19:49:42 +1300},
	Doi = {10.1145/2847263.2847265},
	Isbn = {978-1-4503-3856-1},
	Keywords = {FPGA-NN; bandwidth utilization, convolutional neural network (cnn), dynamic-precision data quantization, embedded fpga},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {26--35},
	Publisher = {ACM},
	Series = {FPGA '16},
	Title = {Going Deeper with Embedded FPGA Platform for Convolutional Neural Network},
	Url = {http://doi.acm.org/10.1145/2847263.2847265},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2847263.2847265},
	Bdsk-Url-2 = {https://doi.org/10.1145/2847263.2847265}}

@inproceedings{Zhang:2017:IPO:3020078.3021698,
	Acmid = {3021698},
	Address = {New York, NY, USA},
	Author = {Zhang, Jialiang and Li, Jing},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 19:34:22 +1300},
	Date-Modified = {2018-11-25 19:34:32 +1300},
	Doi = {10.1145/3020078.3021698},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGA-NN; convolutional neural networks, fpga, hardware accelerator, opencl},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {25--34},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Improving the Performance of OpenCL-based FPGA Accelerator for Convolutional Neural Network},
	Url = {http://doi.acm.org/10.1145/3020078.3021698},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021698},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021698}}

@inproceedings{Langhammer:2015:FDB:2684746.2689071,
	Acmid = {2689071},
	Address = {New York, NY, USA},
	Author = {Langhammer, Martin and Pasca, Bogdan},
	Booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 16:46:58 +1300},
	Date-Modified = {2018-11-25 16:47:11 +1300},
	Doi = {10.1145/2684746.2689071},
	Isbn = {978-1-4503-3315-3},
	Keywords = {FPGA, altera, arria10, dsp, floating-point, single-precision},
	Location = {Monterey, California, USA},
	Numpages = {9},
	Pages = {117--125},
	Publisher = {ACM},
	Series = {FPGA '15},
	Title = {Floating-Point DSP Block Architecture for FPGAs},
	Url = {http://doi.acm.org/10.1145/2684746.2689071},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689071},
	Bdsk-Url-2 = {https://doi.org/10.1145/2684746.2689071}}

@inproceedings{Aydonat:2017:ODL:3020078.3021738,
	Acmid = {3021738},
	Address = {New York, NY, USA},
	Author = {Aydonat, Utku and O'Connell, Shane and Capalija, Davor and Ling, Andrew C. and Chiu, Gordon R.},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 15:35:07 +1300},
	Date-Modified = {2018-11-25 15:35:29 +1300},
	Doi = {10.1145/3020078.3021738},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGA-NN, convolutional neural networks, deep neural networks},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {55--64},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {An OpenCL Deep Learning Accelerator on Arria 10},
	Url = {http://doi.acm.org/10.1145/3020078.3021738},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021738},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021738}}

@inproceedings{10.1007/11550822_51,
	Abstract = {In this paper, we analyze characteristics of GA-based learning method of Binary Neural Networks (BNN). First, we consider coding methods to a chromosome in a GA and discuss the necessary chromosome length for a learning of BNN. Then, we compare some selection methods in a GA. We show that the learning results can be obtained in the less number of generations by properly setting selection methods and parameters in a GA. We also show that the quality of the learning results can be almost the same as that of the conventional method. These results can be verified by numerical experiments.},
	Address = {Berlin, Heidelberg},
	Author = {Hirane, Tatsuya and Toryu, Tetsuya and Nakano, Hidehiro and Miyauchi, Arata},
	Booktitle = {Artificial Neural Networks: Biological Inspirations -- ICANN 2005},
	Date-Added = {2018-11-25 00:34:18 +1300},
	Date-Modified = {2018-11-25 14:47:15 +1300},
	Editor = {Duch, W{\l}odzis{\l}aw and Kacprzyk, Janusz and Oja, Erkki and Zadro{\.{z}}ny, S{\l}awomir},
	Isbn = {978-3-540-28754-4},
	Keywords = {BNN, Genetic Algorithm,},
	Pages = {323--329},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Analysis for Characteristics of GA-Based Learning Method of Binary Neural Networks},
	Year = {2005}}

@inproceedings{Umuroglu:2017:FFF:3020078.3021744,
	Acmid = {3021744},
	Address = {New York, NY, USA},
	Author = {Umuroglu, Yaman and Fraser, Nicholas J. and Gambardella, Giulio and Blott, Michaela and Leong, Philip and Jahre, Magnus and Vissers, Kees},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-25 00:31:00 +1300},
	Date-Modified = {2019-04-28 15:11:49 +1200},
	Doi = {10.1145/3020078.3021744},
	Isbn = {978-1-4503-4354-1},
	Keywords = {BNN; FPGA, binarized neural network, binary neural network, hardware acceleration, neural networks, reconfigurable logic},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {65--74},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {FINN: A Framework for Fast, Scalable Binarized Neural Network Inference},
	Url = {http://doi.acm.org/10.1145/3020078.3021744},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021744},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021744}}

@article{Yamada1992,
	Abstract = {This paper presents an ultra-high-speed sorter based upon a simplified parallel sorting algorithm using a binary neural network which consists both of binary neurons and of AND-OR synaptic connections to solve sorting problems at two and only two clock cycles. Our simplified algorithm is based on the super parallel sorting algorithm proposed by Takefuji and Lee. Nevertheless, our algorithm does not need any adders, while Takefuji's algorithm needs n{\texttimes}(n−1) analog adders of which each has multiple input ports. For an example of the simplified parallel sorter, a hardware design and its implementation will be introduced in this paper, which performs a sorting operation at two clock cycles. Both results of a logic circuit simulation and of an algorithm simulation show the justice of our hardware implementation even if in the practical size of the problem.},
	Author = {Yamada, Manabu and Nakagawa, Tohru and Kitagawa, Hajime},
	Date-Added = {2018-11-25 00:23:46 +1300},
	Date-Modified = {2018-11-25 00:24:03 +1300},
	Day = {01},
	Doi = {10.1007/BF00228719},
	Issn = {1573-1979},
	Journal = {Analog Integrated Circuits and Signal Processing},
	Keywords = {BNN; Parallel processing; Sorting Algorithm},
	Month = {Nov},
	Number = {4},
	Pages = {389--393},
	Title = {A super parallel sorter using a binary neural network with AND-OR synaptic connections},
	Url = {https://doi.org/10.1007/BF00228719},
	Volume = {2},
	Year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00228719}}

@inproceedings{10.1007/978-3-642-42042-9_86,
	Abstract = {This paper studies application of the dynamic binary neural network to control signal of switching circuits. The network is characterized by the signum activation function and ternary weighting parameters. In the application, the teacher signal is one binary periodic orbit corresponding to the controls. The learning algorithm is based on the genetic algorithm. As an application object, we consider a control signal of the basic matrix converter. Performing a basic numerical experiment, we have confirmed that the teacher signal is stored successfully and is stabilized automatically.},
	Address = {Berlin, Heidelberg},
	Author = {Nakayama, Yuta and Kouzuki, Ryota and Saito, Toshimichi},
	Booktitle = {Neural Information Processing},
	Date-Added = {2018-11-25 00:10:15 +1300},
	Date-Modified = {2018-11-25 00:20:07 +1300},
	Editor = {Lee, Minho and Hirose, Akira and Hou, Zeng-Guang and Kil, Rhee Man},
	Isbn = {978-3-642-42042-9},
	Keywords = {BNN; Dynamic NN; Control signal, Genetic Algorithm},
	Pages = {697--704},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Application of the Dynamic Binary Neural Network to Switching Circuits},
	Year = {2013}}

@inproceedings{10.1007/978-3-642-59041-2_23,
	Abstract = {In the very near future large amounts of Remotely Sensed data will become available on a daily basis. Unfortunately, it is not clear if the processing methods are available to deal with this data in a timely fashion. This paper describes research towards an approach which will allow a user to perform a rapid pre-search of large amounts of image data for regions of interest based on texture. The method is based on a novel neural network architecture (ADAM) that is designed primarily for speed of operation by making use of computationally simple pre-processing and only uses Boolean operations in the weights of the network. To facilitate interactive use of the network, it is capable of rapid training. The paper outlines the neural network, its application to RS data in comparison with other methods, and briefly describes a fast hardware implementation of the network.},
	Address = {Berlin, Heidelberg},
	Author = {Austin, Jim},
	Booktitle = {Neurocomputation in Remote Sensing Data Analysis},
	Date-Added = {2018-11-24 23:37:04 +1300},
	Date-Modified = {2018-11-24 23:37:33 +1300},
	Editor = {Kanellopoulos, Ioannis and Wilkinson, Graeme G. and Roli, Fabio and Austin, James},
	Isbn = {978-3-642-59041-2},
	Keywords = {BNN, Image Segmentation, Data Analysis},
	Pages = {202--213},
	Publisher = {Springer Berlin Heidelberg},
	Title = {High Speed Image Segmentation Using a Binary Neural Network},
	Year = {1997}}

@inproceedings{10.1007/978-81-322-2208-8_43,
	Abstract = {In this paper, a quantum based binary neural network learning algorithm is proposed for solving two class problems. The proposed method constructively forms the neural network architecture and weights are decided by quantum computing concept. The use of quantum computing optimizes the network structure and the performance in terms of number of neurons at hidden layer and classification accuracy. This approach is compared with MTiling-real networks algorithm and it is found that there is a significant improvement in terms of number of neurons at the hidden layer, number of iterations, training accuracy and generalization accuracy.},
	Address = {New Delhi},
	Author = {Patel, Om Prakash and Tiwari, Aruna},
	Booktitle = {Computational Intelligence in Data Mining - Volume 2},
	Date-Added = {2018-11-24 23:17:08 +1300},
	Date-Modified = {2018-11-24 23:17:53 +1300},
	Editor = {Jain, Lakhmi C. and Behera, Himansu Sekhar and Mandal, Jyotsna Kumar and Mohapatra, Durga Prasad},
	Isbn = {978-81-322-2208-8},
	Keywords = {BNN; Quantum Algorithm; Neural Network},
	Pages = {473--482},
	Publisher = {Springer India},
	Title = {Quantum Based Learning with Binary Neural Network},
	Year = {2015}}

@incollection{NIPS2016_6573,
	Author = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
	Booktitle = {Advances in Neural Information Processing Systems 29},
	Date-Added = {2018-11-24 21:41:12 +1300},
	Date-Modified = {2018-11-24 21:42:24 +1300},
	Editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	Keywords = {BNN; Neural Network},
	Pages = {4107--4115},
	Publisher = {Curran Associates, Inc.},
	Title = {Binarized Neural Networks},
	Url = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf},
	Year = {2016},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf}}

@incollection{NIPS2015_5647,
	Annote = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf},
	Author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
	Booktitle = {Advances in Neural Information Processing Systems 28},
	Date-Added = {2018-11-24 21:39:32 +1300},
	Date-Modified = {2019-07-23 17:14:58 +1200},
	Editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	Keywords = {BNN; Machine Learning; Computer Vision; Pattern Recognition; Neural and Evolutionary Computing},
	Pages = {3123--3131},
	Publisher = {Curran Associates, Inc.},
	Title = {BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
	Year = {2015},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf}}

@inproceedings{Nurvitadhi:2017:FBG:3020078.3021740,
	Acmid = {3021740},
	Address = {New York, NY, USA},
	Author = {Nurvitadhi, Eriko and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan and Subhaschandra, Suchit and Boudoukh, Guy},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-11-24 20:45:12 +1300},
	Date-Modified = {2018-11-24 20:45:24 +1300},
	Doi = {10.1145/3020078.3021740},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGA-NN; FPGA, GPU, accelerator, deep learning, intel stratix 10},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {5--14},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Can FPGAs Beat GPUs in Accelerating Next-Generation Deep Neural Networks?},
	Url = {http://doi.acm.org/10.1145/3020078.3021740},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021740},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021740}}

@inproceedings{Cheng_2014_CVPR,
	Author = {Cheng, Ming-Ming and Zhang, Ziming and Lin, Wen-Yan and Torr, Philip},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-11-23 10:09:08 +1300},
	Date-Modified = {2018-11-23 10:09:18 +1300},
	Keywords = {CNN; BING},
	Month = {June},
	Title = {BING: Binarized Normed Gradients for Objectness Estimation at 300fps},
	Year = {2014}}

@article{bhandari2012design,
	Author = {Bhandari, Sachin and Tiwari, Aruna},
	Date-Added = {2018-11-22 18:18:02 +1300},
	Date-Modified = {2018-11-22 18:18:27 +1300},
	Keywords = {BNN; Clustering, ETL, Fuzzy logic},
	Title = {Design and implementation of binary neural network learning with fuzzy clustering},
	Year = {2012}}

@misc{Muselli99hammingclustering:,
	Author = {Marco Muselli and Diego Liberati},
	Date-Added = {2018-11-22 17:37:41 +1300},
	Date-Modified = {2018-11-22 17:37:52 +1300},
	Keywords = {BNN},
	Title = {Hamming Clustering: A New Approach to Rule Extraction},
	Year = {1999}}

@inproceedings{817981,
	Abstract = {A new constructive learning algorithm, called Hamming clustering (HC), for binary neural networks is proposed. It is able to generate a set of rules in if-then form underlying an unknown classification problem starting from a training set of samples. The performance of HC has been evaluated through a variety of artificial and real-world benchmarks. In particular, its application in the diagnosis of breast cancer has led to the derivation of a reduced set of rules solving the associated classification problem.},
	Author = {M. Muselli and D. Liberati},
	Booktitle = {1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)},
	Date-Added = {2018-11-22 16:46:03 +1300},
	Date-Modified = {2018-11-22 16:46:11 +1300},
	Doi = {10.1049/cp:19991161},
	Issn = {0537-9989},
	Keywords = {BNN; neural nets;learning algorithm;Hamming clustering;binary neural networks;breast cancer;pattern classification;patient diagnosis;Boolean function},
	Month = {Sept},
	Pages = {515-520 vol.2},
	Title = {Rule extraction from binary neural networks},
	Volume = {2},
	Year = {1999},
	Bdsk-Url-1 = {https://doi.org/10.1049/cp:19991161}}

@inproceedings{5178979,
	Abstract = {This paper presents a learning algorithm of digital binary neural networks for approximation of desired Boolean functions. In the learning, the genetic algorithms is used with flexible fitness that tolerates error: it is suitable to reduce the number of hidden neurons and to tolerate noise and outliers. We then apply the algorithm to design of cellular automata with rich spatio-temporal patterns and various applications. Performing basic numerical experiment, the algorithm efficiency is confirmed.},
	Author = {S. Kabeya and T. Abe and T. Saito},
	Booktitle = {2009 International Joint Conference on Neural Networks},
	Date-Added = {2018-11-22 11:37:15 +1300},
	Date-Modified = {2018-11-22 11:37:25 +1300},
	Doi = {10.1109/IJCNN.2009.5178979},
	Issn = {2161-4393},
	Keywords = {BNN; Boolean functions;cellular automata;function approximation;genetic algorithms;learning (artificial intelligence);neural nets;flexible learning algorithm;error tolerance;digital binary neural networks;Boolean function approximation;genetic algorithms;cellular automata;spatio-temporal patterns;Neural networks;Signal processing algorithms;Neurons;Boolean functions;Approximation algorithms;Genetic algorithms;Noise reduction;Nonlinear dynamical systems;USA Councils;Algorithm design and analysis},
	Month = {June},
	Pages = {1476-1480},
	Title = {A GA-based flexible learning algorithm with error tolerance for digital binary neural networks},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2009.5178979}}

@inproceedings{714090,
	Abstract = {This paper considers the use of binary neural networks for pattern classification. An expand-and-truncate learning (ETL) algorithm is used to determine the required number of neurons as well as the connecting weights in a three-layered feedforward network for classifying input patterns. The ETL algorithm is guaranteed to find a network for any binary-to-binary mappings. The ETL algorithm's performance in pattern classification is tested using a breast cancer database that have been used for benchmarking performance other machine learning methods.},
	Author = {C. H. Chu and J. H. Kim},
	Booktitle = {Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)},
	Date-Added = {2018-11-21 23:44:42 +1300},
	Date-Modified = {2019-06-05 21:20:32 +1200},
	Doi = {10.1109/IJCNN.1993.714090},
	Keywords = {BNN; feedforward neural nets;pattern classification;learning (artificial intelligence);medical diagnostic computing;geometrical learning;binary neural networks;pattern classification;expand-and-truncate learning algorithm;connecting weights;three-layered feedforward network;binary-to-binary mappings;breast cancer;Pattern classification;Neural networks;Neurons;Machine learning algorithms;Testing;Breast cancer;Databases;Hamming distance;Computer networks;Joining processes},
	Month = {Oct},
	Pages = {1039-1042 vol.1},
	Title = {Pattern classification by geometrical learning of binary neural networks},
	Volume = {1},
	Year = {1993},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1993.714090}}

@inproceedings{203280,
	Abstract = {A novel design technique for asynchronous binary neural networks is proposed. This design uses linear programming to design two architectures: (i) a fully connected network that reads a N-digit cue and classifies it into a category represented by a N-digit pattern: and (ii) a two-layer network (with lateral connections) that has M neurons in the first layer and L neurons in the second layer; the network reads an M-digit cue to the first layer and associates it with a second-layer L-digit pattern. In both cases, the objective function is a weighted sum of the number of errors that can be corrected by the network. A cue with this number of errors (or fewer) is guaranteed to converge to the correct pattern. An economical VLSI realization of the designed networks can be easily accomplished.&lt;&lt;ETX&gt;&gt;},
	Author = {M. Kam and J. C. Chow and R. Fischl},
	Booktitle = {29th IEEE Conference on Decision and Control},
	Date-Added = {2018-11-21 23:33:12 +1300},
	Date-Modified = {2018-11-21 23:33:20 +1300},
	Doi = {10.1109/CDC.1990.203280},
	Keywords = {BNN, linear programming;neural nets;asynchronous binary neural networks;linear programming;design technique;fully connected network;two-layer network;lateral connections;Neural networks;Linear programming;Neurons;Algorithm design and analysis;Error correction;Convergence;Neurofeedback;Hamming distance;Very large scale integration;Stochastic processes},
	Month = {Dec},
	Pages = {2766-2767 vol.5},
	Title = {Design of two architectures of asynchronous binary neural networks using linear programming},
	Year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1109/CDC.1990.203280}}

@inproceedings{1259700,
	Abstract = {Learning problem for neural networks has widely been investigated in last two decades. Kim and Park [J.H. Kim et al., Jan. 1995] proposed one approach based on geometric technique, called "expand and truncate learning (ETL)". ETL is proposed to construct a three-layer binary neural network (BNN) for training a Boolean function of n (Boolean) variables. It is claimed by Kim and Park in [J.H. Kim et al., Jan. 1995] that, neural networks constructed according to this technique are much smaller. This paper investigates usefulness of these ideas for data classification. Data classification in real world involves multiple classes. For solving this problem, there are many techniques based on statistical principles, clustering approaches, etc. Application of binary neural networks for multiple outputs are important in practice. We propose a method for construction of a binary neural net based on generalization of ETL to more than two classes. Our method simplifies the resulting neural network architecture.},
	Author = {Yi Xu and N. S. Chaudhari},
	Booktitle = {Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693)},
	Date-Added = {2018-11-21 23:18:59 +1300},
	Date-Modified = {2018-11-21 23:19:06 +1300},
	Doi = {10.1109/ICMLC.2003.1259700},
	Keywords = {BNN; neural nets;pattern classification;learning (artificial intelligence);Boolean functions;binary neural networks;geometric technique;expand and truncate learning;Boolean function;data classification;core vertex;neural network training;Neural networks;Neurons;Artificial neural networks;Power line communications;Machine learning algorithms;Application software;Boolean functions;Pattern classification;Function approximation;Pattern matching},
	Month = {Nov},
	Pages = {1343-1348 Vol.3},
	Title = {Application of binary neural networks for classification},
	Volume = {3},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICMLC.2003.1259700}}

@inproceedings{480879,
	Abstract = {Hidden periodicities play a very important role in the regulatory and structural functioning of genomic DNA strands. Primarily, it concerns the fundamental three-periodicity inherent to protein coding regions in all taxonomic groups, two-periodicity in introns of eukaryots as well as periodicities related to helix and chromatin pitches, while the other periodicities appear to be species specific. Rather roughly (and without sharp boundary) the underlying periodicities may be divided by two groups. In the first case the periodicities are due to particular nucleotides (or very short oligomers) quasi-regularly positioned in a seemingly random background. This type of regularity can be identified via either standard frequency analysis or more elaborate Fourier methods. For the second group a periodicity is related to the quasi-random replacements in initially complete repeating motifs (situation typical, e.g., for modifications of satellites). In the last case the statistical reconstruction of underlying repeats is a much less trivial task. The authors show that this problem can successfully be solved with multi-symbol extension of energy-minimizing neural networks (EMNN). The reconstruction of underlying motifs may shed additional light on the evolutionary and functional modifications in various genomes.},
	Author = {V. R. Chechetkin and A. A. Ezhov and L. A. Knizhnikova and A. Y. Turygin},
	Booktitle = {The Second International Symposium on Neuroinformatics and Neurocomputers},
	Date-Added = {2018-11-21 23:03:36 +1300},
	Date-Modified = {2018-11-21 23:03:46 +1300},
	Doi = {10.1109/ISNINC.1995.480879},
	Keywords = {NN; DNA;neural nets;sequences;genomic DNA sequences;neural networks;fundamental three-periodicity;protein coding regions;taxonomic groups;two-periodicity;introns;eukaryots;chromatin pitches;helix pitches;nucleotides;quasi-random replacements;statistical reconstruction;energy-minimizing neural networks;Intelligent networks;Genomics;Bioinformatics;DNA;Sequences;Neural networks;Neurons;Proteins;Technological innovation;Frequency},
	Month = {Sept},
	Pages = {346-352},
	Title = {Study of underlying repeats in genomic DNA sequences with neural networks},
	Year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNINC.1995.480879}}

@inproceedings{5380141,
	Abstract = {In this paper we propose a new method to analyze the similarity/dissimilarity of DNA sequences which can be used in human identification field. This method is based on the graphical representation proposed by Randic et al [M. Randic, M. Vracko, L. Nella, P. Dejan, Chem. (2003)]. Instead of calculating the leading eigenvalues of the matrix for graphical representation we smooth the zigzag curve and calculate its curvature. Similarity between DNA sequences are decided by neural network. Our method is useful for human identification in criminal investigations and in genetic disease. Our results verify the validity of our method.},
	Author = {R. Rafeh and M. Mesgar},
	Booktitle = {2009 Second International Conference on Computer and Electrical Engineering},
	Date-Added = {2018-11-21 22:54:01 +1300},
	Date-Modified = {2018-11-21 22:54:11 +1300},
	Doi = {10.1109/ICCEE.2009.132},
	Keywords = {NN, biology computing;DNA;eigenvalues and eigenfunctions;forensic science;matrix algebra;medical computing;neural nets;neural network;human identification;DNA sequences;graphical representation;criminal investigations;genetic disease;matrix eigenvalues;Neural networks;Humans;Sequences;Computer networks;DNA computing;Eigenvalues and eigenfunctions;Artificial neural networks;Genetics;Diseases;Forensics;University Timetabling;Optimization Problems;Constraint Programming;Linear Programming},
	Month = {Dec},
	Pages = {64-67},
	Title = {Neural Network in Human Identification by DNA Sequences},
	Volume = {2},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCEE.2009.132}}

@inproceedings{497852,
	Abstract = {The four nitrogenous bases of DNA spell out the recipes from which proteins are made. A gene typically contains five thousand or so bases but often only a small percentage of these are protein coding. Computer based prediction systems are increasingly relied upon as submissions to the major genetic databases are growing exponentially. Several systems exist to locate coding regions (exons) and noncoding regions (introns) within genomic DNA; the common models used are neural networks and Markov chains (M. Borodovsky and J. McIninch (1993), A. Krogh et al. (1994). One of the most successful programs is called GRAIL. Currently, two versions of GRAIL are available: GRAIL-I (E. Uberbacher and R. Mural (1991), and GRAIL-II (Y. Xu et al. (1994). In GRAIL-I, a neural network receives its inputs from seven statistical measures taken on a 99 base window. Performance is improved in GRAIL-II by the addition of variable length windows, neural nets trained to locate intron/exon boundaries, and a number of steps designed to evaluate candidate exons and eliminate improbable ones. Both versions of GRAIL predict coding regions in human DNA. A simulation of GRAIL-I was carried out with the goal of improving classification performance without resorting to the additional measures used in GRAIL-II. The intention was then to supplement the resulting module with modules based on physiochemical measures of DNA (such as melting profiles, twist and wedge angles) to enable precise exon prediction in plant sequences.},
	Author = {L. Roberts and N. Steele and C. Reeves and G. J. King},
	Booktitle = {1995 Fourth International Conference on Artificial Neural Networks},
	Date-Added = {2018-11-21 22:10:13 +1300},
	Date-Modified = {2018-11-21 22:10:22 +1300},
	Doi = {10.1049/cp:19950589},
	Issn = {0537-9989},
	Keywords = {NN; biology computing;DNA;genetics;neural nets;learning (artificial intelligence);proteins;neural network training;coding region identification;genomic DNA;nitrogenous bases;Markov chains;computer based prediction systems;exons;introns;GRAIL;GRAIL-I;GRAIL-II;statistical measures;variable length windows;intron/exon boundaries;human DNA;classification performance;physiochemical measures;precise exon prediction;plant sequences;Biomedical computing;DNA;Genetics;Neural networks;Learning systems;Proteins},
	Month = {June},
	Pages = {399-403},
	Title = {Training neural networks to identify coding regions in genomic DNA},
	Year = {1995},
	Bdsk-Url-1 = {https://doi.org/10.1049/cp:19950589}}

@article{Stanley:2002:ENN:638553.638554,
	Acmid = {638554},
	Address = {Cambridge, MA, USA},
	Author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	Date-Added = {2018-11-14 20:01:01 +1300},
	Date-Modified = {2018-11-14 20:01:09 +1300},
	Doi = {10.1162/106365602320169811},
	Issn = {1063-6560},
	Issue_Date = {Summer 2002},
	Journal = {Evol. Comput.},
	Keywords = {Evolutionary, competing conventions, genetic algorithms, network topologies, neural networks, neuroevolution, speciation},
	Month = jun,
	Number = {2},
	Numpages = {29},
	Pages = {99--127},
	Publisher = {MIT Press},
	Title = {Evolving Neural Networks Through Augmenting Topologies},
	Url = {http://dx.doi.org/10.1162/106365602320169811},
	Volume = {10},
	Year = {2002},
	Bdsk-Url-1 = {http://dx.doi.org/10.1162/106365602320169811}}

@article{2018arXiv180402464M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402464M},
	Archiveprefix = {arXiv},
	Author = {{Miconi}, T. and {Clune}, J. and {Stanley}, K.~O.},
	Date-Added = {2018-11-13 19:47:52 +1300},
	Date-Modified = {2018-11-13 19:48:02 +1300},
	Eprint = {1804.02464},
	Journal = {ArXiv e-prints},
	Keywords = {DNN, Neural and Evolutionary Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = apr,
	Title = {{Differentiable plasticity: training plastic neural networks with backpropagation}},
	Year = 2018}

@inproceedings{10.1007/3-540-61108-8_27,
	Abstract = {In this paper, evolution strategies (ESs) --- a class of evolutionary algorithms using normally distributed mutations, recombination, deterministic selection of the $\mu$>1 best offspring individuals, and the principle of self-adaptation for the collective on-line learning of strategy parameters --- are described by demonstrating their differences to genetic algorithms. By comparison of the algorithms, it is argued that the application of canonical genetic algorithms for continuous parameter optimization problems implies some difficulties caused by the encoding of continuous object variables by binary strings and the constant mutation rate used in genetic algorithms. Because they utilize a problem-adequate representation and a suitable self-adaptive step size control guaranteeing linear convergence for strictly convex problems, evolution strategies are argued to be more adequate for continuous problems.},
	Address = {Berlin, Heidelberg},
	Author = {B{\"a}ck, Thomas},
	Booktitle = {Artificial Evolution},
	Date-Added = {2018-11-13 16:18:55 +1300},
	Date-Modified = {2018-11-13 16:19:16 +1300},
	Editor = {Alliot, Jean-Marc and Lutton, Evelyne and Ronald, Edmund and Schoenauer, Marc and Snyers, Dominique},
	Isbn = {978-3-540-49948-0},
	Keywords = {Evolutionary, evolution strategies, Genetic Algorithm},
	Pages = {1--20},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Evolution strategies: An alternative evolutionary algorithm},
	Year = {1996}}

@inproceedings{4424711,
	Abstract = {Differential evolution has shown to be a very powerful, yet simple, population-based optimization approach. The nature of its reproduction operator limits its application to continuous-valued search spaces. However, a simple discretization procedure can be used to convert floating-point solution vectors into discrete-valued vectors. This paper considers three approaches in which differential evolution can be used to solve problems with binary-valued parameters. The first approach is based on a homomorphous mapping, while the second approach interprets the floating-point solution vector as a vector of probabilities, used to decide on the appropriate binary value. The third approach normalizes solution vectors and then discretize these normalized vectors to form a bitstring. Empirical results are provided to illustrate the efficiency of both methods in comparison with particle swarm optimizers.},
	Author = {A. P. Engelbrecht and G. Pampara},
	Booktitle = {2007 IEEE Congress on Evolutionary Computation},
	Date-Added = {2018-11-13 15:23:49 +1300},
	Date-Modified = {2018-11-13 15:24:01 +1300},
	Doi = {10.1109/CEC.2007.4424711},
	Issn = {1089-778X},
	Keywords = {Evolutionary; evolutionary computation;optimisation;binary differential evolution;population-based optimization;reproduction operator;continuous-valued search space;simple discretization procedure;floating-point solution vectors;discrete-valued vectors;binary-valued parameters;homomorphous mapping;normalized vectors;particle swarm optimizers;Optimization methods;Genetic mutations;Particle swarm optimization;Probability density function;Arithmetic;Biological cells;Difference equations;Differential equations;Stochastic processes;Discrete transforms},
	Month = {Sept},
	Pages = {1942-1947},
	Title = {Binary differential evolution strategies},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEC.2007.4424711}}

@article{Wierstra:2014:NES:2627435.2638566,
	Acmid = {2638566},
	Author = {Wierstra, Daan and Schaul, Tom and Glasmachers, Tobias and Sun, Yi and Peters, Jan and Schmidhuber, J\"{u}rgen},
	Date-Added = {2018-11-13 15:10:29 +1300},
	Date-Modified = {2018-11-13 15:10:50 +1300},
	Issn = {1532-4435},
	Issue_Date = {January 2014},
	Journal = {J. Mach. Learn. Res.},
	Keywords = {Evolutionary, black-box optimization, evolution strategies, natural gradient, sampling, stochastic search},
	Month = jan,
	Number = {1},
	Numpages = {32},
	Pages = {949--980},
	Publisher = {JMLR.org},
	Title = {Natural Evolution Strategies},
	Url = {http://dl.acm.org/citation.cfm?id=2627435.2638566},
	Volume = {15},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2627435.2638566}}

@article{Sehnke:2010aa,
	Abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradient by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the parameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
	Author = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
	Booktitle = {The 18th International Conference on Artificial Neural Networks, ICANN 2008},
	Da = {2010/05/01/},
	Date-Added = {2018-11-13 15:07:22 +1300},
	Date-Modified = {2018-11-13 15:07:34 +1300},
	Doi = {https://doi.org/10.1016/j.neunet.2009.12.004},
	Isbn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Evolutionary, Policy gradients; Stochastic optimisation; Reinforcement learning; Robotics; Control},
	Number = {4},
	Pages = {551--559},
	Title = {Parameter-exploring policy gradients},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S0893608009003220},
	Volume = {23},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608009003220},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2009.12.004}}

@article{Williams1992,
	Abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	Author = {Williams, Ronald J.},
	Date-Added = {2018-11-13 15:01:38 +1300},
	Date-Modified = {2018-11-13 15:02:10 +1300},
	Day = {01},
	Doi = {10.1007/BF00992696},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Keywords = {Evolutionary, Gradient Methods, Reinforcement Learning, REINFORCE},
	Month = {May},
	Number = {3},
	Pages = {229--256},
	Title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	Url = {https://doi.org/10.1007/BF00992696},
	Volume = {8},
	Year = {1992},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF00992696}}

@inproceedings{Morse:2016:SEO:2908812.2908916,
	Acmid = {2908916},
	Address = {New York, NY, USA},
	Author = {Morse, Gregory and Stanley, Kenneth O.},
	Booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
	Date-Added = {2018-11-12 23:49:41 +1300},
	Date-Modified = {2018-11-12 23:49:51 +1300},
	Doi = {10.1145/2908812.2908916},
	Isbn = {978-1-4503-4206-3},
	Keywords = {Evolutionary, artificial intelligence, deep learning, machine learning, neural networks, pattern recognition and classification},
	Location = {Denver, Colorado, USA},
	Numpages = {8},
	Pages = {477--484},
	Publisher = {ACM},
	Series = {GECCO '16},
	Title = {Simple Evolutionary Optimization Can Rival Stochastic Gradient Descent in Neural Networks},
	Url = {http://doi.acm.org/10.1145/2908812.2908916},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2908812.2908916},
	Bdsk-Url-2 = {https://doi.org/10.1145/2908812.2908916}}

@article{2018arXiv180703247L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703247L},
	Archiveprefix = {arXiv},
	Author = {{Liu}, R. and {Lehman}, J. and {Molino}, P. and {Petroski Such}, F. and {Frank}, E. and {Sergeev}, A. and {Yosinski}, J.},
	Date-Added = {2018-11-12 20:13:43 +1300},
	Date-Modified = {2018-11-12 20:14:06 +1300},
	Eprint = {1807.03247},
	Journal = {ArXiv e-prints},
	Keywords = {CNN, Computer Vision, Pattern Recognition, Machine Learning, Statistics, Machine Learning},
	Month = jul,
	Primaryclass = {cs.CV},
	Title = {{An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution}},
	Year = 2018}

@article{2017arXiv171206564Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171206564Z},
	Archiveprefix = {arXiv},
	Author = {{Zhang}, X. and {Clune}, J. and {Stanley}, K.~O.},
	Date-Added = {2018-11-12 19:47:24 +1300},
	Date-Modified = {2018-11-12 19:47:36 +1300},
	Eprint = {1712.06564},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary, Neural and Evolutionary Computing},
	Month = dec,
	Title = {{On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent}},
	Year = 2017}

@article{Assuncao2018,
	Abstract = {Deep evolutionary network structured representation (DENSER) is a novel evolutionary approach for the automatic generation of deep neural networks (DNNs) which combines the principles of genetic algorithms (GAs) with those of dynamic structured grammatical evolution (DSGE). The GA-level encodes the macro structure of evolution, i.e., the layers, learning, and/or data augmentation methods (among others); the DSGE-level specifies the parameters of each GA evolutionary unit and the valid range of the parameters. The use of a grammar makes DENSER a general purpose framework for generating DNNs: one just needs to adapt the grammar to be able to deal with different network and layer types, problems, or even to change the range of the parameters. DENSER is tested on the automatic generation of convolutional neural networks (CNNs) for the CIFAR-10 dataset, with the best performing networks reaching accuracies of up to 95.22{\%}. Furthermore, we take the fittest networks evolved on the CIFAR-10, and apply them to classify MNIST, Fashion-MNIST, SVHN, Rectangles, and CIFAR-100. The results show that the DNNs discovered by DENSER during evolution generalise, are robust, and scale. The most impressive result is the 78.75{\%} classification accuracy on the CIFAR-100 dataset, which, to the best of our knowledge, sets a new state-of-the-art on methods that seek to automatically design CNNs.},
	Author = {Assun{\c{c}}ao, Filipe and Louren{\c{c}}o, Nuno and Machado, Penousal and Ribeiro, Bernardete},
	Date-Added = {2018-11-12 17:44:52 +1300},
	Date-Modified = {2018-11-12 17:45:26 +1300},
	Day = {27},
	Doi = {10.1007/s10710-018-9339-y},
	Issn = {1573-7632},
	Journal = {Genetic Programming and Evolvable Machines},
	Keywords = {Evolutionary, Deep Neural Network, Neuroevolution},
	Month = {Sep},
	Title = {DENSER: deep evolutionary network structured representation},
	Url = {https://doi.org/10.1007/s10710-018-9339-y},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s10710-018-9339-y}}

@article{Mirjalili_2012,
	Author = {SeyedAli Mirjalili and Siti Zaiton Mohd Hashim},
	Date-Added = {2018-11-09 11:12:56 +1300},
	Date-Modified = {2018-11-09 11:13:25 +1300},
	Doi = {10.7763/ijmlc.2012.v2.114},
	Journal = {International Journal of Machine Learning and Computing},
	Keywords = {Other, Learning algorithm, Physics},
	Pages = {204--208},
	Publisher = {{EJournal} Publishing},
	Title = {{BMOA}: Binary Magnetic Optimization Algorithm},
	Url = {https://doi.org/10.7763%2Fijmlc.2012.v2.114},
	Year = 2012,
	Bdsk-Url-1 = {https://doi.org/10.7763%2Fijmlc.2012.v2.114},
	Bdsk-Url-2 = {https://doi.org/10.7763/ijmlc.2012.v2.114}}

@inproceedings{8056823,
	Abstract = {Convolutional neural networks (CNNs) are deployed in a wide range of image recognition, scene segmentation and object detection applications. Achieving state of the art accuracy in CNNs often results in large models and complex topologies that require significant compute resources to complete in a timely manner. Binarised neural networks (BNNs) have been proposed as an optimised variant of CNNs, which constrain the weights and activations to +1 or -1 and thus offer compact models and lower computational complexity per operation. This paper presents a high performance BNN accelerator on the Intel{\textregistered}Xeon+FPGA{\texttrademark} platform. The proposed accelerator is designed to take advantage of the Xeon+FPGA system in a way that a specialised FPGA architecture can be targeted for the most compute intensive parts of the BNN whilst other parts of the topology can be handled by the Xeon{\texttrademark} CPU. The implementation is evaluated by comparing the raw compute performance and energy efficiency for key layers in standard CNN topologies against an Nvidia Titan X Pascal GPU and other published FPGA BNN accelerators. The results show that our single-package integrated Arria{\texttrademark} 10 FPGA accelerator coupled with a high-end Xeon CPU can offer comparable performance and better energy efficiency than a high-end discrete Titan X GPU card. In addition, our solution delivers the best performance compared to previous BNN FPGA implementations.},
	Author = {D. J. M. Moss and E. Nurvitadhi and J. Sim and A. Mishra and D. Marr and S. Subhaschandra and P. H. W. Leong},
	Booktitle = {2017 27th International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-11-05 22:28:08 +1300},
	Date-Modified = {2018-12-06 14:59:09 +1300},
	Doi = {10.23919/FPL.2017.8056823},
	Issn = {1946-1488},
	Keywords = {BNN, CNN, FPGA, computational complexity;field programmable gate arrays;graphics processing units;image recognition;neural nets;object detection;high performance binary neural networks;convolutional neural networks;CNN;Xeon CPU;Intel Xeon+FPGA platform;FPGA BNN accelerators;Nvidia Titan X Pascal GPU;specialised FPGA architecture;Xeon+FPGA system;high performance BNN accelerator;lower computational complexity;complex topologies;object detection applications;scene segmentation;image recognition;Field programmable gate arrays;Graphics processing units;Topology;Computer architecture;Performance evaluation;Network topology;IP networks},
	Month = {Sept},
	Pages = {1-4},
	Title = {High performance binary neural networks on the Xeon-FPGA platform},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.23919/FPL.2017.8056823}}

@article{2017arXiv170402081J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170402081J},
	Archiveprefix = {arXiv},
	Author = {{Javad Shafiee}, M. and {Barshan}, E. and {Wong}, A.},
	Date-Added = {2018-10-29 11:34:29 +0000},
	Date-Modified = {2018-10-29 11:34:29 +0000},
	Eprint = {1704.02081},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = apr,
	Title = {{Evolution in Groups: A deeper look at synaptic cluster driven evolution of deep neural networks}},
	Year = 2017}

@article{2018arXiv180905989W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180905989W},
	Archiveprefix = {arXiv},
	Author = {{Wong}, A. and {Javad Shafiee}, M. and {Chwyl}, B. and {Li}, F.},
	Date-Added = {2018-10-29 06:56:37 +0000},
	Date-Modified = {2018-10-29 06:57:00 +0000},
	Eprint = {1809.05989},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Neural and Evolutionary Computing; Artificial Intelligence, Computer Vision; Pattern Recognition},
	Month = sep,
	Title = {{FermiNets: Learning generative machines to generate efficient neural networks via generative synthesis}},
	Year = 2018}

@article{2016arXiv160604393J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604393J},
	Archiveprefix = {arXiv},
	Author = {{Javad Shafiee}, M. and {Mishra}, A. and {Wong}, A.},
	Date-Added = {2018-10-29 06:49:10 +0000},
	Date-Modified = {2018-10-29 06:49:40 +0000},
	Eprint = {1606.04393},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Computer Vision; Pattern Recognition; Machine Learning; Neural and Evolutionary Computing; Statistics; Machine Learning},
	Month = jun,
	Primaryclass = {cs.CV},
	Title = {{Deep Learning with Darwin: Evolutionary Synthesis of Deep Neural Networks}},
	Year = 2016}

@article{2015arXiv151108228K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151108228K},
	Archiveprefix = {arXiv},
	Author = {{Kaiser}, {\L}. and {Sutskever}, I.},
	Date-Added = {2018-10-28 08:28:38 +0000},
	Date-Modified = {2018-10-28 08:28:51 +0000},
	Eprint = {1511.08228},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Machine Learning, Neural and Evolutionary Computing},
	Month = nov,
	Title = {{Neural GPUs Learn Algorithms}},
	Year = 2015}

@article{2015arXiv150205477S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205477S},
	Archiveprefix = {arXiv},
	Author = {{Schulman}, J. and {Levine}, S. and {Moritz}, P. and {Jordan}, M.~I. and {Abbeel}, P.},
	Date-Added = {2018-10-26 09:31:15 +0000},
	Date-Modified = {2018-10-26 09:31:24 +0000},
	Eprint = {1502.05477},
	Journal = {ArXiv e-prints},
	Keywords = {DNN; Machine Learning},
	Month = feb,
	Title = {{Trust Region Policy Optimization}},
	Year = 2015,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBfLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRGVjZW50cmFsaXplZC9EaXN0cmlidXRlZDpEZWNlbnRyYWxpemVkIGFuZCBBc3luY2hyb25vdXMgQWxnb3JpdGhtcy5iaWJPEQI6AAAAAAI6AAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRGlzdHJpYnV0ZWQvRGVjZW50I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAADURlY2VudHJhbGl6ZWQAAAIAdS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpEZWNlbnRyYWxpemVkOkRpc3RyaWJ1dGVkL0RlY2VudHJhbGl6ZWQgYW5kIEFzeW5jaHJvbm91cyBBbGdvcml0aG1zLmJpYgAADgB0ADkARABpAHMAdAByAGkAYgB1AHQAZQBkAC8ARABlAGMAZQBuAHQAcgBhAGwAaQB6AGUAZAAgAGEAbgBkACAAQQBzAHkAbgBjAGgAcgBvAG4AbwB1AHMAIABBAGwAZwBvAHIAaQB0AGgAbQBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBzVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9EZWNlbnRyYWxpemVkL0Rpc3RyaWJ1dGVkOkRlY2VudHJhbGl6ZWQgYW5kIEFzeW5jaHJvbm91cyBBbGdvcml0aG1zLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAhgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAALE}}

@article{2016arXiv160909106H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160909106H},
	Archiveprefix = {arXiv},
	Author = {{Ha}, D. and {Dai}, A. and {Le}, Q.~V.},
	Date-Added = {2018-10-26 09:08:27 +0000},
	Date-Modified = {2018-10-26 09:08:35 +0000},
	Eprint = {1609.09106},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Machine Learning},
	Month = sep,
	Title = {{HyperNetworks}},
	Year = 2016}

@inproceedings{7780459,
	Abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8; deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	Author = {K. He and X. Zhang and S. Ren and J. Sun},
	Booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-10-25 23:41:31 +0000},
	Date-Modified = {2018-12-01 13:39:34 +1300},
	Doi = {10.1109/CVPR.2016.90},
	Issn = {1063-6919},
	Keywords = {CNN, image classification; AI; neural nets;object detection, RESNET},
	Month = {June},
	Pages = {770-778},
	Title = {Deep Residual Learning for Image Recognition},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/CVPR.2016.90}}

@article{726791,
	Abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	Author = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
	Date-Added = {2018-10-25 01:06:25 +0000},
	Date-Modified = {2018-10-25 01:06:33 +0000},
	Doi = {10.1109/5.726791},
	Issn = {0018-9219},
	Journal = {Proceedings of the IEEE},
	Keywords = {NN; optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
	Month = {Nov},
	Number = {11},
	Pages = {2278-2324},
	Title = {Gradient-based learning applied to document recognition},
	Volume = {86},
	Year = {1998},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBVLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0hhbW1pbmcgQ2x1c3RlcmluZy0gQSBOZXcgQXBwcm9hY2ggdG8gUnVsZSBFeHRyYWN0aW9uLmJpYk8RAhwAAAAAAhwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9IYW1taW5nIENsdXN0ZXJpbmcjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAADQk5OAAACAGsvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Qk5OOkhhbW1pbmcgQ2x1c3RlcmluZy0gQSBOZXcgQXBwcm9hY2ggdG8gUnVsZSBFeHRyYWN0aW9uLmJpYgAADgB0ADkASABhAG0AbQBpAG4AZwAgAEMAbAB1AHMAdABlAHIAaQBuAGcALQAgAEEAIABOAGUAdwAgAEEAcABwAHIAbwBhAGMAaAAgAHQAbwAgAFIAdQBsAGUAIABFAHgAdAByAGEAYwB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBpVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9CTk4vSGFtbWluZyBDbHVzdGVyaW5nLSBBIE5ldyBBcHByb2FjaCB0byBSdWxlIEV4dHJhY3Rpb24uYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJAB8AAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAApw=},
	Bdsk-Url-1 = {https://doi.org/10.1109/5.726791}}

@inproceedings{7966159,
	Abstract = {We introduce FxpNet, a framework to train deep convolutional neural networks with low bit-width arithmetics in both forward pass and backward pass. During training FxpNet further reduces the bit-width of stored parameters (also known as primal parameters) by adaptively updating their fixed-point formats. These primal parameters are usually represented in the full resolution of floating-point values in previous binarized and quantized neural networks. In FxpNet, during forward pass fixed-point primal weights and activations are first binarized before computation, while in backward pass all gradients are represented as low resolution fixed-point values and then accumulated to corresponding fixed-point primal parameters. To have highly efficient implementations in FPGAs, ASICs and other dedicated devices, FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM (FxpADAM) methods to further reduce the required floating-point operations, which will save considerable power and chip area. The evaluation on CIFAR-10 dataset indicates the effectiveness that FxpNet with 12-bit primal parameters and 12-bit gradients achieves comparable prediction accuracy with state-of-the-art binarized and quantized neural networks.},
	Author = {X. Chen and X. Hu and H. Zhou and N. Xu},
	Booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
	Date-Added = {2018-10-24 07:44:36 +0000},
	Date-Modified = {2018-10-24 07:44:36 +0000},
	Doi = {10.1109/IJCNN.2017.7966159},
	Issn = {2161-4407},
	Keywords = {application specific integrated circuits;convolution;field programmable gate arrays;fixed point arithmetic;floating point arithmetic;neural nets;FxpNet;deep convolutional neural network;fixed-point representation;bit-width arithmetics;forward pass;backward pass;floating-point values;binarized neural networks;quantized neural networks;fixed-point primal weights;low resolution fixed-point values;fixed-point primal parameters;FPGAs;ASICs;integer batch normalization;IBN;fixed-point ADAM;FxpADAM;CIFAR-10 dataset;12-bit primal parameters;12-bit gradients;Quantization (signal);Training;Field programmable gate arrays;Neural networks;Convolution;Kernel;Acceleration},
	Month = {May},
	Pages = {2494-2501},
	Title = {FxpNet: Training a deep convolutional neural network in fixed-point representation},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2017.7966159}}

@inproceedings{Nakahara:2018:LYB:3174243.3174266,
	Acmid = {3174266},
	Address = {New York, NY, USA},
	Author = {Nakahara, Hiroki and Yonekawa, Haruyoshi and Fujii, Tomoya and Sato, Shimpei},
	Booktitle = {Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-10-22 09:26:07 +0000},
	Date-Modified = {2018-10-22 09:26:17 +0000},
	Doi = {10.1145/3174243.3174266},
	Isbn = {978-1-4503-5614-5},
	Keywords = {BNN; binarized deep neural network, convolutional deep neural network, object detection},
	Location = {Monterey, CALIFORNIA, USA},
	Numpages = {10},
	Pages = {31--40},
	Publisher = {ACM},
	Series = {FPGA '18},
	Title = {A Lightweight YOLOv2: A Binarized CNN with A Parallel Support Vector Regression for an FPGA},
	Url = {http://doi.acm.org/10.1145/3174243.3174266},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3174243.3174266},
	Bdsk-Url-2 = {https://doi.org/10.1145/3174243.3174266}}

@inproceedings{Guo2018FBNAAF,
	Author = {P. Guo and H. Ma and Ruizhi Chen and Pin Li and Shaolin Xie and Donglin Wang},
	Booktitle = {FPL 2018},
	Date-Added = {2018-10-22 09:01:32 +0000},
	Date-Modified = {2018-10-22 09:11:30 +0000},
	Keywords = {BNN, Neural Network, FPGA, Accelerator},
	Organization = {Trinity College, Dublin},
	Title = {FBNA: A Fully Binarized Neural Network Accelerator},
	Volume = {1},
	Year = {2018}}

@inproceedings{Yang:2018:FOB:3218603.3218615,
	Acmid = {3218615},
	Address = {New York, NY, USA},
	Articleno = {50},
	Author = {Yang, Li and He, Zhezhi and Fan, Deliang},
	Booktitle = {Proceedings of the International Symposium on Low Power Electronics and Design},
	Date-Added = {2018-10-22 05:09:45 +0000},
	Date-Modified = {2018-10-22 05:09:59 +0000},
	Doi = {10.1145/3218603.3218615},
	Isbn = {978-1-4503-5704-3},
	Keywords = {BNN; Binarized convolutional neural network (BNN), Convolutional neural network (CNN), field-programmable gate array (FPGA)},
	Location = {Seattle, WA, USA},
	Numpages = {6},
	Pages = {50:1--50:6},
	Publisher = {ACM},
	Series = {ISLPED '18},
	Title = {A Fully Onchip Binarized Convolutional Neural Network FPGA Impelmentation with Accurate Inference},
	Url = {http://doi.acm.org/10.1145/3218603.3218615},
	Year = {2018},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3218603.3218615},
	Bdsk-Url-2 = {https://doi.org/10.1145/3218603.3218615}}

@article{2018arXiv180903368P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180903368P},
	Archiveprefix = {arXiv},
	Author = {{Peters}, J.~W.~T. and {Welling}, M.},
	Date-Added = {2018-10-22 01:43:56 +0000},
	Date-Modified = {2018-10-22 01:44:09 +0000},
	Eprint = {1809.03368},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning; Statistics},
	Month = sep,
	Title = {{Probabilistic Binary Neural Networks}},
	Year = 2018}

@article{2018arXiv180909244B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180909244B},
	Archiveprefix = {arXiv},
	Author = {{Baluja}, S. and {Marwood}, D. and {Covell}, M. and {Johnston}, N.},
	Date-Added = {2018-10-22 01:40:27 +0000},
	Date-Modified = {2018-10-22 01:40:43 +0000},
	Eprint = {1809.09244},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning; Statistics},
	Month = sep,
	Title = {{No Multiplication? No Floating Point? No Problem! Training Networks for Efficient Inference}},
	Year = 2018}

@article{2018arXiv181002068F,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv181002068F},
	Archiveprefix = {arXiv},
	Author = {{Fu}, C. and {Zhu}, S. and {Su}, H. and {Lee}, C.-E. and {Zhao}, J.},
	Date-Added = {2018-10-22 01:15:11 +0000},
	Date-Modified = {2018-10-22 01:15:49 +0000},
	Eprint = {1810.02068},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning; Artificial Intelligence; Hardware Architecture; Computer Vision; Pattern Recognition; Statistics},
	Month = oct,
	Title = {{Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA}},
	Year = 2018}

@article{2017arXiv170302660R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170302660R},
	Archiveprefix = {arXiv},
	Author = {{Rajeswaran}, A. and {Lowrey}, K. and {Todorov}, E. and {Kakade}, S.},
	Date-Added = {2018-10-19 10:46:17 +0000},
	Date-Modified = {2018-10-19 10:46:39 +0000},
	Eprint = {1703.02660},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary, Machine Learning, Artificial Intelligence, Robotics, Systems and Control},
	Month = mar,
	Title = {{Towards Generalization and Simplicity in Continuous Control}},
	Year = 2017}

@article{2017arXiv171206567P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171206567P},
	Archiveprefix = {arXiv},
	Author = {{Petroski Such}, F. and {Madhavan}, V. and {Conti}, E. and {Lehman}, J. and {Stanley}, K.~O. and {Clune}, J.},
	Date-Added = {2018-10-19 10:41:55 +0000},
	Date-Modified = {2019-01-11 17:06:46 +1300},
	Eprint = {1712.06567},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary, Neural and Evolutionary Computing, Machine Learning; Uber},
	Month = dec,
	Title = {{Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning}},
	Year = 2017}

@article{2018arXiv180307055M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180307055M},
	Archiveprefix = {arXiv},
	Author = {{Mania}, H. and {Guy}, A. and {Recht}, B.},
	Date-Added = {2018-10-19 10:39:53 +0000},
	Date-Modified = {2018-10-19 10:40:20 +0000},
	Eprint = {1803.07055},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Machine Learning, Artificial Intelligence, Mathematics, Optimization and Control, Statistics, Machine Learning},
	Month = mar,
	Title = {{Simple random search provides a competitive approach to reinforcement learning}},
	Year = 2018}

@article{2017arXiv171003748B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171003748B},
	Archiveprefix = {arXiv},
	Author = {{Bansal}, T. and {Pachocki}, J. and {Sidor}, S. and {Sutskever}, I. and {Mordatch}, I.},
	Date-Added = {2018-10-19 10:33:38 +0000},
	Date-Modified = {2018-10-19 10:34:00 +0000},
	Eprint = {1710.03748},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Artificial Intelligence},
	Month = oct,
	Primaryclass = {cs.AI},
	Title = {{Emergent Complexity via Multi-Agent Competition}},
	Year = 2017}

@inproceedings{pmlr-v37-schaul15,
	Abstract = {Value functions are a core component of reinforcement learning. The main idea is to to construct a single function approximator V(s; theta) that estimates the long-term reward from any state s, using parameters θ. In this paper we introduce universal value function approximators (UVFAs) V(s,g;theta) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
	Address = {Lille, France},
	Author = {Tom Schaul and Daniel Horgan and Karol Gregor and David Silver},
	Booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
	Date-Added = {2018-10-19 10:22:36 +0000},
	Date-Modified = {2018-10-19 10:23:02 +0000},
	Editor = {Francis Bach and David Blei},
	Keywords = {Evolutionary, Google, DeepMind, Neural Network},
	Month = {07--09 Jul},
	Pages = {1312--1320},
	Pdf = {http://proceedings.mlr.press/v37/schaul15.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Universal Value Function Approximators},
	Url = {http://proceedings.mlr.press/v37/schaul15.html},
	Volume = {37},
	Year = {2015},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v37/schaul15.html}}

@article{2018arXiv180403867P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403867P},
	Archiveprefix = {arXiv},
	Author = {{Prabhu}, A. and {Batchu}, V. and {Gajawada}, R. and {Aurobindo Munagala}, S. and {Namboodiri}, A.},
	Date-Added = {2018-10-13 07:48:52 +0000},
	Date-Modified = {2018-10-13 07:49:08 +0000},
	Eprint = {1804.03867},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Vision; Pattern Recognition},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{Hybrid Binary Networks: Optimizing for Accuracy, Efficiency and Memory}},
	Year = 2018}

@inproceedings{bmxnet,
	Acmid = {3129393},
	Address = {New York, NY, USA},
	Annote = {https://arxiv.org/abs/1705.09864
https://github.com/hpi-xnor/BMXNet
},
	Author = {Yang, Haojin and Fritzsche, Martin and Bartz, Christian and Meinel, Christoph},
	Booktitle = {Proceedings of the 2017 ACM on Multimedia Conference},
	Date-Added = {2018-10-13 07:42:15 +0000},
	Date-Modified = {2018-10-13 07:45:31 +0000},
	Doi = {10.1145/3123266.3129393},
	Isbn = {978-1-4503-4906-2},
	Keywords = {BNN; binary neural networks, computer vision, machine learning, open source},
	Location = {Mountain View, California, USA},
	Numpages = {4},
	Pages = {1209--1212},
	Publisher = {ACM},
	Series = {MM '17},
	Title = {BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet},
	Url = {http://doi.acm.org/10.1145/3123266.3129393},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3123266.3129393},
	Bdsk-Url-2 = {https://doi.org/10.1145/3123266.3129393}}

@inproceedings{8429420,
	Abstract = {Deep neural networks have achieved impressive results in computer vision and machine learning. Unfortunately, state-of-the-art networks are extremely compute-and memory-intensive which makes them unsuitable for mW-devices such as IoT end-nodes. Aggressive quantization of these networks dramatically reduces the computation and memory footprint. Binary-weight neural networks (BWNs) follow this trend, pushing weight quantization to the limit. Hardware accelerators for BWNs presented up to now have focused on core efficiency, disregarding I/O bandwidth and system-level efficiency that are crucial for deployment of accelerators in ultra-low power devices. We present Hyperdrive: a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel binary-weight streaming approach, and capable of handling high-resolution images by virtue of its systolic-scalable architecture. We achieve a 5.9 TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than state-of-the-art BNN accelerators, even if our core uses resource-intensive FP16 arithmetic for increased robustness.},
	Author = {R. Andri and L. Cavigelli and D. Rossi and L. Benini},
	Booktitle = {2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)},
	Date-Added = {2018-10-12 06:40:47 +0000},
	Date-Modified = {2018-10-12 06:40:54 +0000},
	Doi = {10.1109/ISVLSI.2018.00099},
	Issn = {2159-3477},
	Keywords = {BNN; computer vision;feedforward neural nets;Internet of Things;learning (artificial intelligence);low-power electronics;microprocessor chips;hardware accelerators;core efficiency;I/O bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable architecture;state-of-the-art BNN accelerators;resource-intensive FP16 arithmetic;TOp/s/W system-level efficiency;binary-weight streaming approach;BWN;hyperdrive;weight quantization;binary-weight neural networks;memory footprint;aggressive quantization;mW-devices;memory-intensive;machine learning;computer vision;impressive results;deep neural networks;mW IoT end-nodes;systolically scalable binary-weight CNN inference engine;Frequency modulation;Computer architecture;Quantization (signal);Neural networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary Weights Neural Networks;IoT},
	Month = {July},
	Pages = {509-515},
	Title = {Hyperdrive: A Systolically Scalable Binary-Weight CNN Inference Engine for mW IoT End-Nodes},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISVLSI.2018.00099}}

@inproceedings{8457633,
	Abstract = {This paper proposes ReBNet, an end-to-end framework for training reconfigurable binary neural networks on software and developing efficient accelerators for execution on FPGA. Binary neural networks offer an intriguing opportunity for deploying large-scale deep learning models on resource-constrained devices. Binarization reduces the memory footprint and replaces the power-hungry matrix-multiplication with light-weight XnorPopcount operations. However, binary networks suffer from a degraded accuracy compared to their fixed-point counterparts. We show that the state-of-the-art methods for optimizing binary networks accuracy, significantly increase the implementation cost and complexity. To compensate for the degraded accuracy while adhering to the simplicity of binary networks, we devise the first reconfigurable scheme that can adjust the classification accuracy based on the application. Our proposition improves the classification accuracy by representing features with multiple levels of residual binarization. Unlike previous methods, our approach does not exacerbate the area cost of the hardware accelerator. Instead, it provides a tradeoff between throughput and accuracy while the area overhead of multi-level binarization is negligible.},
	Author = {M. Ghasemzadeh and M. Samragh and F. Koushanfar},
	Booktitle = {2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	Date-Added = {2018-10-12 06:35:49 +0000},
	Date-Modified = {2018-10-12 06:36:55 +0000},
	Doi = {10.1109/FCCM.2018.00018},
	Issn = {2576-2621},
	Keywords = {BNN; field programmable gate arrays;learning (artificial intelligence);matrix multiplication;neural nets;ReBNet;residual binarized neural network;large-scale deep learning models;power-hungry matrix-multiplication;light-weight XnorPopcount operations;fixed-point counterparts;FPGA;memory footprint;hardware accelerator;Hardware;Neural networks;Training;Field programmable gate arrays;Parallel processing;Cost function;Libraries;Deep neural networks;Reconfigurable computing;Domain customized computing;Binary neural network;Residual binarization},
	Month = {April},
	Pages = {57-64},
	Title = {ReBNet: Residual Binarized Neural Network},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FCCM.2018.00018}}

@inproceedings{8461456,
	Abstract = {With the ever growing popularity of deep learning, the tremendous complexity of deep neural networks is becoming problematic when one considers inference on resource constrained platforms. Binary networks have emerged as a potential solution, however, they exhibit a fundamentallimi-tation in realizing gradient-based learning as their activations are non-differentiable. Current work has so far relied on approximating gradients in order to use the back-propagation algorithm via the straight through estimator (STE). Such approximations harm the quality of the training procedure causing a noticeable gap in accuracy between binary neural networks and their full precision baselines. We present a novel method to train binary activated neural networks using true gradient-based learning. Our idea is motivated by the similarities between clipping and binary activation functions. We show that our method has minimal accuracy degradation with respect to the full precision baseline. Finally, we test our method on three benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show that continuous binarization using true gradient-based learning achieves an accuracy within 1.5% of the floating-point baseline, as compared to accuracy drops as high as 6% when training the same binary activated network using the STE.},
	Author = {C. Sakr and J. Choi and Z. Wang and K. Gopalakrishnan and N. Shanbhag},
	Booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	Date-Added = {2018-10-12 06:28:18 +0000},
	Date-Modified = {2018-10-12 06:28:24 +0000},
	Doi = {10.1109/ICASSP.2018.8461456},
	Issn = {2379-190X},
	Keywords = {BNN; gradient methods;learning (artificial intelligence);neural nets;gradient-based training;deep binary activated neural networks;continuous binarization;deep learning;tremendous complexity;resource constrained platforms;training procedure;binary activation functions;minimal accuracy degradation;gradient-based learning;back-propagation algorithm;straight through estimator;floating-point baseline;STE;Training;Neural networks;Complexity theory;Machine learning;Stochastic processes;Perturbation methods;Approximation algorithms;deep learning;binary neural networks;activation functions},
	Month = {April},
	Pages = {2346-2350},
	Title = {True Gradient-Based Training of Deep Binary Activated Neural Networks Via Continuous Binarization},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICASSP.2018.8461456}}

@inproceedings{8373076,
	Abstract = {Deploying state-of-the-art CNNs requires power-hungry processors and off-chip memory. This precludes the implementation of CNNs in low-power embedded systems. Recent research shows CNNs sustain extreme quantization, binarizing their weights and intermediate feature maps, thereby saving 8-32x memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount operations. We present XNORBIN, a flexible accelerator for binary CNNs with computation tightly coupled to memory for aggressive data reuse supporting even non-trivial network topologies with large feature map volumes. Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of 95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.},
	Author = {A. A. Bahou and G. Karunaratne and R. Andri and L. Cavigelli and L. Benini},
	Booktitle = {2018 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS)},
	Date-Added = {2018-10-12 06:22:47 +0000},
	Date-Modified = {2018-10-12 06:22:55 +0000},
	Doi = {10.1109/CoolChips.2018.8373076},
	Issn = {2473-4683},
	Keywords = {BNN; embedded systems;low-power electronics;neural nets;power aware computing;binary convolutional neural networks;off-chip memory;low-power embedded systems;extreme quantization;flexible accelerator;aggressive data;nontrivial network topologies;feature map volumes;energy efficiency;hardware accelerator;binary CNN;weight binarization;collapsing energy-intensive sum-of-products;XNOR-and-popcount operations;Hardware;Convolutional neural networks;System-on-chip;Computational modeling;Computer architecture;Program processors},
	Month = {April},
	Pages = {1-3},
	Title = {XNORBIN: A 95 TOp/s/W hardware accelerator for binary convolutional neural networks},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/CoolChips.2018.8373076}}

@article{8103902,
	Abstract = {Binary weight convolutional neural networks (BCNNs) can achieve near state-of-the-art classification accuracy and have far less computation complexity compared with traditional CNNs using high-precision weights. Due to their binary weights, BCNNs are well suited for vision-based Internet-of-Things systems being sensitive to power consumption. BCNNs make it possible to achieve very high throughput with moderate power dissipation. In this paper, an energy-efficient architecture for BCNNs is proposed. It fully exploits the binary weights and other hardware-friendly characteristics of BCNNs. A judicious processing schedule is proposed so that off-chip I/O access is minimized and activations are maximally reused. To significantly reduce the critical path delay, we introduce optimized compressor trees and approximate binary multipliers with two novel compensation schemes. The latter is able to save significant hardware resource, and almost no computation accuracy is compromised. Taking advantage of error resiliency of BCNNs, an innovative approximate adder is developed, which significantly reduces the silicon area and data path delay. Thorough error analysis and extensive experimental results on several data sets show that the approximate adders in the data path cause negligible accuracy loss. Moreover, algorithmic transformations for certain layers of BCNNs and a memory-efficient quantization scheme are incorporated to further reduce the energy cost and on-chip storage requirement. Finally, the proposed BCNN hardware architecture is implemented with the SMIC 130-nm technology. The postlayout results demonstrate that our design can achieve an energy efficiency over 2.0TOp/s/W when scaled to 65 nm, which is more than two times better than the prior art.},
	Author = {Y. Wang and J. Lin and Z. Wang},
	Date-Added = {2018-10-12 06:20:05 +0000},
	Date-Modified = {2018-10-12 06:20:11 +0000},
	Doi = {10.1109/TVLSI.2017.2767624},
	Issn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {BNN;adders;convolution;energy conservation;error analysis;feedforward neural nets;multiplying circuits;neural chips;trees (mathematics);energy-efficient architecture;binary weight convolutional neural networks;high-precision weights;binary weights;approximate binary multipliers;BCNN hardware architecture;energy efficiency;classification accuracy;data path delay;processing schedule;off-chip I/O access;critical path delay;optimized compressor trees;approximate adder;error analysis;memory-efficient quantization;on-chip storage requirement;size 65.0 nm;Computer architecture;Hardware;Neural networks;Neurons;Adders;Quantization (signal);Convolution;Approximate computing;binary weight convolutional neural network (BCNN) architecture;convolutional neural network (CNN);deep learning;energy-efficient design;signal processing;VLSI architecture},
	Month = {Feb},
	Number = {2},
	Pages = {280-293},
	Title = {An Energy-Efficient Architecture for Binary Weight Convolutional Neural Networks},
	Volume = {26},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/TVLSI.2017.2767624}}

@inproceedings{8425178,
	Abstract = {Deep learning has revolutionized computer vision and other fields since its big bang in 2012. However, it is challenging to deploy Deep Neural Networks (DNNs) into real-world applications due to their high computational complexity. Binary Neural Networks (BNNs) dramatically reduce computational complexity by replacing most arithmetic operations with bitwise operations. Existing implementations of BNNs have been focusing on GPU or FPGA, and using the conventional image-to-column method that doesn't perform well for binary convolution due to low arithmetic intensity and unfriendly pattern for bitwise operations. We propose BitFlow, a gemm-operator-network three-level optimization framework for fully exploiting the computing power of BNNs on CPU. BitFlow features a new class of algorithm named PressedConv for efficient binary convolution using locality-aware layout and vector parallelism. We evaluate BitFlow with the VGG network. On a single core of Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN implementations, and 11.5x speedup over counterpart full-precision DNNs. Over 64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart full-precision DNNs on GPU (GTX 1080).},
	Author = {Y. Hu and J. Zhai and D. Li and Y. Gong and Y. Zhu and W. Liu and L. Su and J. Jin},
	Booktitle = {2018 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
	Date-Added = {2018-10-12 06:15:46 +0000},
	Date-Modified = {2018-10-12 06:16:05 +0000},
	Doi = {10.1109/IPDPS.2018.00034},
	Issn = {1530-2075},
	Keywords = {BNN;computational complexity;field programmable gate arrays;graphics processing units;learning (artificial intelligence);multiprocessing systems;neural nets;optimisation;parallel processing;vector parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big bang;Deep Neural Networks;high computational complexity;Binary Neural Networks;BNNs;arithmetic operations;bitwise operations;image-to-column method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow features;efficient binary convolution;VGG network;counterpart full-precision DNNs;GPU;Convolution;Neural networks;Layout;Parallel processing;Acceleration;Graphics processing units;Machine learning;Network Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi},
	Month = {May},
	Pages = {244-253},
	Title = {BitFlow: Exploiting Vector Parallelism for Binary Neural Networks on CPU},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPDPS.2018.00034}}

@article{8226999,
	Abstract = {A versatile reconfigurable accelerator architecture for binary/ternary deep neural networks is presented. In-memory neural network processing without any external data accesses, sustained by the symmetry and simplicity of the computation of the binary/ternaty neural network, improves the energy efficiency dramatically. The prototype chip is fabricated, and it achieves 1.4 TOPS (tera operations per second) peak performance with 0.6-W power consumption at 400-MHz clock. The application examination is also conducted.},
	Author = {K. Ando and K. Ueyoshi and K. Orimo and H. Yonekawa and S. Sato and H. Nakahara and S. Takamaeda-Yamazaki and M. Ikebe and T. Asai and T. Kuroda and M. Motomura},
	Date-Added = {2018-10-12 06:15:46 +0000},
	Date-Modified = {2018-10-12 06:15:58 +0000},
	Doi = {10.1109/JSSC.2017.2778702},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {BNN;low-power electronics;neural nets;random-access storage;reconfigurable architectures;deep neural network accelerator;binary/ternary deep neural networks;In-memory neural network processing;binary/ternaty neural network;BRein memory;single-chip binary/ternary reconfigurable in-memory;reconfigurable accelerator architecture;external data access;power 0.6 W;frequency 400 MHz;Biological neural networks;Random access memory;Memory management;Neurons;System-on-chip;Parallel processing;Binary neural networks;in-memory processing;near-memory processing;neural networks;reconfigurable array;ternary neural networks},
	Month = {April},
	Number = {4},
	Pages = {983-994},
	Title = {BRein Memory: A Single-Chip Binary/Ternary Reconfigurable in-Memory Deep Neural Network Accelerator Achieving 1.4 TOPS at 0.6 W},
	Volume = {53},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2017.2778702}}

@inproceedings{8394726,
	Abstract = {To achieve an advanced Internet of Things (IoT), it is necessary to combine artificial intelligence (AI) with IoT. Compact circuits that can operate AI functions will be useful for this purpose. Therefore, we propose stochastic weights binary neural networks (SWBNN). SWBNNs are more accurate than binary neural networks (BNN) with small circuits. BNNs can be realized with small circuits since binary calculation needs simpler circuits than real number calculation. However, BNNs have lower accuracy than networks with real numbers. Thus, the proposed SWBNNs are BNNs that behave stochastically, which makes them more accurate than BNNs. Moreover, SWBNNs can still be achieved with small circuits since they execute binary calculation. As a result, the accuracy for the test data of SWBNNs is closer to the accuracy for learning data than the accuracy for the test data of BNNs is. Especially when using the CIFAR10 database, the difference in the identification accuracy rate between learning data and test data decreased from 6% for BNNs to 2% for SWBNNs. From results of a field-programmable gate array (FPGA) implementation, circuits of SWBNNs are sufficiently small although they are 10% bigger than those of BNNs. Therefore, SWBNNs are more accurate than BNNs, and the circuit costs ofintroducing stochastic weights are low.},
	Author = {Y. Fukuda and T. Kawahara},
	Booktitle = {2018 7th International Symposium on Next Generation Electronics (ISNE)},
	Date-Added = {2018-10-12 06:02:12 +0000},
	Date-Modified = {2018-10-12 06:02:21 +0000},
	Doi = {10.1109/ISNE.2018.8394726},
	Issn = {2378-8607},
	Keywords = {BNN;field programmable gate arrays;Internet of Things;learning (artificial intelligence);neural nets;stochastic processes;binary calculation;stochastic weights binary neural networks;IoT;BNN;SWBNN;FPGA;Internet of things;artificial intelligence;AI;binary neural networks;CIFAR10 database;field-programmable gate array implementation;Next generation networking;Neural networks;IoT;Neural-network;BNN;FPGA;stochastic},
	Month = {May},
	Pages = {1-3},
	Title = {Stochastic weights binary neural networks on FPGA},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISNE.2018.8394726}}

@inproceedings{8457667,
	Abstract = {In binary convolutional neural networks (BCNN), arithmetic operations are replaced by bitwise operations and the required memory size is greatly reduced, which is a good opportunity to accelerate training or inference on FPGAs. This paper proposes a BCNN architecture with a single engine that achieves high resource utilization. The proposed design deploys a large number of processing elements in parallel to increase throughput, and a forwarding scheme to increase resource utilization on the existing engine. In addition, we demonstrate a novel reuse scheme to make fully-connected layers exploit the same engine. The proposed design is combined with an inference environment for comparison and implemented on a Xilinx XCVU190 FPGA. The implemented design uses 61k look-up tables (LUTs), 45k flip-flops (FFs), and 13.9Mbit block RAM (BRAM). In addition, it achieves 61.6 GOPS/kLUT at 240MHz, which is 1.16 times higher than that of the best prior BCNN design, even though it uses a single engine without optimal configurations on each layer.},
	Author = {S. Kim and R. Rutenbar},
	Booktitle = {2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)},
	Date-Added = {2018-10-12 05:55:29 +0000},
	Date-Modified = {2018-10-12 05:55:36 +0000},
	Doi = {10.1109/FCCM.2018.00052},
	Issn = {2576-2621},
	Keywords = {BNN; field programmable gate arrays;flip-flops;logic design;neural chips;neural net architecture;random-access storage;table lookup;BRAM;block RAM;flip-flops;look-up tables;inference environment;fully-connected layers;processing elements;BCNN design;memory size;bitwise operations;arithmetic operations;binary convolutional neural networks;effective resource utilization;accelerator design;Xilinx XCVU190 FPGA;forwarding scheme;high resource utilization;BCNN architecture;frequency 240.0 MHz;storage capacity 13.9 Mbit;Field programmable gate arrays;Resource management;Convolutional neural networks;Engines;Computer science;Acceleration;Machine learning;Binary convolutional neural networks;High resource utilization;FPGA},
	Month = {April},
	Pages = {218-218},
	Title = {Accelerator Design with Effective Resource Utilization for Binary Convolutional Neural Networks on an FPGA},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1109/FCCM.2018.00052}}

@article{6847217,
	Abstract = {Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.},
	Author = {L. Shao and F. Zhu and X. Li},
	Date-Added = {2018-10-11 23:28:26 +0000},
	Date-Modified = {2018-10-11 23:30:04 +0000},
	Doi = {10.1109/TNNLS.2014.2330900},
	Issn = {2162-237X},
	Journal = {IEEE Transactions on Neural Networks and Learning Systems},
	Keywords = {NN; image classification;learning (artificial intelligence);object recognition;visual categorization;transfer learning algorithms;object recognition;image classification;human action recognition;Knowledge transfer;Visualization;Training;Training data;Adaptation models;Learning systems;Testing;Action recognition;image classification;machine learning;object recognition;survey;transfer learning;visual categorization.;Action recognition;image classification;machine learning;object recognition;survey;transfer learning;visual categorization;Algorithms;Humans;Knowledge;Machine Learning;Models, Theoretical;Neural Networks (Computer);Surveys and Questionnaires;Transfer (Psychology);Visual Perception},
	Month = {May},
	Number = {5},
	Pages = {1019-1034},
	Title = {Transfer Learning for Visual Categorization: A Survey},
	Volume = {26},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2014.2330900}}

@article{2016arXiv160204283L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160204283L},
	Archiveprefix = {arXiv},
	Author = {{Lacey}, G. and {Taylor}, G.~W. and {Areibi}, S.},
	Date-Added = {2018-10-10 00:44:17 +0000},
	Date-Modified = {2018-10-10 00:44:26 +0000},
	Eprint = {1602.04283},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA; Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{Deep Learning on FPGAs: Past, Present, and Future}},
	Year = 2016}

@article{Gadea-Girones:2018aa,
	Abstract = {In the optimization of deep neural networks (DNNs) via evolutionary algorithms (EAs) and the implementation of the training necessary for the creation of the objective function, there is often a trade-off between efficiency and flexibility. Pure software solutions implemented on general-purpose processors tend to be slow because they do not take advantage of the inherent parallelism of these devices, whereas hardware realizations based on heterogeneous platforms (combining central processing units (CPUs), graphics processing units (GPUs) and/or field-programmable gate arrays (FPGAs)) are designed based on different solutions using methodologies supported by different languages and using very different implementation criteria. This paper first presents a study that demonstrates the need for a heterogeneous (CPU-GPU-FPGA) platform to accelerate the optimization of artificial neural networks (ANNs) using genetic algorithms. Second, the paper presents implementations of the calculations related to the individuals evaluated in such an algorithm on different (CPU- and FPGA-based) platforms, but with the same source files written in OpenCL. The implementation of individuals on remote, low-cost FPGA systems on a chip (SoCs) is found to enable the achievement of good efficiency in terms of performance per watt.},
	An = {PMC5982427},
	Author = {Gadea-Giron{\'e}s, Rafael and Colom-Palero, Ricardo and Herrero-Bosch, Vicente},
	Date = {2018/05/},
	Date-Added = {2018-10-10 00:41:23 +0000},
	Date-Modified = {2018-10-10 00:41:45 +0000},
	Db = {PMC},
	Doi = {10.3390/s18051384},
	Isbn = {1424-8220},
	J1 = {Sensors (Basel)},
	Journal = {Sensors (Basel, Switzerland)},
	Keywords = {Evolutionary; SoC; OpenCL},
	Month = {05},
	Number = {5},
	Pages = {1384},
	Publisher = {MDPI},
	Title = {Optimization of Deep Neural Networks Using SoCs with OpenCL},
	Ty = {JOUR},
	U1 = {29710875{$[$}pmid{$]$}; sensors-18-01384{$[$}PII{$]$}},
	Url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982427/},
	Volume = {18},
	Year = {2018},
	Year1 = {2018/04/30},
	Year2 = {2018/03/08/received},
	Year3 = {2018/04/27/accepted},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5982427/},
	Bdsk-Url-2 = {https://doi.org/10.3390/s18051384}}

@inproceedings{2017arXiv171205877J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171205877J},
	Archiveprefix = {arXiv},
	Author = {{Jacob}, B. and {Kligys}, S. and {Chen}, B. and {Zhu}, M. and {Tang}, M. and {Howard}, A. and {Adam}, H. and {Kalenichenko}, D.},
	Booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-10-10 00:07:20 +0000},
	Date-Modified = {2018-12-04 23:27:14 +1300},
	Eprint = {1712.05877},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = jun,
	Title = {{Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference}},
	Year = 2018}

@inproceedings{Soudry2014ExpectationBP,
	Abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient
descent-based methods, such as BackPropagation (BP). Inference in probabilistic
graphical models is often done using variational Bayes methods, such as Expectation
Propagation (EP). We show how an EP based approach can also be used
to train deterministic MNNs. Specifically, we approximate the posterior of the
weights given the data using a ``mean-field'' factorized distribution, in an online
setting. Using online EP and the central limit theorem we find an analytical approximation
to the Bayes update of this posterior, as well as the resulting Bayes
estimates of the weights and outputs.
Despite a different origin, the resulting algorithm, Expectation BackPropagation
(EBP), is very similar to BP in form and efficiency. However, it has several additional
advantages: (1) Training is parameter-free, given initial conditions (prior)
and the MNN architecture. This is useful for large-scale problems, where parameter
tuning is a major challenge. (2) The weights can be restricted to have discrete
values. This is especially useful for implementing trained MNNs in precision limited
hardware chips, thus improving their speed and energy efficiency by several
orders of magnitude.
We test the EBP algorithm numerically in eight binary text classification tasks.
In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning
rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with
binary weights usually perform better than MNNs with continuous (real) weights
- if we average the MNN output using the inferred posterior.},
	Annote = {https://www.semanticscholar.org/paper/Expectation-Backpropagation%3A-Parameter-Free-of-with-Soudry-Hubara/0b88ed755c4dcd0ac3d25842a5bbbe4ebcefeeec

https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf},
	Author = {Daniel Soudry and Itay Hubara and Ron Meir},
	Booktitle = {NIPS},
	Date-Added = {2018-10-09 23:57:10 +0000},
	Date-Modified = {2018-10-10 00:00:53 +0000},
	Keywords = {DNN; Backpropagation; Discrete weight space; Continuos weight space},
	Title = {Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights},
	Year = {2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBULi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vU3VydmV5IFJlcG9ydCBvbiBDcnlwdG9ncmFwaHkgQmFzZWQgb24gTmV1cmFsIE5ldHdvcmsuYmliTxECFgAAAAACFgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1N1cnZleSBSZXBvcnQgb24gQyNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAGovOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46U3VydmV5IFJlcG9ydCBvbiBDcnlwdG9ncmFwaHkgQmFzZWQgb24gTmV1cmFsIE5ldHdvcmsuYmliAA4AdAA5AFMAdQByAHYAZQB5ACAAUgBlAHAAbwByAHQAIABvAG4AIABDAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAgAEIAYQBzAGUAZAAgAG8AbgAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAaFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vU3VydmV5IFJlcG9ydCBvbiBDcnlwdG9ncmFwaHkgQmFzZWQgb24gTmV1cmFsIE5ldHdvcmsuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAHsAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAClQ==}}

@article{2018arXiv180607550Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180607550Z},
	Archiveprefix = {arXiv},
	Author = {{Zhu}, S. and {Dong}, X. and {Su}, H.},
	Date-Added = {2018-10-09 23:07:05 +0000},
	Date-Modified = {2018-10-09 23:07:12 +0000},
	Eprint = {1806.07550},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	Month = jun,
	Title = {{Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?}},
	Year = 2018}

@article{2016arXiv160207360I,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160207360I},
	Archiveprefix = {arXiv},
	Author = {{Iandola}, F.~N. and {Han}, S. and {Moskewicz}, M.~W. and {Ashraf}, K. and {Dally}, W.~J. and {Keutzer}, K.},
	Date-Added = {2018-10-01 09:23:08 +0000},
	Date-Modified = {2018-10-01 09:23:15 +0000},
	Eprint = {1602.07360},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $\lt$0.5MB model size}},
	Year = 2016}

@article{2017arXiv170507175P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170507175P},
	Archiveprefix = {arXiv},
	Author = {{Pedersoli}, F. and {Tzanetakis}, G. and {Tagliasacchi}, A.},
	Date-Added = {2018-09-30 04:37:12 +0000},
	Date-Modified = {2018-09-30 04:37:18 +0000},
	Eprint = {1705.07175},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, 62M45, I.2.6},
	Month = may,
	Primaryclass = {cs.DC},
	Title = {{Espresso: Efficient Forward Propagation for BCNNs}},
	Year = 2017}

@article{2018arXiv180700343A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180700343A},
	Archiveprefix = {arXiv},
	Author = {{Agrawal}, A. and {Jaiswal}, A. and {Han}, B. and {Srinivasan}, G. and {Roy}, K.},
	Date-Added = {2018-09-30 04:23:17 +0000},
	Date-Modified = {2018-09-30 04:23:29 +0000},
	Eprint = {1807.00343},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science; Emerging Technologies},
	Month = jul,
	Title = {{Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays}},
	Year = 2018}

@article{2018arXiv180703010C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180703010C},
	Archiveprefix = {arXiv},
	Author = {{Conti}, F. and {Davide Schiavone}, P. and {Benini}, L.},
	Date-Added = {2018-09-30 04:20:35 +0000},
	Date-Modified = {2018-09-30 04:20:42 +0000},
	Eprint = {1807.03010},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Neural and Evolutionary Computing, Computer Science - Hardware Architecture, Computer Science - Machine Learning},
	Month = jul,
	Title = {{XNOR Neural Engine: a Hardware Accelerator IP for 21.6 fJ/op Binary Neural Network Inference}},
	Year = 2018}

@article{2018arXiv180801990C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180801990C},
	Archiveprefix = {arXiv},
	Author = {{Cakir}, F. and {He}, K. and {Sclaroff}, S.},
	Date-Added = {2018-09-30 02:43:46 +0000},
	Date-Modified = {2018-10-09 23:14:42 +0000},
	Eprint = {1808.01990},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = aug,
	Title = {{Hashing with Binary Matrix Pursuit}},
	Year = 2018}

@article{2017arXiv171107971W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171107971W},
	Archiveprefix = {arXiv},
	Author = {{Wang}, X. and {Girshick}, R. and {Gupta}, A. and {He}, K.},
	Date-Added = {2018-09-30 02:43:14 +0000},
	Date-Modified = {2018-09-30 02:44:18 +0000},
	Eprint = {1711.07971},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Primaryclass = {cs.CV},
	Title = {{Non-local Neural Networks}},
	Year = 2017}

@article{2018arXiv180802631Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180802631Z},
	Archiveprefix = {arXiv},
	Author = {{Zhuang}, B. and {Shen}, C. and {Reid}, I.},
	Date-Added = {2018-09-30 01:58:28 +0000},
	Date-Modified = {2018-09-30 01:58:33 +0000},
	Eprint = {1808.02631},
	Journal = {ArXiv e-prints},
	Keywords = {BNN;Computer Science - Computer Vision and Pattern Recognition},
	Month = aug,
	Primaryclass = {cs.CV},
	Title = {{Training Compact Neural Networks with Binary Weights and Low Precision Activations}},
	Year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBkLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0RFTlNFUi0gRGVlcCBFdm9sdXRpb25hcnkgTmV0d29yayBTdHJ1Y3R1cmVkIFJlcHJlc2VudGF0aW9uLmJpYk8RAkwAAAAAAkwAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9ERU5TRVItIERlZXAgRXZvbHUjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAMRXZvbHV0aW9uYXJ5AAIAei86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpFdm9sdXRpb25hcnk6REVOU0VSLSBEZWVwIEV2b2x1dGlvbmFyeSBOZXR3b3JrIFN0cnVjdHVyZWQgUmVwcmVzZW50YXRpb24uYmliAA4AgAA/AEQARQBOAFMARQBSAC0AIABEAGUAZQBwACAARQB2AG8AbAB1AHQAaQBvAG4AYQByAHkAIABOAGUAdAB3AG8AcgBrACAAUwB0AHIAdQBjAHQAdQByAGUAZAAgAFIAZQBwAHIAZQBzAGUAbgB0AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAeFVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0RFTlNFUi0gRGVlcCBFdm9sdXRpb25hcnkgTmV0d29yayBTdHJ1Y3R1cmVkIFJlcHJlc2VudGF0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACLAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAts=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBiLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OL1ZlcnkgZGVlcCBjb252b2x1dGlvbmFsIG5ldHdvcmtzIGZvciBsYXJnZS1zY2FsZSBpbWFnZSByZWNvZ25pdGlvbi5iaWJPEQJOAAAAAAJOAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fVmVyeSBkZWVwIGNvbnZvbHV0I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAA0NOTgAAAgB4LzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkNOTjpWZXJ5IGRlZXAgY29udm9sdXRpb25hbCBuZXR3b3JrcyBmb3IgbGFyZ2Utc2NhbGUgaW1hZ2UgcmVjb2duaXRpb24uYmliAA4AjgBGAFYAZQByAHkAIABkAGUAZQBwACAAYwBvAG4AdgBvAGwAdQB0AGkAbwBuAGEAbAAgAG4AZQB0AHcAbwByAGsAcwAgAGYAbwByACAAbABhAHIAZwBlAC0AcwBjAGEAbABlACAAaQBtAGEAZwBlACAAcgBlAGMAbwBnAG4AaQB0AGkAbwBuAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgB2VXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4vVmVyeSBkZWVwIGNvbnZvbHV0aW9uYWwgbmV0d29ya3MgZm9yIGxhcmdlLXNjYWxlIGltYWdlIHJlY29nbml0aW9uLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACJAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAts=}}

@article{2018arXiv180810631K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180810631K},
	Archiveprefix = {arXiv},
	Author = {{Krestinskaya}, O. and {Salama}, K.~N. and {Pappachen James}, A.},
	Date-Added = {2018-09-30 01:58:11 +0000},
	Date-Modified = {2018-09-30 01:58:18 +0000},
	Eprint = {1808.10631},
	Journal = {ArXiv e-prints},
	Keywords = {BNN;Computer Science - Emerging Technologies, Computer Science - Artificial Intelligence},
	Month = aug,
	Title = {{Learning in Memristive Neural Network Architectures using Analog Backpropagation Circuits}},
	Year = 2018}

@article{2018arXiv180910463B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180910463B},
	Archiveprefix = {arXiv},
	Author = {{Bethge}, J. and {Yang}, H. and {Bartz}, C. and {Meinel}, C.},
	Date-Added = {2018-09-30 01:49:26 +0000},
	Date-Modified = {2018-09-30 01:49:44 +0000},
	Eprint = {1809.10463},
	Journal = {ArXiv e-prints},
	Keywords = {BNN;Computer Science; Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = sep,
	Title = {{Learning to Train a Binary Neural Network}},
	Year = 2018}

@article{2018JInst..13P7027D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018JInst..13P7027D},
	Archiveprefix = {arXiv},
	Author = {{Duarte}, J. and {Han}, S. and {Harris}, P. and {Jindariani}, S. and {Kreinar}, E. and {Kreis}, B. and {Ngadiuba}, J. and {Pierini}, M. and {Rivera}, R. and {Tran}, N. and {Wu}, Z.},
	Date-Added = {2018-09-30 01:22:32 +0000},
	Date-Modified = {2018-10-10 00:48:54 +0000},
	Doi = {10.1088/1748-0221/13/07/P07027},
	Eprint = {1804.06913},
	Journal = {Journal of Instrumentation},
	Keywords = {FPGA-Fin; NN; Physics},
	Month = jul,
	Pages = {P07027},
	Primaryclass = {physics.ins-det},
	Title = {{Fast inference of deep neural networks in FPGAs for particle physics}},
	Volume = 13,
	Year = 2018,
	Bdsk-Url-1 = {https://doi.org/10.1088/1748-0221/13/07/P07027}}

@inproceedings{8052915,
	Abstract = {As a popular deep learning technique, convolutional neural network has been widely used in many tasks such as image classification and object recognition. Convolutional neural network exploits spatial correlations in the images by performing convolution operations in local receptive fields. Convolutional neural networks are preferred over fully connected neural networks because they have fewer weights and are easier to train. Many research works have been conducted to reduce the computational complexity and memory requirements of convolutional neural network, to make it applicable to the low-power embedded applications with limited memories. This paper presents the architecture design of convolutional neural network with binary weights and activations, also known as binary neural network, on an FPGA platform. Weights and input activations are binarized with only two values, +1 and -1. This reduces all the fixed point multiplication operations in convolutional layers and fully connected layers to 1-bit XNOR operations. The proposed design uses only on-chip memories. Furthermore, an efficient implementation of batch normalization operation is introduced. When evaluating the CIFAR-10 benchmark, the proposed FPGA design can achieve a processing rate of 332,158 images per second with with accuracy of 86.06% using 1-bit quantized weights and activations.},
	Author = {Y. Zhou and S. Redkar and X. Huang},
	Booktitle = {2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)},
	Date-Added = {2018-09-29 09:57:58 +0000},
	Date-Modified = {2018-09-29 09:58:39 +0000},
	Doi = {10.1109/MWSCAS.2017.8052915},
	Keywords = {BNN;field programmable gate arrays;fixed point arithmetic;image classification;learning (artificial intelligence);neural nets;object recognition;quantisation (signal);1-bit XNOR operation;CIFAR-10 benchmark;FPGA platform;batch normalization operation;computational complexity reduction;convolutional neural network;deep learning binary neural network;fixed point multiplication operation;local receptive fields;low-power embedded applications;memory requirement reduction;on-chip memories;spatial correlation;Biological neural networks;Convolution;Field programmable gate arrays;Hardware;Memory management;Training},
	Month = {Aug},
	Pages = {281-284},
	Title = {Deep learning binary neural network on an FPGA},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/MWSCAS.2017.8052915}}

@article{5159360,
	Abstract = {Implementing linearly nonseparable Boolean functions (non-LSBF) has been an important and yet challenging task due to the extremely high complexity of this kind of functions and the exponentially increasing percentage of the number of non-LSBF in the entire set of Boolean functions as the number of input variables increases. In this paper, an algorithm named DNA-like learning and decomposing algorithm (DNA-like LDA) is proposed, which is capable of effectively implementing non-LSBF. The novel algorithm first trains the DNA-like offset sequence and decomposes non-LSBF into logic XOR operations of a sequence of LSBF, and then determines the weight-threshold values of the multilayer perceptron (MLP) that perform both the decompositions of LSBF and the function mapping the hidden neurons to the output neuron. The algorithm is validated by two typical examples about the problem of approximating the circular region and the well-known &lt;i&gt;n&lt;/i&gt; -bit parity Boolean function (PBF).},
	Author = {F. Chen and G. Chen and Q. He and G. He and X. Xu},
	Date-Added = {2018-09-29 09:56:02 +0000},
	Date-Modified = {2018-09-29 09:56:11 +0000},
	Doi = {10.1109/TNN.2009.2023122},
	Issn = {1045-9227},
	Journal = {IEEE Transactions on Neural Networks},
	Keywords = {BNN; Boolean functions;learning (artificial intelligence);multilayer perceptrons;binary neural network;linearly nonseparable Boolean functions;nonLSBF;DNA-like learning and decomposing algorithm;DNA-like LDA;DNA-like offset sequence;logic XOR operation;weight-threshold value;multilayer perceptron;function mapping;parity Boolean function;Neural networks;Boolean functions;Neurons;Multilayer perceptrons;Linear discriminant analysis;Logic;Backpropagation algorithms;Input variables;Mathematics;Binary neural network;DNA-like learning and decomposing algorithm (DNA-like LDA);linearly nonseparable Boolean function (non-LSBF);multilayer perceptron (MLP);parity Boolean function (PBF);Algorithms;Artificial Intelligence;DNA;Linear Models;Neural Networks (Computer)},
	Month = {Aug},
	Number = {8},
	Pages = {1293-1301},
	Title = {Universal Perceptron and DNA-Like Learning Algorithm for Binary Neural Networks: Non-LSBF Implementation},
	Volume = {20},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNN.2009.2023122}}

@inproceedings{7016218,
	Abstract = {This paper presents a new method in forecasting Philippine Peso to US Dollar exchange rate. Compared to the conventional way, in which the Philippine Dealing System (PDS), as monitored by the Central Bank, determines the rate by analysing demand and supply, the use of artificial neural network, having consumer price index, inflation rate, lending interest rate and purchasing power of the peso as the inputs is presented in this paper. Though foreign exchange rates vary on a daily basis, the output of this paper is prediction of the average foreign exchange rate every month. Artificial Neural Network serves as a powerful tool in forecasting Philippine Peso to US Dollar exchange rate not requiring expert knowledge in banking and finance thus letting the public gain access to a helpful beacon which is the foreign exchange rate. However, the accuracy of the forecast using artificial neural network is highly dependent on the volume of the training data, in this paper, an alternative algorithm that will increase the accuracy of the conventional artificial neural network with limited volume of training data is presented and analyze.},
	Author = {M. L. R. Torregoza and E. P. Dadios},
	Booktitle = {2014 International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment and Management (HNICEM)},
	Date-Added = {2018-09-29 02:58:04 +0000},
	Date-Modified = {2018-10-10 06:51:39 +0000},
	Doi = {10.1109/HNICEM.2014.7016218},
	Keywords = {Evolutionary;banking;forecasting theory;genetic algorithms;neural nets;pricing;hybrid genetic algorithm neural network;Philippine peso US dollar exchange rate forecasting;Philippine dealing system;PDS;central bank;demand and supply analysis;artificial neural network;consumer price index;interest rate;inflation rate;lending interest rate;purchasing power;foreign exchange rates;banking;training data;Exchange rates;Genetic algorithms;Artificial neural networks;Forecasting;Conferences;Economic indicators;Artificial Neural Network;forecasting;prediction;exchange rate;evolutionary algorithm},
	Month = {Nov},
	Pages = {1-5},
	Title = {Comparison of neural network and hybrid genetic algorithm-neural network in forecasting of Philippine Peso-US Dollar exchange rate},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/HNICEM.2014.7016218}}

@inproceedings{7838429,
	Abstract = {On-chip implementation of large-scale neural networks with emerging synaptic devices is attractive but challenging, primarily due to the pre-mature analog properties of today's resistive memory technologies. This work aims to realize a large-scale neural network using today's available binary RRAM devices for image recognition. We propose a methodology to binarize the neural network parameters with a goal of reducing the precision of weights and neurons to 1-bit for classification and &lt;;8-bit for online training. We experimentally demonstrate the binary neural network (BNN) on Tsinghua's 16 Mb RRAM macro chip fabricated in 130 nm CMOS process. Even under finite bit yield and endurance cycles, the system performance on MNIST handwritten digit dataset achieves ~96.5% accuracy for both classification and online training, close to ~97% accuracy by the ideal software implementation. This work reports the largest scale of the synaptic arrays and achieved the highest accuracy so far.},
	Author = {S. Yu and Z. Li and P. Chen and H. Wu and B. Gao and D. Wang and W. Wu and H. Qian},
	Booktitle = {2016 IEEE International Electron Devices Meeting (IEDM)},
	Date-Added = {2018-09-28 11:32:29 +0000},
	Date-Modified = {2018-09-28 11:32:37 +0000},
	Doi = {10.1109/IEDM.2016.7838429},
	Issn = {2156-017X},
	Keywords = {BNN; CMOS integrated circuits;electronic engineering computing;image recognition;neural nets;resistive RAM;MNIST handwritten digit dataset;CMOS process;Tsinghua;BNN;image recognition;binary RRAM macrochip device;resistive memory technology;pre-mature analog property;synaptic device;large-scale binary neural network;word length 1 bit;storage capacity 16 Mbit;size 130 nm},
	Month = {Dec},
	Pages = {16.2.1-16.2.4},
	Title = {Binary neural network with 16 Mb RRAM macro chip for classification and online training},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/IEDM.2016.7838429}}

@inproceedings{7033335,
	Author = {O. P. Patel and A. Tiwari},
	Booktitle = {2014 International Conference on Information Technology},
	Date-Added = {2018-09-28 10:26:33 +0000},
	Date-Modified = {2018-09-28 10:26:47 +0000},
	Doi = {10.1109/ICIT.2014.29},
	Keywords = {BNN; generalisation (artificial intelligence);learning (artificial intelligence);neural nets;optimisation;pattern classification;quantum computing;quantum based binary neural network learning algorithm;network structure optimisation;neurons;classification accuracy;hidden layer;training accuracy;generalization accuracy;QANN;Neurons;Accuracy;Training;Biological neural networks;Testing;Diabetes;Binary neural network;Quantum processing;Qubits;Back propagation learning},
	Month = {Dec},
	Pages = {270-274},
	Title = {Quantum Inspired Binary Neural Network Algorithm},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIT.2014.29}}

@inproceedings{1206405,
	Abstract = {This paper describes a 3D VLSI Chip for binary neural network classification applications. The 3D circuit includes three layers of MCM integrating 4 chips each making it a total of 12 chips integrated in a volume of (2 /spl times/ 2 /spl times/ 0.7)cm/sup 3/. The architecture is scalable, and real-time binary neural network classifier systems could be built with one, two or all twelve chip solutions. Each basic chip includes an on-chip control unit for programming options of the neural network topology and precision. The system is modular and presents easy expansibility without requiring extra devices. Experimental test results showed that a full recall operation is obtained in less than 1.2/spl mu/s for any topology with 4-bit or 8-bit precision while it is obtained in less than 2.2/spl mu/s for any 16-bit precision. As a consequence the 3D chip is a very powerful reconfigurable and a multiprecision neural chip exhibiting a significant speed of 1.25 GCPS.},
	Author = {A. Bermak},
	Booktitle = {Proceedings of the 2003 International Symposium on Circuits and Systems, 2003. ISCAS '03.},
	Date-Added = {2018-09-28 10:13:06 +0000},
	Date-Modified = {2018-12-06 14:26:04 +1300},
	Doi = {10.1109/ISCAS.2003.1206405},
	Keywords = {BNN; pattern classification;VLSI;multiprecision neural chip},
	Month = {May},
	Pages = {V-V},
	Title = {A highly scalable 3D chip for binary neural network classification applications},
	Volume = {5},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCAS.2003.1206405}}

@article{2009arXiv0904.4587T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2009arXiv0904.4587T},
	Archiveprefix = {arXiv},
	Author = {{Torres-Moreno}, J.-M. and {Gordon}, M.~B.},
	Date-Added = {2018-09-28 09:40:01 +0000},
	Date-Modified = {2018-09-28 09:40:24 +0000},
	Eprint = {0904.4587},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science; Artificial Intelligence; Neural and Evolutionary Computing},
	Month = apr,
	Primaryclass = {cs.AI},
	Title = {{Adaptive Learning with Binary Neurons}},
	Year = 2009}

@inproceedings{616215,
	Abstract = {An "evolutionary neural network (ENN)" is presented for the max cut problem of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem is to find a partition of V into two disjoint subsets such that the cut size be maximized. The cut size is the sum of weights on edges in E whose endpoints belong to different subsets. The ENN combines the evolutionary initialization scheme of the neural state into the energy minimization criteria of the binary neural network. The performance of ENN is evaluated through simulations in randomly weighted complete graphs and unweighted random graphs with up to 1000 vertices. The results show that the evolutionary initialization scheme drastically improves the solution quality. ENN can always find better solutions than the maximum neural network, the mean field annealing, the simulated annealing, and the greedy algorithm.},
	Author = {N. Funabiki and J. Kitamichi and S. Nishikawa},
	Booktitle = {Proceedings of International Conference on Neural Networks (ICNN'97)},
	Date-Added = {2018-09-28 09:23:32 +0000},
	Date-Modified = {2018-10-10 06:34:05 +0000},
	Doi = {10.1109/ICNN.1997.616215},
	Keywords = {Evolutionary; neural nets;genetic algorithms;set theory;graph theory;minimisation;computational complexity;evolutionary neural network algorithm;ENN;max cut problems;undirected graph;NP-hard problem;partition;disjoint subsets;evolutionary initialization scheme;energy minimization criteria;binary neural network;randomly weighted complete graphs;unweighted random graphs;maximum neural network;mean field annealing;simulated annealing;greedy algorithm;Neural networks;Neurons;Simulated annealing;NP-hard problem;Equations;Multi-layer neural network;Computer networks;Minimization;Greedy algorithms;Approximation algorithms},
	Month = {June},
	Pages = {1260-1265 vol.2},
	Title = {An evolutionary neural network algorithm for max cut problems},
	Volume = {2},
	Year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICNN.1997.616215}}

@inproceedings{4790104,
	Abstract = {Analysis of the state space for the fully-connected binary neural network ("the Hopfield model") remains an important objective in utilizing the network in pattern recognition and associative information retrieval. Most of the research pertaining to the network's state space so far concentrated on stable-state enumeration and often it was assumed that the patterns which are to be stored are random. We discuss the case of deterministic known codewords whose storage is required, and show that for this important case bounds on the retrieval probabilities and convergence rates can be achieved. The main tool which we employ is Birth-and-Death Markov chains, describing the Hamming distance of the network's state from the stored patterns. The results are applicable to both the asynchronous network and to the Boltzmann machine, and can be utilized to compare codeword sets in terms of efficiency of their retrieval, when the neural network is used as a content addressable memory.},
	Author = {M. Kam and R. Cheng and A. Guez},
	Booktitle = {1988 American Control Conference},
	Date-Added = {2018-09-28 09:05:13 +0000},
	Date-Modified = {2018-09-28 09:05:27 +0000},
	Doi = {10.23919/ACC.1988.4790104},
	Keywords = {BNN;State-space methods;Neural networks;Information analysis;Pattern analysis;Hopfield neural networks;Pattern recognition;Information retrieval;Convergence;Hamming distance;Content based retrieval},
	Month = {June},
	Pages = {2276-2281},
	Title = {On the State Space of the Binary Neural Network},
	Year = {1988},
	Bdsk-Url-1 = {https://doi.org/10.23919/ACC.1988.4790104}}

@article{2016arXiv160602580F,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160602580F},
	Archiveprefix = {arXiv},
	Author = {{Fernando}, C. and {Banarse}, D. and {Reynolds}, M. and {Besse}, F. and {Pfau}, D. and {Jaderberg}, M. and {Lanctot}, M. and {Wierstra}, D.},
	Date-Added = {2018-09-26 08:54:27 +0000},
	Date-Modified = {2018-10-10 07:00:44 +0000},
	Eprint = {1606.02580},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Computer Science; Neural; Evolutionary Computing; Computer Vision; Pattern Recognition; Machine Learning;},
	Month = jun,
	Title = {{Convolution by Evolution: Differentiable Pattern Producing Networks}},
	Year = 2016}

@article{2017arXiv170303864S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170303864S},
	Archiveprefix = {arXiv},
	Author = {{Salimans}, T. and {Ho}, J. and {Chen}, X. and {Sidor}, S. and {Sutskever}, I.},
	Date-Added = {2018-09-26 08:48:55 +0000},
	Date-Modified = {2019-01-11 17:05:39 +1300},
	Eprint = {1703.03864},
	Journal = {ArXiv e-prints},
	Keywords = {Evolutionary; Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing; OpenAI},
	Month = mar,
	Primaryclass = {stat.ML},
	Title = {{Evolution Strategies as a Scalable Alternative to Reinforcement Learning}},
	Year = 2017}

@article{2018arXiv180801974T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180801974T},
	Archiveprefix = {arXiv},
	Author = {{Tan}, C. and {Sun}, F. and {Kong}, T. and {Zhang}, W. and {Yang}, C. and {Liu}, C.},
	Date-Added = {2018-09-26 08:24:11 +0000},
	Date-Modified = {2018-10-10 00:49:11 +0000},
	Eprint = {1808.01974},
	Journal = {ArXiv e-prints},
	Keywords = {NN; Computer Science - Machine Learning, Statistics - Machine Learning},
	Month = aug,
	Title = {{A Survey on Deep Transfer Learning}},
	Year = 2018}

@article{2018arXiv180804752G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180804752G},
	Archiveprefix = {arXiv},
	Author = {{Guo}, Y.},
	Date-Added = {2018-09-26 07:51:55 +0000},
	Date-Modified = {2018-10-14 00:31:41 +0000},
	Eprint = {1808.04752},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Machine Learning, Neural and Evolutionary Computing, Statistics,Machine Learning},
	Month = aug,
	Title = {{A Survey on Methods and Theories of Quantized Neural Networks}},
	Year = 2018}

@article{2016arXiv161006918A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161006918A},
	Archiveprefix = {arXiv},
	Author = {{Abadi}, M. and {Andersen}, D.~G.},
	Date-Added = {2018-09-26 05:39:12 +0000},
	Date-Modified = {2019-01-11 15:27:35 +1300},
	Eprint = {1610.06918},
	Journal = {ArXiv e-prints},
	Keywords = {DCGAN; Computer Science - Cryptography and Security, Computer Science - Machine Learning, Cryptonet},
	Month = oct,
	Primaryclass = {cs.CR},
	Title = {{Learning to Protect Communications with Adversarial Neural Cryptography}},
	Year = 2016}

@article{2018arXiv180500728V,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180500728V},
	Archiveprefix = {arXiv},
	Author = {{Volz}, V. and {Schrum}, J. and {Liu}, J. and {Lucas}, S.~M. and {Smith}, A. and {Risi}, S.},
	Date-Added = {2018-09-26 05:35:11 +0000},
	Date-Modified = {2018-10-10 00:33:52 +0000},
	Eprint = {1805.00728},
	Journal = {ArXiv e-prints},
	Keywords = {DCGAN; Evolutionary; Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	Month = may,
	Primaryclass = {cs.AI},
	Title = {{Evolving Mario Levels in the Latent Space of a Deep Convolutional Generative Adversarial Network}},
	Year = 2018}

@article{2018arXiv180605695W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180605695W},
	Archiveprefix = {arXiv},
	Author = {{Wilson}, D.~G and {Cussat-Blanc}, S. and {Luga}, H. and {Miller}, J.~F},
	Date-Added = {2018-09-26 05:29:58 +0000},
	Date-Modified = {2018-09-26 05:29:58 +0000},
	Eprint = {1806.05695},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	Month = jun,
	Title = {{Evolving simple programs for playing Atari games}},
	Year = 2018}

@inproceedings{8393327,
	Author = {B. Yang and W. Zhang and L. Gong and H. Ma},
	Booktitle = {2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
	Date-Added = {2018-09-26 01:45:32 +0000},
	Date-Modified = {2018-10-10 07:02:14 +0000},
	Doi = {10.1109/FSKD.2017.8393327},
	Keywords = {Evolutionary; finance; exchange rates;forecasting theory;genetic algorithms;neural nets;stock markets;time series;trees (mathematics);CVFNT model;time series datasets;neural network;finance time series prediction;complex-valued flexible neural tree model;artificial bee colony;forecasting accuracy;genetic algorithm;Shanghai stock index;exchange rates;Neural networks;Time series analysis;Brain modeling;Predictive models;Forecasting;Data models;Indexes;evolutionary method;flexible neural tree;complex-valued;artificial bee colony},
	Month = {July},
	Pages = {54-58},
	Title = {Finance time series prediction using complex-valued flexible neural tree model},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/FSKD.2017.8393327}}

@inproceedings{5432472,
	Author = {L. Liu and M. Deng},
	Booktitle = {2010 Third International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2018-09-26 01:39:28 +0000},
	Date-Modified = {2019-06-05 21:20:37 +1200},
	Doi = {10.1109/WKDD.2010.148},
	Keywords = {Evolutionary; cancer;genetic algorithms;medical image processing;neural nets;artificial neural network;breast cancer;women;adaptive genetic algorithm;macro-search capability;global optimization;computational cost;Wisions breast cancer data set;Artificial neural networks;Breast cancer;Genetic algorithms;Genetic mutations;Flowcharts;Economic forecasting;Space technology;Data mining;Conference management;Knowledge management;adaptive genetic algorithm;neural network;weights and thresholds;breast cancer},
	Month = {Jan},
	Pages = {593-596},
	Title = {An Evolutionary Artificial Neural Network Approach for Breast Cancer Diagnosis},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/WKDD.2010.148}}

@inproceedings{Krizhevsky2999257,
	Acmid = {2999257},
	Address = {USA},
	Author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	Booktitle = {Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1},
	Date-Added = {2018-09-25 11:16:05 +0000},
	Date-Modified = {2018-12-01 13:40:11 +1300},
	Keywords = {CNN, ImageNet, ILSVRC, Convolution, AlexNet},
	Location = {Lake Tahoe, Nevada},
	Numpages = {9},
	Pages = {1097--1105},
	Publisher = {Curran Associates Inc.},
	Series = {NIPS'12},
	Title = {ImageNet Classification with Deep Convolutional Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2999134.2999257}}

@webpage{AIWinter-Andrey,
	Author = {Kurenkov, Andrey},
	Date-Added = {2018-09-25 10:57:03 +0000},
	Date-Modified = {2018-10-10 09:00:52 +0000},
	Keywords = {AI, Neural Networks, History},
	Lastchecked = {25-Sep-2018},
	Month = {September},
	Title = {A 'Brief' History of Neural Nets and Deep Learning},
	Url = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning},
	Urldate = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning},
	Year = {2018},
	Bdsk-Url-1 = {http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning}}

@article{Linnainmaa1976,
	Abstract = {The article describes analytic and algorithmic methods for determining the coefficients of the Taylor expansion of an accumulated rounding error with respect to the local rounding errors, and hence determining the influence of the local errors on the accumulated error. Second and higher order coefficients are also discussed, and some possible methods of reducing the extensive storage requirements are analyzed.},
	Author = {Linnainmaa, Seppo},
	Date-Added = {2018-09-25 08:37:46 +0000},
	Date-Modified = {2018-09-25 08:38:04 +0000},
	Day = {01},
	Doi = {10.1007/BF01931367},
	Issn = {1572-9125},
	Journal = {BIT Numerical Mathematics},
	Keywords = {NN, Neural Networks, Backpropagation},
	Month = {Jun},
	Number = {2},
	Pages = {146--160},
	Title = {Taylor expansion of the accumulated rounding error},
	Url = {https://doi.org/10.1007/BF01931367},
	Volume = {16},
	Year = {1976},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF01931367}}

@book{Minsky:1988:PEE:50066,
	Address = {Cambridge, MA, USA},
	Author = {Minsky, Marvin L. and Papert, Seymour A.},
	Date-Added = {2018-09-25 08:29:03 +0000},
	Date-Modified = {2018-09-25 08:29:27 +0000},
	Isbn = {0-262-63111-3},
	Keywords = {NN, Neural Networks},
	Publisher = {MIT Press},
	Title = {Perceptrons: Expanded Edition},
	Year = {1988},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB+Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgR0EtYmFzZWQgZmxleGlibGUgbGVhcm5pbmcgYWxnb3JpdGhtIHdpdGggZXJyb3IgdG9sZXJhbmNlIGZvciBkaWdpdGFsIGJpbmFyeSBuZXVyYWwgbmV0d29ya3MuYmliTxECvgAAAAACvgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0EgR0EtYmFzZWQgZmxleGlibCNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANCTk4AAAIAlC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpCTk46QSBHQS1iYXNlZCBmbGV4aWJsZSBsZWFybmluZyBhbGdvcml0aG0gd2l0aCBlcnJvciB0b2xlcmFuY2UgZm9yIGRpZ2l0YWwgYmluYXJ5IG5ldXJhbCBuZXR3b3Jrcy5iaWIADgDGAGIAQQAgAEcAQQAtAGIAYQBzAGUAZAAgAGYAbABlAHgAaQBiAGwAZQAgAGwAZQBhAHIAbgBpAG4AZwAgAGEAbABnAG8AcgBpAHQAaABtACAAdwBpAHQAaAAgAGUAcgByAG8AcgAgAHQAbwBsAGUAcgBhAG4AYwBlACAAZgBvAHIAIABkAGkAZwBpAHQAYQBsACAAYgBpAG4AYQByAHkAIABuAGUAdQByAGEAbAAgAG4AZQB0AHcAbwByAGsAcwAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAklVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvQk5OL0EgR0EtYmFzZWQgZmxleGlibGUgbGVhcm5pbmcgYWxnb3JpdGhtIHdpdGggZXJyb3IgdG9sZXJhbmNlIGZvciBkaWdpdGFsIGJpbmFyeSBuZXVyYWwgbmV0d29ya3MuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAKUAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADZw==}}

@inproceedings{6173439,
	Abstract = {Hardware/software partitioning is a crucial problem in hardware/software co-design. In this paper, we deeply investigate genetic algorithm (GA) for hardware/software partitioning, our co-design targets a heterogeneous multicore system on chip (SoC) which consists of several different types of processing engines(PE), Communicating structure adopts NOC, We use GA for four task graphs to simulate the hardware/software partitioning, experiments show our method is an effective hardware/software partitioning algorithm.},
	Author = {L. Luo and H. He and Q. Dou and W. Xu},
	Booktitle = {2012 Second International Conference on Intelligent System Design and Engineering Application},
	Date-Added = {2018-09-09 11:15:43 +0000},
	Date-Modified = {2018-09-09 11:16:08 +0000},
	Doi = {10.1109/ISdea.2012.501},
	Keywords = {Evolutionary, genetic algorithms;graph theory;hardware-software codesign;multiprocessing systems;system-on-chip;hardware-software partitioning;genetic algorithm;hardware-software codesign;heterogeneous multicore system on chip;processing engines;communicating structure;NOC;task graphs;Software;Hardware;Partitioning algorithms;Software algorithms;Genetic algorithms;Heuristic algorithms;System-on-a-chip;Hardware/Software Partitioning;Genetic Algorithm(GA);Heterogeneous Multicore SOC},
	Month = {Jan},
	Pages = {1267-1270},
	Title = {Hardware/Software Partitioning for Heterogeneous Multicore SoC Using Genetic Algorithm},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISdea.2012.501}}

@inproceedings{Zhang:2016:ECI:2934583.2934644,
	Acmid = {2934644},
	Address = {New York, NY, USA},
	Author = {Zhang, Chen and Wu, Di and Sun, Jiayu and Sun, Guangyu and Luo, Guojie and Cong, Jason},
	Booktitle = {Proceedings of the 2016 International Symposium on Low Power Electronics and Design},
	Date-Added = {2018-08-25 11:20:53 +0000},
	Date-Modified = {2018-09-26 07:26:01 +0000},
	Doi = {10.1145/2934583.2934644},
	Isbn = {978-1-4503-4185-1},
	Keywords = {CNN-FPGA, FPGA, CNN, Pipeline, Cluster},
	Location = {San Francisco Airport, CA, USA},
	Numpages = {6},
	Pages = {326--331},
	Publisher = {ACM},
	Series = {ISLPED '16},
	Title = {Energy-Efficient CNN Implementation on a Deeply Pipelined FPGA Cluster},
	Url = {http://doi.acm.org/10.1145/2934583.2934644},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2934583.2934644},
	Bdsk-Url-2 = {https://doi.org/10.1145/2934583.2934644}}

@article{2017arXiv170402019C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170402019C},
	Archiveprefix = {arXiv},
	Author = {{Chaudhuri}, R. and {Fiete}, I.},
	Date-Added = {2018-07-09 10:12:02 +0000},
	Date-Modified = {2018-07-09 10:12:09 +0000},
	Eprint = {1704.02019},
	Journal = {ArXiv e-prints},
	Keywords = {BNN, Quantitative Biology - Neurons and Cognition, Computer Science - Neural and Evolutionary Computing},
	Month = apr,
	Primaryclass = {q-bio.NC},
	Title = {{Associative content-addressable networks with exponentially many robust stable states}},
	Year = 2017}

@article{32008,
	Author = {M. Verleysen and B. Sirletti and A. M. Vandemeulebroecke and P. G. A. Jespers},
	Date-Added = {2018-07-09 10:11:47 +0000},
	Date-Modified = {2018-07-09 10:11:54 +0000},
	Doi = {10.1109/4.32008},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {BNN, VLSI;computerised pattern recognition;content-addressable storage;integrated memory circuits;learning systems;neural nets;programming;CAM;Hopfield neural network;VLSI circuit;analogue VLSI implementation;content-addressable memory;fully interconnected neural network;learning algorithm;pattern-recognition applications;programming;storage capacity;synapse weights;synaptic cells;Artificial intelligence;Artificial neural networks;Biological neural networks;Biology computing;Circuits;Hopfield neural networks;Neural networks;Neurons;Pattern recognition;Very large scale integration},
	Month = {Jun},
	Number = {3},
	Pages = {562-569},
	Title = {Neural networks for high-storage content-addressable memory: VLSI circuit and learning algorithm},
	Volume = {24},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1109/4.32008}}

@article{Brodsky:93,
	Abstract = {The content-addressable network (CAN) is an efficient, intrinsically discrete training algorithm for binary-valued classification networks. The binary nature of the CAN network permits accelerated learning and significantly reduced hardware-implementation requirements. A multilayer optoelectronic CAN network employing matrix--vector multiplication was constructed. The network learned and correctly classified trained patterns, gaining a measure of fault tolerance by learning associative solutions to optical hardware imperfections. Operation of this system is possible owing to the reduced hardware accuracy requirements of the CAN learning algorithm.},
	Author = {Stephen A. Brodsky and Gary C. Marsden and Clark C. Guest},
	Date-Added = {2018-07-05 08:41:33 +0000},
	Date-Modified = {2018-07-05 08:41:42 +0000},
	Doi = {10.1364/AO.32.001338},
	Journal = {Appl. Opt.},
	Keywords = {BNN, Cylindrical lenses; Light valves; Neural networks; Optical components; Optical neural systems; Parallel processing},
	Month = {Mar},
	Number = {8},
	Pages = {1338--1345},
	Publisher = {OSA},
	Title = {Optical matrix--vector implementation of the content-addressable network},
	Url = {http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338},
	Volume = {32},
	Year = {1993},
	Bdsk-Url-1 = {http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338},
	Bdsk-Url-2 = {https://doi.org/10.1364/AO.32.001338}}

@article{LMNN-UoE-UK,
	Adsurl = {http://homepages.inf.ed.ac.uk/rbf/EUCOGNITION/BRIEFINGS/billings.pdf},
	Archiveprefix = {UoE-UK},
	Author = {{Billings}, Guy},
	Date-Added = {2018-06-22 10:47:53 +0000},
	Date-Modified = {2018-06-22 10:47:53 +0000},
	Eprint = {1502.04390},
	Journal = {Euroinformatics Doctoral Training Centre},
	Keywords = {NN, Learning, Numerical Analysis},
	Primaryclass = {cs.LG},
	Title = {{Learning and Memory in Neural Networks}},
	Year = 2004}

@article{PETERSON1989475,
	Author = {Carsten Peterson and Eric Hartman},
	Date-Added = {2018-06-22 10:47:31 +0000},
	Date-Modified = {2018-12-06 13:49:53 +1300},
	Doi = {https://doi.org/10.1016/0893-6080(89)90045-2},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {NN, Neural network, Bidirectional, Generalization, Content addressable memory, Mean field theory, Learning algorithm},
	Number = {6},
	Pages = {475 - 494},
	Title = {Explorations of the mean field theory learning algorithm},
	Url = {http://www.sciencedirect.com/science/article/pii/0893608089900452},
	Volume = {2},
	Year = {1989},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/0893608089900452},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(89)90045-2}}

@article{2018arXiv180305900V,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180305900V},
	Archiveprefix = {arXiv},
	Author = {{Venieris}, S.~I. and {Kouris}, A. and {Bouganis}, C.-S.},
	Date-Added = {2018-06-22 10:47:07 +0000},
	Date-Modified = {2018-06-22 10:47:16 +0000},
	Eprint = {1803.05900},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA-NN, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Learning},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{Toolflows for Mapping Convolutional Neural Networks on FPGAs: A Survey and Future Directions}},
	Year = 2018}

@inproceedings{Suda:2016:TOF:2847263.2847276,
	Acmid = {2847276},
	Address = {New York, NY, USA},
	Author = {Suda, Naveen and Chandra, Vikas and Dasika, Ganesh and Mohanty, Abinash and Ma, Yufei and Vrudhula, Sarma and Seo, Jae-sun and Cao, Yu},
	Booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-06-22 10:45:58 +0000},
	Date-Modified = {2018-06-22 10:46:11 +0000},
	Doi = {10.1145/2847263.2847276},
	Isbn = {978-1-4503-3856-1},
	Keywords = {FPGA-NN, convolutional neural networks, fpga, opencl, optimization},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {16--25},
	Publisher = {ACM},
	Series = {FPGA '16},
	Title = {Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks},
	Url = {http://doi.acm.org/10.1145/2847263.2847276},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2847263.2847276},
	Bdsk-Url-2 = {https://doi.org/10.1145/2847263.2847276}}

@inproceedings{7760779,
	Author = {Wenlai Zhao and Haohuan Fu and W. Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	Booktitle = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Date-Added = {2018-06-22 10:44:51 +0000},
	Date-Modified = {2018-09-26 08:03:08 +0000},
	Doi = {10.1109/ASAP.2016.7760779},
	Keywords = {FPGA-NN, field programmable gate arrays;floating point arithmetic;neural nets;32bit floating-point arithmetic;FPGA-based framework;bandwidth resources;convolutional neural networks;hardware resources;streaming datapath;Bandwidth;Computational modeling;Convolution;Field programmable gate arrays;Neural networks;Runtime;Training},
	Month = {July},
	Pages = {107-114},
	Title = {F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASAP.2016.7760779}}

@inproceedings{374468,
	Author = {A. V. Krishnamoorthy and S. A. Brodsky and C. C. Guest and G. C. Marsden and M. Blume and G. Yayla and J. Merckle and S. C. Esener},
	Booktitle = {Neural Networks, 1994. IEEE World Congress on Computational Intelligence., 1994 IEEE International Conference on},
	Date-Added = {2018-06-22 10:44:13 +0000},
	Date-Modified = {2018-09-26 08:04:11 +0000},
	Doi = {10.1109/ICNN.1994.374468},
	Keywords = {BNN, learning (artificial intelligence);neural chips;neural net architecture;optical neural nets;3D optoelectronic neural system;D-STOP system;Accelerated Learning;connectivity;content addressable network learning algorithm;discrete learning algorithm;dual-scale topology optoelectronic processor neural network;fully-parallel neural networks;generic gradient-descent learning rules;hardware efficient learning;optoelectronic hardware tradeoffs;scalable optically interconnected neural network architecture;Backpropagation algorithms;Hardware;Integrated circuit interconnections;Neural networks;Neurons;Neurotransmitters;Optical interconnections;Optical network units;Optical transmitters;Power system interconnection},
	Month = {Jun},
	Pages = {1998-2003 vol.3},
	Title = {Hardware efficient learning on a 3-D optoelectronic neural system},
	Volume = {3},
	Year = {1994},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICNN.1994.374468}}

@inproceedings{226962,
	Author = {S. A. Brodsky and C. C. Guest},
	Booktitle = {[Proceedings 1992] IJCNN International Joint Conference on Neural Networks},
	Date-Added = {2018-06-22 10:43:49 +0000},
	Date-Modified = {2018-06-22 10:43:58 +0000},
	Doi = {10.1109/IJCNN.1992.226962},
	Keywords = {BNN, backpropagation;content-addressable storage;optical neural nets;VLSI;backpropagation;content addressable networks;discrete mappings;error-free solution;fast convergence rate;initialization;zero error solutions;Backpropagation algorithms;Computer errors;Costs;Error correction;Hardware;Optical computing;Optical devices;Optical fiber networks;Optical network units;Very large scale integration},
	Month = {Jun},
	Pages = {352-357 vol.2},
	Title = {Content addressable networks for initialization of backpropagation with zero error solutions},
	Volume = {2},
	Year = {1992},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBtLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS1OTi9GLUNOTi0gQW4gRlBHQS1iYXNlZCBmcmFtZXdvcmsgZm9yIHRyYWluaW5nIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYk8RAngAAAAAAngAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9GLUNOTi0gQW4gRlBHQS1iYXMjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAHRlBHQS1OTgAAAgCDLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkZQR0EtTk46Ri1DTk4tIEFuIEZQR0EtYmFzZWQgZnJhbWV3b3JrIGZvciB0cmFpbmluZyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4AnABNAEYALQBDAE4ATgAtACAAQQBuACAARgBQAEcAQQAtAGIAYQBzAGUAZAAgAGYAcgBhAG0AZQB3AG8AcgBrACAAZgBvAHIAIAB0AHIAYQBpAG4AaQBuAGcAIABDAG8AbgB2AG8AbAB1AHQAaQBvAG4AYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAIFVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0ZQR0EtTk4vRi1DTk4tIEFuIEZQR0EtYmFzZWQgZnJhbWV3b3JrIGZvciB0cmFpbmluZyBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAJQAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADEA==},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1992.226962}}

@inproceedings{5726804,
	Author = {S. A. Brodsky and C. C. Guest},
	Booktitle = {1990 IJCNN International Joint Conference on Neural Networks},
	Date-Added = {2018-06-22 10:43:21 +0000},
	Date-Modified = {2018-06-22 10:43:33 +0000},
	Doi = {10.1109/IJCNN.1990.137846},
	Keywords = {BNN, content-addressable storage;learning systems;neural nets;arbitrary bit-level significance;binary backpropagation;bit connection weights;content addressable memory;continuous backpropagation network learning model;local computation;pseudoanalog extension},
	Month = {June},
	Pages = {205-210 vol.3},
	Title = {Binary backpropagation in content addressable memory},
	Year = {1990},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.1990.137846}}

@article{31325,
	Author = {M. Verleysen and B. Sirletti and A. Vandemeulebroecke and P. G. A. Jespers},
	Date-Added = {2018-06-22 10:42:54 +0000},
	Date-Modified = {2018-06-22 10:43:03 +0000},
	Doi = {10.1109/31.31325},
	Issn = {0098-4094},
	Journal = {IEEE Transactions on Circuits and Systems},
	Keywords = {BNN, CMOS integrated circuits;VLSI;content-addressable storage;neural nets;Hopfield neural network;VLSI;content-addressable memory;fully interconnected neural network;high-storage capacity;implementation;learning algorithm;neural networks;optimization;pattern recognition;programming algorithm;retrieval capabilities;small area;speed capabilities;CADCAM;Cams;Computer aided manufacturing;Content based retrieval;Hopfield neural networks;Integrated circuit interconnections;Neural networks;Neurons;Pattern recognition;Very large scale integration},
	Month = {May},
	Number = {5},
	Pages = {762-766},
	Title = {A high-storage capacity content-addressable memory and its learning algorithm},
	Volume = {36},
	Year = {1989},
	Bdsk-Url-1 = {https://doi.org/10.1109/31.31325}}

@article{2014arXiv1409.5185L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.5185L},
	Archiveprefix = {arXiv},
	Author = {{Lee}, C.-Y. and {Xie}, S. and {Gallagher}, P. and {Zhang}, Z. and {Tu}, Z.},
	Date-Added = {2018-06-11 04:57:33 +0000},
	Date-Modified = {2018-06-11 04:57:33 +0000},
	Eprint = {1409.5185},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = sep,
	Primaryclass = {stat.ML},
	Title = {{Deeply-Supervised Nets}},
	Year = 2014}

@article{2013arXiv1306.0239T,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1306.0239T},
	Archiveprefix = {arXiv},
	Author = {{Tang}, Y.},
	Date-Added = {2018-06-11 04:57:10 +0000},
	Date-Modified = {2018-06-11 04:57:10 +0000},
	Eprint = {1306.0239},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Deep Learning using Linear Support Vector Machines}},
	Year = 2013}

@article{2016arXiv160504711L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160504711L},
	Archiveprefix = {arXiv},
	Author = {{Li}, F. and {Zhang}, B. and {Liu}, B.},
	Date-Added = {2018-06-11 04:56:39 +0000},
	Date-Modified = {2018-06-11 04:56:39 +0000},
	Eprint = {1605.04711},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{Ternary Weight Networks}},
	Year = 2016}

@article{BALDOMINOS201838,
	Author = {Alejandro Baldominos and Yago Saez and Pedro Isasi},
	Date-Added = {2018-06-02 05:38:07 +0000},
	Date-Modified = {2018-06-02 05:38:25 +0000},
	Doi = {https://doi.org/10.1016/j.neucom.2017.12.049},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {CNN, Neuroevolution, Evolutionary algorithms, Convolutional neural networks, Automatic topology design, Genetic algorithms, Grammatical evolution},
	Pages = {38 - 52},
	Title = {Evolutionary convolutional neural networks: An application to handwriting recognition},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231217319112},
	Volume = {283},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217319112},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.12.049}}

@article{FERREIRA2018205,
	Author = {Martha Dais Ferreira and D{\'e}bora Cristina Corr{\^e}a and Luis Gustavo Nonato and Rodrigo Fernandes de Mello},
	Date-Added = {2018-06-02 05:37:45 +0000},
	Date-Modified = {2018-06-02 05:37:53 +0000},
	Doi = {https://doi.org/10.1016/j.eswa.2017.10.052},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {CNN, Convolutional neural network, Architecture assessment, Dynamical systems, Handwritten digit recognition, Face recognition, Object recognition},
	Pages = {205 - 217},
	Title = {Designing architectures of convolutional neural networks to solve practical problems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417417307340},
	Volume = {94},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417307340},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.10.052}}

@article{LI2018154,
	Author = {Guoqi Li and Lei Deng and Lei Tian and Haotian Cui and Wentao Han and Jing Pei and Luping Shi},
	Date-Added = {2018-06-02 05:36:48 +0000},
	Date-Modified = {2018-06-02 05:37:00 +0000},
	Doi = {https://doi.org/10.1016/j.neucom.2017.06.058},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {BNN, Deep learning, Neural network applications, Discrete state transition, Discrete weight space},
	Pages = {154 - 162},
	Title = {Training deep neural networks with discrete state transition},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231217311864},
	Volume = {272},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217311864},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.06.058}}

@article{LIANG20181072,
	Author = {Shuang Liang and Shouyi Yin and Leibo Liu and Wayne Luk and Shaojun Wei},
	Date-Added = {2018-06-02 05:36:24 +0000},
	Date-Modified = {2018-06-02 05:36:30 +0000},
	Doi = {https://doi.org/10.1016/j.neucom.2017.09.046},
	Issn = {0925-2312},
	Journal = {Neurocomputing},
	Keywords = {BNN, Binarized neural network, Hardware accelerator, FPGA},
	Pages = {1072 - 1086},
	Title = {FP-BNN: Binarized neural network on FPGA},
	Url = {http://www.sciencedirect.com/science/article/pii/S0925231217315655},
	Volume = {275},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0925231217315655},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neucom.2017.09.046}}

@article{SHOEMAKER1991231,
	Author = {Patrick A. Shoemaker and Michael J. Carlin and Randy L. Shimabukuro},
	Date-Added = {2018-06-02 05:35:50 +0000},
	Date-Modified = {2018-06-02 05:35:58 +0000},
	Doi = {https://doi.org/10.1016/0893-6080(91)90007-R},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {BNN, Neural networks, Learning algorithms, Back propagation, Trinary, VLSI implementations, Nonvolatile weights},
	Number = {2},
	Pages = {231 - 241},
	Title = {Back propagation learning with trinary quantization of weight updates},
	Url = {http://www.sciencedirect.com/science/article/pii/089360809190007R},
	Volume = {4},
	Year = {1991},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/089360809190007R},
	Bdsk-Url-2 = {https://doi.org/10.1016/0893-6080(91)90007-R}}

@article{DENG201849,
	Author = {Lei Deng and Peng Jiao and Jing Pei and Zhenzhi Wu and Guoqi Li},
	Date-Added = {2018-06-02 05:35:28 +0000},
	Date-Modified = {2018-06-02 05:35:34 +0000},
	Doi = {https://doi.org/10.1016/j.neunet.2018.01.010},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {BNN, GXNOR-Net, Discrete state transition, Ternary neural networks, Sparse binary networks},
	Pages = {49 - 58},
	Title = {GXNOR-Net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework},
	Url = {http://www.sciencedirect.com/science/article/pii/S0893608018300108},
	Volume = {100},
	Year = {2018},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0893608018300108},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2018.01.010}}

@article{2017arXiv170300810S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170300810S},
	Archiveprefix = {arXiv},
	Author = {{Shwartz-Ziv}, R. and {Tishby}, N.},
	Date-Added = {2018-06-02 03:06:54 +0000},
	Date-Modified = {2018-06-02 03:07:05 +0000},
	Eprint = {1703.00810},
	Journal = {ArXiv e-prints},
	Keywords = {DNN, Computer Science - Learning},
	Month = mar,
	Primaryclass = {cs.LG},
	Title = {{Opening the Black Box of Deep Neural Networks via Information}},
	Year = 2017}

@article{2016arXiv160606160Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160606160Z},
	Archiveprefix = {arXiv},
	Author = {{Zhou}, S. and {Wu}, Y. and {Ni}, Z. and {Zhou}, X. and {Wen}, H. and {Zou}, Y.},
	Date-Added = {2018-05-30 01:32:32 +0000},
	Date-Modified = {2018-08-31 09:56:59 +0000},
	Eprint = {1606.06160},
	Journal = {ArXiv e-prints},
	Keywords = {BNN, Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = jun,
	Title = {{DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients}},
	Year = 2016}

@article{2017arXiv170602379L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170602379L},
	Annote = {Same paper as:

Towards a Deeper Understanding of Training Quantized Neural Networks
Published in https://www.padl.ws
Principled Approaches to Deep Learning
ICML 2017, Sydney, Australia
August 10, 2017
https://www.padl.ws/papers/Paper%2016.pdf},
	Archiveprefix = {arXiv},
	Author = {{Li}, H. and {De}, S. and {Xu}, Z. and {Studer}, C. and {Samet}, H. and {Goldstein}, T.},
	Date-Added = {2018-05-30 01:03:06 +0000},
	Date-Modified = {2018-11-28 15:14:52 +1300},
	Eprint = {1706.02379},
	Journal = {ArXiv e-prints},
	Keywords = {DNN, Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Training Quantized Nets: A Deeper Understanding}},
	Year = 2017}

@techreport{abdelouahab:hal-01695375,
	Author = {ABDELOUAHAB, Kamel and Pelcat, Maxime and Berry, Fran{\c c}ois and S{\'e}rot, Jocelyn},
	Date-Added = {2018-05-28 11:51:04 +0000},
	Date-Modified = {2018-05-28 11:51:35 +0000},
	Hal_Id = {hal-01695375},
	Hal_Version = {v2},
	Institution = {{Universit{\'e} Clermont Auvergne ; Institut Pascal, Clermont Ferrand ; IETR/INSA Rennes}},
	Keywords = {FPGA-NN, FPGA, CNN, HDL, Hardware},
	Month = Jan,
	Pdf = {https://hal.archives-ouvertes.fr/hal-01695375/file/hal-accelerating-cnn.pdf},
	Title = {{Accelerating CNN inference on FPGAs: A Survey}},
	Type = {Research Report},
	Url = {https://hal.archives-ouvertes.fr/hal-01695375},
	Year = {2018},
	Bdsk-Url-1 = {https://hal.archives-ouvertes.fr/hal-01695375}}

@article{HASSABIS2017245,
	Author = {Demis Hassabis and Dharshan Kumaran and Christopher Summerfield and Matthew Botvinick},
	Date-Added = {2018-05-25 05:39:29 +0000},
	Date-Modified = {2018-05-25 05:39:29 +0000},
	Doi = {https://doi.org/10.1016/j.neuron.2017.06.011},
	Issn = {0896-6273},
	Journal = {Neuron},
	Keywords = {artificial intelligence, brain, cognition, neural network, learning},
	Number = {2},
	Pages = {245 - 258},
	Title = {Neuroscience-Inspired Artificial Intelligence},
	Url = {http://www.sciencedirect.com/science/article/pii/S0896627317305093},
	Volume = {95},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0896627317305093},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neuron.2017.06.011}}

@article{2016arXiv161200796K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200796K},
	Archiveprefix = {arXiv},
	Author = {{Kirkpatrick}, J. and {Pascanu}, R. and {Rabinowitz}, N. and {Veness}, J. and {Desjardins}, G. and {Rusu}, A.~A. and {Milan}, K. and {Quan}, J. and {Ramalho}, T. and {Grabska-Barwinska}, A. and {Hassabis}, D. and {Clopath}, C. and {Kumaran}, D. and {Hadsell}, R.},
	Date-Added = {2018-05-25 05:39:08 +0000},
	Date-Modified = {2018-05-25 05:39:08 +0000},
	Eprint = {1612.00796},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Overcoming catastrophic forgetting in neural networks}},
	Year = 2016}

@article{2015arXiv151004189N,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151004189N},
	Archiveprefix = {arXiv},
	Author = {{N{\o}kland}, A.},
	Date-Added = {2018-05-21 00:54:01 +0000},
	Date-Modified = {2018-05-21 00:54:01 +0000},
	Eprint = {1510.04189},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = oct,
	Primaryclass = {stat.ML},
	Title = {{Improving Back-Propagation by Adding an Adversarial Gradient}},
	Year = 2015}

@article{2015arXiv150303562C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150303562C},
	Archiveprefix = {arXiv},
	Author = {{Cheng}, Z. and {Soudry}, D. and {Mao}, Z. and {Lan}, Z.},
	Date-Added = {2018-05-19 11:32:04 +0000},
	Date-Modified = {2018-10-09 23:48:40 +0000},
	Eprint = {1503.03562},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	Month = mar,
	Title = {{Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation}},
	Year = 2015}

@article{2016arXiv161103530Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161103530Z},
	Archiveprefix = {arXiv},
	Author = {{Zhang}, C. and {Bengio}, S. and {Hardt}, M. and {Recht}, B. and {Vinyals}, O.},
	Date-Added = {2018-05-19 05:48:30 +0000},
	Date-Modified = {2018-05-19 05:48:30 +0000},
	Eprint = {1611.03530},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Understanding deep learning requires rethinking generalization}},
	Year = 2016}

@article{2017arXiv170107875A,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107875A},
	Archiveprefix = {arXiv},
	Author = {{Arjovsky}, M. and {Chintala}, S. and {Bottou}, L.},
	Date-Added = {2018-05-19 04:48:30 +0000},
	Date-Modified = {2018-05-19 04:48:30 +0000},
	Eprint = {1701.07875},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = jan,
	Primaryclass = {stat.ML},
	Title = {{Wasserstein GAN}},
	Year = 2017}

@article{2015arXiv151202479M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151202479M},
	Archiveprefix = {arXiv},
	Author = {{Montavon}, G. and {Bach}, S. and {Binder}, A. and {Samek}, W. and {M{\"u}ller}, K.-R.},
	Date-Added = {2018-05-16 10:38:31 +0000},
	Date-Modified = {2018-05-16 10:38:31 +0000},
	Eprint = {1512.02479},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Explaining NonLinear Classification Decisions with Deep Taylor Decomposition}},
	Year = 2015}

@article{journal.pone.0130140,
	Abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
	Author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Gr{\'e}goire AND Klauschen, Frederick AND M{\"u}ller, Klaus-Robert AND Samek, Wojciech},
	Date-Added = {2018-05-16 09:58:57 +0000},
	Date-Modified = {2018-05-16 09:59:52 +0000},
	Doi = {10.1371/journal.pone.0130140},
	Journal = {PLOS ONE},
	Keywords = {DNN, Taylor Series, MNIST, ImageNet, Pixel-wise},
	Month = {07},
	Number = {7},
	Pages = {1-46},
	Publisher = {Public Library of Science},
	Title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
	Url = {https://doi.org/10.1371/journal.pone.0130140},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1371/journal.pone.0130140}}

@article{kim51bitwise,
	Author = {Kim, Minje and Smaragdis, Paris},
	Date-Added = {2018-05-16 00:48:16 +0000},
	Date-Modified = {2018-10-10 08:59:49 +0000},
	Journal = {Urbana},
	Keywords = {BNN, QaD, IBM, Signals, Denoise},
	Pages = {61801},
	Title = {Bitwise Neural Networks for Efficient Single-Channel Source Separation},
	Volume = {51},
	Year = {2018}}

@article{2018arXiv180409154G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180409154G},
	Archiveprefix = {arXiv},
	Author = {{Giacomello}, E. and {Lanzi}, P.~L. and {Loiacono}, D.},
	Date-Added = {2018-05-15 11:53:51 +0000},
	Date-Modified = {2018-05-15 11:53:51 +0000},
	Eprint = {1804.09154},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Human-Computer Interaction, Statistics - Machine Learning},
	Month = apr,
	Primaryclass = {cs.LG},
	Title = {{DOOM Level Generation using Generative Adversarial Networks}},
	Year = 2018,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCBLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vT24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliTxECzAAAAAACzAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H09uIFBpeGVsLVdpc2UgRXhwbCNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAJcvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46T24gUGl4ZWwtV2lzZSBFeHBsYW5hdGlvbnMgZm9yIE5vbi1MaW5lYXIgQ2xhc3NpZmllciBEZWNpc2lvbnMgYnkgTGF5ZXItV2lzZSBSZWxldmFuY2UgUHJvcGFnYXRpb24uYmliAAAOAM4AZgBPAG4AIABQAGkAeABlAGwALQBXAGkAcwBlACAARQB4AHAAbABhAG4AYQB0AGkAbwBuAHMAIABmAG8AcgAgAE4AbwBuAC0ATABpAG4AZQBhAHIAIABDAGwAYQBzAHMAaQBmAGkAZQByACAARABlAGMAaQBzAGkAbwBuAHMAIABiAHkAIABMAGEAeQBlAHIALQBXAGkAcwBlACAAUgBlAGwAZQB2AGEAbgBjAGUAIABQAHIAbwBwAGEAZwBhAHQAaQBvAG4ALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAJVVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL09uIFBpeGVsLVdpc2UgRXhwbGFuYXRpb25zIGZvciBOb24tTGluZWFyIENsYXNzaWZpZXIgRGVjaXNpb25zIGJ5IExheWVyLVdpc2UgUmVsZXZhbmNlIFByb3BhZ2F0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAqAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAN4},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OL05vbi1sb2NhbCBOZXVyYWwgTmV0d29ya3MuYmliTxEBrAAAAAABrAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HU5vbi1sb2NhbCBOZXVyYWwgTmV0d29ya3MuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAANDTk4AAAIATy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk46Tm9uLWxvY2FsIE5ldXJhbCBOZXR3b3Jrcy5iaWIAAA4APAAdAE4AbwBuAC0AbABvAGMAYQBsACAATgBlAHUAcgBhAGwAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAE1Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NOTi9Ob24tbG9jYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAYAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIQ}}

@inproceedings{5234726,
	Author = {X. Chen and Q. Ma and T. Alkharobi},
	Booktitle = {2009 2nd IEEE International Conference on Computer Science and Information Technology},
	Date-Added = {2018-05-15 11:27:59 +0000},
	Date-Modified = {2018-05-15 11:27:59 +0000},
	Doi = {10.1109/ICCSIT.2009.5234726},
	Keywords = {Fourier series;data mining;pattern clustering;radial basis function networks;Fourier component neural network;Gauss series clustering neural network;Taylor component neural network;Taylor series;mining;radial basis function neuron;Computer science;Educational institutions;Electronic mail;Gaussian processes;Information science;Input variables;Neural networks;Neurons;Taylor series;Transfer functions;Fourier component neural network;Gauss series Clustering neural network;Taylor component neural network;Taylor series neural network;prediction;stock price},
	Month = {Aug},
	Pages = {291-294},
	Title = {New neural networks based on Taylor series and their research},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCSIT.2009.5234726}}

@inproceedings{li2013,
	Abstract = {This study adopts popular back-propagation neural network to make one-period-ahead prediction of the stock price. A model based on Taylor series by using both fundamental and technical indicators EPS and MACD as input data is built for an empirical study. Leading Taiwanese companies in non-hi-tech industry such as Formosa Plastics, Yieh Phui Steel, Evergreen Marine, and Chang Hwa Bank are picked as targets to analyze their reasonable prices and moving trends. The performance of this model shows remarkable return and high accuracy in making long/short strategies.},
	Author = {Li, Jung Bin and Wu, Chien Ho},
	Booktitle = {Innovation for Applied Science and Technology},
	Date-Added = {2018-05-15 11:27:42 +0000},
	Date-Modified = {2019-04-29 12:00:28 +1200},
	Doi = {10.4028/www.scientific.net/AMM.284-287.3020},
	Keywords = {NN-Fin; Neural Network (NN), BPN, Taylor Series, Price Forecast},
	Month = {3},
	Pages = {3020--3024},
	Publisher = {Trans Tech Publications},
	Series = {Applied Mechanics and Materials},
	Title = {An Efficient Neural Network Model with Taylor Series-Based Data Pre-Processing for Stock Price Forecast},
	Volume = {284},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.4028/www.scientific.net/AMM.284-287.3020}}

@article{2016arXiv160205897D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160205897D},
	Archiveprefix = {arXiv},
	Author = {{Daniely}, A. and {Frostig}, R. and {Singer}, Y.},
	Date-Added = {2018-05-15 11:27:25 +0000},
	Date-Modified = {2018-05-15 11:27:25 +0000},
	Eprint = {1602.05897},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Statistics - Machine Learning},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity}},
	Year = 2016}

@article{10.1002-mma.2641,
	Abstract = {This paper focuses on learning algorithms for approximating functional data that are chosen from some Hilbert spaces. An effective algorithm, called Hilbert parallel overrelaxation backpropagation (HPORBP) algorithm, is proposed for training the Hilbert feedforward neural networks that are extensions of feedforward neural networks from Euclidean space to some Hilbert spaces. Furthermore, the convergence of the iterative HPORBP algorithm is analyzed, and a deterministic convergence theorem is proposed for the HPORBP algorithm on the basis of the perturbation results of Mangasarian and Solodov. Some experimental results of learning functional data on some Hilbert spaces illustrate the convergence theorem and show that the proposed HPORBP algorithm has a better accuracy than the Hilbert backpropagation algorithm. Copyright {\copyright} 2012 John Wiley \& Sons, Ltd.},
	Author = {Zhao Jianwei},
	Date-Added = {2018-05-15 11:26:53 +0000},
	Date-Modified = {2018-10-10 08:58:50 +0000},
	Doi = {10.1002/mma.2641},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mma.2641},
	Journal = {Mathematical Methods in the Applied Sciences},
	Keywords = {functional data, feedforward neural network, learning algorithm, convergence},
	Month = {November},
	Number = {17},
	Pages = {2111-2121},
	Title = {Functional data learning by Hilbert feedforward neural networks},
	Url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Volume = {35},
	Year = {2012},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mma.2641},
	Bdsk-Url-2 = {https://doi.org/10.1002/mma.2641}}

@article{CASTRO2005967,
	Abstract = {The Fuzzy ARTMAP algorithm has been proven to be one of the premier neural network architectures for classification problems. One of the properties of Fuzzy ARTMAP, which can be both an asset and a liability, is its capacity to produce new nodes (templates) on demand to represent classification categories. This property allows Fuzzy ARTMAP to automatically adapt to the database without having to a priori specify its network size. On the other hand, it has the undesirable side effect that large databases might produce a large network size (node proliferation) that can dramatically slow down the training speed of the algorithm. To address the slow convergence speed of Fuzzy ARTMAP for large database problems, we propose the use of space-filling curves, specifically the Hilbert space-filling curves (HSFC). Hilbert space-filling curves allow us to divide the problem into smaller sub-problems, each focusing on a smaller than the original dataset. For learning each partition of data, a different Fuzzy ARTMAP network is used. Through this divide-and-conquer approach we are avoiding the node proliferation problem, and consequently we speedup Fuzzy ARTMAP's training. Results have been produced for a two-class, 16-dimensional Gaussian data, and on the Forest database, available at the UCI repository. Our results indicate that the Hilbert space-filling curve approach reduces the time that it takes to train Fuzzy ARTMAP without affecting the generalization performance attained by Fuzzy ARTMAP trained on the original large dataset. Given that the resulting smaller datasets that the HSFC approach produces can independently be learned by different Fuzzy ARTMAP networks, we have also implemented and tested a parallel implementation of this approach on a Beowulf cluster of workstations that further speeds up Fuzzy ARTMAP's convergence to a solution for large database problems.},
	Author = {Jos{\'e} Castro and Michael Georgiopoulos and Ronald Demara and Avelino Gonzalez},
	Date-Added = {2018-05-15 11:26:41 +0000},
	Date-Modified = {2018-05-15 11:26:41 +0000},
	Doi = {https://doi.org/10.1016/j.neunet.2005.01.007},
	Issn = {0893-6080},
	Journal = {Neural Networks},
	Keywords = {Fuzzy-ARTMAP, Hilbert space-filling curve, Data mining, Data-partitioning},
	Number = {7},
	Pages = {967 - 984},
	Title = {Data-partitioning using the Hilbert space filling curves: Effect on the speed of convergence of Fuzzy ARTMAP for large database problems},
	Url = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Volume = {18},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S089360800500047X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neunet.2005.01.007}}

@article{2018arXiv180501934C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180501934C},
	Archiveprefix = {arXiv},
	Author = {{Chen}, C. and {Chen}, Q. and {Xu}, J. and {Koltun}, V.},
	Date-Added = {2018-05-15 11:26:25 +0000},
	Date-Modified = {2018-05-15 11:26:25 +0000},
	Eprint = {1805.01934},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Learning},
	Month = may,
	Primaryclass = {cs.CV},
	Title = {{Learning to See in the Dark}},
	Year = 2018}

@inproceedings{2017arXiv171111294L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171111294L},
	Archiveprefix = {arXiv},
	Author = {{Lin}, X. and {Zhao}, C. and {Pan}, W.},
	Booktitle = {31st Conference on Neural Information Processing Systems},
	Date-Added = {2018-05-15 11:26:04 +0000},
	Date-Modified = {2018-11-26 21:20:19 +1300},
	Eprint = {1711.11294},
	Journal = {NIPS 2017},
	Keywords = {BNN; Computer Science; Learning; Statistics; Machine Learning},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Towards Accurate Binary Convolutional Neural Network}},
	Year = 2017}

@article{2017arXiv170906206Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170906206Y},
	Archiveprefix = {arXiv},
	Author = {{Yin}, S. and {Venkataramanaiah}, S.~K. and {Chen}, G.~K. and {Krishnamurthy}, R. and {Cao}, Y. and {Chakrabarti}, C. and {Seo}, J.-s.},
	Date-Added = {2018-05-15 11:24:06 +0000},
	Date-Modified = {2018-10-11 08:40:58 +0000},
	Eprint = {1709.06206},
	Journal = {ArXiv e-prints},
	Keywords = {SNN; Neural and Evolutionary Computing},
	Month = sep,
	Title = {{Algorithm and Hardware Design of Discrete-Time Spiking Neural Networks Based on Back Propagation with Binary Activations}},
	Year = 2017}

@inproceedings{5949465,
	Author = {J. Ranhel and C. V. Lima and J. L. R. Monteiro and J. E. Kogler and M. L. Netto},
	Booktitle = {2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI)},
	Date-Added = {2018-05-15 11:24:06 +0000},
	Date-Modified = {2018-05-15 11:24:06 +0000},
	Doi = {10.1109/FOCI.2011.5949465},
	Keywords = {neural nets;storage management;PNG attributes;binary counters;bistable memory;parallel computing;polychronous group;spiking neural network;Biological system modeling;Computational modeling;Delay;Fires;Firing;Kernel;Neurons;bistable neural memory;neural counters;neural hierarchical organization;neural stack counter;polychronization;spiking neural networks},
	Month = {April},
	Pages = {66-73},
	Title = {Bistable memory and binary counters in spiking neural network},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCI.2011.5949465}}

@conference{2016arXiv161105128Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161105128Y},
	Archiveprefix = {arXiv},
	Author = {{Yang}, T.-J. and {Chen}, Y.-H. and {Sze}, V.},
	Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Date-Added = {2018-05-11 02:51:58 +0000},
	Date-Modified = {2018-11-26 20:59:17 +1300},
	Eprint = {1611.05128},
	Journal = {ArXiv e-prints},
	Keywords = {CNN; Computer Vision; Pattern Recognition},
	Month = jul,
	Primaryclass = {cs.CV},
	Title = {{Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning}},
	Year = 2017}

@conference{Chen2018UnderstandingTL,
	Author = {Yu-hsin Chen and Tien-Ju Yang and Joel S. Emer and Vivienne Sze},
	Booktitle = {SysML Conference},
	Date-Added = {2018-05-11 00:57:52 +0000},
	Date-Modified = {2018-11-26 19:52:57 +1300},
	Keywords = {NN, DNN, Hardware, FPGA, ASIC},
	Month = {February},
	Title = {Understanding the Limitations of Existing Energy-Efficient Design Approaches for Deep Neural Networks},
	Year = {2018}}

@inproceedings{2017arXiv170309039S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170309039S},
	Archiveprefix = {arXiv},
	Author = {{Sze}, V. and {Chen}, Y.-H. and {Yang}, T.-J. and {Emer}, J.},
	Booktitle = {Proceedings of the IEEE},
	Date-Added = {2018-05-10 05:28:12 +0000},
	Date-Modified = {2018-11-26 19:56:15 +1300},
	Eprint = {1703.09039},
	Journal = {Proceedings of the IEEE},
	Keywords = {NN, Computer Science; Computer Vision; Pattern Recognition},
	Month = dec,
	Number = {12},
	Primaryclass = {cs.CV},
	Title = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
	Volume = {105},
	Year = 2017}

@article{2016arXiv161207625S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161207625S},
	Archiveprefix = {arXiv},
	Author = {{Sze}, V. and {Chen}, Y.-H. and {Emer}, J. and {Suleiman}, A. and {Zhang}, Z.},
	Date-Added = {2018-05-10 05:20:22 +0000},
	Date-Modified = {2018-05-10 05:20:22 +0000},
	Eprint = {1612.07625},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{Hardware for Machine Learning: Challenges and Opportunities}},
	Year = 2016}

@article{2018arXiv180403230Y,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180403230Y},
	Archiveprefix = {arXiv},
	Author = {{Yang}, T.-J. and {Howard}, A. and {Chen}, B. and {Zhang}, X. and {Go}, A. and {Sze}, V. and {Adam}, H.},
	Date-Added = {2018-05-10 04:40:55 +0000},
	Date-Modified = {2018-05-10 04:40:55 +0000},
	Eprint = {1804.03230},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications}},
	Year = 2018}

@article{7738524,
	Author = {Y. H. Chen and T. Krishna and J. S. Emer and V. Sze},
	Date-Added = {2018-05-10 03:54:03 +0000},
	Date-Modified = {2018-10-17 00:13:32 +0000},
	Doi = {10.1109/JSSC.2016.2616357},
	Issn = {0018-9200},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {CNN; DRAM chips;data flow computing;energy conservation;feedforward neural nets;learning (artificial intelligence);neural net architecture;power aware computing;reconfigurable architectures;AI systems;AlexNet;CNN shapes;DRAM accesses;Eyeriss;MAC;RS dataflow reconfiguration;;convolutional layers;data movement energy cost;dataflow processing;deep convolutional neural networks;energy efficiency;energy-efficient reconfigurable accelerator;multiply and accumulation;off-chip DRAM;reconfiguring architecture;row stationary;spatial architecture;Clocks;Computer architecture;Hardware;Neural networks;Random access memory;Shape;Throughput;Convolutional neural networks (CNNs);dataflow processing;deep learning;energy-efficient accelerators;spatial architecture},
	Month = {Jan},
	Number = {1},
	Pages = {127-138},
	Title = {Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks},
	Volume = {52},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2016.2616357}}

@article{2014arXiv1412.6980K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1412.6980K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Ba}, J.},
	Date-Added = {2018-05-09 11:47:33 +0000},
	Date-Modified = {2018-05-09 11:47:33 +0000},
	Eprint = {1412.6980},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{Adam: A Method for Stochastic Optimization}},
	Year = 2014}

@inproceedings{8302078,
	Author = {X. Xu and J. Amaro and S. Caulfield and A. Forembski and G. Falcao and D. Moloney},
	Booktitle = {2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
	Date-Added = {2018-05-09 11:20:24 +0000},
	Date-Modified = {2018-09-26 08:02:09 +0000},
	Doi = {10.1109/CISP-BMEI.2017.8302078},
	Keywords = {computer vision;convolution;feedforward neural nets;image classification;image processing;multiprocessing systems;object recognition;stereo image processing;CNN;3D volumetric representation;CNNs;Movidius Neural Compute Stick;USB;VOLA;Volumetric Accelerator;computational requirements;computer vision;dedicated CNN hardware blocks;low-power processing unit;synthetic 3D voxelized point-clouds generation method;trained model;training data;volumetric data;voxelized point-clouds classification;Computational modeling;Graphics processing units;Object recognition;Solid modeling;Task analysis;Three-dimensional displays;Training;Convolutional Neural Networks;Embedded Systems;Point-clouds},
	Month = {Oct},
	Pages = {1-7},
	Title = {Convolutional neural network on neural compute stick for voxelized point-clouds classification},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/CISP-BMEI.2017.8302078}}

@article{2015arXiv151003009L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151003009L},
	Archiveprefix = {arXiv},
	Author = {{Lin}, Z. and {Courbariaux}, M. and {Memisevic}, R. and {Bengio}, Y.},
	Date-Added = {2018-05-09 11:05:15 +0000},
	Date-Modified = {2018-05-09 11:05:15 +0000},
	Eprint = {1510.03009},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = oct,
	Primaryclass = {cs.LG},
	Title = {{Neural Networks with Few Multiplications}},
	Year = 2015}

@inproceedings{7929192,
	Author = {E. Nurvitadhi and D. Sheffield and Jaewoong Sim and A. Mishra and G. Venkatesh and D. Marr},
	Booktitle = {2016 International Conference on Field-Programmable Technology (FPT)},
	Date-Added = {2018-05-06 09:18:40 +0000},
	Date-Modified = {2018-10-22 08:34:35 +0000},
	Doi = {10.1109/FPT.2016.7929192},
	Keywords = {BNN, application specific integrated circuits;field programmable gate arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria 10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural networks;deep neural network;hardware acceleration;Biological neural networks;Field programmable gate arrays;Graphics processing units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep learning;FPGA;GPU;binarized neural networks;data analytics;hardware accelerator},
	Month = {Dec},
	Pages = {77-84},
	Title = {Accelerating Binarized Neural Networks: Comparison of FPGA, CPU, GPU, and ASIC},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2016.7929192}}

@inproceedings{Zhao:2017:ABC:3020078.3021741,
	Acmid = {3021741},
	Address = {New York, NY, USA},
	Author = {Zhao, Ritchie and Song, Weinan and Zhang, Wentao and Xing, Tianwei and Lin, Jeng-Hau and Srivastava, Mani and Gupta, Rajesh and Zhang, Zhiru},
	Booktitle = {Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-05-06 08:57:34 +0000},
	Date-Modified = {2018-05-06 08:57:34 +0000},
	Doi = {10.1145/3020078.3021741},
	Isbn = {978-1-4503-4354-1},
	Keywords = {FPGAs, binarized, binarized convolutional networks, deep learning, high-level synthesis, reconfigurable computing},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {15--24},
	Publisher = {ACM},
	Series = {FPGA '17},
	Title = {Accelerating Binarized Convolutional Neural Networks with Software-Programmable FPGAs},
	Url = {http://doi.acm.org/10.1145/3020078.3021741},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3020078.3021741},
	Bdsk-Url-2 = {https://doi.org/10.1145/3020078.3021741}}

@article{2016arXiv160106071K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160106071K},
	Archiveprefix = {arXiv},
	Author = {{Kim}, M. and {Smaragdis}, P.},
	Date-Added = {2018-05-06 05:27:19 +0000},
	Date-Modified = {2018-05-06 05:27:19 +0000},
	Eprint = {1601.06071},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	Month = jan,
	Primaryclass = {cs.LG},
	Title = {{Bitwise Neural Networks}},
	Year = 2016}

@inbook{Baez2011,
	Abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a ``cobordism'': a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and ``quantum topology''. But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of ``closed symmetric monoidal category''. We assume no prior knowledge of category theory, proof theory or computer science.},
	Address = {Berlin, Heidelberg},
	Author = {Baez, J. and Stay, M.},
	Booktitle = {New Structures for Physics},
	Date-Added = {2018-04-29 11:38:03 +0000},
	Date-Modified = {2018-04-29 11:38:23 +0000},
	Doi = {10.1007/978-3-642-12821-9_2},
	Editor = {Coecke, Bob},
	Isbn = {978-3-642-12821-9},
	Keywords = {Category Theory, Physics, Logic, Computability},
	Pages = {95--172},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Physics, Topology, Logic and Computation: A Rosetta Stone},
	Url = {https://doi.org/10.1007/978-3-642-12821-9_2},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-3-642-12821-9_2}}

@article{2017arXiv170404865J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170404865J},
	Archiveprefix = {arXiv},
	Author = {{Juefei-Xu}, F. and {Naresh Boddeti}, V. and {Savvides}, M.},
	Date-Added = {2018-04-28 05:48:26 +0000},
	Date-Modified = {2018-04-28 05:48:26 +0000},
	Eprint = {1704.04865},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	Month = apr,
	Primaryclass = {cs.CV},
	Title = {{Gang of GANs: Generative Adversarial Networks with Maximum Margin Ranking}},
	Year = 2017}

@article{5607329,
	Abstract = {This paper presents the development and implementation of a generalized backpropagation multilayer perceptron (MLP) architecture described in VLSI hardware description language (VHDL). The development of hardware platforms has been complicated by the high hardware cost and quantity of the arithmetic operations required in online artificial neural networks (ANNs), i.e., general purpose ANNs with learning capability. Besides, there remains a dearth of hardware platforms for design space exploration, fast prototyping, and testing of these networks. Our general purpose architecture seeks to fill that gap and at the same time serve as a tool to gain a better understanding of issues unique to ANNs implemented in hardware, particularly using field programmable gate array (FPGA). The challenge is thus to find an architecture that minimizes hardware costs, while maximizing performance, accuracy, and parameterization. This work describes a platform that offers a high degree of parameterization, while maintaining generalized network design with performance comparable to other hardware-based MLP implementations. Application of the hardware implementation of ANN with backpropagation learning algorithm for a realistic application is also presented.},
	Author = {A. Gomperts and A. Ukil and F. Zurfluh},
	Date-Added = {2018-04-28 04:52:24 +0000},
	Date-Modified = {2018-04-28 04:52:24 +0000},
	Doi = {10.1109/TII.2010.2085006},
	Issn = {1551-3203},
	Journal = {IEEE Transactions on Industrial Informatics},
	Keywords = {backpropagation;field programmable gate arrays;hardware description languages;multilayer perceptrons;FPGA;VLSI hardware description language;arithmetic operation;artificial neural network;backpropagation multilayer perceptron;fast prototyping;field programmable gate array;general purpose neural network;hardware-based MLP;learning capability;online application;space exploration;Backpropagation;NIR spectra calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware implementation;multilayer perceptron;neural network;spectroscopy},
	Month = {Feb},
	Number = {1},
	Pages = {78-89},
	Title = {Development and Implementation of Parameterized FPGA-Based General Purpose Neural Networks for Online Applications},
	Volume = {7},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/TII.2010.2085006}}

@inproceedings{Sun:2018:FPR:3201607.3201741,
	Acmid = {3201741},
	Address = {Piscataway, NJ, USA},
	Author = {Sun, Xiaoyu and Peng, Xiaochen and Chen, Pai-Yu and Liu, Rui and Seo, Jae-sun and Yu, Shimeng},
	Booktitle = {Proceedings of the 23rd Asia and South Pacific Design Automation Conference},
	Date-Added = {2018-04-28 04:34:43 +0000},
	Date-Modified = {2018-10-15 10:21:08 +0000},
	Keywords = {BNN, P-BNN, CSM, MNIST},
	Location = {Jeju, Republic of Korea},
	Numpages = {6},
	Pages = {574--579},
	Publisher = {IEEE Press},
	Series = {ASPDAC '18},
	Title = {Fully Parallel RRAM Synaptic Array for Implementing Binary Neural Network with (+1, -1) Weights and (+1, 0) Neurons},
	Url = {http://dl.acm.org/citation.cfm?id=3201607.3201741},
	Year = {2018},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3201607.3201741}}

@article{2018arXiv180200904L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200904L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y. and {Ren}, F.},
	Date-Added = {2018-04-28 04:09:52 +0000},
	Date-Modified = {2018-10-11 08:39:19 +0000},
	Eprint = {1802.00904},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Computer Science; Computer Vision; Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{Build a Compact Binary Neural Network through Bit-level Sensitivity and Data Pruning}},
	Year = 2018}

@conference{8541786,
	Adsurl = {http://hdl.handle.net/1854/LU-8541786},
	Archiveprefix = {arXiv},
	Author = {{Leroux}, S. and {Bohez}, S. and {Verbelen}, T. and {Vankeirsbilck}, B. and {Simoens}, P. and {Dhoedt}, B.},
	Booktitle = {31st Conference on Neural Information Processing Systems},
	Date-Added = {2018-04-28 03:56:37 +0000},
	Date-Modified = {2018-12-06 11:17:15 +1300},
	Eprint = {8541786},
	Journal = {NIPS 2017},
	Keywords = {BNN; Computer Science; Neural and Evolutionary Computing; Computer Vision; Pattern Recognition},
	Month = Dec,
	Organization = {NIPS},
	Pages = {1-4},
	Title = {{Transfer Learning with Binary Neural Networks}},
	Year = 2017}

@article{2017arXiv170905306G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170905306G},
	Archiveprefix = {arXiv},
	Author = {{Guan}, T. and {Zeng}, X. and {Seok}, M.},
	Date-Added = {2018-04-28 03:26:42 +0000},
	Date-Modified = {2018-10-11 08:41:32 +0000},
	Eprint = {1709.05306},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Neural and Evolutionary Computing},
	Month = sep,
	Title = {{Recursive Binary Neural Network Learning Model with 2.28b/Weight Storage Requirement}},
	Year = 2017}

@inproceedings{McDanel:2017:EBN:3108009.3108031,
	Acmid = {3108031},
	Address = {USA},
	Author = {McDanel, Bradley and Teerapittayanon, Surat and Kung, H.T.},
	Booktitle = {Proceedings of the 2017 International Conference on Embedded Wireless Systems and Networks},
	Date-Added = {2018-04-28 01:31:18 +0000},
	Date-Modified = {2018-04-28 01:34:56 +0000},
	Isbn = {978-0-9949886-1-4},
	Keywords = {BNN, eBNN, CNN, MNIST, CIFAR},
	Location = {Uppsala, Sweden},
	Numpages = {6},
	Pages = {168--173},
	Publisher = {Junction Publishing},
	Series = {EWSN \&\#8217;17},
	Title = {Embedded Binarized Neural Networks},
	Url = {http://dl.acm.org/citation.cfm?id=3108009.3108031},
	Year = {2017},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3108009.3108031}}

@article{2016arXiv160907061H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160907061H},
	Archiveprefix = {arXiv},
	Author = {{Hubara}, I. and {Courbariaux}, M. and {Soudry}, D. and {El-Yaniv}, R. and {Bengio}, Y.},
	Date-Added = {2018-04-27 00:08:19 +0000},
	Date-Modified = {2018-12-02 19:10:57 +1300},
	Eprint = {1609.07061},
	Journal = {Journal Of Machine Learning Research 18},
	Keywords = {NN, Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = {January},
	Number = {1},
	Pages = {6869-6898},
	Title = {{Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations}},
	Volume = {18},
	Year = 2017}

@inproceedings{Tang2017HowTT,
	Author = {Wei Tang and Gang Hua and Liang Wang},
	Booktitle = {AAAI},
	Date-Added = {2018-04-26 22:22:30 +0000},
	Date-Modified = {2018-04-26 22:22:52 +0000},
	Keywords = {BNN, NN, Training},
	Title = {How to Train a Compact Binary Neural Network with High Accuracy?},
	Year = {2017}}

@inproceedings{Wei:2017:ASA:3061639.3062207,
	Acmid = {3062207},
	Address = {New York, NY, USA},
	Articleno = {29},
	Author = {Wei, Xuechao and Yu, Cody Hao and Zhang, Peng and Chen, Youxiang and Wang, Yuxin and Hu, Han and Liang, Yun and Cong, Jason},
	Booktitle = {Proceedings of the 54th Annual Design Automation Conference 2017},
	Date-Added = {2018-04-25 23:12:26 +0000},
	Date-Modified = {2018-04-26 22:23:20 +0000},
	Doi = {10.1145/3061639.3062207},
	Isbn = {978-1-4503-4927-7},
	Keywords = {CNN, FPGA, Linear Algebra, Systolic Array},
	Location = {Austin, TX, USA},
	Numpages = {6},
	Pages = {29:1--29:6},
	Publisher = {ACM},
	Series = {DAC '17},
	Title = {Automated Systolic Array Architecture Synthesis for High Throughput CNN Inference on FPGAs},
	Url = {http://doi.acm.org/10.1145/3061639.3062207},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3061639.3062207},
	Bdsk-Url-2 = {https://doi.org/10.1145/3061639.3062207}}

@article{1998adap.org..6001P,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/1998adap.org..6001P},
	Author = {{Pang}, X. and {Werbos}, P.},
	Date-Added = {2018-04-24 23:59:42 +0000},
	Date-Modified = {2018-04-24 23:59:42 +0000},
	Journal = {Advances in Astrophysics},
	Keywords = {Adaptation, Noise, and Self-Organizing Systems, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Quantitative Biology - Neurons and Cognition},
	Month = jun,
	Title = {{Neural network design for J function approximation in dynamic programming}},
	Year = 1998}

@article{2018arXiv180200438Z,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180200438Z},
	Archiveprefix = {arXiv},
	Author = {{Zohouri}, H.~R. and {Podobas}, A. and {Matsuoka}, S.},
	Date-Added = {2018-04-24 23:57:57 +0000},
	Date-Modified = {2018-04-24 23:57:57 +0000},
	Eprint = {1802.00438},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{Combined Spatial and Temporal Blocking for High-Performance Stencil Computation on FPGAs Using OpenCL}},
	Year = 2018}

@article{2017arXiv170703049M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170703049M},
	Archiveprefix = {arXiv},
	Author = {{Mostafa}, H. and {Pedroni}, B. and {Sheik}, S. and {Cauwenberghs}, G.},
	Date-Added = {2018-04-24 23:57:22 +0000},
	Date-Modified = {2018-06-22 10:46:48 +0000},
	Eprint = {1707.03049},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA-NN, Computer Science - Neural and Evolutionary Computing},
	Month = jun,
	Title = {{Hardware-efficient on-line learning through pipelined truncated-error backpropagation in binary-state networks}},
	Year = 2017}

@article{2018arXiv180303790S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180303790S},
	Archiveprefix = {arXiv},
	Author = {{Shen}, J. and {Qiao}, Y. and {Huang}, Y. and {Wen}, M. and {Zhang}, C.},
	Date-Added = {2018-04-24 23:57:08 +0000},
	Date-Modified = {2018-04-24 23:57:08 +0000},
	Eprint = {1803.03790},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Hardware Architecture},
	Month = mar,
	Title = {{Towards a Multi-array Architecture for Accelerating Large-scale Matrix Multiplication on FPGAs}},
	Year = 2018}

@article{2016arXiv161200694H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161200694H},
	Archiveprefix = {arXiv},
	Author = {{Han}, S. and {Kang}, J. and {Mao}, H. and {Hu}, Y. and {Li}, X. and {Li}, Y. and {Xie}, D. and {Luo}, H. and {Yao}, S. and {Wang}, Y. and {Yang}, H. and {Dally}, W.~J.},
	Date-Added = {2018-04-24 10:49:59 +0000},
	Date-Modified = {2018-04-24 10:49:59 +0000},
	Eprint = {1612.00694},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language},
	Month = dec,
	Primaryclass = {cs.CL},
	Title = {{ESE: Efficient Speech Recognition Engine with Sparse LSTM on FPGA}},
	Year = 2016}

@article{2011arXiv1107.1831H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1107.1831H},
	Archiveprefix = {arXiv},
	Author = {{Homescu}, C.},
	Date-Added = {2018-04-15 11:10:18 +0000},
	Date-Modified = {2018-04-15 11:10:18 +0000},
	Eprint = {1107.1831},
	Journal = {ArXiv e-prints},
	Keywords = {Quantitative Finance - Computational Finance},
	Month = jul,
	Primaryclass = {q-fin.CP},
	Title = {{Adjoints and Automatic (Algorithmic) Differentiation in Computational Finance}},
	Year = 2011}

@inproceedings{1188677,
	Author = {A. A. Gaffar and O. Mencer and W. Luk and P. Y. K. Cheung and N. Shirazi},
	Booktitle = {2002 IEEE International Conference on Field-Programmable Technology, 2002. (FPT). Proceedings.},
	Date-Added = {2018-04-15 10:59:42 +0000},
	Date-Modified = {2018-04-15 10:59:42 +0000},
	Doi = {10.1109/FPT.2002.1188677},
	Keywords = {FIR filters;VLSI;circuit CAD;circuit optimisation;data flow graphs;differentiation;digital filters;digital signal processing chips;discrete Fourier transforms;field programmable gate arrays;floating point arithmetic;high level synthesis;integrated circuit design;sensitivity analysis;DFT implementation;FIR filter implementation;FPGA;VLSI circuits;arithmetic operations;automatic bitwidth analysis;automatic differentiation;dataflow graph representation;discrete Fourier transform implementation;floating-point bitwidth analysis;floating-point designs;high-level programming;high-level synthesis;mathematical method;precision analysis;sensitivity analysis;user-defined numerical constraints;Arithmetic;Automatic programming;Circuits;Data analysis;Discrete Fourier transforms;Field programmable gate arrays;Finite impulse response filter;High level synthesis;Sensitivity analysis;Very large scale integration},
	Month = {Dec},
	Pages = {158-165},
	Title = {Floating-point bitwidth analysis via automatic differentiation},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/FPT.2002.1188677}}

@inproceedings{Elliott-2009-beautiful-differentiation,
	Author = {Conal Elliott},
	Date-Added = {2018-04-13 05:07:34 +0000},
	Date-Modified = {2018-04-13 05:07:34 +0000},
	Journal = {International Conference on Functional Programming (ICFP)},
	Keywords = {Automatic Differentiation, Vector Spaces, Haskell},
	Month = sep,
	Number = {ICFP},
	Title = {Beautiful Differentiation},
	Url = {http://conal.net/papers/beautiful-differentiation/},
	Year = {2009},
	Bdsk-Url-1 = {http://conal.net/papers/beautiful-differentiation/}}

@article{Elliott-2017-compiling-to-categories,
	Articleno = {48},
	Author = {Conal Elliott},
	Date-Added = {2018-04-07 11:10:44 +0000},
	Date-Modified = {2018-04-07 11:10:44 +0000},
	Doi = {http://dx.doi.org/10.1145/3110271},
	Journal = {Proc. ACM Program. Lang.},
	Keywords = {Automatic Differentiation, Category Theory, Haskell},
	Month = sep,
	Number = {ICFP},
	Numpages = {24},
	Title = {Compiling To Categories},
	Url = {http://conal.net/papers/compiling-to-categories},
	Volume = {1},
	Year = {2017},
	Bdsk-Url-1 = {http://conal.net/papers/compiling-to-categories},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3110271}}

@inproceedings{Elliott-2018-ad-icfp,
	Abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm can be specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.
},
	Author = {Conal Elliott},
	Booktitle = {Proceedings of the ACM on Programming Languages (ICFP)},
	Date-Added = {2018-04-06 11:42:18 +0000},
	Date-Modified = {2018-12-03 19:46:22 +1300},
	Eprint = {1804.00746},
	Keywords = {Haskell, Automatic Differentiation, Math, Category Theory},
	Month = {March},
	Title = {The simple essence of automatic differentiation},
	Url = {http://conal.net/papers/essence-of-ad/},
	Volume = {abs/1804.00746},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1804.00746},
	Bdsk-Url-2 = {https://arxiv.org/pdf/1804.00746}}

@article{Karczmarczuk2001,
	Abstract = {We present a purely functional implementation of the computational differentiation tools---the well known numeric (i.e., not symbolic) techniques which permit one to compute point-wise derivatives of functions defined by computer programs economically and exactly (with machine precision). We show how the use of lazy evaluation permits a transparent and elegant construction of the entire infinite tower of derivatives of higher order for any expressions present in the program. The formalism may be useful in various problems of scientific computing which often demand a hard and ungracious human preprocessing before writing the final code. Some concrete examples are given.},
	Author = {Karczmarczuk, Jerzy},
	Date-Added = {2018-04-06 04:46:21 +0000},
	Date-Modified = {2018-04-06 04:49:58 +0000},
	Day = {01},
	Doi = {10.1023/A:1011501232197},
	Issn = {1573-0557},
	Journal = {Higher-Order and Symbolic Computation},
	Keywords = {Mathematics, Derivates, Automatic Differentiation, Computer Science},
	Month = {Mar},
	Number = {1},
	Pages = {35--57},
	Title = {Functional Differentiation of Computer Programs},
	Url = {https://doi.org/10.1023/A:1011501232197},
	Volume = {14},
	Year = {2001},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1011501232197}}

@article{2014arXiv1404.7456G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1404.7456G},
	Archiveprefix = {arXiv},
	Author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A.},
	Date-Added = {2018-04-04 09:50:26 +0000},
	Date-Modified = {2018-09-26 08:04:03 +0000},
	Eprint = {1404.7456},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Symbolic Computation, Statistics - Machine Learning, Automatic Differentiation, Automatic Differentiation, Automatic Differentiation, G.1.4, I.2.6},
	Month = apr,
	Primaryclass = {cs.LG},
	Title = {{Automatic Differentiation of Algorithms for Machine Learning}},
	Year = 2014}

@article{Gremse:2016aa,
	Abstract = {Many scientific problems such as classifier training or medical image reconstruction can be expressed as minimization of differentiable real-valued cost functions and solved with iterative gradient-based methods. Adjoint algorithmic differentiation (AAD) enables automated computation of gradients of such cost functions implemented as computer programs. To backpropagate adjoint derivatives, excessive memory is potentially required to store the intermediate partial derivatives on a dedicated data structure, referred to as the ``tape''. Parallelization is difficult because threads need to synchronize their accesses during taping and backpropagation. This situation is aggravated for many-core architectures, such as Graphics Processing Units (GPUs), because of the large number of light-weight threads and the limited memory size in general as well as per thread. We show how these limitations can be mediated if the cost function is expressed using GPU-accelerated vector and matrix operations which are recognized as intrinsic functions by our AAD software. We compare this approach with naive and vectorized implementations for CPUs. We use four increasingly complex cost functions to evaluate the performance with respect to memory consumption and gradient computation times. Using vectorization, CPU and GPU memory consumption could be substantially reduced compared to the naive reference implementation, in some cases even by an order of complexity. The vectorization allowed usage of optimized parallel libraries during forward and reverse passes which resulted in high speedups for the vectorized CPU version compared to the naive reference implementation. The GPU version achieved an additional speedup of 7.5 $\pm$4.4, showing that the processing power of GPUs can be utilized for AAD using this concept. Furthermore, we show how this software can be systematically extended for more complex problems such as nonlinear absorption reconstruction for fluorescence-mediated tomography.},
	An = {PMC4772124},
	Author = {Gremse, Felix and H{\"o}fter, Andreas and Razik, Lukas and Kiessling, Fabian and Naumann, Uwe},
	Date = {2016/03/01},
	Date-Added = {2018-04-04 08:10:03 +0000},
	Date-Modified = {2018-04-04 08:10:23 +0000},
	Db = {PMC},
	Doi = {10.1016/j.cpc.2015.10.027},
	Isbn = {0010-4655},
	J1 = {Comput Phys Commun},
	Journal = {Computer physics communications},
	Keywords = {GPU, Automatic Differentiation, Math, Neural Network},
	Month = {03},
	Pages = {300--311},
	Title = {GPU-Accelerated Adjoint Algorithmic Differentiation},
	Ty = {JOUR},
	U1 = {26941443{$[$}pmid{$]$}},
	Url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Volume = {200},
	Year = {2016},
	Bdsk-Url-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4772124/},
	Bdsk-Url-2 = {https://dx.doi.org/10.1016/j.cpc.2015.10.027}}

@article{6701396,
	Author = {D. Neil and S. C. Liu},
	Date-Added = {2018-03-20 09:15:46 +0000},
	Date-Modified = {2018-03-20 09:15:46 +0000},
	Doi = {10.1109/TVLSI.2013.2294916},
	Issn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {field programmable gate arrays;neural nets;CPU;MNIST handwritten digit classification;Minitaur;event-driven FPGA;event-driven neural network accelerator;field-programmable gate array-based system;neural networks;newsgroups classification data;robotics;spiking deep network;spiking network accelerator;Biological neural networks;Clocks;Computer architecture;Field programmable gate arrays;Mathematical model;Neurons;Performance evaluation;Deep belief networks;field programmable arrays;machine learning;neural networks;restricted Boltzmann machines;spiking neural networks},
	Month = {Dec},
	Number = {12},
	Pages = {2621-2628},
	Title = {Minitaur, an Event-Driven FPGA-Based Spiking Network Accelerator},
	Volume = {22},
	Year = {2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2013.2294916}}

@book{Make-Your-Own-Neural-Network,
	Adsurl = {https://www.amazon.co.uk/Make-Your-Own-Neural-Network/dp/1530826608},
	Author = {{Rashid}, Tariq},
	Date-Added = {2018-03-03 10:33:43 +0000},
	Date-Modified = {2018-03-03 10:33:43 +0000},
	Keywords = {Computer Science, Neural Network, Python},
	Month = March,
	Title = {{Make Your Own Neural Network}},
	Year = 2016}

@article{798320,
	Author = {C. Elliott},
	Date-Added = {2018-02-26 09:50:52 +0000},
	Date-Modified = {2018-02-26 09:50:52 +0000},
	Doi = {10.1109/32.798320},
	Issn = {0098-5589},
	Journal = {IEEE Transactions on Software Engineering},
	Keywords = {computer animation;multimedia computing;simulation languages;Fran;Haskell;declarative host language;embedded domain-specific vocabulary;embedded modeling language approach;growth;interactive 3D animation;interactive multimedia animation;modeled animation;motion;Animation;Automatic programming;Computer graphics;Computer languages;Domain specific languages;Functional programming;Programming profession;Shape;Vocabulary;Writing},
	Month = {May},
	Number = {3},
	Pages = {291-308},
	Title = {An embedded modeling language approach to interactive 3D and multimedia animation},
	Volume = {25},
	Year = {1999},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/32.798320}}

@techreport{Hudak94vs.ada,
	Author = {Paul Hudak and Mark P. Jones},
	Date-Added = {2018-02-26 09:38:35 +0000},
	Date-Modified = {2018-02-26 09:38:35 +0000},
	Keywords = {DARPA, haskell, Cpp, Awk},
	Title = {vs. Ada vs. C++ vs. Awk vs. ... An Experiment in Software Prototyping Productivity Available from http://www.haskell.org/papers/NSWC/jfp.ps},
	Year = {1994}}

@inproceedings{Totoo:2012:HVF:2364474.2364483,
	Acmid = {2364483},
	Address = {New York, NY, USA},
	Author = {Totoo, Prabhat and Deligiannis, Pantazis and Loidl, Hans-Wolfgang},
	Booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2018-02-26 08:51:00 +0000},
	Date-Modified = {2018-02-26 08:51:00 +0000},
	Doi = {10.1145/2364474.2364483},
	Isbn = {978-1-4503-1577-7},
	Keywords = {barnes-hut, f\#, haskell, n-body, parallelism, scala},
	Location = {Copenhagen, Denmark},
	Numpages = {12},
	Pages = {49--60},
	Publisher = {ACM},
	Series = {FHPC '12},
	Title = {Haskell vs. F\# vs. Scala: A High-level Language Features and Parallelism Support Comparison},
	Url = {http://doi.acm.org/10.1145/2364474.2364483},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2364474.2364483},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364483}}

@inproceedings{6209130,
	Author = {M. Fenwick and C. Sesanker and M. R. Schiller and H. J. Ellis and M. L. Hinman and J. Vyas and M. R. Gryk},
	Booktitle = {2012 Ninth International Conference on Information Technology - New Generations},
	Date-Added = {2018-02-26 08:28:41 +0000},
	Date-Modified = {2018-02-26 08:28:41 +0000},
	Doi = {10.1109/ITNG.2012.21},
	Keywords = {Java;LISP;bioinformatics;functional programming;learning (artificial intelligence);public domain software;software engineering;Haskell;Java;LISP;Python;algorithm development;bioinformatics;complex data operations;complex mathematical notions;data processing;functional computing;functional languages;functional programming accessibility;functional programming techniques;learning curve;learning resources;machine learning;multilanguage source-code repository;open-source Sandbox;scientific communities;software integration;Bioinformatics;Data visualization;Functional programming;Nuclear magnetic resonance;Proteins;Schedules;Transient analysis;Clojure;Haskell;Java;LISP;NMR;bioinformatics;functional-programming},
	Month = {April},
	Pages = {89-94},
	Title = {An Open-Source Sandbox for Increasing the Accessibility of Functional Programming to the Bioinformatics and Scientific Communities},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBrLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vR2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWJPEQJyAAAAAAJyAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fR2FuZyBvZiBHQU5zLSBHZW5lI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABURDR0FOAAACAIEvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RENHQU46R2FuZyBvZiBHQU5zLSBHZW5lcmF0aXZlIEFkdmVyc2FyaWFsIE5ldHdvcmtzIHdpdGggTWF4aW11bSBNYXJnaW4gUmFua2luZy5iaWIAAA4AnABNAEcAYQBuAGcAIABvAGYAIABHAEEATgBzAC0AIABHAGUAbgBlAHIAYQB0AGkAdgBlACAAQQBkAHYAZQByAHMAYQByAGkAYQBsACAATgBlAHQAdwBvAHIAawBzACAAdwBpAHQAaAAgAE0AYQB4AGkAbQB1AG0AIABNAGEAcgBnAGkAbgAgAFIAYQBuAGsAaQBuAGcALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAH9Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0RDR0FOL0dhbmcgb2YgR0FOcy0gR2VuZXJhdGl2ZSBBZHZlcnNhcmlhbCBOZXR3b3JrcyB3aXRoIE1heGltdW0gTWFyZ2luIFJhbmtpbmcuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACSAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAwg=},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ITNG.2012.21}}

@inproceedings{Pop:2010:ERH:1863543.1863595,
	Acmid = {1863595},
	Address = {New York, NY, USA},
	Author = {Pop, Iustin},
	Booktitle = {Proceedings of the 15th ACM SIGPLAN International Conference on Functional Programming},
	Date-Added = {2018-02-26 08:27:07 +0000},
	Date-Modified = {2018-02-26 08:27:07 +0000},
	Doi = {10.1145/1863543.1863595},
	Isbn = {978-1-60558-794-3},
	Keywords = {ganeti, haskell, python, system administration},
	Location = {Baltimore, Maryland, USA},
	Numpages = {6},
	Pages = {369--374},
	Publisher = {ACM},
	Series = {ICFP '10},
	Title = {Experience Report: Haskell As a Reagent: Results and Observations on the Use of Haskell in a Python Project},
	Url = {http://doi.acm.org/10.1145/1863543.1863595},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1863543.1863595},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1863543.1863595}}

@inproceedings{Nanz:2015:CSP:2818754.2818848,
	Acmid = {2818848},
	Address = {Piscataway, NJ, USA},
	Author = {Nanz, Sebastian and Furia, Carlo A.},
	Booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
	Date-Added = {2018-02-26 07:03:13 +0000},
	Date-Modified = {2018-04-15 11:11:54 +0000},
	Isbn = {978-1-4799-1934-5},
	Keywords = {Languages, Haskell, C++},
	Location = {Florence, Italy},
	Numpages = {11},
	Pages = {778--788},
	Publisher = {IEEE Press},
	Series = {ICSE '15},
	Title = {A Comparative Study of Programming Languages in Rosetta Code},
	Url = {http://dl.acm.org/citation.cfm?id=2818754.2818848},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2818754.2818848}}

@article{2010arXiv1009.0305R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2010arXiv1009.0305R},
	Archiveprefix = {arXiv},
	Author = {{Rabah}, S. and {Li}, J. and {Liu}, M. and {Lai}, Y.},
	Date-Added = {2018-02-22 23:19:52 +0000},
	Date-Modified = {2018-02-22 23:19:52 +0000},
	Eprint = {1009.0305},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Programming Languages, D.3},
	Month = sep,
	Primaryclass = {cs.PL},
	Title = {{Comparative Studies of 10 Programming Languages within 10 Diverse Criteria -- a Team 7 COMP6411-S10 Term Report}},
	Year = 2010}

@article{2017arXiv171109846J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171109846J},
	Archiveprefix = {arXiv},
	Author = {{Jaderberg}, M. and {Dalibard}, V. and {Osindero}, S. and {Czarnecki}, W.~M. and {Donahue}, J. and {Razavi}, A. and {Vinyals}, O. and {Green}, T. and {Dunning}, I. and {Simonyan}, K. and {Fernando}, C. and {Kavukcuoglu}, K.},
	Date-Added = {2018-02-21 06:49:04 +0000},
	Date-Modified = {2018-02-21 06:49:04 +0000},
	Eprint = {1711.09846},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Population Based Training of Neural Networks}},
	Year = 2017}

@article{2014arXiv1410.5401G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1410.5401G},
	Archiveprefix = {arXiv},
	Author = {{Graves}, A. and {Wayne}, G. and {Danihelka}, I.},
	Date-Added = {2018-02-21 06:07:10 +0000},
	Date-Modified = {2018-02-21 06:07:10 +0000},
	Eprint = {1410.5401},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing},
	Month = oct,
	Title = {{Neural Turing Machines}},
	Year = 2014}

@article{2015arXiv150308895S,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150308895S},
	Archiveprefix = {arXiv},
	Author = {{Sukhbaatar}, S. and {Szlam}, A. and {Weston}, J. and {Fergus}, R.},
	Date-Added = {2018-02-21 05:45:01 +0000},
	Date-Modified = {2018-02-21 05:45:01 +0000},
	Eprint = {1503.08895},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	Month = mar,
	Title = {{End-To-End Memory Networks}},
	Year = 2015}

@article{2014arXiv1409.0473B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1409.0473B},
	Archiveprefix = {arXiv},
	Author = {{Bahdanau}, D. and {Cho}, K. and {Bengio}, Y.},
	Date-Added = {2018-02-21 05:29:04 +0000},
	Date-Modified = {2018-02-21 05:29:04 +0000},
	Eprint = {1409.0473},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	Month = sep,
	Primaryclass = {cs.CL},
	Title = {{Neural Machine Translation by Jointly Learning to Align and Translate}},
	Year = 2014}

@article{2015arXiv150203044X,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150203044X},
	Archiveprefix = {arXiv},
	Author = {{Xu}, K. and {Ba}, J. and {Kiros}, R. and {Cho}, K. and {Courville}, A. and {Salakhutdinov}, R. and {Zemel}, R. and {Bengio}, Y.},
	Date-Added = {2018-02-21 04:55:23 +0000},
	Date-Modified = {2018-02-21 04:55:23 +0000},
	Eprint = {1502.03044},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}},
	Year = 2015}

@article{2015arXiv150301007J,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150301007J},
	Archiveprefix = {arXiv},
	Author = {{Joulin}, A. and {Mikolov}, T.},
	Date-Added = {2018-02-21 04:03:18 +0000},
	Date-Modified = {2018-02-21 04:03:18 +0000},
	Eprint = {1503.01007},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	Month = mar,
	Title = {{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}},
	Year = 2015}

@article{2014arXiv1406.6247M,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.6247M},
	Archiveprefix = {arXiv},
	Author = {{Mnih}, V. and {Heess}, N. and {Graves}, A. and {Kavukcuoglu}, K.},
	Date-Added = {2018-02-21 04:01:10 +0000},
	Date-Modified = {2018-02-21 04:01:10 +0000},
	Eprint = {1406.6247},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Recurrent Models of Visual Attention}},
	Year = 2014}

@article{2013arXiv1308.0850G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1308.0850G},
	Archiveprefix = {arXiv},
	Author = {{Graves}, A.},
	Date-Added = {2018-02-21 03:44:05 +0000},
	Date-Modified = {2018-02-21 03:44:05 +0000},
	Eprint = {1308.0850},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computation and Language},
	Month = aug,
	Title = {{Generating Sequences With Recurrent Neural Networks}},
	Year = 2013}

@article{2015arXiv150204390D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204390D},
	Archiveprefix = {arXiv},
	Author = {{Dauphin}, Y.~N. and {de Vries}, H. and {Bengio}, Y.},
	Date-Added = {2018-02-21 02:45:02 +0000},
	Date-Modified = {2018-02-21 02:45:02 +0000},
	Eprint = {1502.04390},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Numerical Analysis},
	Month = feb,
	Primaryclass = {cs.LG},
	Title = {{Equilibrated adaptive learning rates for non-convex optimization}},
	Year = 2015}

@article{2016arXiv160604934K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160604934K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P. and {Salimans}, T. and {Jozefowicz}, R. and {Chen}, X. and {Sutskever}, I. and {Welling}, M.},
	Date-Added = {2018-02-20 10:42:29 +0000},
	Date-Modified = {2018-02-20 10:42:29 +0000},
	Eprint = {1606.04934},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Improving Variational Inference with Inverse Autoregressive Flow}},
	Year = 2016}

@article{2015arXiv150204623G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150204623G},
	Archiveprefix = {arXiv},
	Author = {{Gregor}, K. and {Danihelka}, I. and {Graves}, A. and {Jimenez Rezende}, D. and {Wierstra}, D.},
	Date-Added = {2018-02-20 10:42:05 +0000},
	Date-Modified = {2018-02-20 10:42:05 +0000},
	Eprint = {1502.04623},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = feb,
	Primaryclass = {cs.CV},
	Title = {{DRAW: A Recurrent Neural Network For Image Generation}},
	Year = 2015}

@article{2016arXiv160603657C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603657C},
	Archiveprefix = {arXiv},
	Author = {{Chen}, X. and {Duan}, Y. and {Houthooft}, R. and {Schulman}, J. and {Sutskever}, I. and {Abbeel}, P.},
	Date-Added = {2018-02-20 00:17:44 +0000},
	Date-Modified = {2018-02-20 00:17:44 +0000},
	Eprint = {1606.03657},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
	Year = 2016}

@article{2016arXiv160509674H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160509674H},
	Archiveprefix = {arXiv},
	Author = {{Houthooft}, R. and {Chen}, X. and {Duan}, Y. and {Schulman}, J. and {De Turck}, F. and {Abbeel}, P.},
	Date-Added = {2018-02-19 23:51:31 +0000},
	Date-Modified = {2018-02-19 23:51:31 +0000},
	Eprint = {1605.09674},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
	Month = may,
	Primaryclass = {cs.LG},
	Title = {{VIME: Variational Information Maximizing Exploration}},
	Year = 2016}

@article{2016arXiv160603476H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160603476H},
	Archiveprefix = {arXiv},
	Author = {{Ho}, J. and {Ermon}, S.},
	Date-Added = {2018-02-19 10:11:33 +0000},
	Date-Modified = {2018-02-19 10:11:33 +0000},
	Eprint = {1606.03476},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Artificial Intelligence},
	Month = jun,
	Primaryclass = {cs.LG},
	Title = {{Generative Adversarial Imitation Learning}},
	Year = 2016}

@article{SalimansGZCRC16,
	Archiveprefix = {arXiv},
	Author = {Tim Salimans and Ian J. Goodfellow and Wojciech Zaremba and Vicki Cheung and Alec Radford and Xi Chen},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/SalimansGZCRC16},
	Date-Added = {2018-02-19 09:53:01 +0000},
	Date-Modified = {2018-10-29 11:30:01 +0000},
	Eprint = {1606.03498},
	Journal = {CoRR},
	Keywords = {DCGAN, MNIST, Semi-Supervised learning, minibatch},
	Timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
	Title = {Improved Techniques for Training GANs},
	Url = {http://arxiv.org/abs/1606.03498},
	Volume = {abs/1606.03498},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1606.03498}}

@article{2014arXiv1406.2661G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1406.2661G},
	Archiveprefix = {arXiv},
	Author = {{Goodfellow}, I.~J. and {Pouget-Abadie}, J. and {Mirza}, M. and {Xu}, B. and {Warde-Farley}, D. and {Ozair}, S. and {Courville}, A. and {Bengio}, Y.},
	Date-Added = {2018-02-19 09:28:01 +0000},
	Date-Modified = {2018-02-19 09:28:01 +0000},
	Eprint = {1406.2661},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = jun,
	Primaryclass = {stat.ML},
	Title = {{Generative Adversarial Networks}},
	Year = 2014}

@article{2013arXiv1312.6114K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
	Archiveprefix = {arXiv},
	Author = {{Kingma}, D.~P and {Welling}, M.},
	Date-Added = {2018-02-19 09:25:50 +0000},
	Date-Modified = {2018-02-19 09:25:50 +0000},
	Eprint = {1312.6114},
	Journal = {ArXiv e-prints},
	Keywords = {Statistics - Machine Learning, Computer Science - Learning},
	Month = dec,
	Primaryclass = {stat.ML},
	Title = {{Auto-Encoding Variational Bayes}},
	Year = 2013,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCHLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0Egc3ltbWV0cmljIGtleSBjcnlwdG9ncmFwaHkgdXNpbmcgZ2VuZXRpYyBhbGdvcml0aG0gYW5kIGVycm9yIGJhY2sgcHJvcGFnYXRpb24gbmV1cmFsIG5ldHdvcmsuYmliTxEC2gAAAAAC2gACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0Egc3ltbWV0cmljIGtleSBjciNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAxFdm9sdXRpb25hcnkAAgCdLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkV2b2x1dGlvbmFyeTpBIHN5bW1ldHJpYyBrZXkgY3J5cHRvZ3JhcGh5IHVzaW5nIGdlbmV0aWMgYWxnb3JpdGhtIGFuZCBlcnJvciBiYWNrIHByb3BhZ2F0aW9uIG5ldXJhbCBuZXR3b3JrLmJpYgAADgDGAGIAQQAgAHMAeQBtAG0AZQB0AHIAaQBjACAAawBlAHkAIABjAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAgAHUAcwBpAG4AZwAgAGcAZQBuAGUAdABpAGMAIABhAGwAZwBvAHIAaQB0AGgAbQAgAGEAbgBkACAAZQByAHIAbwByACAAYgBhAGMAawAgAHAAcgBvAHAAYQBnAGEAdABpAG8AbgAgAG4AZQB1AHIAYQBsACAAbgBlAHQAdwBvAHIAawAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAm1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRXZvbHV0aW9uYXJ5L0Egc3ltbWV0cmljIGtleSBjcnlwdG9ncmFwaHkgdXNpbmcgZ2VuZXRpYyBhbGdvcml0aG0gYW5kIGVycm9yIGJhY2sgcHJvcGFnYXRpb24gbmV1cmFsIG5ldHdvcmsuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACuAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA4w=}}

@article{DBLP:journals/corr/OordKK16,
	Archiveprefix = {arXiv},
	Author = {A{\"{a}}ron van den Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/OordKK16},
	Date-Added = {2018-02-19 09:01:51 +0000},
	Date-Modified = {2018-02-19 09:05:45 +0000},
	Eprint = {1601.06759},
	Journal = {CoRR},
	Keywords = {RNN, PixelRNN, BiLSTM, MNIST},
	Timestamp = {Wed, 07 Jun 2017 14:40:22 +0200},
	Title = {Pixel Recurrent Neural Networks},
	Url = {http://arxiv.org/abs/1601.06759},
	Volume = {abs/1601.06759},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1601.06759}}

@article{DBLP:journals/corr/KulkarniWKT15,
	Archiveprefix = {arXiv},
	Author = {Tejas D. Kulkarni and Will Whitney and Pushmeet Kohli and Joshua B. Tenenbaum},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.org/rec/bib/journals/corr/KulkarniWKT15},
	Date-Added = {2018-02-19 04:44:04 +0000},
	Date-Modified = {2018-02-19 04:44:43 +0000},
	Eprint = {1503.03167},
	Journal = {CoRR},
	Keywords = {CNN, IGN, SGVB, DC-IGN},
	Timestamp = {Wed, 07 Jun 2017 14:40:29 +0200},
	Title = {Deep Convolutional Inverse Graphics Network},
	Url = {http://arxiv.org/abs/1503.03167},
	Volume = {abs/1503.03167},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/abs/1503.03167}}

@article{2017arXiv170100160G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170100160G},
	Archiveprefix = {arXiv},
	Author = {{Goodfellow}, I.},
	Date-Added = {2018-02-16 06:51:27 +0000},
	Date-Modified = {2018-02-16 06:51:27 +0000},
	Eprint = {1701.00160},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = dec,
	Primaryclass = {cs.LG},
	Title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
	Year = 2017}

@article{2017arXiv170900199H,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170900199H},
	Archiveprefix = {arXiv},
	Author = {{Hadad}, N. and {Wolf}, L. and {Shahar}, M.},
	Date-Added = {2018-02-07 01:32:29 +0000},
	Date-Modified = {2018-02-07 01:32:29 +0000},
	Eprint = {1709.00199},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Statistics - Machine Learning},
	Month = sep,
	Primaryclass = {cs.LG},
	Title = {{Two-Step Disentanglement for Financial Data}},
	Year = 2017}

@inproceedings{10.1007/3-540-28438-9_2,
	Abstract = {Backwards calculation of derivatives -- sometimes called the reverse mode, the full adjoint method, or backpropagation -- has been developed and applied in many fields. This paper reviews several strands of history, advanced capabilities and types of application -- particularly those which are crucial to the development of brain-like capabilities in intelligent control and artificial intelligence.},
	Address = {Berlin, Heidelberg},
	Author = {Werbos, Paul J.},
	Booktitle = {Automatic Differentiation: Applications, Theory, and Implementations},
	Date-Added = {2018-02-07 01:11:34 +0000},
	Date-Modified = {2018-04-15 11:11:31 +0000},
	Editor = {B{\"u}cker, Martin and Corliss, George and Naumann, Uwe and Hovland, Paul and Norris, Boyana},
	Isbn = {978-3-540-28438-3},
	Keywords = {NN, Backpropagation},
	Pages = {15--34},
	Publisher = {Springer Berlin Heidelberg},
	Title = {Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities},
	Year = {2006}}

@article{2017arXiv170107274L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170107274L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y.},
	Date-Added = {2018-02-06 09:19:00 +0000},
	Date-Modified = {2018-02-06 09:19:00 +0000},
	Eprint = {1701.07274},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning},
	Month = jan,
	Primaryclass = {cs.LG},
	Title = {{Deep Reinforcement Learning: An Overview}},
	Year = 2017}

@incollection{NIPS2017_6917,
	Author = {XIAO, SHUAI and Farajtabar, Mehrdad and Ye, Xiaojing and Yan, Junchi and Yang, Xiaokang and Song, Le and Zha, Hongyuan},
	Booktitle = {Advances in Neural Information Processing Systems 30},
	Date-Added = {2018-02-06 09:12:15 +0000},
	Date-Modified = {2018-02-06 09:13:20 +0000},
	Editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	Keywords = {DNN, Generative Adversarial NN, Point processes},
	Pages = {3250--3259},
	Publisher = {Curran Associates, Inc.},
	Title = {Wasserstein Learning of Deep Generative Point Process Models},
	Url = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf},
	Year = {2017},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/6917-wasserstein-learning-of-deep-generative-point-process-models.pdf}}

@book{TheWayOfTheTurtle,
	Author = {Curtis M. Faith},
	Date-Added = {2018-02-06 08:38:52 +0000},
	Date-Modified = {2018-02-06 08:40:50 +0000},
	Keywords = {Finance, Economics Personal, Professional Development},
	Month = {March},
	Number = {9780071486644},
	Publisher = {McGraw-Hill Osborne Media},
	Title = {Way of the Turtle},
	Year = {2007}}

@article{2015arXiv150205767G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv150205767G},
	Archiveprefix = {arXiv},
	Author = {{Gunes Baydin}, A. and {Pearlmutter}, B.~A. and {Andreyevich Radul}, A. and {Siskind}, J.~M.},
	Date-Added = {2018-02-06 01:01:08 +0000},
	Date-Modified = {2018-09-26 08:04:03 +0000},
	Eprint = {1502.05767},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Symbolic Computation, Computer Science - Learning, Automatic Differentiation, Automatic Differentiation, Automatic Differentiation, G.1.4, I.2.6},
	Month = feb,
	Primaryclass = {cs.SC},
	Title = {{Automatic differentiation in machine learning: a survey}},
	Year = 2015}

@misc{2015arXiv151106434R-sc,
	Author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	Date-Added = {2018-02-05 09:50:12 +0000},
	Date-Modified = {2018-02-23 00:38:59 +0000},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	Url = {https://github.com/Newmu/dcgan_code},
	Year = 2015,
	Bdsk-Url-1 = {https://github.com/Newmu/dcgan_code}}

@article{2015arXiv151106434R,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151106434R},
	Archiveprefix = {arXiv},
	Author = {{Radford}, A. and {Metz}, L. and {Chintala}, S.},
	Date-Added = {2018-02-05 09:50:12 +0000},
	Date-Modified = {2018-02-05 09:50:12 +0000},
	Eprint = {1511.06434},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Learning, Computer Science - Computer Vision and Pattern Recognition},
	Month = nov,
	Primaryclass = {cs.LG},
	Title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
	Year = 2015}

@misc{Grenade-Github-Code,
	Address = {San Francisco, CA, USA},
	Author = {Campbell, Huw},
	Date-Added = {2018-02-05 09:27:10 +0000},
	Date-Modified = {2018-02-23 00:39:26 +0000},
	Day = {24},
	Keywords = {functional programming, DNN, CNN, sourcecode},
	Month = {June},
	Publisher = {Github},
	Title = {Grenade},
	Url = {https://github.com/HuwCampbell/grenade},
	Year = {2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB1Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRGVjZW50cmFsaXplZC9CaXRXb3JrZXIsIGEgRGVjZW50cmFsaXplZCBEaXN0cmlidXRlZCBDb21wdXRpbmcgU3lzdGVtIEJhc2VkIG9uIEJpdFRvcnJlbnQuYmliTxECkgAAAAACkgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0JpdFdvcmtlciwgYSBEZWNlbiNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAA1EZWNlbnRyYWxpemVkAAACAIsvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RGVjZW50cmFsaXplZDpCaXRXb3JrZXIsIGEgRGVjZW50cmFsaXplZCBEaXN0cmlidXRlZCBDb21wdXRpbmcgU3lzdGVtIEJhc2VkIG9uIEJpdFRvcnJlbnQuYmliAAAOAKAATwBCAGkAdABXAG8AcgBrAGUAcgAsACAAYQAgAEQAZQBjAGUAbgB0AHIAYQBsAGkAegBlAGQAIABEAGkAcwB0AHIAaQBiAHUAdABlAGQAIABDAG8AbQBwAHUAdABpAG4AZwAgAFMAeQBzAHQAZQBtACAAQgBhAHMAZQBkACAAbwBuACAAQgBpAHQAVABvAHIAcgBlAG4AdAAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAiVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRGVjZW50cmFsaXplZC9CaXRXb3JrZXIsIGEgRGVjZW50cmFsaXplZCBEaXN0cmlidXRlZCBDb21wdXRpbmcgU3lzdGVtIEJhc2VkIG9uIEJpdFRvcnJlbnQuYmliAAATAAEvAAAVAAIAC///AAAACAANABoAJACcAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAzI=},
	Bdsk-Url-1 = {https://github.com/HuwCampbell/grenade}}

@inproceedings{Zhang:2015:OFA:2684746.2689060,
	Acmid = {2689060},
	Address = {New York, NY, USA},
	Author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
	Booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
	Date-Added = {2018-02-05 09:06:58 +0000},
	Date-Modified = {2018-09-26 08:04:22 +0000},
	Doi = {10.1145/2684746.2689060},
	Isbn = {978-1-4503-3315-3},
	Keywords = {Acceleration, convolutional neural network, fpga, roofline model},
	Location = {Monterey, California, USA},
	Numpages = {10},
	Pages = {161--170},
	Publisher = {ACM},
	Series = {FPGA '15},
	Title = {Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks},
	Url = {http://doi.acm.org/10.1145/2684746.2689060},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684746.2689060},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2684746.2689060}}

@article{2016arXiv161102450W,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv161102450W},
	Archiveprefix = {arXiv},
	Author = {{Wang}, D. and {An}, J. and {Xu}, K.},
	Date-Added = {2018-02-05 09:06:58 +0000},
	Date-Modified = {2018-06-22 10:46:32 +0000},
	Eprint = {1611.02450},
	Journal = {ArXiv e-prints},
	Keywords = {FPGA-NN, Computer Science - Hardware Architecture},
	Month = nov,
	Title = {{PipeCNN: An OpenCL-Based FPGA Accelerator for Large-Scale Convolution Neuron Networks}},
	Year = 2016}

@article{anonymous2018wavelet,
	Author = {Anonymous},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-12-02 12:41:25 +1300},
	Journal = {International Conference on Learning Representations},
	Keywords = {CNN, Wavelet, MIST, MathConvNet},
	Title = {Wavelet Pooling for Convolutional Neural Networks},
	Url = {https://openreview.net/forum?id=rkhlb8lCZ},
	Year = {2018},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCALi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ3J5cHRvL0ZQR0EtQmFzZWQgSGlnaC1QZXJmb3JtYW5jZSBQYXJhbGxlbCBBcmNoaXRlY3R1cmUgZm9yIEhvbW9tb3JwaGljIENvbXB1dGluZyBvbiBFbmNyeXB0ZWQgRGF0YS5iaWJPEQLCAAAAAALCAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRlBHQS1CYXNlZCBIaWdoLVBlI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABkNyeXB0bwACAJYvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Q3J5cHRvOkZQR0EtQmFzZWQgSGlnaC1QZXJmb3JtYW5jZSBQYXJhbGxlbCBBcmNoaXRlY3R1cmUgZm9yIEhvbW9tb3JwaGljIENvbXB1dGluZyBvbiBFbmNyeXB0ZWQgRGF0YS5iaWIADgDEAGEARgBQAEcAQQAtAEIAYQBzAGUAZAAgAEgAaQBnAGgALQBQAGUAcgBmAG8AcgBtAGEAbgBjAGUAIABQAGEAcgBhAGwAbABlAGwAIABBAHIAYwBoAGkAdABlAGMAdAB1AHIAZQAgAGYAbwByACAASABvAG0AbwBtAG8AcgBwAGgAaQBjACAAQwBvAG0AcAB1AHQAaQBuAGcAIABvAG4AIABFAG4AYwByAHkAcAB0AGUAZAAgAEQAYQB0AGEALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAJRVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL0NyeXB0by9GUEdBLUJhc2VkIEhpZ2gtUGVyZm9ybWFuY2UgUGFyYWxsZWwgQXJjaGl0ZWN0dXJlIGZvciBIb21vbW9ycGhpYyBDb21wdXRpbmcgb24gRW5jcnlwdGVkIERhdGEuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAKcAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADbQ==},
	Bdsk-Url-1 = {https://openreview.net/forum?id=rkhlb8lCZ}}

@book{Nayak:2017aa,
	Author = {Nayak, Sarat and Bihari Misra, Bijan and Behera, Dr. H.},
	Date = {2017/01/01},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.4018/978-1-5225-0788-8.ch022},
	Journal = {Nature-Inspired Computing: Concepts, Methodologies, Tools, and Applications},
	Keywords = {HONN, Pi-Sigma, Genetic Algorithms},
	Month = {01},
	N2 = {This chapter presents two higher order neural networks (HONN) for efficient prediction of stock market behavior. The models include Pi-Sigma, and Sigma-Pi higher order neural network models. Along with the traditional gradient descent learning, how the evolutionary computation technique such as genetic algorithm (GA) can be used effectively for the learning process is also discussed here. The learning process is made adaptive to handle the noise and uncertainties associated with stock market data. Further, different prediction approaches are discussed here and application of HONN for time series forecasting is illustrated with real life data taken from a number of stock markets across the globe.},
	Title = {Adaptive Hybrid Higher Order Neural Networks for Prediction of Stock Market Behavior},
	Ty = {BOOK},
	Year = {2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.4018/978-1-5225-0788-8.ch022}}

@article{CHONG2017187,
	Abstract = {We offer a systematic analysis of the use of deep learning networks for stock market analysis and prediction. Its ability to extract features from a large set of raw data without relying on prior knowledge of predictors makes deep learning potentially attractive for stock market prediction at high frequencies. Deep learning algorithms vary considerably in the choice of network structure, activation function, and other model parameters, and their performance is known to depend heavily on the method of data representation. Our study attempts to provides a comprehensive and objective assessment of both the advantages and drawbacks of deep learning algorithms for stock market analysis and prediction. Using high-frequency intraday stock returns as input data, we examine the effects of three unsupervised feature extraction methods---principal component analysis, autoencoder, and the restricted Boltzmann machine---on the network's overall ability to predict future market behavior. Empirical results suggest that deep neural networks can extract additional information from the residuals of the autoregressive model and improve prediction performance; the same cannot be said when the autoregressive model is applied to the residuals of the network. Covariance estimation is also noticeably improved when the predictive network is applied to covariance-based market structure analysis. Our study offers practical insights and potentially useful directions for further investigation into how deep learning networks can be effectively used for stock market analysis and prediction.},
	Author = {Eunsuk Chong and Chulwoo Han and Frank C. Park},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.eswa.2017.04.030},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {Stock market prediction, Deep learning, Multilayer neural network, Covariance estimation},
	Pages = {187 - 205},
	Title = {Deep learning networks for stock market analysis and prediction: Methodology, data representations, and case studies},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Volume = {83},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417417302750},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2017.04.030}}

@inproceedings{HuangY.2016Etmt,
	Author = {Huang, Y. and Huang, K. and Wang, Y. and Zhang, H. and Guan, J. and Zhou, S.},
	Copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {9783319422961},
	Issn = {03029743},
	Journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	Keywords = {Convolutional Neural Network ; Deep Neural Network ; Financial Trend Prediction ; Twitter Mood},
	Pages = {449--460},
	Publisher = {Springer Verlag},
	Title = {Exploiting twitter moods to boost financial trend prediction based on deep network models},
	Volume = {9773},
	Year = {2016}}

@article{diartificial,
	Author = {Di Persio, Luca and Honchar, Oleksandr},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Economics and Management Systems},
	Keywords = {CNN, RNN, LSTM},
	Month = {January},
	Pages = {5},
	Title = {Artificial neural networks approach to the forecast of stock market price movements},
	Volume = {1},
	Year = {2016}}

@techreport{ghoshal2017reading,
	Author = {Ghoshal, Sid and Roberts, Stephen},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Institution = {Technical report},
	Keywords = {CNN, RNN, Technical Analysis},
	Title = {Reading the Tea Leaves: A Neural Network Perspective on Technical Trading},
	Year = {2017}}

@article{aggarwal2017deep,
	Author = {Aggarwal, Saurabh and Aggarwal, Somya},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Computer Applications},
	Keywords = {DNN, Finance, Portfolio, LSTM},
	Number = {2},
	Publisher = {Foundation of Computer Science},
	Title = {Deep Investment in Financial Markets using Deep Learning Models},
	Volume = {162},
	Year = {2017}}

@article{di2016artificial,
	Author = {Di Persio, Luca and Honchar, Oleksandr},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Journal = {International Journal of Circuits, Systems and Signal Processing},
	Keywords = {CNN, MLP, LSTM, Wavelet},
	Pages = {403--413},
	Title = {Artificial Neural Networks architectures for stock price prediction: comparisons and applications},
	Volume = {10},
	Year = {2016}}

@article{dixon2016classification,
	Author = {Dixon, Matthew Francis and Klabjan, Diego and Bang, Jin Hoon},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {DNN, Futures, Finance},
	Title = {Classification-based Financial Markets Prediction using Deep Neural Networks},
	Year = {2016}}

@misc{essay59381,
	Author = {M. {Kooijman}},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {Haskell, functional programming},
	Month = {December},
	Title = {Haskell as a higher order structural hardware description language},
	Url = {http://essay.utwente.nl/59381/},
	Year = {2009},
	Bdsk-Url-1 = {http://essay.utwente.nl/59381/}}

@conference{L:08,
	Author = {Philip H.W. Leong},
	Booktitle = {Proc. 4th IEEE International Symposium on Electronic Design, Test and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {FPGA, Trends},
	Location = {Hong Kong},
	Note = {\textbf{Invited}},
	Pages = {137--141},
	Title = {Recent Trends in {FPGA} Architectures and Applications},
	Url = {rtfpga_delta08.pdf},
	Year = {2008},
	Bdsk-Url-1 = {rtfpga_delta08.pdf}}

@inproceedings{Pike:2009:RYO:1596638.1596646,
	Acmid = {1596646},
	Address = {New York, NY, USA},
	Author = {Pike, Lee and Brown, Geoffrey and Goodloe, Alwyn},
	Booktitle = {Proceedings of the 2Nd ACM SIGPLAN Symposium on Haskell},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1596638.1596646},
	Isbn = {978-1-60558-508-6},
	Keywords = {emulation, functional programming, physical-layer protocol testing},
	Location = {Edinburgh, Scotland},
	Numpages = {8},
	Pages = {61--68},
	Publisher = {ACM},
	Series = {Haskell '09},
	Title = {Roll Your Own Test Bed for Embedded Real-time Protocols: A Haskell Experience},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Year = {2009},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA5Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliTxEBrAAAAAABrAACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////HkVuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAJOTgACAE8vOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk46RW5kLVRvLUVuZCBNZW1vcnkgTmV0d29ya3MuYmliAAAOAD4AHgBFAG4AZAAtAFQAbwAtAEUAbgBkACAATQBlAG0AbwByAHkAIABOAGUAdAB3AG8AcgBrAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAE1Vc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OL0VuZC1Uby1FbmQgTWVtb3J5IE5ldHdvcmtzLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAYAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAIQ},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1596638.1596646},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1596638.1596646}}

@inproceedings{Schrage:2005:HRD:1088348.1088351,
	Acmid = {1088351},
	Address = {New York, NY, USA},
	Author = {Schrage, Martijn M. and van IJzendoorn, Arjan and van der Gaag, Linda C.},
	Booktitle = {Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1088348.1088351},
	Isbn = {1-59593-071-X},
	Keywords = {application, bayesian networks, graphical user interface, haskell, wxHaskell},
	Location = {Tallinn, Estonia},
	Numpages = {10},
	Pages = {17--26},
	Publisher = {ACM},
	Series = {Haskell '05},
	Title = {Haskell Ready to Dazzle the Real World},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1088348.1088351},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1088348.1088351}}

@article{Sezer:2017aa,
	Author = {Sezer, Omer Berat and Ozbayoglu, Murat and Dogdu, Erdogan},
	Booktitle = {Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 --November 1, 2017, Chicago, Illinois, USA},
	Da = {2017/01/01/},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.procs.2017.09.031},
	Isbn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Stock Trading; Stock Market; Deep Neural-Network; Evolutionary Algorithms; Technical Analysis},
	Number = {Supplement C},
	Pages = {473--480},
	Title = {A Deep Neural-Network Based Stock Trading System Based on Evolutionary Optimized Technical Analysis Parameters},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Volume = {114},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050917318252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2017.09.031}}

@inproceedings{Kablan:2009aa,
	Author = {A. Kablan},
	Booktitle = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ADVCOMP.2009.23},
	Journal = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Journal1 = {2009 Third International Conference on Advanced Engineering Computing and Applications in Sciences},
	Keywords = {decision making; expert systems; financial data processing; fuzzy reasoning; neural nets; pattern recognition; stock markets; adaptive neuro fuzzy inference systems; automated trading strategy; decision making; efficient market hypothesis; expert system; financial forecasting; financial markets; financial time series; fuzzy reasoning; high frequency financial trading; neural networks; pattern recognition; Adaptive systems; Economic forecasting; Expert systems; Finance; Frequency; Fuzzy reasoning; Fuzzy systems; Humans; Neural networks; Pattern recognition; efficient market hypothesis; financial prediction; high frequency trading; neuro-fuzzy inference system},
	Pages = {105--110},
	Title = {Adaptive Neuro Fuzzy Inference Systems for High Frequency Financial Trading and Forecasting},
	Ty = {CONF},
	Year = {2009},
	Year1 = {11-16 Oct. 2009},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ADVCOMP.2009.23}}

@article{2003cond.mat..4469K,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2003cond.mat..4469K},
	Author = {{Kondratenko}, V.~V. and {Kuperin}, Y.~A},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {cond-mat/0304469},
	Journal = {eprint arXiv:cond-mat/0304469},
	Keywords = {Condensed Matter - Disordered Systems and Neural Networks, Quantitative Finance - Statistical Finance},
	Month = apr,
	Title = {{Using Recurrent Neural Networks To Forecasting of Forex}},
	Year = 2003}

@article{2015arXiv151207108G,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2015arXiv151207108G},
	Archiveprefix = {arXiv},
	Author = {{Gu}, J. and {Wang}, Z. and {Kuen}, J. and {Ma}, L. and {Shahroudy}, A. and {Shuai}, B. and {Liu}, T. and {Wang}, X. and {Wang}, L. and {Wang}, G. and {Cai}, J. and {Chen}, T.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1512.07108},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	Month = dec,
	Primaryclass = {cs.CV},
	Title = {{Recent Advances in Convolutional Neural Networks}},
	Year = 2015}

@article{Niedermeier:2014aa,
	Abstract = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	Author = {Niedermeier,A.},
	Date = {2014/8/29},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.3990/1.9789036537322},
	Isbn = {978-90-365-3732-2},
	Keywords = {IR-91607; EWI-25011; METIS-304761},
	M3 = {PhD Thesis - Research UT, graduation UT},
	Month = {8},
	N2 = {Data driven streaming applications are quite common in modern multimedia and wireless applications, like for example video and audio processing. The main components of these applications are Digital Signal Processing (DSP) algorithms. These algorithms are not extremely complex in terms of their structure and the operations that make up the algorithms are fairly simple (usually binary mathematical operations like addition and multiplication). What makes it challenging to implement and execute these algorithms efficiently is their large degree of fine-grained parallelism and the required throughput. DSP algorithms can usually be described as dataflow graphs with nodes corresponding to operations and edges between the nodes expressing data dependencies. A node fires, i.e. executes, as soon as all required input data has arrived at its input edge(s). To execute DSP algorithms efficiently while maintaining flexibility, coarse-grained reconfigurable arrays (CGRAs) can be used. CGRAs are composed of a set of small, reconfigurable cores, interconnected in e.g. a two dimensional array. Each core by itself is not very powerful, yet the complete array of cores forms an efficient architecture with a high throughput due to its ability to efficiently execute operations in parallel. In this thesis, we present a CGRA targeted at data driven streaming DSP applications that contain a large degree of fine grained parallelism, such as matrix manipulations or filter algorithms. Along with the architecture, also a programming language is presented that can directly describe DSP applications as dataflow graphs which are then automatically mapped and executed on the architecture. In contrast to previously published work on CGRAs, the guiding principle and inspiration for the presented CGRA and its corresponding programming paradigm is the dataflow principle. The result of this work is a completely integrated framework targeted at streaming DSP algorithms, consisting of a CGRA, a programming language and a compiler. The complete system is based on dataflow principles. We conclude that by using an architecture that is based on dataflow principles and a corresponding programming paradigm that can directly express dataflow graphs, DSP algorithms can be implemented in a very intuitive and straightforward manner.},
	Title = {A fine-grained parallel dataflow-inspired architecture for streaming applications},
	Ty = {THES},
	U2 = {10.3990/1.9789036537322},
	Year = {2014},
	Year1 = {2014/8/29},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036537322}}

@inbook{Smit:2010aa,
	Abstract = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	Annote = {eemcs-eprint-19169},
	Author = {Smit,Gerardus Johannes Maria and Kuper,Jan and Baaij,C. P. R.},
	Booktitle = {Dagstuhl Seminar on Dynamically Reconfigurable Architectures},
	Date = {2010/12/14},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.4230/OASIcs.WCET.2010.136},
	Keywords = {IR-75334; METIS-275806; Hardware design; EC Grant Agreement nr.: FP7/248465; Streaming Applications; EWI-19169; mathematical specification},
	M3 = {Conference contribution},
	Month = {12},
	N2 = {Today the hardware for embedded systems is often specified in VHDL. However, VHDL describes the system at a rather low level, which is cumbersome and may lead to design faults in large real life applications. There is a need of higher level abstraction mechanisms. In the embedded systems group of the University of Twente we are working on systematic and transformational methods to design hardware architectures, both multi core and single core. The main line in this approach is to start with a straightforward (often mathematical) specification of the problem. The next step is to find some adequate transformations on this specification, in particular to find specific optimizations, to be able to distribute the application over different cores. The result of these transformations is then translated into the functional programming language Haskell since Haskell is close to mathematics and such a translation often is straightforward. Besides, the Haskell code is executable, so one immediately has a simulation of the intended system. Next, the resulting Haskell specification is given to a compiler, called C{\"e}aSH (for CAES LAnguage for Synchronous Hardware) which translates the specification into VHDL. The resulting VHDL is synthesizable, so from there on standard VHDL-tooling can be used for synthesis. In this work we primarily focus on streaming applications: i.e. applications that can be modeled as data-flow graphs. At the moment the C{\"e}aSH system is ready in prototype form and in the presentation we will give several examples of how it can be used. In these examples it will be shown that the specification code is clear and concise. Furthermore, it is possible to use powerful abstraction mechanisms, such as polymorphism, higher order functions, pattern matching, lambda abstraction, partial application. These features allow a designer to describe circuits in a more natural and concise way than possible with the language elements found in the traditional hardware description languages. In addition we will give some examples of transformations that are possible in a mathematical specification, and which do not suffer from the problems encountered in, e.g., automatic parallelization of nested for-loops in C-programs.},
	Pages = {11},
	Publisher = {Internationales Begegnungs- und Forschungszentrum fur Informatik (IBFI)},
	Title = {A mathematical approach towards hardware design},
	Title1 = {Dagstuhl Seminar Proceedings},
	Ty = {CHAP},
	U2 = {10.4230/OASIcs.WCET.2010.136},
	Year = {2010},
	Year1 = {2010/12/14},
	Bdsk-Url-1 = {https://dx.doi.org/10.4230/OASIcs.WCET.2010.136}}

@article{Wester:2015aa,
	Abstract = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	Author = {Wester,Rinse},
	Date = {2015/7/3},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.3990/1.9789036538879},
	Isbn = {978-90-365-3887-9},
	Keywords = {Higher-order functions; EWI-26125; IR-96278; METIS-310874; transformations; Hardware design},
	M3 = {PhD Thesis - Research UT, graduation UT},
	Month = {7},
	N2 = {The amount of resources available on reconfigurable logic devices like FPGAs has seen a tremendous growth over the last thirty years. During this period, the amount of programmable resources (CLBs and RAMs) has increased by more than three orders of magnitude. Programming these reconfigurable architectures has been dominated by the hardware description languages VHDL and Verilog. However, it has become generally accepted that these languages do not provide adequate abstraction mechanisms to deliver the design productivity for designing more and more complex applications. To raise the abstraction level, techniques to translate high-level languages to hardware have been developed based on imperative languages like C. Parallelism is achieved by parallelization of for-loops. Whether parallelization of loops is possible, is determined using dependency analysis which is a very hard problem. To mitigate this problem, other abstractions are needed to express parallelism. In this thesis, parallelism is expressed using higher-order functions, an abstraction commonly used in functional programming languages. The main contribution of this thesis is a design methodology based on exploiting regularity of higher-order functions. A mathematical formula, e.g., a DSP algorithm, is first formulated using higher-order functions. Then, transformation rules are applied to these higher-order functions to distribute computations over space and time. Using these transformations, an optimal trade-off can be made between space and time. Finally, hardware is generated using the CLaSH compiler by translating the result of the transformation to VHDL. In this thesis, we derive transformation rules for several higher-order functions and prove that the transformations are meaning-preserving. After transformation, a mathematically equivalent description is derived in which the computations are distributed over space and time. The designer can control the amount of parallelism using a parameter that is introduced by the transformation. Transformation rules for both one-dimensional higher-order functions and two-dimensional higher- order functions have been derived and applied to several case studies: a dot product, a particle filter and stencil computations.},
	Publisher = {Universiteit Twente},
	Title = {A transformation-based approach to hardware design using higher-order functions},
	Ty = {THES},
	U2 = {10.3990/1.9789036538879},
	Year = {2015},
	Year1 = {2015/7/3},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxCRLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0EgRGVlcCBOZXVyYWwtTmV0d29yayBCYXNlZCBTdG9jayBUcmFkaW5nIFN5c3RlbSBCYXNlZCBvbiBFdm9sdXRpb25hcnkgT3B0aW1pemVkIFRlY2huaWNhbCBBbmFseXNpcyBQYXJhbWV0ZXJzLnJpc08RAwgAAAAAAwgAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BIERlZXAgTmV1cmFsLU5ldHcjRkZGRkZGRkYucmlzAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAGTk4tRmluAAIApy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTi1GaW46QSBEZWVwIE5ldXJhbC1OZXR3b3JrIEJhc2VkIFN0b2NrIFRyYWRpbmcgU3lzdGVtIEJhc2VkIG9uIEV2b2x1dGlvbmFyeSBPcHRpbWl6ZWQgVGVjaG5pY2FsIEFuYWx5c2lzIFBhcmFtZXRlcnMucmlzAAAOAOYAcgBBACAARABlAGUAcAAgAE4AZQB1AHIAYQBsAC0ATgBlAHQAdwBvAHIAawAgAEIAYQBzAGUAZAAgAFMAdABvAGMAawAgAFQAcgBhAGQAaQBuAGcAIABTAHkAcwB0AGUAbQAgAEIAYQBzAGUAZAAgAG8AbgAgAEUAdgBvAGwAdQB0AGkAbwBuAGEAcgB5ACAATwBwAHQAaQBtAGkAegBlAGQAIABUAGUAYwBoAG4AaQBjAGEAbAAgAEEAbgBhAGwAeQBzAGkAcwAgAFAAYQByAGEAbQBlAHQAZQByAHMALgByAGkAcwAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAKVVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9BIERlZXAgTmV1cmFsLU5ldHdvcmsgQmFzZWQgU3RvY2sgVHJhZGluZyBTeXN0ZW0gQmFzZWQgb24gRXZvbHV0aW9uYXJ5IE9wdGltaXplZCBUZWNobmljYWwgQW5hbHlzaXMgUGFyYW1ldGVycy5yaXMAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkALgAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAADxA==},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB+Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0FkYXB0aXZlIE5ldXJvIEZ1enp5IEluZmVyZW5jZSBTeXN0ZW1zIGZvciBIaWdoIEZyZXF1ZW5jeSBGaW5hbmNpYWwgVHJhZGluZyBhbmQgRm9yZWNhc3RpbmcucmlzTxECugAAAAACugACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H0FkYXB0aXZlIE5ldXJvIEZ1eiNGRkZGRkZGRi5yaXMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgCULzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpBZGFwdGl2ZSBOZXVybyBGdXp6eSBJbmZlcmVuY2UgU3lzdGVtcyBmb3IgSGlnaCBGcmVxdWVuY3kgRmluYW5jaWFsIFRyYWRpbmcgYW5kIEZvcmVjYXN0aW5nLnJpcwAOAMAAXwBBAGQAYQBwAHQAaQB2AGUAIABOAGUAdQByAG8AIABGAHUAegB6AHkAIABJAG4AZgBlAHIAZQBuAGMAZQAgAFMAeQBzAHQAZQBtAHMAIABmAG8AcgAgAEgAaQBnAGgAIABGAHIAZQBxAHUAZQBuAGMAeQAgAEYAaQBuAGEAbgBjAGkAYQBsACAAVAByAGEAZABpAG4AZwAgAGEAbgBkACAARgBvAHIAZQBjAGEAcwB0AGkAbgBnAC4AcgBpAHMADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCSVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi1GaW4vQWRhcHRpdmUgTmV1cm8gRnV6enkgSW5mZXJlbmNlIFN5c3RlbXMgZm9yIEhpZ2ggRnJlcXVlbmN5IEZpbmFuY2lhbCBUcmFkaW5nIGFuZCBGb3JlY2FzdGluZy5yaXMAEwABLwAAFQACAAv//wAAAAgADQAaACQApQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAANj},
	Bdsk-File-3 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBaLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL1VzaW5nIFJlY3VycmVudCBOZXVyYWwgTmV0d29ya3MgVG8gRm9yZWNhc3Rpbmcgb2YgRm9yZXguYmliTxECKgAAAAACKgACAAAJTWFjaW50b3NoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEJEAAH/////H1VzaW5nIFJlY3VycmVudCBOZSNGRkZGRkZGRi5iaWIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAP////8AAAAAAAAAAAAAAAAAAQAEAAAKIGN1AAAAAAAAAAAAAAAAAAZOTi1GaW4AAgBwLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOk5OLUZpbjpVc2luZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIFRvIEZvcmVjYXN0aW5nIG9mIEZvcmV4LmJpYgAOAHgAOwBVAHMAaQBuAGcAIABSAGUAYwB1AHIAcgBlAG4AdAAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzACAAVABvACAARgBvAHIAZQBjAGEAcwB0AGkAbgBnACAAbwBmACAARgBvAHIAZQB4AC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgBuVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9OTi1GaW4vVXNpbmcgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3JrcyBUbyBGb3JlY2FzdGluZyBvZiBGb3JleC5iaWIAEwABLwAAFQACAAv//wAAAAgADQAaACQAgQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKv},
	Bdsk-Url-1 = {https://dx.doi.org/10.3990/1.9789036538879}}

@inproceedings{Wester:2012aa,
	Author = {R. Wester and C. Baaij and J. Kuper},
	Booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2012.6339258},
	Isbn = {1946-147X},
	Journal = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Journal1 = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Keywords = {field programmable gate arrays; functional languages; hardware description languages; logic design; mathematical analysis; particle filtering (numerical methods); program compilers; C\&{\#}x03BB;aSH HDL; DSP application; FPGA; Haskell; adequate abstraction mechanisms; functional hardware description language; higher level abstraction mechanism; higher-order function; mathematical definition; particle filtering; polymorphism; two step hardware design method; Atmospheric measurements; Design methodology; Equations; Hardware; Mathematical model; Particle measurements; Systematics},
	Pages = {181--188},
	Title = {A two step hardware design method using C\&{\#}x03BB;aSH},
	Ty = {CONF},
	Year = {2012},
	Year1 = {29-31 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339258}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5faB,
	Abstract = {Functional hardware description languages are a class of hardware description languages that emphasize on the ability to express higher level structural properties, such a parameterization and regularity. Due to such features as higher-order functions and polymorphism, parameterization in functional hardware description languages is more natural than the parameterization support found in the more traditional hardware description languages, like VHDL and Verilog. We de- velop a new functional hardware description language, CλasH, that borrows both the syntax and semantics from the general-purpose functional programming language Haskell.},
	Author = {C.P.R. Baaij},
	Booktitle = {ClasH - From Haskell To Hardware},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {Haskell, CLaSH},
	Month = {12},
	Pages = {101},
	Publisher = {University of Twente},
	Title = {ClasH - From Haskell To Hardware},
	Year = {2009}}

@misc{essay70777,
	Abstract = {ClaSH is a functional hardware description language (HDL) developed at the CAES
group of the University of Twente. ClaSH borrows both the syntax and semantics from the general-purpose functional programming language Haskell, meaning that circuit designers can define their circuits with regular Haskell syntax.

In this thesis, research is done on the co-simulation of ClaSH and traditional HDLs. The Verilog Procedural Interface (VPI), as defined in the IEEE 1364 standard, is used to set-up the communication and to control a Verilog simulator. An implementation is made, as will be described in this thesis, to show the practical feasibility of co-simulation of ClaSH and Verilog.},
	Author = {J.G.J. {Verheij}},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Keywords = {CLaSH, Haskell, Simulation, HDL},
	Month = {August},
	Title = {Co-simulation between C$\lambda$aSH and traditional HDLs},
	Url = {http://essay.utwente.nl/70777/},
	Year = {2016},
	Bdsk-Url-1 = {http://essay.utwente.nl/70777/}}

@inbook{5ecc1e5c8c7644a9bf1ff0c60033d5fa,
	Abstract = {As embedded systems are becoming increasingly complex, the design process and verification have become very time-consuming. Additionally, specifying hardware manually in a low-level hardware description language like VHDL is usually an error-prone task. In our group, a tool (the ClaSH compiler) was developed to generate fully synthesisable VHDL code from a specification given in the functional programming language Haskell. In this paper, we present a comparison between two implementations of the same design by using ClaSH and hand-written VHDL. The design is a simple dataflow processor. As measures of interest area, performance, power consumption and source lines of code (SLOC) are used. The obtained results indicate that the ClaSH -generated VHDL code as well as the netlist after synthesis and place and route are functionally correct. The placed and routed hand-written VHDL code has also the correct behaviour. Furthermore, a similar performance is achieved. The power consumption is even lower for the ClaSH implementation. The SLOC for ClaSH is considerably smaller and it is possible to specify the design in a much higher level of abstraction compared to VHDL.},
	Author = {A. Niedermeier and Rinse Wester and Rinse Wester and C.P.R. Baaij and Jan Kuper and Smit, {Gerardus Johannes Maria}},
	Booktitle = {Proceedings of the Workshop on PROGram for Research on Embedded Systems and Software (PROGRESS 2010)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-90-73461-67-3},
	Keywords = {METIS-277454, IR-75095, EWI-18902},
	Month = {11},
	Pages = {216--221},
	Publisher = {Technology Foundation (STW)},
	Title = {Comparing CλaSH and VHDL by implementing a dataflow processor},
	Year = {2010}}

@inproceedings{6339201,
	Author = {B. N. Uchevler and K. Svarstad},
	Booktitle = {22nd International Conference on Field Programmable Logic and Applications (FPL)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/FPL.2012.6339201},
	Issn = {1946-147X},
	Keywords = {circuit complexity;functional languages;hardware description languages;high level synthesis;program compilers;program verification;reconfigurable architectures;CLaSH;RT level VHDL;digital circuit description;digital circuit verification;dynamic reconfigurable system modeling;electronic design complexity;formal verification;functional HDL;high-level Haskell description translation;higher-order functions;parametrization;partial evaluation technique;polymorphism;run-time reconfigurable systems;synthesis tool chain;Communications technology;Consumer electronics;Educational institutions;Field programmable gate arrays;Hardware;Mathematical model;Unified modeling language},
	Month = {Aug},
	Pages = {481-482},
	Title = {Modeling of dynamic reconfigurable systems with Haskell},
	Year = {2012},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBnLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYk8RAmIAAAAAAmIAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////x9BIFB5dGhvbmljIEFwcHJvYWMjRkZGRkZGRkYuYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAERlBHQQACAH0vOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6RlBHQTpBIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYgAADgCWAEoAQQAgAFAAeQB0AGgAbwBuAGkAYwAgAEEAcABwAHIAbwBhAGMAaAAgAGYAbwByACAAUgBhAHAAaQBkACAASABhAHIAZAB3AGEAcgBlACAAUAByAG8AdABvAHQAeQBwAGkAbgBnACAAYQBuAGQAIABJAG4AcwB0AHIAdQBtAGUAbgB0AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAe1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BIFB5dGhvbmljIEFwcHJvYWNoIGZvciBSYXBpZCBIYXJkd2FyZSBQcm90b3R5cGluZyBhbmQgSW5zdHJ1bWVudGF0aW9uLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAjgAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAL0},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2012.6339201}}

@inbook{fff28539e56047c4adca5cc3c9c29cec,
	Abstract = {CλaSH, a functional hardware description language based on Haskell, has several abstraction mechanisms that allow a hardware designer to describe architectures in a short and concise way. In this paper we evaluate CλaSH on a complex DSP application, a Polyphase Filter Bank as it is used in the ASTRON APERTIF project. The Polyphase Filter Bank is implemented in two steps: first in Haskell as being close to a standard mathematical specification, then in CλaSH which is derived from the Haskell formulation by applying only minor changes. We show that the CλaSH formulation can be directly mapped to hardware, thus exploiting the parallelism and concurrency that is present in the original mathematical specification.},
	Author = {Rinse Wester and Dimitrios Sarakiotis and Eric Kooistra and Jan Kuper},
	Booktitle = {Communicating Process Architectures 2012},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-0-9565409-5-9},
	Keywords = {EWI-22586, EC Grant Agreement nr.: FP7/248465, Specification, METIS-289800, APERTIF Project, CλaSH, IR-82307},
	Month = {8},
	Note = {eemcs-eprint-22586},
	Pages = {53--64},
	Publisher = {Open Channel Publishing},
	Title = {Specification of APERTIF Polyphase Filter Bank in CλaSH},
	Year = {2012}}

@inproceedings{6523639,
	Author = {B. N. Uchevler and K. Svarstad and J. Kuper and C. Baaij},
	Booktitle = {International Symposium on Quality Electronic Design (ISQED)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ISQED.2013.6523639},
	Issn = {1948-3287},
	Keywords = {field programmable gate arrays;hardware description languages;logic design;CLaSH;FPGA design;RT level;Suzaku-sz410 board;digital circuit verification;dynamic reconfigurable designs;formal verification;functional HDL;functional programming abstractions;high-level Haskell descriptions;high-level descriptions;high-level structures;higher-order functions;partial evaluation implementation technique;run-time reconfigurable systems;synthesizable VHDL;system-level modelling;Consumer electronics;Digital signal processing;Field programmable gate arrays;Finite impulse response filters;Hardware;Software;Unified modeling language;Functional HDL;Partial Evaluation;Run-Time Reconfiguration;Self-Reconfiguration},
	Month = {March},
	Pages = {379-385},
	Title = {System-level modelling of dynamic reconfigurable designs using functional programming abstractions},
	Year = {2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ISQED.2013.6523639}}

@inproceedings{Oancea:2012:FSG:2364474.2364484,
	Acmid = {2364484},
	Address = {New York, NY, USA},
	Author = {Oancea, Cosmin E. and Andreetta, Christian and Berthold, Jost and Frisch, Alain and Henglein, Fritz},
	Booktitle = {Proceedings of the 1st ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/2364474.2364484},
	Isbn = {978-1-4503-1577-7},
	Keywords = {autoparallelization, functional language, memory coalescing, strength reduction, tiling},
	Location = {Copenhagen, Denmark},
	Numpages = {12},
	Pages = {61--72},
	Publisher = {ACM},
	Series = {FHPC '12},
	Title = {Financial Software on GPUs: Between Haskell and Fortran},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2364474.2364484},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2364474.2364484}}

@inproceedings{Funie:2014aa,
	Author = {A. I. Funie and M. Salmon and W. Luk},
	Booktitle = {2014 13th International Conference on Machine Learning and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ICMLA.2014.11},
	Journal = {2014 13th International Conference on Machine Learning and Applications},
	Journal1 = {2014 13th International Conference on Machine Learning and Applications},
	Keywords = {economics; field programmable gate arrays; foreign exchange trading; genetic algorithms; particle swarm optimisation; economic value; field programmable gate array technology; financial markets; foreign exchange market data; genetic programming; high frequency trading strategies; hybrid evolutionary algorithm; monitor market stability; particle swarm optimisation; Algorithm design and analysis; Genetics; Noise; Prediction algorithms; Sociology; Statistics; Testing},
	Pages = {29--34},
	Title = {A Hybrid Genetic-Programming Swarm-Optimisation Approach for Examining the Nature and Stability of High Frequency Trading Strategies},
	Ty = {CONF},
	Year = {2014},
	Year1 = {3-6 Dec. 2014},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICMLA.2014.11}}

@inproceedings{Lockwood:2012aa,
	Author = {J. W. Lockwood and A. Gupte and N. Mehta and M. Blott and T. English and K. Vissers},
	Booktitle = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/HOTI.2012.15},
	Isbn = {1550-4794},
	Journal = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Journal1 = {2012 IEEE 20th Annual Symposium on High-Performance Interconnects},
	Keywords = {IP networks; electronic engineering computing; electronic trading; field programmable gate arrays; local area networks; memory protocols; microprocessor chips; network interfaces; Ethernet line rate; FPGA IP library; FPGA hardware; HFT platform; I-O interface; alternative hybrid architecture; bit rate 10 Gbit/s; computers software; custom 1U FPGA appliance; electronic trading; financial protocol parser; fixed end-to-end latency; hardware acceleration; high-frequency trading platform; high-performance network adapter; low-latency library; memory interface; pre-built infrastructure; software implementation; time 1 mus; Field programmable gate arrays; Hardware; IP networks; Libraries; Protocols; Registers; Software; Algorithmic; FPGA; HFT; latency; trading},
	Pages = {9--16},
	Title = {A Low-Latency Library in FPGA Hardware for High-Frequency Trading (HFT)},
	Ty = {CONF},
	Year = {2012},
	Year1 = {22-24 Aug. 2012},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/HOTI.2012.15}}

@inproceedings{Zoican:2016aa,
	Author = {S. Zoican and M. Vochin},
	Booktitle = {2016 International Conference on Communications (COMM)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/ICComm.2016.7528289},
	Journal = {2016 International Conference on Communications (COMM)},
	Journal1 = {2016 International Conference on Communications (COMM)},
	Keywords = {financial data processing; computing system; financial market literature; high frequency trading applications; high frequency trading financial applications; high processing speed; high-frequency traders; low latency technology; low network latency; medium cost technology; network architectures; optimal trading speed; Bandwidth; Computer architecture; Computers; Graphics processing units; Instruction sets; Parallel processing; Servers; computer unified device architecture; high frequency trading algorithms; network latency},
	Pages = {139--144},
	Title = {Computing system and network architectures in high frequency trading financial applications},
	Ty = {CONF},
	Year = {2016},
	Year1 = {9-10 June 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ICComm.2016.7528289}}

@inproceedings{Litz:2011:DPE:2088256.2088268,
	Acmid = {2088268},
	Address = {New York, NY, USA},
	Author = {Litz, Heiner and Leber, Christian and Geib, Benjamin},
	Booktitle = {Proceedings of the Fourth Workshop on High Performance Computational Finance},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/2088256.2088268},
	Isbn = {978-1-4503-1108-3},
	Keywords = {DSL, FAST, FIX, FPGA, decoder, domain specific language, high throughput, low latency, stock, trading},
	Location = {Seattle, Washington, USA},
	Numpages = {8},
	Pages = {31--38},
	Publisher = {ACM},
	Series = {WHPCF '11},
	Title = {DSL Programmable Engine for High Frequency Trading Acceleration},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/2088256.2088268},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/2088256.2088268}}

@inproceedings{Woods:2008aa,
	Author = {N. A. Woods and T. VanCourt},
	Booktitle = {2008 International Conference on Field Programmable Logic and Applications},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-09-26 08:04:22 +0000},
	Doi = {10.1109/FPL.2008.4629954},
	Isbn = {1946-147X},
	Journal = {2008 International Conference on Field Programmable Logic and Applications},
	Journal1 = {2008 International Conference on Field Programmable Logic and Applications},
	Keywords = {Monte Carlo methods; field programmable gate arrays; financial data processing; FPGA acceleration; finance; multicore processor; pricing simulations; quasiMonte Carlo methods; Acceleration; Computational modeling; Field programmable gate arrays; Finance; Monte Carlo methods; Multicore processing; Pricing; Runtime; Security; Yield estimation},
	Pages = {335--340},
	Title = {FPGA acceleration of quasi-Monte Carlo in finance},
	Ty = {CONF},
	Year = {2008},
	Year1 = {8-10 Sept. 2008},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/FPL.2008.4629954}}

@article{Tian:2010:HQC:1862648.1862656,
	Acmid = {1862656},
	Address = {New York, NY, USA},
	Articleno = {26},
	Author = {Tian, Xiang and Benkrid, Khaled},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1145/1862648.1862656},
	Issn = {1936-7406},
	Issue_Date = {November 2010},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {CPU, FPGA, GPU, Maxwell, Quasi-Monte Carlo simulations, option pricing},
	Month = nov,
	Number = {4},
	Numpages = {22},
	Pages = {26:1--26:22},
	Publisher = {ACM},
	Title = {High-Performance Quasi-Monte Carlo Financial Simulation: FPGA vs. GPP vs. GPU},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Volume = {3},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/1862648.1862656},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/1862648.1862656}}

@inproceedings{Dvorak:2014aa,
	Author = {M. Dvo{\v r}{\'a}k and J. Ko{\v r}enek},
	Booktitle = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/DDECS.2014.6868785},
	Journal = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Journal1 = {17th International Symposium on Design and Diagnostics of Electronic Circuits \& Systems},
	Keywords = {electronic trading; field programmable gate arrays; logic design; FPGA; QDR SRAM; algorithmic trading; best bid price; best offer price; financial instrument; hardware architecture; hardware market state; high frequency trading; lookup latency-memory utilization trade-off; low latency book handling; low latency trading system; market data processing; storage capacity 144 Mbit; Algorithm design and analysis; Feeds; Field programmable gate arrays; Hardware; Instruments; Memory management},
	Pages = {175--178},
	Title = {Low latency book handling in FPGA for high frequency trading},
	Ty = {CONF},
	Year = {2014},
	Year1 = {23-25 April 2014},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBfLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRXF1aWxpYnJhdGVkIGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzIGZvciBub24tY29udmV4IG9wdGltaXphdGlvbi5iaWJPEQJEAAAAAAJEAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRXF1aWxpYnJhdGVkIGFkYXB0I0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAAAk5OAAIAdS86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpOTjpFcXVpbGlicmF0ZWQgYWRhcHRpdmUgbGVhcm5pbmcgcmF0ZXMgZm9yIG5vbi1jb252ZXggb3B0aW1pemF0aW9uLmJpYgAADgCKAEQARQBxAHUAaQBsAGkAYgByAGEAdABlAGQAIABhAGQAYQBwAHQAaQB2AGUAIABsAGUAYQByAG4AaQBuAGcAIAByAGEAdABlAHMAIABmAG8AcgAgAG4AbwBuAC0AYwBvAG4AdgBlAHgAIABvAHAAdABpAG0AaQB6AGEAdABpAG8AbgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAc1VzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4vRXF1aWxpYnJhdGVkIGFkYXB0aXZlIGxlYXJuaW5nIHJhdGVzIGZvciBub24tY29udmV4IG9wdGltaXphdGlvbi5iaWIAABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAIYAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAACzg==},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/DDECS.2014.6868785}}

@article{Thomas:2013aa,
	Author = {D. B. Thomas and W. Luk},
	Booktitle = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/TVLSI.2012.2228017},
	Isbn = {1063-8210},
	Journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Journal1 = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	Keywords = {Gaussian distribution; adders; field programmable gate arrays; graphics processing units; random number generation; shift registers; table lookup; Virtex-5 FPGA; adders; block-memory resources; field-programmable gate array; frequency 1.2 GHz; frequency 400 MHz; graphics processing unit; independent Gaussian samples; logic resources; lookup-tables; multiplierless algorithm; multivariate Gaussian distribution; multivariate Gaussian vectors; multivariate generator; numerical simulation; pair-wise correlations; random number generation; read-only memories; registers; scalar Gaussian generator; uniform distribution; Covariance matrix; Field programmable gate arrays; Generators; Matrix decomposition; Standards; Table lookup; Vectors; Field-programmable gate array (FPGA); Monte Carlo simulation; multivariate samples; random number generation},
	Number = {12},
	Pages = {2193--2205},
	Title = {Multiplierless Algorithm for Multivariate Gaussian Random Number Generation in FPGAs},
	Ty = {JOUR},
	Vo = {21},
	Volume = {21},
	Year = {2013},
	Year1 = {Dec. 2013},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/TVLSI.2012.2228017}}

@article{2017arXiv171105860H-2,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv171105860H},
	Archiveprefix = {arXiv},
	Author = {{Hao}, Y.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1711.05860},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Hardware Architecture, Computer Science - Neural and Evolutionary Computing},
	Month = nov,
	Primaryclass = {cs.CV},
	Title = {{A General Neural Network Hardware Architecture on FPGA}},
	Year = 2017}

@article{2017arXiv170206392L,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170206392L},
	Archiveprefix = {arXiv},
	Author = {{Li}, Y. and {Liu}, Z. and {Xu}, K. and {Yu}, H. and {Ren}, F.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1702.06392},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Hardware Architecture, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, C.3},
	Month = feb,
	Primaryclass = {cs.DC},
	Title = {{A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks}},
	Year = 2017}

@article{2017arXiv170808917D,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170808917D},
	Archiveprefix = {arXiv},
	Author = {{Ding}, C. and {Liao}, S. and {Wang}, Y. and {Li}, Z. and {Liu}, N. and {Zhuo}, Y. and {Wang}, C. and {Qian}, X. and {Bai}, Y. and {Yuan}, G. and {Ma}, X. and {Zhang}, Y. and {Tang}, J. and {Qiu}, Q. and {Lin}, X. and {Yuan}, B.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1708.08917},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Artificial Intelligence, Computer Science - Learning, Statistics - Machine Learning},
	Month = aug,
	Primaryclass = {cs.CV},
	Title = {{CirCNN: Accelerating and Compressing Deep Neural Networks Using Block-CirculantWeight Matrices}},
	Year = 2017}

@inproceedings{Zhao:2016aa,
	Author = {Wenlai Zhao and Haohuan Fu and W. Luk and Teng Yu and Shaojun Wang and Bo Feng and Yuchun Ma and Guangwen Yang},
	Booktitle = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-09-26 08:03:08 +0000},
	Doi = {10.1109/ASAP.2016.7760779},
	Journal = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Journal1 = {2016 IEEE 27th International Conference on Application-specific Systems, Architectures and Processors (ASAP)},
	Keywords = {field programmable gate arrays; floating point arithmetic; neural nets; 32bit floating-point arithmetic; FPGA-based framework; bandwidth resources; convolutional neural networks; hardware resources; streaming datapath; Bandwidth; Computational modeling; Convolution; Field programmable gate arrays; Neural networks; Runtime; Training},
	Pages = {107--114},
	Title = {F-CNN: An FPGA-based framework for training Convolutional Neural Networks},
	Ty = {CONF},
	Year = {2016},
	Year1 = {6-8 July 2016},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBWLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXNPEQIcAAAAAAIcAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fQWNjZWxlcmF0aW5nIExhcmdlI0ZGRkZGRkZGLnJpcwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABEZQR0EAAgBsLzpVc2VyczpyaHZ0OkRldjpDTk4tUGhkOlJlZmVyZW5jZXM6Q2l0YXRpb25zOkZQR0E6QWNjZWxlcmF0aW5nIExhcmdlLVNjYWxlIEhQQyBBcHBsaWNhdGlvbnMgVXNpbmcgRlBHQXMucmlzAA4AdAA5AEEAYwBjAGUAbABlAHIAYQB0AGkAbgBnACAATABhAHIAZwBlAC0AUwBjAGEAbABlACAASABQAEMAIABBAHAAcABsAGkAYwBhAHQAaQBvAG4AcwAgAFUAcwBpAG4AZwAgAEYAUABHAEEAcwAuAHIAaQBzAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIAalVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRlBHQS9BY2NlbGVyYXRpbmcgTGFyZ2UtU2NhbGUgSFBDIEFwcGxpY2F0aW9ucyBVc2luZyBGUEdBcy5yaXMAEwABLwAAFQACAAv//wAAAAgADQAaACQAfQAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAKd},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAxLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vV2Fzc2Vyc3RlaW4gR0FOLmJpYk8RAYoAAAAAAYoAAgAACU1hY2ludG9zaAAAAAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xNXYXNzZXJzdGVpbiBHQU4uYmliAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAEABAAACiBjdQAAAAAAAAAAAAAAAAAFRENHQU4AAAIARy86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpEQ0dBTjpXYXNzZXJzdGVpbiBHQU4uYmliAAAOACgAEwBXAGEAcwBzAGUAcgBzAHQAZQBpAG4AIABHAEEATgAuAGIAaQBiAA8AFAAJAE0AYQBjAGkAbgB0AG8AcwBoABIARVVzZXJzL3JodnQvRGV2L0NOTi1QaGQvUmVmZXJlbmNlcy9DaXRhdGlvbnMvRENHQU4vV2Fzc2Vyc3RlaW4gR0FOLmJpYgAAEwABLwAAFQACAAv//wAAAAgADQAaACQAWAAAAAAAAAIBAAAAAAAAAAUAAAAAAAAAAAAAAAAAAAHm},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/ASAP.2016.7760779}}

@inproceedings{Abdelouahab:2017:WTH:3131885.3131937,
	Acmid = {3131937},
	Address = {New York, NY, USA},
	Author = {Abdelouahab, Kamel and Pelcat, Maxime and Berry, Francois},
	Booktitle = {Proceedings of the 11th International Conference on Distributed Smart Cameras},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-09-26 08:04:22 +0000},
	Doi = {10.1145/3131885.3131937},
	Isbn = {978-1-4503-5487-5},
	Keywords = {FPGA, TanH, Harware, Acceleration},
	Location = {Stanford, CA, USA},
	Numpages = {3},
	Pages = {199--201},
	Publisher = {ACM},
	Series = {ICDSC 2017},
	Title = {Why TanH is a Hardware Friendly Activation Function for CNNs},
	Url = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org.ezproxy.auckland.ac.nz/10.1145/3131885.3131937},
	Bdsk-Url-2 = {https://dx.doi.org/10.1145/3131885.3131937}}

@article{2017arXiv170304691B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2017arXiv170304691B},
	Archiveprefix = {arXiv},
	Author = {{Borovykh}, A. and {Bohte}, S. and {Oosterlee}, C.~W.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-10-24 07:44:45 +0000},
	Eprint = {1703.04691},
	Journal = {ArXiv e-prints},
	Keywords = {BNN; Statistics; Machine Learning},
	Month = mar,
	Primaryclass = {stat.ML},
	Title = {{Conditional Time Series Forecasting with Convolutional Neural Networks}},
	Year = 2017,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB0Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OLUZQR0EvT3B0aW1pemluZyBGUEdBLWJhc2VkIEFjY2VsZXJhdG9yIERlc2lnbiBmb3IgRGVlcCBDb252b2x1dGlvbmFsIE5ldXJhbCBOZXR3b3Jrcy5iaWJPEQKQAAAAAAKQAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fT3B0aW1pemluZyBGUEdBLWJhI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAACENOTi1GUEdBAAIAii86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk4tRlBHQTpPcHRpbWl6aW5nIEZQR0EtYmFzZWQgQWNjZWxlcmF0b3IgRGVzaWduIGZvciBEZWVwIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgAOAKgAUwBPAHAAdABpAG0AaQB6AGkAbgBnACAARgBQAEcAQQAtAGIAYQBzAGUAZAAgAEEAYwBjAGUAbABlAHIAYQB0AG8AcgAgAEQAZQBzAGkAZwBuACAAZgBvAHIAIABEAGUAZQBwACAAQwBvAG4AdgBvAGwAdQB0AGkAbwBuAGEAbAAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCIVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4tRlBHQS9PcHRpbWl6aW5nIEZQR0EtYmFzZWQgQWNjZWxlcmF0b3IgRGVzaWduIGZvciBEZWVwIENvbnZvbHV0aW9uYWwgTmV1cmFsIE5ldHdvcmtzLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJACbAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAAy8=},
	Bdsk-File-2 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxB6Li4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvQ05OLUZQR0EvUGlwZUNOTi0gQW4gT3BlbkNMLUJhc2VkIE9wZW4tU291cmNlIEZQR0EgQWNjZWxlcmF0b3IgZm9yIENvbnZvbHV0aW9uIE5ldXJhbCBOZXR3b3Jrcy5iaWJPEQKoAAAAAAKoAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fUGlwZUNOTi0gQW4gT3BlbkNMI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAACENOTi1GUEdBAAIAkC86VXNlcnM6cmh2dDpEZXY6Q05OLVBoZDpSZWZlcmVuY2VzOkNpdGF0aW9uczpDTk4tRlBHQTpQaXBlQ05OLSBBbiBPcGVuQ0wtQmFzZWQgT3Blbi1Tb3VyY2UgRlBHQSBBY2NlbGVyYXRvciBmb3IgQ29udm9sdXRpb24gTmV1cmFsIE5ldHdvcmtzLmJpYgAOALQAWQBQAGkAcABlAEMATgBOAC0AIABBAG4AIABPAHAAZQBuAEMATAAtAEIAYQBzAGUAZAAgAE8AcABlAG4ALQBTAG8AdQByAGMAZQAgAEYAUABHAEEAIABBAGMAYwBlAGwAZQByAGEAdABvAHIAIABmAG8AcgAgAEMAbwBuAHYAbwBsAHUAdABpAG8AbgAgAE4AZQB1AHIAYQBsACAATgBlAHQAdwBvAHIAawBzAC4AYgBpAGIADwAUAAkATQBhAGMAaQBuAHQAbwBzAGgAEgCOVXNlcnMvcmh2dC9EZXYvQ05OLVBoZC9SZWZlcmVuY2VzL0NpdGF0aW9ucy9DTk4tRlBHQS9QaXBlQ05OLSBBbiBPcGVuQ0wtQmFzZWQgT3Blbi1Tb3VyY2UgRlBHQSBBY2NlbGVyYXRvciBmb3IgQ29udm9sdXRpb24gTmV1cmFsIE5ldHdvcmtzLmJpYgATAAEvAAAVAAIAC///AAAACAANABoAJAChAAAAAAAAAgEAAAAAAAAABQAAAAAAAAAAAAAAAAAAA00=}}

@inproceedings{Ding:2015:DLE:2832415.2832572,
	Acmid = {2832572},
	Author = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
	Booktitle = {Proceedings of the 24th International Conference on Artificial Intelligence},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Isbn = {978-1-57735-738-4},
	Keywords = {Deep learning, CNN, NN, S&P},
	Location = {Buenos Aires, Argentina},
	Numpages = {7},
	Pages = {2327--2333},
	Publisher = {AAAI Press},
	Series = {IJCAI'15},
	Title = {Deep Learning for Event-driven Stock Prediction},
	Url = {http://dl.acm.org/citation.cfm?id=2832415.2832572},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2832415.2832572}}

@inproceedings{Chen:2016aa,
	Author = {J. F. Chen and W. L. Chen and C. P. Huang and S. H. Huang and A. P. Chen},
	Booktitle = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-10-10 00:34:11 +0000},
	Doi = {10.1109/CCBD.2016.027},
	Journal = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Journal1 = {2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
	Keywords = {CNN-FIN; decision support systems; feature extraction; feedforward neural nets; financial data processing; learning (artificial intelligence); stock markets; time series; FinTech; Taiwan Stock Index Futures; artificial intelligence; deep convolutional neural networks; deep learning; feature extraction; financial markets; financial time-series data analysis; historical datasets; intelligent trading decision support system; multimedia fields; next financial technology generation; numerical features; planar feature representation; time-series data prediction; time-series data processing; trading simulation application; Data models; Feature extraction; Machine learning; Market research; Neural networks; Time series analysis; Training; Deep learning; convolutional neural networks; data visualization; machine learning; trend prediction},
	Pages = {87--92},
	Title = {Financial Time-Series Data Analysis Using Deep Convolutional Neural Networks},
	Ty = {CONF},
	Year = {2016},
	Year1 = {16-18 Nov. 2016},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CCBD.2016.027}}

@inproceedings{Tsantekidis:2017aa,
	Abstract = {In today's financial markets, where most trades are performed in their entirety by electronic means and the largest fraction of them is completely automated, an opportunity has risen from analyzing this vast amount of transactions. Since all the transactions are recorded in great detail, investors can analyze all the generated data and detect repeated patterns of the price movements. Being able to detect them in advance, allows them to take profitable positions or avoid anomalous events in the financial markets. In this work we proposed a deep learning methodology, based on Convolutional Neural Networks (CNNs), that predicts the price movements of stocks, using as input large-scale, high-frequency time-series derived from the order book of financial exchanges. The dataset that we use contains more than 4 million limit order events and our comparison with other methods, like Multilayer Neural Networks and Support Vector Machines, shows that CNNs are better suited for this kind of task.},
	Author = {A. Tsantekidis and N. Passalis and A. Tefas and J. Kanniainen and M. Gabbouj and A. Iosifidis},
	Booktitle = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {10.1109/CBI.2017.23},
	Journal = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Journal1 = {2017 IEEE 19th Conference on Business Informatics (CBI)},
	Keywords = {economic forecasting; feedforward neural nets; learning (artificial intelligence); pricing; stock markets; time series; CNN; convolutional neural networks; deep learning methodology; financial exchanges; financial markets; input large-scale high-frequency time-series; limit order book; price movements; stock price forecasting; stock price movement prediction; transaction analysis; Convolution; Data models; Machine learning; Market research; Mathematical model; Neural networks; Support vector machines; Convolutional Neural Networks; Large scale financial data; Limit Orderbook},
	Pages = {7--12},
	Title = {Forecasting Stock Prices from the Limit Order Book Using Convolutional Neural Networks},
	Ty = {CONF},
	Vo = {01},
	Volume = {01},
	Year = {2017},
	Year1 = {24-27 July 2017},
	Bdsk-Url-1 = {https://dx.doi.org/10.1109/CBI.2017.23}}

@article{Gunduz:2017aa,
	Author = {Gunduz, Hakan and Yaslan, Yusuf and Cataltepe, Zehra},
	Da = {2017/12/01/},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Doi = {https://doi.org/10.1016/j.knosys.2017.09.023},
	Isbn = {0950-7051},
	Journal = {Knowledge-Based Systems},
	Keywords = {Stock market prediction; Deep learning; Borsa Istanbul; Convolutional neural networks; CNN; Feature selection; Feature correlations},
	Number = {Supplement C},
	Pages = {138--148},
	Title = {Intraday prediction of Borsa Istanbul using convolutional neural networks and feature correlations},
	Ty = {JOUR},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Volume = {137},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950705117304252},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.knosys.2017.09.023}}

@article{2016arXiv160306995C,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160306995C},
	Archiveprefix = {arXiv},
	Author = {{Cui}, Z. and {Chen}, W. and {Chen}, Y.},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1603.06995},
	Journal = {ArXiv e-prints},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = mar,
	Primaryclass = {cs.CV},
	Title = {{Multi-Scale Convolutional Neural Networks for Time Series Classification}},
	Year = 2016}

@article{2017arXiv171105860H,
	Adsnote = {International Journal of Computer Science and Information Technologies (IJCSIT{\textregistered}) is published using an open access publishing model, which makes the full-text of all peer-reviewed papers freely available online with no subscription or registration barriers.},
	Adsurl = {http://ijcsit.com/docs/Volume%207/vol7issue5/ijcsit20160705014.pdf},
	Archiveprefix = {pdf},
	Author = {{Bhandare}, Ashwin, {Bhide}, Maithili, {Gokhale}, Pranav, {Chandavarkar}, Rohan,},
	Date-Added = {2018-02-05 09:05:50 +0000},
	Date-Modified = {2018-02-05 09:05:50 +0000},
	Eprint = {1711.05860},
	Journal = {International Journal of Computer Science and Information Technologies},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	Month = 5,
	Primaryclass = {cs.CV},
	Title = {{Applications of Convolutional Neural Networks}},
	Year = 2016,
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxBiLi4vUmVmZXJlbmNlcy9DaXRhdGlvbnMvTk4tRmluL0RlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWJPEQJKAAAAAAJKAAIAAAlNYWNpbnRvc2gAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkQAAf////8fRGVlcCBJbnZlc3RtZW50IGluI0ZGRkZGRkZGLmJpYgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA/////wAAAAAAAAAAAAAAAAABAAQAAAogY3UAAAAAAAAAAAAAAAAABk5OLUZpbgACAHgvOlVzZXJzOnJodnQ6RGV2OkNOTi1QaGQ6UmVmZXJlbmNlczpDaXRhdGlvbnM6Tk4tRmluOkRlZXAgSW52ZXN0bWVudCBpbiBGaW5hbmNpYWwgTWFya2V0cyB1c2luZyBEZWVwIExlYXJuaW5nIE1vZGVscy5iaWIADgCIAEMARABlAGUAcAAgAEkAbgB2AGUAcwB0AG0AZQBuAHQAIABpAG4AIABGAGkAbgBhAG4AYwBpAGEAbAAgAE0AYQByAGsAZQB0AHMAIAB1AHMAaQBuAGcAIABEAGUAZQBwACAATABlAGEAcgBuAGkAbgBnACAATQBvAGQAZQBsAHMALgBiAGkAYgAPABQACQBNAGEAYwBpAG4AdABvAHMAaAASAHZVc2Vycy9yaHZ0L0Rldi9DTk4tUGhkL1JlZmVyZW5jZXMvQ2l0YXRpb25zL05OLUZpbi9EZWVwIEludmVzdG1lbnQgaW4gRmluYW5jaWFsIE1hcmtldHMgdXNpbmcgRGVlcCBMZWFybmluZyBNb2RlbHMuYmliABMAAS8AABUAAgAL//8AAAAIAA0AGgAkAIkAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAC1w==}}
