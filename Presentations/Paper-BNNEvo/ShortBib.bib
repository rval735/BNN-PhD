%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for RHVT at 2019-10-08 16:19:43 +1300


%% Saved with string encoding Unicode (UTF-8)

@article{Blott:2018:FRE:3299999.3242897,
	Acmid = {3242897},
	Address = {New York, NY, USA},
	Articleno = {16},
	Author = {Blott, Michaela and Preusser, Thomas B. and Fraser, Nicholas J. and Gambardella, Giulio and O'brien, Kenneth and Umuroglu, Yaman and Leeser, Miriam and Vissers, Kees},
	Date-Added = {2018-12-16 22:41:43 +1300},
	Date-Modified = {2019-04-28 15:11:33 +1200},
	Doi = {10.1145/3242897},
	Issn = {1936-7406},
	Issue_Date = {December 2018},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {BNN; FINN, FPGA, Neural network, artificial intelligence, convolutional neural networks, hardware accellerator, inference, quantized neural networks},
	Month = dec,
	Number = {3},
	Numpages = {23},
	Pages = {16:1--16:23},
	Publisher = {ACM},
	Title = {FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks},
	Volume = {11},
	Year = {2018}}
	
@article{electronics8060661,
	Abstract = {In this work, we review Binarized Neural Networks (BNNs). BNNs are deep neural networks that use binary values for activations and weights, instead of full precision values. With binary values, BNNs can execute computations using bitwise operations, which reduces execution time. Model sizes of BNNs are much smaller than their full precision counterparts. While the accuracy of a BNN model is generally less than full precision models, BNNs have been closing accuracy gap and are becoming more accurate on larger datasets like ImageNet. BNNs are also good candidates for deep learning implementations on FPGAs and ASICs due to their bitwise efficiency. We give a tutorial of the general BNN methodology and review various contributions, implementations and applications of BNNs.},
	Article-Number = {661},
	Author = {Simons, Taylor and Lee, Dah-Jye},
	Date-Added = {2019-10-08 15:58:12 +1300},
	Date-Modified = {2019-10-08 15:58:34 +1300},
	Doi = {10.3390/electronics8060661},
	Issn = {2079-9292},
	Journal = {Electronics},
	Keywords = {BNN, Survey, backpropagation},
	Number = {6},
	Title = {A Review of Binarized Neural Networks},
	Volume = {8},
	Year = {2019},
	Bdsk-Url-1 = {https://www.mdpi.com/2079-9292/8/6/661},
	Bdsk-Url-2 = {https://doi.org/10.3390/electronics8060661}}
	
@article{7460958,
	Abstract = {Learning algorithms are being increasingly adopted in various applications. However, further expansion will require methods that work more automatically. To enable this level of automation, a more powerful solution representation is needed. However, by increasing the representation complexity, a second problem arises. The search space becomes huge, and therefore, an associated scalable and efficient searching algorithm is also required. To solve both the problems, first a powerful representation is proposed that unifies most of the neural networks features from the literature into one representation. Second, a new diversity preserving method called spectrum diversity is created based on the new concept of chromosome spectrum that creates a spectrum out of the characteristics and frequency of alleles in a chromosome. The combination of spectrum diversity with a unified neuron representation enables the algorithm to either surpass or equal NeuroEvolution of Augmenting Topologies on all of the five classes of problems tested. Ablation tests justify the good results, showing the importance of added new features in the unified neuron representation. Part of the success is attributed to the novelty-focused evolution and good scalability with a chromosome size provided by spectrum diversity. Thus, this paper sheds light on a new representation and diversity preserving mechanism that should impact algorithms and applications to come.},
	Author = {Danilo {Vargas}, Junichi {Murata}},
	Date-Added = {2019-02-25 23:58:32 +1300},
	Date-Modified = {2019-04-17 21:06:05 +1200},
	Doi = {10.1109/TNNLS.2016.2551748},
	Issn = {2162-237X},
	Journal = {IEEE Transactions on Neural Networks and Learning Systems},
	Keywords = {Evolutionary; computational complexity;evolutionary computation;learning (artificial intelligence);neural nets;search problems;spectrum-diverse neuroevolution;unified neural models;learning algorithms;solution representation;representation complexity;search space;efficient searching algorithm;neural networks features;diversity preserving method;chromosome spectrum;NeuroEvolution;augmenting topologies;unified neuron representation;chromosome size;Neurons;Biological neural networks;Topology;Network topology;Biological cells;Encoding;Technological innovation;General artificial intelligence;neuroevolution;neuroEvolution of Augmenting Topology (NEAT);reinforcement learning;spectrum diversity;topology and weight evolving artificial neural network (TWEANN);unified neuron model; SUNA},
	Month = {Aug},
	Number = {8},
	Pages = {1759-1773},
	Title = {Spectrum-Diverse Neuroevolution With Unified Neural Models},
	Volume = {28},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/TNNLS.2016.2551748}}
	

@book{Sher2013NeuroEvol,
	Abstract = {This chapter discusses the numerous reasons for why one might wish to study the subject of neuroevolution. I cover a number of different applications of such a system, giving examples and scenarios of a neuroevolutionary system being applied within a variety of different fields. A discussion then follows on where all of this research is heading, and what the next step within this field might be. Finally, a whirlwind introduction of the book is given, with a short summary of what is covered in every chapter.},
	Address = {New York, NY},
	Author = {Sher, Gene I.},
	Booktitle = {Handbook of Neuroevolution Through Erlang},
	Date-Added = {2018-12-12 19:57:45 +1300},
	Date-Modified = {2018-12-12 19:59:29 +1300},
	Doi = {10.1007/978-1-4614-4463-3_1},
	Isbn = {978-1-4614-4463-3},
	Keywords = {Evolutionary, Neuroevolution, Erlang, Finance},
	Pages = {1--39},
	Publisher = {Springer New York},
	Title = {Handbook of Neuroevolution Through Erlang},
	Year = {2013},
}
	
%%

@phdthesis{stanley:phd04,
	Author = {Kenneth O. Stanley},
	Date-Added = {2019-04-28 16:54:44 +1200},
	Date-Modified = {2019-04-28 16:54:59 +1200},
	Keywords = {Books, NeuroEvolution, NEAT},
	School = {Department of Computer Sciences, The University of Texas at Austin},
	Title = {Efficient Evolution of Neural Networks Through Complexification},
	Year = {2004},
	}
	
%%

@inproceedings{10.1007/978-3-319-46493-0_32,
	Abstract = {We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}faster convolutional operations (in terms of number of the high precision operations) and 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than {\$}{\$}16{\backslash},{\backslash}{\%}{\$}{\$}16{\%}in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.},
	Address = {Cham},
	Author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
	Booktitle = {Computer Vision -- ECCV 2016},
	Date-Added = {2019-04-04 23:37:48 +1300},
	Date-Modified = {2019-04-04 23:38:31 +1300},
	Editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	Isbn = {978-3-319-46493-0},
	Keywords = {BNN, XNOR, ImageNet, CNN, AlexNet},
	Pages = {525--542},
	Publisher = {Springer International Publishing},
	Title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
	Year = {2016}}
	
%%

@inproceedings{Han:2016:EEI:3001136.3001163,
	Acmid = {3001163},
	Address = {Piscataway, NJ, USA},
	Author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
	Booktitle = {Proceedings of the 43rd International Symposium on Computer Architecture},
	Date-Added = {2018-12-11 11:22:33 +1300},
	Date-Modified = {2018-12-11 11:22:42 +1300},
	Doi = {10.1109/ISCA.2016.30},
	Isbn = {978-1-4673-8947-1},
	Keywords = {FPGA-NN, ASIC, algorithm-hardware co-design, deep learning, hardware acceleration, model compression},
	Location = {Seoul, Republic of Korea},
	Numpages = {12},
	Pages = {243--254},
	Publisher = {IEEE Press},
	Series = {ISCA '16},
	Title = {EIE: Efficient Inference Engine on Compressed Deep Neural Network},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISCA.2016.30}}
	
%%

@article{Kung2018,
	Abstract = {Memory performance is a key bottleneck for deep learning systems. Binarization of both activations and weights is one promising approach that can best scale to realize the highest energy efficient system using the lowest possible precision. In this paper, we utilize and analyze the binarized neural network in doing human detection on infrared images. Our results show comparable algorithmic performance of binarized versus 32bit floating-point networks, with the added benefit of greatly simplified computation and reduced memory overhead. In addition, we present a system architecture designed specifically for computation using binary representation that achieves at least 4{\texttimes} speedup and the energy is improved by three orders of magnitude over GPU.},
	Author = {Kung, Jaeha and Zhang, David and van der Wal, Gooitzen and Chai, Sek and Mukhopadhyay, Saibal},
	Date-Added = {2019-04-28 14:28:44 +1200},
	Date-Modified = {2019-04-28 14:32:52 +1200},
	Day = {01},
	Doi = {10.1007/s11265-017-1255-5},
	Issn = {1939-8115},
	Journal = {Journal of Signal Processing Systems},
	Keywords = {BNN, object detection, GPU, Infrared, FPGA},
	Month = {Jun},
	Number = {6},
	Pages = {877--890},
	Title = {Efficient Object Detection Using Embedded Binarized Neural Networks},
	Volume = {90},
	Year = {2018},
	Bdsk-Url-1 = {https://doi.org/10.1007/s11265-017-1255-5}}
	
%%

@article{Dietterich:2000:HRL:1622262.1622268,
	Acmid = {1622268},
	Address = {USA},
	Author = {Dietterich, Thomas G.},
	Date-Added = {2019-04-18 23:38:36 +1200},
	Date-Modified = {2019-04-18 23:40:14 +1200},
	Issn = {1076-9757},
	Issue_Date = {August 2000},
	Journal = {J. Artif. Int. Res.},
	Keywords = {NN, OpenAI Gym, Reinforcement Learning},
	Month = nov,
	Number = {1},
	Numpages = {77},
	Pages = {227--303},
	Publisher = {AI Access Foundation},
	Title = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition},
	Volume = {13},
	Year = {2000},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1622262.1622268}}
	
%%

@inproceedings{Strens:2000:BFR:645529.658114,
	Acmid = {658114},
	Address = {San Francisco, CA, USA},
	Author = {Strens, Malcolm J. A.},
	Booktitle = {Proceedings of the Seventeenth International Conference on Machine Learning},
	Date-Added = {2019-04-14 16:42:28 +1200},
	Date-Modified = {2019-04-14 16:43:57 +1200},
	Isbn = {1-55860-707-2},
	Keywords = {NN, Reinforcement Learning, OpenAI Gym},
	Numpages = {8},
	Pages = {943--950},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Series = {ICML '00},
	Title = {A Bayesian Framework for Reinforcement Learning},
	Year = {2000},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=645529.658114}}
	
%%

@electronic{nchainv0OpenAI,
	Author = {blole},
	Date-Added = {2019-05-02 22:52:37 +1200},
	Date-Modified = {2019-05-02 22:54:27 +1200},
	Keywords = {WebPage, OpenAI Gym, SUNA},
	Lastchecked = {2/May/2019},
	Title = {Algorithm on NChain-v0},
	Url = {https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/},
	Year = {2016},
	Bdsk-Url-1 = {https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/}}
	
%%

@article{2016arXiv160601540B,
	Adsnote = {Provided by the SAO/NASA Astrophysics Data System},
	Archiveprefix = {arXiv},
	Author = {{Brockman}, G. and {Cheung}, V. and {Pettersson}, L. and {Schneider}, J. and {Schulman}, J. and {Tang}, J. and {Zaremba}, W.},
	Date-Added = {2019-03-13 17:26:41 +1300},
	Date-Modified = {2019-03-13 17:26:58 +1300},
	Eprint = {1606.01540},
	Journal = {arXiv e-prints},
	Keywords = {Other, OpenAI, Machine Learning, Artificial Intelligence},
	Month = jun,
	Title = {{OpenAI Gym}},
	Year = 2016}

@INPROCEEDINGS{8587580, 
author={Chun-Yan {Lo} and Francis C. M. {Lau} and Chiu-Wing {Sham}}, 
booktitle={International Conference on Advanced Technologies for Communications (ATC)}, 
title={Fixed-Point Implementation of Convolutional Neural Networks for Image Classification}, 
year={2018}, 
volume={}, 
number={}, 
pages={105-109}, 
keywords={C++ language;convolutional neural nets;field programmable gate arrays;fixed point arithmetic;handwritten character recognition;image classification;C++ programming;4bit fixed-point arithmetic;FPGA;handwritten digits classification;floating-point classifier;8bit additions;fixed-point convolutional neural network classifier;image classification;Training;Field programmable gate arrays;Testing;Standards;Table lookup;Adders;Frequency measurement}, 
doi={10.1109/ATC.2018.8587580}, 
ISSN={2162-1039}, 
month={Oct},}	