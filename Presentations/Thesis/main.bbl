% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{none/global//global/global}
  \entry{Minsky:1988:PEE:50066}{book}{}
    \name{author}{2}{}{%
      {{hash=MML}{%
         family={Minsky},
         familyi={M\bibinitperiod},
         given={Marvin\bibnamedelima L.},
         giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
      {{hash=PSA}{%
         family={Papert},
         familyi={P\bibinitperiod},
         given={Seymour\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{NN, Neural Networks}
    \strng{namehash}{MMLPSA1}
    \strng{fullhash}{MMLPSA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{isbn}{0-262-63111-3}
    \field{title}{Perceptrons: Expanded Edition}
    \list{location}{1}{%
      {Cambridge, MA, USA}%
    }
    \field{year}{1988}
  \endentry

  \entry{Linnainmaa1976}{article}{}
    \name{author}{1}{}{%
      {{hash=LS}{%
         family={Linnainmaa},
         familyi={L\bibinitperiod},
         given={Seppo},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{NN, Neural Networks, Backpropagation}
    \strng{namehash}{LS1}
    \strng{fullhash}{LS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The article describes analytic and algorithmic methods for determining the
  coefficients of the Taylor expansion of an accumulated rounding error with
  respect to the local rounding errors, and hence determining the influence of
  the local errors on the accumulated error. Second and higher order
  coefficients are also discussed, and some possible methods of reducing the
  extensive storage requirements are analyzed.%
    }
    \verb{doi}
    \verb 10.1007/BF01931367
    \endverb
    \field{issn}{1572-9125}
    \field{number}{2}
    \field{pages}{146\bibrangedash 160}
    \field{title}{Taylor expansion of the accumulated rounding error}
    \verb{url}
    \verb https://doi.org/10.1007/BF01931367
    \endverb
    \field{volume}{16}
    \field{journaltitle}{BIT Numerical Mathematics}
    \field{year}{1976}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{gd-Haskell}{article}{}
    \name{author}{1}{}{%
      {{hash=CHB}{%
         family={Curry},
         familyi={C\bibinitperiod},
         given={Haskell\bibnamedelima B.},
         giveni={H\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \keyw{Other, gradient descent,}
    \strng{namehash}{CHB1}
    \strng{fullhash}{CHB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{258\bibrangedash 261}
    \field{title}{The method of steepest descent for non-linearminimization
  problems}
    \field{volume}{2}
    \field{journaltitle}{Quarterly of Applied Mathematics}
    \field{year}{1944}
  \endentry

  \entry{AIWinter-Andrey}{misc}{}
    \name{author}{1}{}{%
      {{hash=KA}{%
         family={Kurenkov},
         familyi={K\bibinitperiod},
         given={Andrey},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{AI, Neural Networks, History}
    \strng{namehash}{KA2}
    \strng{fullhash}{KA2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{A 'Brief' History of Neural Nets and Deep Learning}
    \verb{url}
    \verb http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-ne
    \verb ts-and-deep-learning
    \endverb
    \field{year}{2018}
    \warn{\item Invalid format of field 'month' \item Invalid format of field
  'urldate'}
  \endentry

  \entry{ILSVRC15}{article}{}
    \name{author}{12}{}{%
      {{hash=RO}{%
         family={Russakovsky},
         familyi={R\bibinitperiod},
         given={Olga},
         giveni={O\bibinitperiod},
      }}%
      {{hash=DJ}{%
         family={Deng},
         familyi={D\bibinitperiod},
         given={Jia},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SH}{%
         family={Su},
         familyi={S\bibinitperiod},
         given={Hao},
         giveni={H\bibinitperiod},
      }}%
      {{hash=KJ}{%
         family={Krause},
         familyi={K\bibinitperiod},
         given={Jonathan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Satheesh},
         familyi={S\bibinitperiod},
         given={Sanjeev},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Sean},
         giveni={S\bibinitperiod},
      }}%
      {{hash=HZ}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Zhiheng},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Karpathy},
         familyi={K\bibinitperiod},
         given={Andrej},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Khosla},
         familyi={K\bibinitperiod},
         given={Aditya},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Bernstein},
         familyi={B\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BAC}{%
         family={Berg},
         familyi={B\bibinitperiod},
         given={Alexander\bibnamedelima C.},
         giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=FFL}{%
         family={Fei-Fei},
         familyi={F\bibinithyphendelim F\bibinitperiod},
         given={Li},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{Other, ImageNet, ILSVRC}
    \strng{namehash}{RO+1}
    \strng{fullhash}{RODJSHKJSSMSHZKAKABMBACFFL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1007/s11263-015-0816-y
    \endverb
    \field{number}{3}
    \field{pages}{211\bibrangedash 252}
    \field{title}{{ImageNet Large Scale Visual Recognition Challenge}}
    \field{volume}{115}
    \field{journaltitle}{International Journal of Computer Vision (IJCV)}
    \field{year}{2015}
  \endentry

  \entry{Krizhevsky2999257}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KA}{%
         family={Krizhevsky},
         familyi={K\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SI}{%
         family={Sutskever},
         familyi={S\bibinitperiod},
         given={Ilya},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HGE}{%
         family={Hinton},
         familyi={H\bibinitperiod},
         given={Geoffrey\bibnamedelima E.},
         giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates Inc.}%
    }
    \keyw{CNN, ImageNet, ILSVRC, Convolution, AlexNet}
    \strng{namehash}{KASIHGE1}
    \strng{fullhash}{KASIHGE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 25th International Conference on
  Neural Information Processing Systems - Volume 1}
    \field{pages}{1097\bibrangedash 1105}
    \field{series}{NIPS'12}
    \field{title}{ImageNet Classification with Deep Convolutional Neural
  Networks}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=2999134.2999257
    \endverb
    \list{location}{1}{%
      {Lake Tahoe, Nevada}%
    }
    \field{year}{2012}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{5206848}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=DJ}{%
         family={{Deng}},
         familyi={D\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=DW}{%
         family={{Dong}},
         familyi={D\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=SR}{%
         family={{Socher}},
         familyi={S\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=LL}{%
         family={{Li}},
         familyi={L\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=K}{%
         family={{Kai Li}},
         familyi={K\bibinitperiod},
      }}%
      {{hash=L}{%
         family={{Li Fei-Fei}},
         familyi={L\bibinitperiod},
      }}%
    }
    \keyw{NN, ImageNet, computer vision;image resolution;image
  retrieval;Internet;multimedia computing;ontologies (artificial
  intelligence);trees (mathematics);very large databases;visual
  databases;ImageNet database;large-scale hierarchical image
  database;Internet;image retrieval;multimedia data;large-scale
  ontology;wordNet structure;image resolution;subtree;computer
  vision;Large-scale systems;Image
  databases;Explosions;Internet;Robustness;Information retrieval;Image
  retrieval;Multimedia databases;Ontologies;Spine}
    \strng{namehash}{DJ+1}
    \strng{fullhash}{DJDWSRLLKL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The explosion of image data on the Internet has the potential to foster
  more sophisticated and robust models and algorithms to index, retrieve,
  organize and interact with images and multimedia data. But exactly how such
  data can be harnessed and organized remains a critical problem. We introduce
  here a new database called ``ImageNet'', a large-scale ontology of images
  built upon the backbone of the WordNet structure. ImageNet aims to populate
  the majority of the 80,000 synsets of WordNet with an average of 500-1000
  clean and full resolution images. This will result in tens of millions of
  annotated images organized by the semantic hierarchy of WordNet. This paper
  offers a detailed analysis of ImageNet in its current state: 12 subtrees with
  5247 synsets and 3.2 million images in total. We show that ImageNet is much
  larger in scale and diversity and much more accurate than the current image
  datasets. Constructing such a large-scale database is a challenging task. We
  describe the data collection scheme with Amazon Mechanical Turk. Lastly, we
  illustrate the usefulness of ImageNet through three simple applications in
  object recognition, image classification and automatic object clustering. We
  hope that the scale, accuracy, diversity and hierarchical structure of
  ImageNet can offer unparalleled opportunities to researchers in the computer
  vision community and beyond.%
    }
    \field{booktitle}{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}
    \verb{doi}
    \verb 10.1109/CVPR.2009.5206848
    \endverb
    \field{issn}{1063-6919}
    \field{pages}{248\bibrangedash 255}
    \field{title}{ImageNet: A large-scale hierarchical image database}
    \field{year}{2009}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5432472}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=LL}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Deng},
         familyi={D\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; cancer;genetic algorithms;medical image
  processing;neural nets;artificial neural network;breast cancer;women;adaptive
  genetic algorithm;macro-search capability;global optimization;computational
  cost;Wisions breast cancer data set;Artificial neural networks;Breast
  cancer;Genetic algorithms;Genetic mutations;Flowcharts;Economic
  forecasting;Space technology;Data mining;Conference management;Knowledge
  management;adaptive genetic algorithm;neural network;weights and
  thresholds;breast cancer}
    \strng{namehash}{LLDM1}
    \strng{fullhash}{LLDM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{2010 Third International Conference on Knowledge
  Discovery and Data Mining}
    \verb{doi}
    \verb 10.1109/WKDD.2010.148
    \endverb
    \field{pages}{593\bibrangedash 596}
    \field{title}{An Evolutionary Artificial Neural Network Approach for Breast
  Cancer Diagnosis}
    \field{year}{2010}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8393327}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=YB}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
      {{hash=ZW}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=GL}{%
         family={Gong},
         familyi={G\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=MH}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; finance; exchange rates;forecasting theory;genetic
  algorithms;neural nets;stock markets;time series;trees (mathematics);CVFNT
  model;time series datasets;neural network;finance time series
  prediction;complex-valued flexible neural tree model;artificial bee
  colony;forecasting accuracy;genetic algorithm;Shanghai stock index;exchange
  rates;Neural networks;Time series analysis;Brain modeling;Predictive
  models;Forecasting;Data models;Indexes;evolutionary method;flexible neural
  tree;complex-valued;artificial bee colony}
    \strng{namehash}{YB+1}
    \strng{fullhash}{YBZWGLMH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{2017 13th International Conference on Natural
  Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}
    \verb{doi}
    \verb 10.1109/FSKD.2017.8393327
    \endverb
    \field{pages}{54\bibrangedash 58}
    \field{title}{Finance time series prediction using complex-valued flexible
  neural tree model}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{s18051306}{article}{}
    \name{author}{5}{}{%
      {{hash=CM}{%
         family={Coutinho},
         familyi={C\bibinitperiod},
         given={Murilo},
         giveni={M\bibinitperiod},
      }}%
      {{hash=dOAR}{%
         prefix={de},
         prefixi={d\bibinitperiod},
         family={Oliveira\bibnamedelima Albuquerque},
         familyi={O\bibinitperiod\bibinitdelim A\bibinitperiod},
         given={Robson},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Borges},
         familyi={B\bibinitperiod},
         given={F{\'a}bio},
         giveni={F\bibinitperiod},
      }}%
      {{hash=GVLJ}{%
         family={Garc{\'\i}a\bibnamedelima Villalba},
         familyi={G\bibinitperiod\bibinitdelim V\bibinitperiod},
         given={Luis\bibnamedelima Javier},
         giveni={L\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=KTH}{%
         family={Kim},
         familyi={K\bibinitperiod},
         given={Tai-Hoon},
         giveni={T\bibinithyphendelim H\bibinitperiod},
      }}%
    }
    \keyw{DCGAN, Cryptography, Adversarial Neural Cryptography, Cryptonet}
    \strng{namehash}{CM+1}
    \strng{fullhash}{CMOARdBFGVLJKTH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Researches in Artificial Intelligence (AI) have achieved many important
  breakthroughs, especially in recent years. In some cases, AI learns alone
  from scratch and performs human tasks faster and better than humans. With the
  recent advances in AI, it is natural to wonder whether Artificial Neural
  Networks will be used to successfully create or break cryptographic
  algorithms. Bibliographic review shows the main approach to this problem have
  been addressed throughout complex Neural Networks, but without understanding
  or proving the security of the generated model. This paper presents an
  analysis of the security of cryptographic algorithms generated by a new
  technique called Adversarial Neural Cryptography (ANC). Using the proposed
  network, we show limitations and directions to improve the current approach
  of ANC. Training the proposed Artificial Neural Network with the improved
  model of ANC, we show that artificially intelligent agents can learn the
  unbreakable One-Time Pad (OTP) algorithm, without human knowledge, to
  communicate securely through an insecure communication channel. This paper
  shows in which conditions an AI agent can learn a secure encryption scheme.
  However, it also shows that, without a stronger adversary, it is more likely
  to obtain an insecure one.%
    }
    \verb{doi}
    \verb 10.3390/s18051306
    \endverb
    \field{issn}{1424-8220}
    \field{number}{5}
    \field{title}{Learning Perfectly Secure Cryptography to Protect
  Communications with Adversarial Neural Cryptography}
    \verb{url}
    \verb http://www.mdpi.com/1424-8220/18/5/1306
    \endverb
    \field{volume}{18}
    \field{journaltitle}{Sensors}
    \field{year}{2018}
  \endentry

  \entry{7332968}{article}{}
    \name{author}{4}{}{%
      {{hash=ZX}{%
         family={{Zhang}},
         familyi={Z\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=ZJ}{%
         family={{Zou}},
         familyi={Z\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HK}{%
         family={{He}},
         familyi={H\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={{Sun}},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{CNN, VGGNet, Acceleration;Image
  reconstruction;Optimization;Accuracy;Object detection;Covariance
  matrices;Life estimation;Convolutional neural networks;acceleration;image
  classification;object detection}
    \strng{namehash}{ZX+1}
    \strng{fullhash}{ZXZJHKSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper aims to accelerate the test-time computation of convolutional
  neural networks (CNNs), especially very deep CNNs [1] that have substantially
  impacted the computer vision community. Unlike previous methods that are
  designed for approximating linear filters or linear responses, our method
  takes the nonlinear units into account. We develop an effective solution to
  the resulting nonlinear optimization problem without the need of stochastic
  gradient descent (SGD). More importantly, while previous methods mainly focus
  on optimizing one or two layers, our nonlinear method enables an asymmetric
  reconstruction that reduces the rapidly accumulated error when multiple
  (e.g., $\ge$ 10) layers are approximated. For the widely used very deep
  VGG-16 model [1] , our method achieves a whole-model speedup of 4$\times$
  with merely a 0.3 percent increase of top-5 error in ImageNet classification.
  Our 4$\times$ accelerated VGG-16 model also shows a graceful accuracy
  degradation for object detection when plugged into the Fast R-CNN detector
  [2] .%
    }
    \verb{doi}
    \verb 10.1109/TPAMI.2015.2502579
    \endverb
    \field{issn}{1939-3539}
    \field{number}{10}
    \field{pages}{1943\bibrangedash 1955}
    \field{title}{Accelerating Very Deep Convolutional Networks for
  Classification and Detection}
    \field{volume}{38}
    \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7780459}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HK}{%
         family={He},
         familyi={H\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZX}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Ren},
         familyi={R\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{CNN, image classification; AI; neural nets;object detection, RESNET}
    \strng{namehash}{HK+1}
    \strng{fullhash}{HKZXRSSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Deeper neural networks are more difficult to train. We present a residual
  learning framework to ease the training of networks that are substantially
  deeper than those used previously. We explicitly reformulate the layers as
  learning residual functions with reference to the layer inputs, instead of
  learning unreferenced functions. We provide comprehensive empirical evidence
  showing that these residual networks are easier to optimize, and can gain
  accuracy from considerably increased depth. On the ImageNet dataset we
  evaluate residual nets with a depth of up to 152 layers - 8; deeper than VGG
  nets [40] but still having lower complexity. An ensemble of these residual
  nets achieves 3.57% error on the ImageNet test set. This result won the 1st
  place on the ILSVRC 2015 classification task. We also present analysis on
  CIFAR-10 with 100 and 1000 layers. The depth of representations is of central
  importance for many visual recognition tasks. Solely due to our extremely
  deep representations, we obtain a 28% relative improvement on the COCO object
  detection dataset. Deep residual nets are foundations of our submissions to
  ILSVRC; COCO 2015 competitions1, where we also won the 1st places on the
  tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO
  segmentation.%
    }
    \field{booktitle}{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}
    \verb{doi}
    \verb 10.1109/CVPR.2016.90
    \endverb
    \field{issn}{1063-6919}
    \field{pages}{770\bibrangedash 778}
    \field{title}{Deep Residual Learning for Image Recognition}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8533530}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=GT}{%
         family={{Geng}},
         familyi={G\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=WT}{%
         family={{Wang}},
         familyi={W\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={{Sanaullah}},
         familyi={S\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=YC}{%
         family={{Yang}},
         familyi={Y\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=PR}{%
         family={{Patel}},
         familyi={P\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=HM}{%
         family={{Herbordt}},
         familyi={H\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{FPGA-NN, training, CNN, convolutional neural nets;field programmable
  gate arrays;logic design;random-access storage;resource allocation;BRAM
  utilization;fine-grained pipelined manner;inter-FPGA
  bandwidth;deeply-pipelined FPGA clusters;FPGA-based CNN accelerators;off-chip
  memory bottlenecks;multiFPGA designs;CNN training logic;energy-efficient
  training;DSP slice utilization;convolutional neural
  networks;Microsoft;resource utilization;multiple FPGA;RTL implementation;CNN
  weight allocation;weight load balancing;cloud computing service
  providers;Amazon;Alibaba;FPDeep scalable framework;Field programmable gate
  arrays;Training;Graphics processing units;Parallel processing;Load
  management;Resource management;System-on-chip;CNN Training;FPGA Cluster;High
  Performance Computing}
    \strng{namehash}{GT+1}
    \strng{fullhash}{GTWTSAYCPRHM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    To improve flexibility and energy efficiency of Convolutional Neural
  Networks, a number of cloud computing service providers-including Microsoft,
  Amazon, and Alibaba-are using FPGA-based CNN accelerators. However, the
  growing size and complexity of neural networks, coupled with communication
  and off-chip memory bottlenecks, make it increasingly difficult for
  multi-FPGA designs to achieve high resource utilization and performance,
  especially when training. In this work, we present new results for a scalable
  framework, FPDeep, which helps users efficiently map CNN training logic to
  multiple FPGAs and automatically generates the resulting RTL implementation.
  FPDeep is equipped with two mechanisms to facilitate high-performance and
  energy-efficient training. First, FPDeep improves DSP slice utilization
  across FPGAs by balancing workload using dedicated partition and mapping
  strategies. Second, only on-chip memory is used in the CONV layers: a) FPDeep
  balances CNN weight allocation among FPGAs to improve BRAM utilization; b)
  training of CNNs is executed in a fine-grained pipelined manner, minimizing
  the time features need to be cached while waiting for back-propagation
  leading to a reduced storage demand. We evaluate our framework by training
  AlexNet, VGG-16, and VGG-19. Experimental results show FPDeep has good
  scalability to a large number of FPGAs, with the limiting factor being the
  inter-FPGA bandwidth. With 6 transceivers per FPGA, FPDeep shows linearity up
  to 83 FPGAs. FPDeep provides, on average, 6.36x higher energy efficiency than
  GPU servers.%
    }
    \field{booktitle}{2018 28th International Conference on Field Programmable
  Logic and Applications (FPL)}
    \verb{doi}
    \verb 10.1109/FPL.2018.00074
    \endverb
    \field{issn}{1946-1488}
    \field{pages}{394\bibrangedash 3944}
    \field{title}{A Framework for Acceleration of CNN Training on
  Deeply-Pipelined FPGA Clusters with Work and Weight Load Balancing}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{gpipe}{misc}{}
    \name{author}{1}{}{%
      {{hash=HY}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Yanping},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{URL, NN, Training, DNN}
    \strng{namehash}{HY1}
    \strng{fullhash}{HY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Introducing GPipe, an Open Source Library for Efficiently
  Training Large-scale Neural Network Models}
    \verb{url}
    \verb https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-libra
    \verb ry.html
    \endverb
    \field{year}{2019}
  \endentry

  \entry{JMLR:v18:16-456}{article}{}
    \name{author}{5}{}{%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=EYR}{%
         family={El-Yaniv},
         familyi={E\bibinithyphendelim Y\bibinitperiod},
         given={Ran},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{NN, Quantization, Neural Network, Binarization}
    \strng{namehash}{HI+1}
    \strng{fullhash}{HICMSDEYRBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{187}
    \field{pages}{1\bibrangedash 30}
    \field{title}{Quantized Neural Networks: Training Neural Networks with Low
  Precision Weights and Activations}
    \verb{url}
    \verb http://jmlr.org/papers/v18/16-456.html
    \endverb
    \field{volume}{18}
    \field{journaltitle}{Journal of Machine Learning Research}
    \field{year}{2018}
  \endentry

  \entry{Soudry2014ExpectationBP}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={Meir},
         familyi={M\bibinitperiod},
         given={Ron},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{DNN; Backpropagation; Discrete weight space; Continuos weight space}
    \strng{namehash}{SDHIMR1}
    \strng{fullhash}{SDHIMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Multilayer Neural Networks (MNNs) are commonly trained using gradient
  descent-based methods, such as BackPropagation (BP). Inference in
  probabilistic graphical models is often done using variational Bayes methods,
  such as Expectation Propagation (EP). We show how an EP based approach can
  also be used to train deterministic MNNs. Specifically, we approximate the
  posterior of the weights given the data using a ``mean-field'' factorized
  distribution, in an online setting. Using online EP and the central limit
  theorem we find an analytical approximation to the Bayes update of this
  posterior, as well as the resulting Bayes estimates of the weights and
  outputs. Despite a different origin, the resulting algorithm, Expectation
  BackPropagation (EBP), is very similar to BP in form and efficiency. However,
  it has several additional advantages: (1) Training is parameter-free, given
  initial conditions (prior) and the MNN architecture. This is useful for
  large-scale problems, where parameter tuning is a major challenge. (2) The
  weights can be restricted to have discrete values. This is especially useful
  for implementing trained MNNs in precision limited hardware chips, thus
  improving their speed and energy efficiency by several orders of magnitude.
  We test the EBP algorithm numerically in eight binary text classification
  tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal
  constant learning rate (2) previously reported state of the art.
  Interestingly, EBP-trained MNNs with binary weights usually perform better
  than MNNs with continuous (real) weights - if we average the MNN output using
  the inferred posterior.%
    }
    \field{booktitle}{NIPS}
    \field{title}{Expectation Backpropagation: Parameter-Free Training of
  Multilayer Neural Networks with Continuous or Discrete Weights}
    \field{annotation}{%
  https://www.semanticscholar.org/paper/Expectation-Backpropagation%3A-Parameter-Free-of-with-Soudry-Hubara/0b88ed755c4dcd0ac3d25842a5bbbe4ebcefeeec
  https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf%
    }
    \field{year}{2014}
  \endentry

  \entry{Martinez2020Training}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=MB}{%
         family={Martinez},
         familyi={M\bibinitperiod},
         given={Brais},
         giveni={B\bibinitperiod},
      }}%
      {{hash=YJ}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Bulat},
         familyi={B\bibinitperiod},
         given={Adrian},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TG}{%
         family={Tzimiropoulos},
         familyi={T\bibinitperiod},
         given={Georgios},
         giveni={G\bibinitperiod},
      }}%
    }
    \keyw{BNN, Training, Binary, Floating-point}
    \strng{namehash}{MB+1}
    \strng{fullhash}{MBYJBATG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{International Conference on Learning Representations}
    \field{title}{Training binary neural networks with real-to-binary
  convolutions}
    \verb{url}
    \verb https://openreview.net/forum?id=BJg4NgBKvH
    \endverb
    \field{year}{2020}
  \endentry

  \entry{Wang_2018_CVPR}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=WP}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Peisong},
         giveni={P\bibinitperiod},
      }}%
      {{hash=HQ}{%
         family={Hu},
         familyi={H\bibinitperiod},
         given={Qinghao},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Yifan},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZC}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Chunjie},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yang},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={Jian},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{BNN; Quantization, Low-bit}
    \strng{namehash}{WP+1}
    \strng{fullhash}{WPHQZYZCLYCJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}
    \field{title}{Two-Step Quantization for Low-Bit Neural Networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Qin_2020}{article}{}
    \name{author}{6}{}{%
      {{hash=QH}{%
         family={Qin},
         familyi={Q\bibinitperiod},
         given={Haotong},
         giveni={H\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Gong},
         familyi={G\bibinitperiod},
         given={Ruihao},
         giveni={R\bibinitperiod},
      }}%
      {{hash=LX}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Xianglong},
         giveni={X\bibinitperiod},
      }}%
      {{hash=BX}{%
         family={Bai},
         familyi={B\bibinitperiod},
         given={Xiao},
         giveni={X\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Song},
         familyi={S\bibinitperiod},
         given={Jingkuan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Sebe},
         familyi={S\bibinitperiod},
         given={Nicu},
         giveni={N\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier BV}%
    }
    \keyw{BNN, Survey, gradient descent}
    \strng{namehash}{QH+1}
    \strng{fullhash}{QHGRLXBXSJSN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1016/j.patcog.2020.107281
    \endverb
    \field{issn}{0031-3203}
    \field{pages}{107281}
    \field{title}{Binary neural networks: A survey}
    \verb{url}
    \verb http://dx.doi.org/10.1016/j.patcog.2020.107281
    \endverb
    \field{volume}{105}
    \field{journaltitle}{Pattern Recognition}
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7966159}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=CX}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=HX}{%
         family={Hu},
         familyi={H\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=ZH}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=XN}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
    }
    \keyw{application specific integrated circuits;convolution;field
  programmable gate arrays;fixed point arithmetic;floating point
  arithmetic;neural nets;FxpNet;deep convolutional neural network;fixed-point
  representation;bit-width arithmetics;forward pass;backward
  pass;floating-point values;binarized neural networks;quantized neural
  networks;fixed-point primal weights;low resolution fixed-point
  values;fixed-point primal parameters;FPGAs;ASICs;integer batch
  normalization;IBN;fixed-point ADAM;FxpADAM;CIFAR-10 dataset;12-bit primal
  parameters;12-bit gradients;Quantization (signal);Training;Field programmable
  gate arrays;Neural networks;Convolution;Kernel;Acceleration}
    \strng{namehash}{CX+1}
    \strng{fullhash}{CXHXZHXN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We introduce FxpNet, a framework to train deep convolutional neural
  networks with low bit-width arithmetics in both forward pass and backward
  pass. During training FxpNet further reduces the bit-width of stored
  parameters (also known as primal parameters) by adaptively updating their
  fixed-point formats. These primal parameters are usually represented in the
  full resolution of floating-point values in previous binarized and quantized
  neural networks. In FxpNet, during forward pass fixed-point primal weights
  and activations are first binarized before computation, while in backward
  pass all gradients are represented as low resolution fixed-point values and
  then accumulated to corresponding fixed-point primal parameters. To have
  highly efficient implementations in FPGAs, ASICs and other dedicated devices,
  FxpNet introduces Integer Batch Normalization (IBN) and Fixed-point ADAM
  (FxpADAM) methods to further reduce the required floating-point operations,
  which will save considerable power and chip area. The evaluation on CIFAR-10
  dataset indicates the effectiveness that FxpNet with 12-bit primal parameters
  and 12-bit gradients achieves comparable prediction accuracy with
  state-of-the-art binarized and quantized neural networks.%
    }
    \field{booktitle}{2017 International Joint Conference on Neural Networks
  (IJCNN)}
    \verb{doi}
    \verb 10.1109/IJCNN.2017.7966159
    \endverb
    \field{issn}{2161-4407}
    \field{pages}{2494\bibrangedash 2501}
    \field{title}{FxpNet: Training a deep convolutional neural network in
  fixed-point representation}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8665777}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HA}{%
         family={{Haidar}},
         familyi={H\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TS}{%
         family={{Tomov}},
         familyi={T\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=DJ}{%
         family={{Dongarra}},
         familyi={D\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HNJ}{%
         family={{Higham}},
         familyi={H\bibinitperiod},
         given={N.\bibnamedelima J.},
         giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \keyw{NN; FP16; coprocessors;floating point arithmetic;iterative
  methods;linear algebra;mathematics computing;tensors;scientific computing
  applications;artificial intelligence;high-performance computing
  applications;general HPC problem;dense matrix;double precision
  solution;FP16-FP64;architecture-specific algorithms;FP16-TC;classical FP16
  arithmetic;FP32 arithmetic;mixed-precision iterative refinement
  solvers;low-precision floating-point arithmetic;fast FP16 arithmetic;GPU
  tensor core;half-precision tensor cores;GEMM accumulation;Graphics processing
  units;Iterative algorithms;Acceleration;Matrices;FP16 Arithmetic;Half
  Precision;Mixed Precision Solvers;Iterative Refinement Computation;GPU
  Computing;Linear Algebra}
    \strng{namehash}{HA+1}
    \strng{fullhash}{HATSDJHNJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Low-precision floating-point arithmetic is a powerful tool for accelerating
  scientific computing applications, especially those in artificial
  intelligence. Here, we present an investigation showing that other
  high-performance computing (HPC) applications can also harness this power.
  Specifically, we use the general HPC problem, Ax b, where A is a large dense
  matrix, and a double precision (FP64) solution is needed for accuracy. Our
  approach is based on mixed-precision (FP16-FP64) iterative refinement, and we
  generalize and extend prior advances into a framework, for which we develop
  architecture-specific algorithms and highly tuned implementations. These new
  methods show how using half-precision Tensor Cores (FP16-TC) for the
  arithmetic can provide up to 4Ã— speedup. This is due to the performance
  boost that the FP16-TC provide as well as to the improved accuracy over the
  classical FP16 arithmetic that is obtained because the GEMM accumulation
  occurs in FP32 arithmetic.%
    }
    \field{booktitle}{SC18: International Conference for High Performance
  Computing, Networking, Storage and Analysis}
    \verb{doi}
    \verb 10.1109/SC.2018.00050
    \endverb
    \field{pages}{603\bibrangedash 613}
    \field{title}{Harnessing GPU Tensor Cores for Fast FP16 Arithmetic to Speed
  up Mixed-Precision Iterative Refinement Solvers}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1007/978-3-319-46493-0_32}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=RM}{%
         family={Rastegari},
         familyi={R\bibinitperiod},
         given={Mohammad},
         giveni={M\bibinitperiod},
      }}%
      {{hash=OV}{%
         family={Ordonez},
         familyi={O\bibinitperiod},
         given={Vicente},
         giveni={V\bibinitperiod},
      }}%
      {{hash=RJ}{%
         family={Redmon},
         familyi={R\bibinitperiod},
         given={Joseph},
         giveni={J\bibinitperiod},
      }}%
      {{hash=FA}{%
         family={Farhadi},
         familyi={F\bibinitperiod},
         given={Ali},
         giveni={A\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=LB}{%
         family={Leibe},
         familyi={L\bibinitperiod},
         given={Bastian},
         giveni={B\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Matas},
         familyi={M\bibinitperiod},
         given={Jiri},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Sebe},
         familyi={S\bibinitperiod},
         given={Nicu},
         giveni={N\bibinitperiod},
      }}%
      {{hash=WM}{%
         family={Welling},
         familyi={W\bibinitperiod},
         given={Max},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \keyw{BNN, XNOR, ImageNet, CNN, AlexNet}
    \strng{namehash}{RM+2}
    \strng{fullhash}{RMOVRJFA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We propose two efficient approximations to standard convolutional neural
  networks: Binary-Weight-Networks and XNOR-Networks. In
  Binary-Weight-Networks, the filters are approximated with binary values
  resulting in 32{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}memory saving.
  In XNOR-Networks, both the filters and the input to convolutional layers are
  binary. XNOR-Networks approximate convolutions using primarily binary
  operations. This results in 58{\$}{\$}{\backslash}times
  {\$}{\$}{\texttimes}faster convolutional operations (in terms of number of
  the high precision operations) and 32{\$}{\$}{\backslash}times
  {\$}{\$}{\texttimes}memory savings. XNOR-Nets offer the possibility of
  running state-of-the-art networks on CPUs (rather than GPUs) in real-time.
  Our binary networks are simple, accurate, efficient, and work on challenging
  visual tasks. We evaluate our approach on the ImageNet classification task.
  The classification accuracy with a Binary-Weight-Network version of AlexNet
  is the same as the full-precision AlexNet. We compare our method with recent
  network binarization methods, BinaryConnect and BinaryNets, and outperform
  these methods by large margins on ImageNet, more than
  {\$}{\$}16{\backslash},{\backslash}{\%}{\$}{\$}16{\%}in top-1 accuracy. Our
  code is available at: http://allenai.org/plato/xnornet.%
    }
    \field{booktitle}{Computer Vision -- ECCV 2016}
    \field{isbn}{978-3-319-46493-0}
    \field{pages}{525\bibrangedash 542}
    \field{title}{XNOR-Net: ImageNet Classification Using Binary Convolutional
  Neural Networks}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2016}
  \endentry

  \entry{10.1007/978-3-319-78890-6_3}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=SJ}{%
         family={Su},
         familyi={S\bibinitperiod},
         given={Jiang},
         giveni={J\bibinitperiod},
      }}%
      {{hash=FNJ}{%
         family={Fraser},
         familyi={F\bibinitperiod},
         given={Nicholas\bibnamedelima J.},
         giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=GG}{%
         family={Gambardella},
         familyi={G\bibinitperiod},
         given={Giulio},
         giveni={G\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Blott},
         familyi={B\bibinitperiod},
         given={Michaela},
         giveni={M\bibinitperiod},
      }}%
      {{hash=DG}{%
         family={Durelli},
         familyi={D\bibinitperiod},
         given={Gianluca},
         giveni={G\bibinitperiod},
      }}%
      {{hash=TDB}{%
         family={Thomas},
         familyi={T\bibinitperiod},
         given={David\bibnamedelima B.},
         giveni={D\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=LPHW}{%
         family={Leong},
         familyi={L\bibinitperiod},
         given={Philip H.\bibnamedelima W.},
         giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod\bibinitdelim
  W\bibinitperiod},
      }}%
      {{hash=CPYK}{%
         family={Cheung},
         familyi={C\bibinitperiod},
         given={Peter Y.\bibnamedelima K.},
         giveni={P\bibinitperiod\bibinitdelim Y\bibinitperiod\bibinitdelim
  K\bibinitperiod},
      }}%
    }
    \name{editor}{6}{}{%
      {{hash=VN}{%
         family={Voros},
         familyi={V\bibinitperiod},
         given={Nikolaos},
         giveni={N\bibinitperiod},
      }}%
      {{hash=HM}{%
         family={Huebner},
         familyi={H\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
      {{hash=KG}{%
         family={Keramidas},
         familyi={K\bibinitperiod},
         given={Georgios},
         giveni={G\bibinitperiod},
      }}%
      {{hash=GD}{%
         family={Goehringer},
         familyi={G\bibinitperiod},
         given={Diana},
         giveni={D\bibinitperiod},
      }}%
      {{hash=AC}{%
         family={Antonopoulos},
         familyi={A\bibinitperiod},
         given={Christos},
         giveni={C\bibinitperiod},
      }}%
      {{hash=DPC}{%
         family={Diniz},
         familyi={D\bibinitperiod},
         given={Pedro\bibnamedelima C.},
         giveni={P\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \keyw{Books, BNN, MNIST, CIFAR}
    \strng{namehash}{SJ+3}
    \strng{fullhash}{SJFNJGGBMDGTDBLPHWCPYK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Modern Convolutional Neural Networks (CNNs) are typically based on floating
  point linear algebra based implementations. Recently, reduced precision
  Neural Networks (NNs) have been gaining popularity as they require
  significantly less memory and computational resources compared to floating
  point. This is particularly important in power constrained compute
  environments. However, in many cases a reduction in precision comes at a
  small cost to the accuracy of the resultant network. In this work, we
  investigate the accuracy-throughput trade-off for various parameter precision
  applied to different types of NN models. We firstly propose a quantization
  training strategy that allows reduced precision NN inference with a lower
  memory footprint and competitive model accuracy. Then, we quantitatively
  formulate the relationship between data representation and hardware
  efficiency. Our experiments finally provide insightful observation. For
  example, one of our tests show 32-bit floating point is more hardware
  efficient than 1-bit parameters to achieve 99{\%} MNIST accuracy. In general,
  2-bit and 4-bit fixed point parameters show better hardware trade-off on
  small-scale datasets like MNIST and CIFAR-10 while 4-bit provide the best
  trade-off in large-scale tasks like AlexNet on ImageNet dataset within our
  tested problem domain.%
    }
    \field{booktitle}{Applied Reconfigurable Computing. Architectures, Tools,
  and Applications}
    \field{isbn}{978-3-319-78890-6}
    \field{pages}{29\bibrangedash 42}
    \field{title}{Accuracy to Throughput Trade-Offs for Reduced Precision
  Neural Networks on Reconfigurable Logic}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2018}
  \endentry

  \entry{8103902}{article}{}
    \name{author}{3}{}{%
      {{hash=WY}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Z.},
         giveni={Z\bibinitperiod},
      }}%
    }
    \keyw{BNN;adders;convolution;energy conservation;error analysis;feedforward
  neural nets;multiplying circuits;neural chips;trees
  (mathematics);energy-efficient architecture;binary weight convolutional
  neural networks;high-precision weights;binary weights;approximate binary
  multipliers;BCNN hardware architecture;energy efficiency;classification
  accuracy;data path delay;processing schedule;off-chip I/O access;critical
  path delay;optimized compressor trees;approximate adder;error
  analysis;memory-efficient quantization;on-chip storage requirement;size 65.0
  nm;Computer architecture;Hardware;Neural networks;Neurons;Adders;Quantization
  (signal);Convolution;Approximate computing;binary weight convolutional neural
  network (BCNN) architecture;convolutional neural network (CNN);deep
  learning;energy-efficient design;signal processing;VLSI architecture}
    \strng{namehash}{WYLJWZ1}
    \strng{fullhash}{WYLJWZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Binary weight convolutional neural networks (BCNNs) can achieve near
  state-of-the-art classification accuracy and have far less computation
  complexity compared with traditional CNNs using high-precision weights. Due
  to their binary weights, BCNNs are well suited for vision-based
  Internet-of-Things systems being sensitive to power consumption. BCNNs make
  it possible to achieve very high throughput with moderate power dissipation.
  In this paper, an energy-efficient architecture for BCNNs is proposed. It
  fully exploits the binary weights and other hardware-friendly characteristics
  of BCNNs. A judicious processing schedule is proposed so that off-chip I/O
  access is minimized and activations are maximally reused. To significantly
  reduce the critical path delay, we introduce optimized compressor trees and
  approximate binary multipliers with two novel compensation schemes. The
  latter is able to save significant hardware resource, and almost no
  computation accuracy is compromised. Taking advantage of error resiliency of
  BCNNs, an innovative approximate adder is developed, which significantly
  reduces the silicon area and data path delay. Thorough error analysis and
  extensive experimental results on several data sets show that the approximate
  adders in the data path cause negligible accuracy loss. Moreover, algorithmic
  transformations for certain layers of BCNNs and a memory-efficient
  quantization scheme are incorporated to further reduce the energy cost and
  on-chip storage requirement. Finally, the proposed BCNN hardware architecture
  is implemented with the SMIC 130-nm technology. The postlayout results
  demonstrate that our design can achieve an energy efficiency over 2.0TOp/s/W
  when scaled to 65 nm, which is more than two times better than the prior
  art.%
    }
    \verb{doi}
    \verb 10.1109/TVLSI.2017.2767624
    \endverb
    \field{issn}{1063-8210}
    \field{number}{2}
    \field{pages}{280\bibrangedash 293}
    \field{title}{An Energy-Efficient Architecture for Binary Weight
  Convolutional Neural Networks}
    \field{volume}{26}
    \field{journaltitle}{IEEE Transactions on Very Large Scale Integration
  (VLSI) Systems}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{SCHMIDHUBER201585}{article}{}
    \name{author}{1}{}{%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={J{\"u}rgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{DNN; Deep learning, Supervised learning, Unsupervised learning,
  Reinforcement learning, Evolutionary computation}
    \strng{namehash}{SJ1}
    \strng{fullhash}{SJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In recent years, deep artificial neural networks (including recurrent ones)
  have won numerous contests in pattern recognition and machine learning. This
  historical survey compactly summarizes relevant work, much of it from the
  previous millennium. Shallow and Deep Learners are distinguished by the depth
  of their credit assignment paths, which are chains of possibly learnable,
  causal links between actions and effects. I review deep supervised learning
  (also recapitulating the history of backpropagation), unsupervised learning,
  reinforcement learning & evolutionary computation, and indirect search for
  short programs encoding deep and large networks.%
    }
    \verb{doi}
    \verb https://doi.org/10.1016/j.neunet.2014.09.003
    \endverb
    \field{issn}{0893-6080}
    \field{pages}{85 \bibrangedash  117}
    \field{title}{Deep learning in neural networks: An overview}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0893608014002135
    \endverb
    \field{volume}{61}
    \field{journaltitle}{Neural Networks}
    \field{year}{2015}
  \endentry

  \entry{2016arXiv160601540B}{article}{}
    \name{author}{7}{}{%
      {{hash=BG}{%
         family={{Brockman}},
         familyi={B\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={{Cheung}},
         familyi={C\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=PL}{%
         family={{Pettersson}},
         familyi={P\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={{Schneider}},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={{Schulman}},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=TJ}{%
         family={{Tang}},
         familyi={T\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=ZW}{%
         family={{Zaremba}},
         familyi={Z\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
    }
    \keyw{Other, OpenAI Gym, Machine Learning, Artificial Intelligence}
    \strng{namehash}{BG+1}
    \strng{fullhash}{BGCVPLSJSJTJZW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1606.01540
    \endverb
    \field{title}{{OpenAI Gym}}
    \field{journaltitle}{arXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{month}{06}
    \field{year}{2016}
  \endentry

  \entry{gpgpu-ai-dominance}{online}{}
    \name{author}{1}{}{%
      {{hash=KJ}{%
         family={Kobielus},
         familyi={K\bibinitperiod},
         given={James},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{WebPage, GPGPU, AI}
    \strng{namehash}{KJ1}
    \strng{fullhash}{KJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{GPUs Continue to Dominate the AI Accelerator Market for Now}
    \verb{url}
    \verb https://www.informationweek.com/big-data/ai-machine-learning/gpus-con
    \verb tinue-to-dominate-the-ai-accelerator-market-for-now/a/d-id/1336475
    \endverb
    \field{year}{2019}
  \endentry

  \entry{7929192}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=NE}{%
         family={Nurvitadhi},
         familyi={N\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Sheffield},
         familyi={S\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sim},
         familyi={S\bibinitperiod},
         given={Jaewoong},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mishra},
         familyi={M\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VG}{%
         family={Venkatesh},
         familyi={V\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=MD}{%
         family={Marr},
         familyi={M\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{BNN, application specific integrated circuits;field programmable gate
  arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria
  10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural
  networks;deep neural network;hardware acceleration;Biological neural
  networks;Field programmable gate arrays;Graphics processing
  units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep
  learning;FPGA;GPU;binarized neural networks;data analytics;hardware
  accelerator; FPGA, GPU}
    \strng{namehash}{NE+1}
    \strng{fullhash}{NESDSJMAVGMD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{2016 International Conference on Field-Programmable
  Technology (FPT)}
    \verb{doi}
    \verb 10.1109/FPT.2016.7929192
    \endverb
    \field{pages}{77\bibrangedash 84}
    \field{title}{Accelerating Binarized Neural Networks: Comparison of FPGA,
  CPU, GPU, and ASIC}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7878541}{article}{}
    \name{author}{4}{}{%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rossi},
         familyi={R\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; CMOS logic circuits;computer
  vision;convolution;coprocessors;embedded systems;image
  classification;integrated circuit design;low-power electronics;neural
  nets;optimisation;power aware computing;system-on-chip;I/O storage;I/O
  bandwidth;algorithmic advancements;binary weights;competitive classification
  accuracy;hard limitations;deeply embedded applications;mobile embedded
  applications;power envelope;energy consumption;system-on-chip integration;CNN
  accelerators;GP-GPUs;power-hungry parallel processors;computational
  effort;human accuracy;image classification;computer vision;convolutional
  neural networks;ultralow power binary-weight CNN acceleration;power
  dissipation;binary-weight CNNs;accelerator;optimization
  opportunities;ASIC;binary weights;convolutional neural networks
  (CNNs);hardware accelerator;Internet of Things (IoT)}
    \strng{namehash}{AR+1}
    \strng{fullhash}{ARCLRDBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Convolutional neural networks (CNNs) have revolutionized the world of
  computer vision over the last few years, pushing image classification beyond
  human accuracy. The computational effort of today's CNNs requires
  power-hungry parallel processors or GP-GPUs. Recent developments in CNN
  accelerators for system-on-chip integration have reduced energy consumption
  significantly. Unfortunately, even these highly optimized devices are above
  the power envelope imposed by mobile and deeply embedded applications and
  face hard limitations caused by CNN weight I/O and storage. This prevents the
  adoption of CNNs in future ultralow power Internet of Things end-nodes for
  near-sensor analytics. Recent algorithmic and theoretical advancements enable
  competitive classification accuracy even when limiting CNNs to binary (+1/-1)
  weights during training. These new findings bring major optimization
  opportunities in the arithmetic core by removing the need for expensive
  multiplications, as well as reducing I/O bandwidth and storage. In this
  paper, we present an accelerator optimized for binary-weight CNNs that
  achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate
  equivalent (MGE) or 1.9 mm<sup>2</sup>and with a power dissipation of 895 Î¼W
  in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms
  the state-of-the-art in terms of energy and area efficiency achieving 61.2
  TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.%
    }
    \verb{doi}
    \verb 10.1109/TCAD.2017.2682138
    \endverb
    \field{issn}{0278-0070}
    \field{number}{1}
    \field{pages}{48\bibrangedash 60}
    \field{title}{YodaNN: An Architecture for Ultralow Power Binary-Weight CNN
  Acceleration}
    \field{volume}{37}
    \field{journaltitle}{IEEE Transactions on Computer-Aided Design of
  Integrated Circuits and Systems}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1145/3154839}{article}{}
    \name{author}{5}{}{%
      {{hash=LY}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Yixing},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Zichuan},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=XK}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={Kai},
         giveni={K\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Hao},
         giveni={H\bibinitperiod},
      }}%
      {{hash=RF}{%
         family={Ren},
         familyi={R\bibinitperiod},
         given={Fengbo},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \keyw{CNN-FPGA, BNN, energy efficiency, deep learning, FPGA,
  high-throughput, convolutional neural network, hardware acceleration, binary
  neural network, GPU}
    \strng{namehash}{LY+1}
    \strng{fullhash}{LYLZXKYHRF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1145/3154839
    \endverb
    \field{issn}{1550-4832}
    \field{number}{2}
    \field{title}{A GPU-Outperforming FPGA Accelerator Architecture for Binary
  Convolutional Neural Networks}
    \verb{url}
    \verb https://doi-org.ezproxy.auckland.ac.nz/10.1145/3154839
    \endverb
    \field{volume}{14}
    \list{location}{1}{%
      {New York, NY, USA}%
    }
    \field{journaltitle}{J. Emerg. Technol. Comput. Syst.}
    \field{month}{07}
    \field{year}{2018}
  \endentry

  \entry{vitis-ai}{misc}{}
    \name{author}{1}{}{%
      {{hash=X}{%
         family={Xilinx},
         familyi={X\bibinitperiod},
      }}%
    }
    \keyw{Webpage, Xilinx, FPGA, Vitis, AI,}
    \strng{namehash}{X1}
    \strng{fullhash}{X1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Vitis AI}
    \verb{url}
    \verb https://www.xilinx.com/products/design-tools/vitis/vitis-ai.html
    \endverb
    \field{year}{2020}
  \endentry

  \entry{altera-ai}{misc}{}
    \name{author}{1}{}{%
      {{hash=I}{%
         family={Intel},
         familyi={I\bibinitperiod},
      }}%
    }
    \keyw{WebPage, Intel, FPGA, Altera, AI}
    \strng{namehash}{I2}
    \strng{fullhash}{I2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{FPGAs for Artificial Intelligence (AI)}
    \verb{url}
    \verb https://www.intel.com/content/www/us/en/artificial-intelligence/progr
    \verb ammable/overview.html
    \endverb
  \endentry

  \entry{zynqDPU}{report}{}
    \name{author}{1}{}{%
      {{hash=X}{%
         family={Xilinx},
         familyi={X\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Xilinx}%
    }
    \keyw{Other, Zynq, Xilinx, DPU}
    \strng{namehash}{X1}
    \strng{fullhash}{X1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{PG338}
    \field{title}{Zynq DPU v3.2}
    \verb{url}
    \verb https://www.xilinx.com/support/documentation/ip_documentation/dpu/v3_
    \verb 2/pg338-dpu.pdf
    \endverb
    \field{type}{Product Guide}
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{electronics8060661}{article}{}
    \name{author}{2}{}{%
      {{hash=ST}{%
         family={Simons},
         familyi={S\bibinitperiod},
         given={Taylor},
         giveni={T\bibinitperiod},
      }}%
      {{hash=LDJ}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={Dah-Jye},
         giveni={D\bibinithyphendelim J\bibinitperiod},
      }}%
    }
    \keyw{BNN, Survey, backpropagation}
    \strng{namehash}{STLDJ1}
    \strng{fullhash}{STLDJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this work, we review Binarized Neural Networks (BNNs). BNNs are deep
  neural networks that use binary values for activations and weights, instead
  of full precision values. With binary values, BNNs can execute computations
  using bitwise operations, which reduces execution time. Model sizes of BNNs
  are much smaller than their full precision counterparts. While the accuracy
  of a BNN model is generally less than full precision models, BNNs have been
  closing accuracy gap and are becoming more accurate on larger datasets like
  ImageNet. BNNs are also good candidates for deep learning implementations on
  FPGAs and ASICs due to their bitwise efficiency. We give a tutorial of the
  general BNN methodology and review various contributions, implementations and
  applications of BNNs.%
    }
    \verb{doi}
    \verb 10.3390/electronics8060661
    \endverb
    \field{issn}{2079-9292}
    \field{number}{6}
    \field{title}{A Review of Binarized Neural Networks}
    \verb{url}
    \verb https://www.mdpi.com/2079-9292/8/6/661
    \endverb
    \field{volume}{8}
    \field{journaltitle}{Electronics}
    \field{year}{2019}
  \endentry

  \entry{Tang2017HowTT}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=TW}{%
         family={Tang},
         familyi={T\bibinitperiod},
         given={Wei},
         giveni={W\bibinitperiod},
      }}%
      {{hash=HG}{%
         family={Hua},
         familyi={H\bibinitperiod},
         given={Gang},
         giveni={G\bibinitperiod},
      }}%
      {{hash=WL}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Liang},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN, NN, Training}
    \strng{namehash}{TWHGWL1}
    \strng{fullhash}{TWHGWL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{AAAI}
    \field{title}{How to Train a Compact Binary Neural Network with High
  Accuracy?}
    \field{annotation}{%
    https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14619%
    }
    \field{year}{2017}
  \endentry

  \entry{Umuroglu:2017:FFF:3020078.3021744}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=UY}{%
         family={Umuroglu},
         familyi={U\bibinitperiod},
         given={Yaman},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=FNJ}{%
         family={Fraser},
         familyi={F\bibinitperiod},
         given={Nicholas\bibnamedelima J.},
         giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=GG}{%
         family={Gambardella},
         familyi={G\bibinitperiod},
         given={Giulio},
         giveni={G\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Blott},
         familyi={B\bibinitperiod},
         given={Michaela},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Leong},
         familyi={L\bibinitperiod},
         given={Philip},
         giveni={P\bibinitperiod},
      }}%
      {{hash=JM}{%
         family={Jahre},
         familyi={J\bibinitperiod},
         given={Magnus},
         giveni={M\bibinitperiod},
      }}%
      {{hash=VK}{%
         family={Vissers},
         familyi={V\bibinitperiod},
         given={Kees},
         giveni={K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{BNN; FPGA, binarized neural network, binary neural network, hardware
  acceleration, neural networks, reconfigurable logic}
    \strng{namehash}{UY+1}
    \strng{fullhash}{UYFNJGGBMLPJMVK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 2017 ACM/SIGDA International Symposium
  on Field-Programmable Gate Arrays}
    \verb{doi}
    \verb 10.1145/3020078.3021744
    \endverb
    \field{isbn}{978-1-4503-4354-1}
    \field{pages}{65\bibrangedash 74}
    \field{series}{FPGA '17}
    \field{title}{FINN: A Framework for Fast, Scalable Binarized Neural Network
  Inference}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3020078.3021744
    \endverb
    \list{location}{1}{%
      {Monterey, California, USA}%
    }
    \field{year}{2017}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{Kung2018}{article}{}
    \name{author}{5}{}{%
      {{hash=KJ}{%
         family={Kung},
         familyi={K\bibinitperiod},
         given={Jaeha},
         giveni={J\bibinitperiod},
      }}%
      {{hash=ZD}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=vdWG}{%
         prefix={van\bibnamedelima der},
         prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod},
         family={Wal},
         familyi={W\bibinitperiod},
         given={Gooitzen},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CS}{%
         family={Chai},
         familyi={C\bibinitperiod},
         given={Sek},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Mukhopadhyay},
         familyi={M\bibinitperiod},
         given={Saibal},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{BNN, object detection, GPU, Infrared, FPGA}
    \strng{namehash}{KJ+1}
    \strng{fullhash}{KJZDWGvdCSMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Memory performance is a key bottleneck for deep learning systems.
  Binarization of both activations and weights is one promising approach that
  can best scale to realize the highest energy efficient system using the
  lowest possible precision. In this paper, we utilize and analyze the
  binarized neural network in doing human detection on infrared images. Our
  results show comparable algorithmic performance of binarized versus 32bit
  floating-point networks, with the added benefit of greatly simplified
  computation and reduced memory overhead. In addition, we present a system
  architecture designed specifically for computation using binary
  representation that achieves at least 4{\texttimes} speedup and the energy is
  improved by three orders of magnitude over GPU.%
    }
    \verb{doi}
    \verb 10.1007/s11265-017-1255-5
    \endverb
    \field{issn}{1939-8115}
    \field{number}{6}
    \field{pages}{877\bibrangedash 890}
    \field{title}{Efficient Object Detection Using Embedded Binarized Neural
  Networks}
    \verb{url}
    \verb https://doi.org/10.1007/s11265-017-1255-5
    \endverb
    \field{volume}{90}
    \field{journaltitle}{Journal of Signal Processing Systems}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{aggarwal2017deep}{article}{}
    \name{author}{2}{}{%
      {{hash=AS}{%
         family={Aggarwal},
         familyi={A\bibinitperiod},
         given={Saurabh},
         giveni={S\bibinitperiod},
      }}%
      {{hash=AS}{%
         family={Aggarwal},
         familyi={A\bibinitperiod},
         given={Somya},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Foundation of Computer Science}%
    }
    \keyw{DNN, Finance, Portfolio, LSTM}
    \strng{namehash}{ASAS1}
    \strng{fullhash}{ASAS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{2}
    \field{title}{Deep Investment in Financial Markets using Deep Learning
  Models}
    \field{volume}{162}
    \field{journaltitle}{International Journal of Computer Applications}
    \field{year}{2017}
  \endentry

  \entry{Jaderberg859}{article}{}
    \name{author}{18}{}{%
      {{hash=JM}{%
         family={Jaderberg},
         familyi={J\bibinitperiod},
         given={Max},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CWM}{%
         family={Czarnecki},
         familyi={C\bibinitperiod},
         given={Wojciech\bibnamedelima M.},
         giveni={W\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=DI}{%
         family={Dunning},
         familyi={D\bibinitperiod},
         given={Iain},
         giveni={I\bibinitperiod},
      }}%
      {{hash=ML}{%
         family={Marris},
         familyi={M\bibinitperiod},
         given={Luke},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LG}{%
         family={Lever},
         familyi={L\bibinitperiod},
         given={Guy},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CAG}{%
         family={Casta{\~n}eda},
         familyi={C\bibinitperiod},
         given={Antonio\bibnamedelima Garcia},
         giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Beattie},
         familyi={B\bibinitperiod},
         given={Charles},
         giveni={C\bibinitperiod},
      }}%
      {{hash=RNC}{%
         family={Rabinowitz},
         familyi={R\bibinitperiod},
         given={Neil\bibnamedelima C.},
         giveni={N\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=MAS}{%
         family={Morcos},
         familyi={M\bibinitperiod},
         given={Ari\bibnamedelima S.},
         giveni={A\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=RA}{%
         family={Ruderman},
         familyi={R\bibinitperiod},
         given={Avraham},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Sonnerat},
         familyi={S\bibinitperiod},
         given={Nicolas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Green},
         familyi={G\bibinitperiod},
         given={Tim},
         giveni={T\bibinitperiod},
      }}%
      {{hash=DL}{%
         family={Deason},
         familyi={D\bibinitperiod},
         given={Louise},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LJZ}{%
         family={Leibo},
         familyi={L\bibinitperiod},
         given={Joel\bibnamedelima Z.},
         giveni={J\bibinitperiod\bibinitdelim Z\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Silver},
         familyi={S\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HD}{%
         family={Hassabis},
         familyi={H\bibinitperiod},
         given={Demis},
         giveni={D\bibinitperiod},
      }}%
      {{hash=KK}{%
         family={Kavukcuoglu},
         familyi={K\bibinitperiod},
         given={Koray},
         giveni={K\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Graepel},
         familyi={G\bibinitperiod},
         given={Thore},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {American Association for the Advancement of Science}%
    }
    \keyw{DNN, DeepMind, 3D mutiplayer, multi-agent systems}
    \strng{namehash}{JM+1}
    \strng{fullhash}{JMCWMDIMLLGCAGBCRNCMASRASNGTDLLJZSDHDKKGT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Artificially intelligent agents are getting better and better at two-player
  games, but most real-world endeavors require teamwork. Jaderberg et al.
  designed a computer program that excels at playing the video game Quake III
  Arena in Capture the Flag mode, where two multiplayer teams compete in
  capturing the flags of the opposing team. The agents were trained by playing
  thousands of games, gradually learning successful strategies not unlike those
  favored by their human counterparts. Computer agents competed successfully
  against humans even when their reaction times were slowed to match those of
  humans.Science, this issue p. 859Reinforcement learning (RL) has shown great
  success in increasingly complex single-agent environments and two-player
  turn-based games. However, the real world contains multiple agents, each
  learning and acting independently to cooperate and compete with other agents.
  We used a tournament-style evaluation to demonstrate that an agent can
  achieve human-level performance in a three-dimensional multiplayer
  first-person video game, Quake III Arena in Capture the Flag mode, using only
  pixels and game points scored as input. We used a two-tier optimization
  process in which a population of independent RL agents are trained
  concurrently from thousands of parallel matches on randomly generated
  environments. Each agent learns its own internal reward signal and rich
  representation of the world. These results indicate the great potential of
  multiagent reinforcement learning for artificial intelligence research.%
    }
    \verb{doi}
    \verb 10.1126/science.aau6249
    \endverb
    \field{issn}{0036-8075}
    \field{number}{6443}
    \field{pages}{859\bibrangedash 865}
    \field{title}{Human-level performance in 3D multiplayer games with
  population-based reinforcement learning}
    \field{volume}{364}
    \field{journaltitle}{Science}
    \field{year}{2019}
  \endentry

  \entry{10.1007/978-3-642-34500-5_75}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=AMM}{%
         family={Alani},
         familyi={A\bibinitperiod},
         given={Mohammed\bibnamedelima M.},
         giveni={M\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
    }
    \name{editor}{4}{}{%
      {{hash=HT}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Tingwen},
         giveni={T\bibinitperiod},
      }}%
      {{hash=ZZ}{%
         family={Zeng},
         familyi={Z\bibinitperiod},
         given={Zhigang},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=LC}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Chuandong},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LCS}{%
         family={Leung},
         familyi={L\bibinitperiod},
         given={Chi\bibnamedelima Sing},
         giveni={C\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Book, Neurocryptography, DES,}
    \strng{namehash}{AMM1}
    \strng{fullhash}{AMM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this paper, we apply a new cryptanalytic attack on DES and Triple-DES.
  The implemented attack is a known-plaintext attack based on neural networks.
  In this attack we trained a neural network to retrieve plaintext from
  ciphertext without retrieving the key used in encryption.%
    }
    \field{booktitle}{Neural Information Processing}
    \field{isbn}{978-3-642-34500-5}
    \field{pages}{637\bibrangedash 646}
    \field{title}{Neuro-Cryptanalysis of DES and Triple-DES}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{2012}
  \endentry

  \entry{Bianchi:2009aa}{article}{}
    \name{author}{4}{}{%
      {{hash=BL}{%
         family={Bianchi},
         familyi={B\bibinitperiod},
         given={Leonora},
         giveni={L\bibinitperiod},
      }}%
      {{hash=DM}{%
         family={Dorigo},
         familyi={D\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GLM}{%
         family={Gambardella},
         familyi={G\bibinitperiod},
         given={Luca\bibnamedelima Maria},
         giveni={L\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=GWJ}{%
         family={Gutjahr},
         familyi={G\bibinitperiod},
         given={Walter\bibnamedelima J.},
         giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \keyw{Other, Metaheuristics, Evolutionary computation, Survey}
    \strng{namehash}{BL+1}
    \strng{fullhash}{BLDMGLMGWJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Metaheuristics are general algorithmic frameworks, often nature-inspired,
  designed to solve complex optimization problems, and they are a growing
  research area since a few decades. In recent years, metaheuristics are
  emerging as successful alternatives to more classical approaches also for
  solving optimization problems that include in their mathematical formulation
  uncertain, stochastic, and dynamic information. In this paper metaheuristics
  such as Ant Colony Optimization, Evolutionary Computation, Simulated
  Annealing, Tabu Search and others are introduced, and their applications to
  the class of Stochastic Combinatorial Optimization Problems (SCOPs) is
  thoroughly reviewed. Issues common to all metaheuristics, open problems, and
  possible directions of research are proposed and discussed. In this survey,
  the reader familiar to metaheuristics finds also pointers to classical
  algorithmic approaches to optimization under uncertainty, and useful
  informations to start working on this problem domain, while the reader new to
  metaheuristics should find a good tutorial in those metaheuristics that are
  currently being applied to optimization under uncertainty, and motivations
  for interest in this field.%
    }
    \verb{doi}
    \verb 10.1007/s11047-008-9098-4
    \endverb
    \field{isbn}{1572-9796}
    \field{number}{2}
    \field{pages}{239\bibrangedash 287}
    \field{title}{A survey on metaheuristics for stochastic combinatorial
  optimization}
    \verb{url}
    \verb https://doi.org/10.1007/s11047-008-9098-4
    \endverb
    \field{volume}{8}
    \field{journaltitle}{Natural Computing}
    \field{year}{2009}
  \endentry

  \entry{8953134}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=VR}{%
         family={{Valencia}},
         familyi={V\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SC}{%
         family={{Sham}},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=SO}{%
         family={{Sinnen}},
         familyi={S\bibinitperiod},
         given={O.},
         giveni={O\bibinitperiod},
      }}%
    }
    \keyw{BNN; Neuroevolution;binary neural networks;BiSUNA;discrete
  optimization}
    \strng{namehash}{VRSCSO1}
    \strng{fullhash}{VRSCSO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    With the explosive interest in the utilization of Neural Networks, several
  approaches have taken place to make them faster, more accurate or power
  efficient; one technique used to simplify inference models is the utilization
  of binary representations for weights, activations, inputs and outputs. This
  paper presents a novel approach to train from scratch Binary Neural Networks
  using neuroevolution as its base technique (gradient descent free), to then
  apply such results to standard Reinforcement Learning environments tested in
  the OpenAI Gym. The results and code can be found in
  https://github.com/rval735/BiSUNA.%
    }
    \field{booktitle}{2019 IEEE Asia Pacific Conference on Circuits and Systems
  (APCCAS)}
    \verb{doi}
    \verb 10.1109/APCCAS47518.2019.8953134
    \endverb
    \field{issn}{null}
    \field{pages}{301\bibrangedash 304}
    \field{title}{Using Neuroevolved Binary Neural Networks to solve
  reinforcement learning environments}
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8977877}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=VR}{%
         family={{Valencia}},
         familyi={V\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SCW}{%
         family={{Sham}},
         familyi={S\bibinitperiod},
         given={C.\bibnamedelima W.},
         giveni={C\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=SO}{%
         family={{Sinnen}},
         familyi={S\bibinitperiod},
         given={O.},
         giveni={O\bibinitperiod},
      }}%
    }
    \keyw{BNN, FPGA;BiSUNA;Binary Neural Network;TWEANN;Evolutionary Algorithm}
    \strng{namehash}{VRSCWSO1}
    \strng{fullhash}{VRSCWSO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The exponential progress of semiconductor tech-nologies has enabled the
  proliferation of deep learning as a prominent area of research, where neural
  networks have demon-strated its effectiveness to solve very hard multi
  dimensional problems. This paper focuses on one in particular, Binary Neural
  Networks (BNN), which use fixed length bits in its connections and logic
  functions to perform excitation operations. Exploiting those characteristics,
  hardware accelerators that integrate field-programmable gate arrays (FPGAs)
  have been adopted to hasten inference of deep learning networks, given its
  proficiency to maximize parallelism and energy efficiency. This work will
  show how the algorithm Binary Spectrum-diverse Unified Neuroevolution
  Architecture (BiSUNA) can perform training and inference on FPGA without the
  need of gradient descent. Source code can be found in
  github.com/rval735/bisunaocl%
    }
    \field{booktitle}{2019 International Conference on Field-Programmable
  Technology (ICFPT)}
    \verb{doi}
    \verb 10.1109/ICFPT47387.2019.00076
    \endverb
    \field{issn}{null}
    \field{pages}{395\bibrangedash 398}
    \field{title}{Evolved Binary Neural Networks Through Harnessing FPGA
  Capabilities}
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{csde-FPGA}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=VR}{%
         family={{Valencia}},
         familyi={V\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SC}{%
         family={{Sham}},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
    }
    \keyw{BiSUNA, FPGA, U50, Xilinx}
    \strng{namehash}{VRSC1}
    \strng{fullhash}{VRSC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Deep Learning has reached a prominent area of research thanks to advances
  in semiconductor technologies, with Deep Neural Network (DNN) as the pivot of
  change. It is capable of solving complex multi-dimensional problems. This
  paper focuses on one particular example, the Binary Neural Network (BNN): it
  uses fixed-length bits in its connections and logic functions to perform
  excitation operations, reducing memory requirements. The conventional use of
  Field Programmable Gate Arrays (FPGAs) dictates inference only of deep
  learning networks. This publication demonstrates how the algorithm Binary
  Spectrum-diverse Unified Neuroevolution Architecture (BiSUNA) can perform
  training and inference on FPGA by dismissing gradient descent, solving
  reinforcement learning and reaching maximum parallelism and energy
  efficiency, up to 16\% faster compared to a CPU. Source code can be found in
  github.com/rval735/bisunaU50.%
    }
    \field{booktitle}{2020 IEEE Asia-Pacific Conference on Computer Science and
  Data Engineering (CSDE)}
    \field{title}{FPGA deployment of neuroevolved Binary Neural Networks}
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1145/3377929.3389933}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=TRHV}{%
         family={Tenorio},
         familyi={T\bibinitperiod},
         given={Raul Horacio\bibnamedelima Valencia},
         giveni={R\bibinitperiod\bibinitdelim H\bibinitperiod\bibinitdelim
  V\bibinitperiod},
      }}%
      {{hash=SCW}{%
         family={Sham},
         familyi={S\bibinitperiod},
         given={Chiu\bibnamedelima Wing},
         giveni={C\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
      {{hash=VDV}{%
         family={Vargas},
         familyi={V\bibinitperiod},
         given={Danilo\bibnamedelima Vasconcellos},
         giveni={D\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \keyw{Neuroevolution, Evolutionary, CPA, binary neural network, BiSUNA,
  adversarial neurocryptography, neuroevolution}
    \strng{namehash}{TRHVSCWVDV1}
    \strng{fullhash}{TRHVSCWVDV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 2020 Genetic and Evolutionary
  Computation Conference Companion}
    \verb{doi}
    \verb 10.1145/3377929.3389933
    \endverb
    \field{isbn}{9781450371278}
    \field{pages}{291\bibrangedash 292}
    \field{series}{GECCO '20}
    \field{title}{Preliminary Study of Applied Binary Neural Networks for
  Neural Cryptography}
    \verb{url}
    \verb https://doi.org/10.1145/3377929.3389933
    \endverb
    \list{location}{1}{%
      {Canc\'{u}n, Mexico}%
    }
    \field{year}{2020}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{csde-ANE}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=VR}{%
         family={{Valencia}},
         familyi={V\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SC}{%
         family={{Sham}},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
    }
    \keyw{Binary Neural Network, BiSUNA, CPA, Adversarial Neurocryptography,
  Neuroevolution}
    \strng{namehash}{VRSC1}
    \strng{fullhash}{VRSC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Adversarial neuro encoding could provide new insights for ciphering
  information with different perspectives. Nevertheless, it is still
  underexplored with a handful of publications on the subject. This work
  proposes the implementation of neuroevolved binary neural networks based on
  boolean logic functions only (BiSUNA) that apply payload ciphering between
  two agents to disperse information from an observer. The BiSUNA framework
  provides three distinctive attributions: it uses an adversarial neural
  encoding environment to improve the system data transmission; one execution
  yields a diversity of results given its population heuristics; lastly, it is
  an unconventional proposal to employ binary neural networks for the solution
  of symmetric ciphered problems.%
    }
    \field{booktitle}{2020 IEEE Asia-Pacific Conference on Computer Science and
  Data Engineering (CSDE)}
    \field{title}{Adversarial Neuro Encoding with Binary Neural Networks}
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1147/rd.33.0210}{article}{}
    \name{author}{1}{}{%
      {{hash=SAL}{%
         family={Samuel},
         familyi={S\bibinitperiod},
         given={A.\bibnamedelima L.},
         giveni={A\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IBM Corp.}%
    }
    \keyw{NN, Machine Learning}
    \strng{namehash}{SAL1}
    \strng{fullhash}{SAL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1147/rd.33.0210
    \endverb
    \field{issn}{0018-8646}
    \field{number}{3}
    \field{pages}{210\bibrangedash 229}
    \field{title}{Some Studies in Machine Learning Using the Game of Checkers}
    \verb{url}
    \verb https://doi.org/10.1147/rd.33.0210
    \endverb
    \field{volume}{3}
    \list{location}{1}{%
      {USA}%
    }
    \field{journaltitle}{IBM J. Res. Dev.}
    \field{month}{07}
    \field{year}{1959}
  \endentry

  \entry{finger1994origins}{book}{}
    \name{author}{2}{}{%
      {{hash=FS}{%
         family={Finger},
         familyi={F\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=POU}{%
         family={Press},
         familyi={P\bibinitperiod},
         given={Oxford\bibnamedelima University},
         giveni={O\bibinitperiod\bibinitdelim U\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Oxford University Press}%
    }
    \keyw{Neuroscience, Neurons, Medical services}
    \strng{namehash}{FSPOU1}
    \strng{fullhash}{FSPOU1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{isbn}{9780195065039}
    \field{series}{Oxford University Press paperback}
    \field{title}{Origins of Neuroscience: A History of Explorations Into Brain
  Function}
    \verb{url}
    \verb https://books.google.ca/books?id=BdRqAAAAMAAJ
    \endverb
    \field{year}{1994}
  \endentry

  \entry{Salvaris2018Book}{book}{}
    \name{author}{3}{}{%
      {{hash=SM}{%
         family={Salvaris},
         familyi={S\bibinitperiod},
         given={Mathew},
         giveni={M\bibinitperiod},
      }}%
      {{hash=DD}{%
         family={Dean},
         familyi={D\bibinitperiod},
         given={Danielle},
         giveni={D\bibinitperiod},
      }}%
      {{hash=TWH}{%
         family={Tok},
         familyi={T\bibinitperiod},
         given={Wee\bibnamedelima Hyong},
         giveni={W\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Apress}%
    }
    \keyw{Evolutionary, Books, Neuroevolution}
    \strng{namehash}{SMDDTWH1}
    \strng{fullhash}{SMDDTWH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This chapter discusses some of the trends in deep learning and related
  fields. We cover specifically which trends might be useful for what tasks as
  well as discuss some of the methods and ideas that could have far-reaching
  implications but have yet to be applied to many real-world problems. We
  finish by covering briefly some of the current limitations of deep learning
  as well as some other areas of AI that seem to hold promise for future AI
  applications, and discuss briefly some of the ethical and legal implications
  of deep learning applications.%
    }
    \field{booktitle}{Deep Learning with Azure: Building and Deploying
  Artificial Intelligence Solutions on the Microsoft AI Platform}
    \verb{doi}
    \verb 10.1007/978-1-4842-3679-6_3
    \endverb
    \field{isbn}{978-1-4842-3679-6}
    \field{title}{Trends in Deep Learning}
    \verb{url}
    \verb https://doi.org/10.1007/978-1-4842-3679-6_3
    \endverb
    \list{location}{1}{%
      {Berkeley, CA}%
    }
    \field{year}{2018}
  \endentry

  \entry{44873}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=HG}{%
         family={Hinton},
         familyi={H\bibinitperiod},
         given={Geoffrey},
         giveni={G\bibinitperiod},
      }}%
      {{hash=VO}{%
         family={Vinyals},
         familyi={V\bibinitperiod},
         given={Oriol},
         giveni={O\bibinitperiod},
      }}%
      {{hash=DJ}{%
         family={Dean},
         familyi={D\bibinitperiod},
         given={Jeffrey},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{NN, DNN, MNIST, Supervised learning}
    \strng{namehash}{HGVODJ1}
    \strng{fullhash}{HGVODJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{NIPS Deep Learning and Representation Learning Workshop}
    \field{title}{Distilling the Knowledge in a Neural Network}
    \verb{url}
    \verb http://arxiv.org/abs/1503.02531
    \endverb
    \field{year}{2015}
  \endentry

  \entry{li2013}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=LJB}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Jung\bibnamedelima Bin},
         giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=WCH}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Chien\bibnamedelima Ho},
         giveni={C\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Trans Tech Publications}%
    }
    \keyw{NN-Fin; Neural Network (NN), BPN, Taylor Series, Price Forecast}
    \strng{namehash}{LJBWCH1}
    \strng{fullhash}{LJBWCH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This study adopts popular back-propagation neural network to make
  one-period-ahead prediction of the stock price. A model based on Taylor
  series by using both fundamental and technical indicators EPS and MACD as
  input data is built for an empirical study. Leading Taiwanese companies in
  non-hi-tech industry such as Formosa Plastics, Yieh Phui Steel, Evergreen
  Marine, and Chang Hwa Bank are picked as targets to analyze their reasonable
  prices and moving trends. The performance of this model shows remarkable
  return and high accuracy in making long/short strategies.%
    }
    \field{booktitle}{Innovation for Applied Science and Technology}
    \verb{doi}
    \verb 10.4028/www.scientific.net/AMM.284-287.3020
    \endverb
    \field{pages}{3020\bibrangedash 3024}
    \field{series}{Applied Mechanics and Materials}
    \field{title}{An Efficient Neural Network Model with Taylor Series-Based
  Data Pre-Processing for Stock Price Forecast}
    \field{volume}{284}
    \field{month}{03}
    \field{year}{2013}
  \endentry

  \entry{7016218}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=TMLR}{%
         family={Torregoza},
         familyi={T\bibinitperiod},
         given={M.\bibnamedelima L.\bibnamedelima R.},
         giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim
  R\bibinitperiod},
      }}%
      {{hash=DEP}{%
         family={Dadios},
         familyi={D\bibinitperiod},
         given={E.\bibnamedelima P.},
         giveni={E\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary;banking;forecasting theory;genetic algorithms;neural
  nets;pricing;hybrid genetic algorithm neural network;Philippine peso US
  dollar exchange rate forecasting;Philippine dealing system;PDS;central
  bank;demand and supply analysis;artificial neural network;consumer price
  index;interest rate;inflation rate;lending interest rate;purchasing
  power;foreign exchange rates;banking;training data;Exchange rates;Genetic
  algorithms;Artificial neural networks;Forecasting;Conferences;Economic
  indicators;Artificial Neural Network;forecasting;prediction;exchange
  rate;evolutionary algorithm}
    \strng{namehash}{TMLRDEP1}
    \strng{fullhash}{TMLRDEP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper presents a new method in forecasting Philippine Peso to US
  Dollar exchange rate. Compared to the conventional way, in which the
  Philippine Dealing System (PDS), as monitored by the Central Bank, determines
  the rate by analysing demand and supply, the use of artificial neural
  network, having consumer price index, inflation rate, lending interest rate
  and purchasing power of the peso as the inputs is presented in this paper.
  Though foreign exchange rates vary on a daily basis, the output of this paper
  is prediction of the average foreign exchange rate every month. Artificial
  Neural Network serves as a powerful tool in forecasting Philippine Peso to US
  Dollar exchange rate not requiring expert knowledge in banking and finance
  thus letting the public gain access to a helpful beacon which is the foreign
  exchange rate. However, the accuracy of the forecast using artificial neural
  network is highly dependent on the volume of the training data, in this
  paper, an alternative algorithm that will increase the accuracy of the
  conventional artificial neural network with limited volume of training data
  is presented and analyze.%
    }
    \field{booktitle}{2014 International Conference on Humanoid,
  Nanotechnology, Information Technology, Communication and Control,
  Environment and Management (HNICEM)}
    \verb{doi}
    \verb 10.1109/HNICEM.2014.7016218
    \endverb
    \field{pages}{1\bibrangedash 5}
    \field{title}{Comparison of neural network and hybrid genetic
  algorithm-neural network in forecasting of Philippine Peso-US Dollar exchange
  rate}
    \field{year}{2014}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7033335}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=POP}{%
         family={Patel},
         familyi={P\bibinitperiod},
         given={O.\bibnamedelima P.},
         giveni={O\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=TA}{%
         family={Tiwari},
         familyi={T\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; generalisation (artificial intelligence);learning (artificial
  intelligence);neural nets;optimisation;pattern classification;quantum
  computing;quantum based binary neural network learning algorithm;network
  structure optimisation;neurons;classification accuracy;hidden layer;training
  accuracy;generalization accuracy;QANN;Neurons;Accuracy;Training;Biological
  neural networks;Testing;Diabetes;Binary neural network;Quantum
  processing;Qubits;Back propagation learning}
    \strng{namehash}{POPTA1}
    \strng{fullhash}{POPTA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{2014 International Conference on Information Technology}
    \verb{doi}
    \verb 10.1109/ICIT.2014.29
    \endverb
    \field{pages}{270\bibrangedash 274}
    \field{title}{Quantum Inspired Binary Neural Network Algorithm}
    \field{year}{2014}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{DBLP:journals/corr/KulkarniWKT15}{article}{}
    \name{author}{4}{}{%
      {{hash=KTD}{%
         family={Kulkarni},
         familyi={K\bibinitperiod},
         given={Tejas\bibnamedelima D.},
         giveni={T\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=WW}{%
         family={Whitney},
         familyi={W\bibinitperiod},
         given={Will},
         giveni={W\bibinitperiod},
      }}%
      {{hash=KP}{%
         family={Kohli},
         familyi={K\bibinitperiod},
         given={Pushmeet},
         giveni={P\bibinitperiod},
      }}%
      {{hash=TJB}{%
         family={Tenenbaum},
         familyi={T\bibinitperiod},
         given={Joshua\bibnamedelima B.},
         giveni={J\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
    }
    \keyw{CNN, IGN, SGVB, DC-IGN}
    \strng{namehash}{KTD+1}
    \strng{fullhash}{KTDWWKPTJB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1503.03167
    \endverb
    \field{title}{Deep Convolutional Inverse Graphics Network}
    \verb{url}
    \verb http://arxiv.org/abs/1503.03167
    \endverb
    \field{volume}{abs/1503.03167}
    \field{journaltitle}{CoRR}
    \field{eprinttype}{arXiv}
    \field{year}{2015}
  \endentry

  \entry{sra2012optimization}{book}{}
    \name{author}{3}{}{%
      {{hash=SS}{%
         family={Sra},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NS}{%
         family={Nowozin},
         familyi={N\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=WS}{%
         family={Wright},
         familyi={W\bibinitperiod},
         given={S.J.},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{Books, NN, SGD, Gradient Descent, ML, DNN}
    \strng{namehash}{SSNSWS1}
    \strng{fullhash}{SSNSWS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{isbn}{9780262016469}
    \field{series}{Neural information processing series}
    \field{title}{Optimization for Machine Learning}
    \verb{url}
    \verb https://books.google.ca/books?id=JPQx7s2L1A8C
    \endverb
    \field{year}{2012}
  \endentry

  \entry{5380141}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=RR}{%
         family={Rafeh},
         familyi={R\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Mesgar},
         familyi={M\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{NN, biology computing;DNA;eigenvalues and eigenfunctions;forensic
  science;matrix algebra;medical computing;neural nets;neural network;human
  identification;DNA sequences;graphical representation;criminal
  investigations;genetic disease;matrix eigenvalues;Neural
  networks;Humans;Sequences;Computer networks;DNA computing;Eigenvalues and
  eigenfunctions;Artificial neural
  networks;Genetics;Diseases;Forensics;University Timetabling;Optimization
  Problems;Constraint Programming;Linear Programming}
    \strng{namehash}{RRMM1}
    \strng{fullhash}{RRMM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this paper we propose a new method to analyze the
  similarity/dissimilarity of DNA sequences which can be used in human
  identification field. This method is based on the graphical representation
  proposed by Randic et al [M. Randic, M. Vracko, L. Nella, P. Dejan, Chem.
  (2003)]. Instead of calculating the leading eigenvalues of the matrix for
  graphical representation we smooth the zigzag curve and calculate its
  curvature. Similarity between DNA sequences are decided by neural network.
  Our method is useful for human identification in criminal investigations and
  in genetic disease. Our results verify the validity of our method.%
    }
    \field{booktitle}{2009 Second International Conference on Computer and
  Electrical Engineering}
    \verb{doi}
    \verb 10.1109/ICCEE.2009.132
    \endverb
    \field{pages}{64\bibrangedash 67}
    \field{title}{Neural Network in Human Identification by DNA Sequences}
    \field{volume}{2}
    \field{year}{2009}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8296359}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=ZC}{%
         family={{Zhang}},
         familyi={Z\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=NT}{%
         family={{Nguyen}},
         familyi={N\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={{Sah}},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=PR}{%
         family={{Ptucha}},
         familyi={P\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=LA}{%
         family={{Loui}},
         familyi={L\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SC}{%
         family={{Salvaggio}},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
    }
    \keyw{NN, batch normalization, computer vision;eigenvalues and
  eigenfunctions;feedforward neural nets;batch-normalized recurrent highway
  networks;gradient control;feed-forward networks;computer vision
  tasks;recurrence loop;eigenvalues;temporal Jacobian;gradient flow;image
  captioning task;Training;Logic gates;Road transportation;Task
  analysis;Transforms;Measurement;Computer architecture;Gradient
  control;recurrent highway network;batch normalization;vanishing
  gradient;exploding gradient}
    \strng{namehash}{ZC+1}
    \strng{fullhash}{ZCNTSSPRLASC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Gradient control plays an important role in feed-forward networks applied
  to various computer vision tasks. Previous work has shown that Recurrent
  Highway Networks minimize the problem of vanishing or exploding gradients.
  They achieve this by setting the eigenvalues of the temporal Jacobian to 1
  across the time steps. In this work, batch normalized recurrent highway
  networks are proposed to control the gradient flow in an improved way for
  network convergence. Specifically, the introduced model can be formed by
  batch normalizing the inputs at each recurrence loop. The proposed model is
  tested on an image captioning task using MSCOCO dataset. Experimental results
  indicate that the batch normalized recurrent highway networks converge faster
  and performs better compared with the traditional LSTM and RHN based models.%
    }
    \field{booktitle}{2017 IEEE International Conference on Image Processing
  (ICIP)}
    \verb{doi}
    \verb 10.1109/ICIP.2017.8296359
    \endverb
    \field{issn}{2381-8549}
    \field{pages}{640\bibrangedash 644}
    \field{title}{Batch-normalized recurrent highway networks}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8114708}{article}{}
    \name{author}{4}{}{%
      {{hash=SV}{%
         family={{Sze}},
         familyi={S\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=CY}{%
         family={{Chen}},
         familyi={C\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YT}{%
         family={{Yang}},
         familyi={Y\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=EJS}{%
         family={{Emer}},
         familyi={E\bibinitperiod},
         given={J.\bibnamedelima S.},
         giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \keyw{NN, artificial intelligence;computational complexity;neural
  nets;energy efficiency;hardware design changes;DNN hardware designs;deep
  neural networks;hardware cost;computation cost reduction;artificial
  intelligence;computational complexity;hardware platforms;hardware
  architecture;DNN hardware implementations;Neurons;Biological neural
  networks;Artificial intelligence;Machine learning;Neural
  networks;Tutorials;Convolutional neural networks;Artificial
  intelligence;Benchmark testing;Computer architecture;ASIC;computer
  architecture;convolutional neural networks;dataflow processing;deep
  learning;deep neural networks;energy-efficient accelerators;low power;machine
  learning;spatial architectures;VLSI}
    \strng{namehash}{SV+1}
    \strng{fullhash}{SVCYYTEJS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Deep neural networks (DNNs) are currently widely used for many artificial
  intelligence (AI) applications including computer vision, speech recognition,
  and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks,
  it comes at the cost of high computational complexity. Accordingly,
  techniques that enable efficient processing of DNNs to improve energy
  efficiency and throughput without sacrificing application accuracy or
  increasing hardware cost are critical to the wide deployment of DNNs in AI
  systems. This article aims to provide a comprehensive tutorial and survey
  about the recent advances toward the goal of enabling efficient processing of
  DNNs. Specifically, it will provide an overview of DNNs, discuss various
  hardware platforms and architectures that support DNNs, and highlight key
  trends in reducing the computation cost of DNNs either solely via hardware
  design changes or via joint hardware design and DNN algorithm changes. It
  will also summarize various development resources that enable researchers and
  practitioners to quickly get started in this field, and highlight important
  benchmarking metrics and design considerations that should be used for
  evaluating the rapidly growing number of DNN hardware designs, optionally
  including algorithmic codesigns, being proposed in academia and industry. The
  reader will take away the following concepts from this article: understand
  the key design considerations for DNNs; be able to evaluate different DNN
  hardware implementations with benchmarks and comparison metrics; understand
  the tradeoffs between various hardware architectures and platforms; be able
  to evaluate the utility of various DNN design techniques for efficient
  processing; and understand recent implementation trends and opportunities.%
    }
    \verb{doi}
    \verb 10.1109/JPROC.2017.2761740
    \endverb
    \field{issn}{1558-2256}
    \field{number}{12}
    \field{pages}{2295\bibrangedash 2329}
    \field{title}{Efficient Processing of Deep Neural Networks: A Tutorial and
  Survey}
    \field{volume}{105}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8832578}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LX}{%
         family={{Liu}},
         familyi={L\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=ZJ}{%
         family={{Zhou}},
         familyi={Z\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=QH}{%
         family={{Qian}},
         familyi={Q\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{NN; exploding gradient, feedforward neural nets;gradient
  methods;gradient instability;deep neural network models;deep feed-forward
  neural networks;activation functions;Training;Neurons;Biological neural
  networks;Backpropagation;Convergence;Task analysis;Activation
  Functions;Gradient Instability;Deep Neural Network}
    \strng{namehash}{LXZJQH1}
    \strng{fullhash}{LXZJQH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The most widely used activation functions in current deep feed-forward
  neural networks are rectified linear units (ReLU), besides many alternatives.
  However, none of these alternatives have managed to consistently outperform
  the rest and there is no unified theory connecting properties of the task and
  networks with properties of activation functions in the sense of efficient
  training. In order to understand the related problems fundamentally, it is
  necessary to figure out possible causes of gradient instability
  mathematically, and how different activation functions can be adopted to
  improve system performances. Theoretical analysis about gradient instability
  is given in the paper, as well as the fundamental explanation for the
  exploding/vanishing gradient and the possible solutions. The performances of
  different activation functions in a given example network are investigated.
  Numerical simulations suggest that the convergence rate of gradient varies
  with the activation function. There is no activation function that performs
  well to all structures. The findings in the paper provide a reference for the
  selection of activation function in the design of deep neural network
  models.%
    }
    \field{booktitle}{2019 Chinese Control And Decision Conference (CCDC)}
    \verb{doi}
    \verb 10.1109/CCDC.2019.8832578
    \endverb
    \field{issn}{1948-9447}
    \field{pages}{3966\bibrangedash 3971}
    \field{title}{Comparison and Evaluation of Activation Functions in Term of
  Gradient Instability in Deep Neural Networks}
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{6847217}{article}{}
    \name{author}{3}{}{%
      {{hash=SL}{%
         family={Shao},
         familyi={S\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=ZF}{%
         family={Zhu},
         familyi={Z\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
      {{hash=LX}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
    }
    \keyw{NN; image classification;learning (artificial intelligence);object
  recognition;visual categorization;transfer learning algorithms;object
  recognition;image classification;human action recognition;Knowledge
  transfer;Visualization;Training;Training data;Adaptation models;Learning
  systems;Testing;Action recognition;image classification;machine
  learning;object recognition;survey;transfer learning;visual
  categorization.;Action recognition;image classification;machine
  learning;object recognition;survey;transfer learning;visual
  categorization;Algorithms;Humans;Knowledge;Machine Learning;Models,
  Theoretical;Neural Networks (Computer);Surveys and Questionnaires;Transfer
  (Psychology);Visual Perception}
    \strng{namehash}{SLZFLX1}
    \strng{fullhash}{SLZFLX1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Regular machine learning and data mining techniques study the training data
  for future inferences under a major assumption that the future data are
  within the same feature space or have the same distribution as the training
  data. However, due to the limited availability of human labeled training
  data, training data that stay in the same feature space or have the same
  distribution as the future data cannot be guaranteed to be sufficient enough
  to avoid the over-fitting problem. In real-world applications, apart from
  data in the target domain, related data in a different domain can also be
  included to expand the availability of our prior knowledge about the target
  future data. Transfer learning addresses such cross-domain learning problems
  by extracting useful information from data in a related domain and
  transferring them for being used in target tasks. In recent years, with
  transfer learning being applied to visual categorization, some typical
  problems, e.g., view divergence in action recognition tasks and concept
  drifting in image classification tasks, can be efficiently solved. In this
  paper, we survey state-of-the-art transfer learning algorithms in visual
  categorization applications, such as object recognition, image
  classification, and human action recognition.%
    }
    \verb{doi}
    \verb 10.1109/TNNLS.2014.2330900
    \endverb
    \field{issn}{2162-237X}
    \field{number}{5}
    \field{pages}{1019\bibrangedash 1034}
    \field{title}{Transfer Learning for Visual Categorization: A Survey}
    \field{volume}{26}
    \field{journaltitle}{IEEE Transactions on Neural Networks and Learning
  Systems}
    \field{year}{2015}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{aiNorvig}{book}{}
    \name{author}{2}{}{%
      {{hash=RS}{%
         family={Russell},
         familyi={R\bibinitperiod},
         given={Stuart},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NP}{%
         family={Norvig},
         familyi={N\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Pearson}%
    }
    \keyw{Book, AI, ML, Unsupervised learning}
    \strng{namehash}{RSNP1}
    \strng{fullhash}{RSNP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{edition}{4}
    \field{title}{Artificial Intelligence: A Modern Approach}
    \field{volume}{1}
    \field{year}{2020}
  \endentry

  \entry{AIFoundations}{book}{}
    \name{author}{1}{}{%
      {{hash=MDLPAK}{%
         family={Mackworth},
         familyi={M\bibinitperiod},
         given={David L Poole; Alan\bibnamedelima K},
         giveni={D\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim
  P\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Cambridge University Press}%
    }
    \keyw{AI,}
    \strng{namehash}{MDLPAK1}
    \strng{fullhash}{MDLPAK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{edition}{2}
    \field{number}{9781107195394}
    \field{title}{Artificial Intelligence, Foundations of Computational Agents}
    \field{volume}{1}
    \field{annotation}{%
  https://www.cambridge.org/ca/academic/subjects/computer-science/artificial-intelligence-and-natural-language-processing/artificial-intelligence-foundations-computational-agents-2nd-edition?format=HB&isbn=9781107195394%
    }
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Zhang2020}{inbook}{}
    \name{author}{3}{}{%
      {{hash=ZJ}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Jingqing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={Yuan},
         familyi={Y\bibinitperiod},
         given={Hang},
         giveni={H\bibinitperiod},
      }}%
      {{hash=DH}{%
         family={Dong},
         familyi={D\bibinitperiod},
         given={Hao},
         giveni={H\bibinitperiod},
      }}%
    }
    \name{editor}{3}{}{%
      {{hash=DH}{%
         family={Dong},
         familyi={D\bibinitperiod},
         given={Hao},
         giveni={H\bibinitperiod},
      }}%
      {{hash=DZ}{%
         family={Ding},
         familyi={D\bibinitperiod},
         given={Zihan},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=ZS}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Shanghang},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Singapore}%
    }
    \keyw{Books, DQN, Deep RL, AlphaZero}
    \strng{namehash}{ZJYHDH1}
    \strng{fullhash}{ZJYHDH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This chapter aims to briefly introduce the fundamentals for deep learning,
  which is the key component of deep reinforcement learning. We will start with
  a naive single-layer network and gradually progress to much more complex but
  powerful architectures such as convolutional neural networks (CNNs) and
  recurrent neural networks (RNNs). We will end this chapter with a couple of
  examples that demonstrate how to implement deep learning models in practice.%
    }
    \verb{doi}
    \verb 10.1007/978-981-15-4095-0_1
    \endverb
    \field{isbn}{978-981-15-4095-0}
    \field{pages}{3\bibrangedash 46}
    \field{title}{Deep Reinforcement Learning: Fundamentals, Research and
  Applications}
    \verb{url}
    \verb https://doi.org/10.1007/978-981-15-4095-0_1
    \endverb
    \list{location}{1}{%
      {Singapore}%
    }
    \field{year}{2020}
  \endentry

  \entry{pmlr-v37-schulman15}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=SJ}{%
         family={Schulman},
         familyi={S\bibinitperiod},
         given={John},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Levine},
         familyi={L\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
      {{hash=AP}{%
         family={Abbeel},
         familyi={A\bibinitperiod},
         given={Pieter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=JM}{%
         family={Jordan},
         familyi={J\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MP}{%
         family={Moritz},
         familyi={M\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=BF}{%
         family={Bach},
         familyi={B\bibinitperiod},
         given={Francis},
         giveni={F\bibinitperiod},
      }}%
      {{hash=BD}{%
         family={Blei},
         familyi={B\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \keyw{DNN, Reinforcement Learning, TRPO, Reinforcement Learning, Atari}
    \strng{namehash}{SJ+1}
    \strng{fullhash}{SJLSAPJMMP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this article, we describe a method for optimizing control policies, with
  guaranteed monotonic improvement. By making several approximations to the
  theoretically-justified scheme, we develop a practical algorithm, called
  Trust Region Policy Optimization (TRPO). This algorithm is effective for
  optimizing large nonlinear policies such as neural networks. Our experiments
  demonstrate its robust performance on a wide variety of tasks: learning
  simulated robotic swimming, hopping, and walking gaits; and playing Atari
  games using images of the screen as input. Despite its approximations that
  deviate from the theory, TRPO tends to give monotonic improvement, with
  little tuning of hyperparameters.%
    }
    \field{pages}{1889\bibrangedash 1897}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{Trust Region Policy Optimization}
    \verb{url}
    \verb http://proceedings.mlr.press/v37/schulman15.html
    \endverb
    \field{volume}{37}
    \list{location}{1}{%
      {Lille, France}%
    }
    \verb{file}
    \verb http://proceedings.mlr.press/v37/schulman15.pdf
    \endverb
    \field{year}{2015}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{mnih-atari-2013}{incollection}{}
    \name{author}{7}{}{%
      {{hash=MV}{%
         family={Mnih},
         familyi={M\bibinitperiod},
         given={Volodymyr},
         giveni={V\bibinitperiod},
      }}%
      {{hash=KK}{%
         family={Kavukcuoglu},
         familyi={K\bibinitperiod},
         given={Koray},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Silver},
         familyi={S\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Graves},
         familyi={G\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=AI}{%
         family={Antonoglou},
         familyi={A\bibinitperiod},
         given={Ioannis},
         giveni={I\bibinitperiod},
      }}%
      {{hash=WD}{%
         family={Wierstra},
         familyi={W\bibinitperiod},
         given={Daan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=RM}{%
         family={Riedmiller},
         familyi={R\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {NIPS}%
    }
    \keyw{DNN, DeepQNetwork, Atari, Reinforcement Learning, Reinforcement
  Learning}
    \strng{namehash}{MV+1}
    \strng{fullhash}{MVKKSDGAAIWDRM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{NIPS Deep Learning Workshop}
    \field{title}{Playing Atari With Deep Reinforcement Learning}
    \field{year}{2013}
  \endentry

  \entry{9063667}{article}{}
    \name{author}{3}{}{%
      {{hash=LAB}{%
         family={Labao},
         familyi={L\bibinitperiod},
         given={Alfonso\bibnamedelima B.},
         giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod},
      }}%
      {{hash=MMAM}{%
         family={Martija},
         familyi={M\bibinitperiod},
         given={Mygel Andrei\bibnamedelima M.},
         giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim
  M\bibinitperiod},
      }}%
      {{hash=NPC}{%
         family={Naval},
         familyi={N\bibinitperiod},
         given={Prospero\bibnamedelima C.},
         giveni={P\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \keyw{DNN, Atari, Reinforcement Learning, A3C,}
    \strng{namehash}{LABMMAMNPC1}
    \strng{fullhash}{LABMMAMNPC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1109/TNNLS.2020.2980743
    \endverb
    \field{number}{3}
    \field{pages}{1162\bibrangedash 1176}
    \field{title}{A3C-GS: Adaptive Moment Gradient Sharing With Locks for
  Asynchronous Actor--Critic Agents}
    \field{volume}{32}
    \field{journaltitle}{IEEE Transactions on Neural Networks and Learning
  Systems}
    \field{year}{2021}
  \endentry

  \entry{8490422}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=TRR}{%
         family={{Torrado}},
         familyi={T\bibinitperiod},
         given={R.\bibnamedelima R.},
         giveni={R\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=BP}{%
         family={{Bontrager}},
         familyi={B\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
      {{hash=TJ}{%
         family={{Togelius}},
         familyi={T\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={{Liu}},
         familyi={L\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=PD}{%
         family={{Perez-Liebana}},
         familyi={P\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{DNN, Reinforcement Learning, DQN, A2C, computer games;learning
  (artificial intelligence);Monte Carlo methods;planning (artificial
  intelligence);tree searching;AI algorithms;domain-specific description
  language;Monte Carlo Tree Search;Reinforcement Learning;deep reinforcement
  learning algorithms;GVGAI games;Arcade Learning Environment;General Video
  Game AI;associated software framework;online planning;GVGAI interface;OpenAI
  Gym;Games;Benchmark testing;Machine learning;Planning;Learning (artificial
  intelligence);deep reinforcement learning;general video game AI;video game
  description language;OpenAI Gym;advantage actor critic;deep Q-learning}
    \strng{namehash}{TRR+1}
    \strng{fullhash}{TRRBPTJLJPD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The General Video Game AI (GVGAI) competition and its associated software
  framework provides a way of benchmarking AI algorithms on a large number of
  games written in a domain-specific description language. While the
  competition has seen plenty of interest, it has so far focused on online
  planning, providing a forward model that allows the use of algorithms such as
  Monte Carlo Tree Search. In this paper, we describe how we interface GVGAI to
  the OpenAI Gym environment, a widely used way of connecting agents to
  reinforcement learning problems. Using this interface, we characterize how
  widely used implementations of several deep reinforcement learning algorithms
  fare on a number of GVGAI games. We further analyze the results to provide a
  first indication of the relative difficulty of these games relative to each
  other, and relative to those in the Arcade Learning Environment under similar
  conditions.%
    }
    \field{booktitle}{2018 IEEE Conference on Computational Intelligence and
  Games (CIG)}
    \verb{doi}
    \verb 10.1109/CIG.2018.8490422
    \endverb
    \field{issn}{2325-4289}
    \field{pages}{1\bibrangedash 8}
    \field{title}{Deep Reinforcement Learning for General Video Game AI}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2017arXiv170302660R}{article}{}
    \name{author}{4}{}{%
      {{hash=RA}{%
         family={{Rajeswaran}},
         familyi={R\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={{Lowrey}},
         familyi={L\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=TE}{%
         family={{Todorov}},
         familyi={T\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={{Kakade}},
         familyi={K\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, Machine Learning, Artificial Intelligence, Robotics,
  Systems and Control}
    \strng{namehash}{RA+1}
    \strng{fullhash}{RALKTEKS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1703.02660
    \endverb
    \field{title}{{Towards Generalization and Simplicity in Continuous
  Control}}
    \field{journaltitle}{ArXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{month}{03}
    \field{year}{2017}
  \endentry

  \entry{anchieh2018instanas}{article}{}
    \name{author}{5}{}{%
      {{hash=CAC}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={An-Chieh},
         giveni={A\bibinithyphendelim C\bibinitperiod},
      }}%
      {{hash=LCH}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Chieh\bibnamedelima Hubert},
         giveni={C\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=JDC}{%
         family={Juan},
         familyi={J\bibinitperiod},
         given={Da-Cheng},
         giveni={D\bibinithyphendelim C\bibinitperiod},
      }}%
      {{hash=WW}{%
         family={Wei},
         familyi={W\bibinitperiod},
         given={Wei},
         giveni={W\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Min},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{DNN, NAS, Neural Architecture Search, ImageNet}
    \strng{namehash}{CAC+1}
    \strng{fullhash}{CACLCHJDCWWSM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Thirty-Fourth AAAI Conference on Artificial Intelligence}
    \field{title}{InstaNAS: Instance-aware Neural Architecture Search}
    \field{year}{2020}
  \endentry

  \entry{935097}{article}{}
    \name{author}{2}{}{%
      {{hash=MJ}{%
         family={{Moody}},
         familyi={M\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={{Saffell}},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{NN-Fin, Reinforcement Learning, DNN, stock
  markets;investment;decision theory;learning (artificial
  intelligence);stochastic systems;optimisation;direct reinforcement
  learning;portfolio optimization;asset allocations;trading
  systems;DR;investment decision-making;stochastic control problem;recurrent
  reinforcement learning;RRL;investment policies;forecasting
  models;risk-adjusted investment return optimization;differential Sharpe
  ratio;transaction costs;financial data;intra-daily currency trader;monthly
  asset allocation system;S&P 500 Stock Index;T-Bills;Investments;Asset
  management;Optimization methods;Portfolios;Decision making;Stochastic
  processes;Adaptive algorithm;Learning;Predictive models;Dynamic programming}
    \strng{namehash}{MJSM1}
    \strng{fullhash}{MJSM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We present methods for optimizing portfolios, asset allocations, and
  trading systems based on direct reinforcement (DR). In this approach,
  investment decision-making is viewed as a stochastic control problem, and
  strategies are discovered directly. We present an adaptive algorithm called
  recurrent reinforcement learning (RRL) for discovering investment policies.
  The need to build forecasting models is eliminated, and better trading
  performance is obtained. The direct reinforcement approach differs from
  dynamic programming and reinforcement algorithms such as TD-learning and
  Q-learning, which attempt to estimate a value function for the control
  problem. We find that the RRL direct reinforcement framework enables a
  simpler problem representation, avoids Bellman's curse of dimensionality and
  offers compelling advantages in efficiency. We demonstrate how direct
  reinforcement can be used to optimize risk-adjusted investment returns
  (including the differential Sharpe ratio), while accounting for the effects
  of transaction costs. In extensive simulation work using real financial data,
  we find that our approach based on RRL produces better trading strategies
  than systems utilizing Q-learning (a value function method). Real-world
  applications include an intra-daily currency trader and a monthly asset
  allocation system for the S&P 500 Stock Index and T-Bills.%
    }
    \verb{doi}
    \verb 10.1109/72.935097
    \endverb
    \field{issn}{1941-0093}
    \field{number}{4}
    \field{pages}{875\bibrangedash 889}
    \field{title}{Learning to trade via direct reinforcement}
    \field{volume}{12}
    \field{journaltitle}{IEEE Transactions on Neural Networks}
    \field{year}{2001}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1145/2949662}{article}{}
    \name{author}{1}{}{%
      {{hash=KM}{%
         family={Krakovsky},
         familyi={K\bibinitperiod},
         given={Marina},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Association for Computing Machinery}%
    }
    \keyw{DNN, Reinforcement Learning, Robotics, Article}
    \strng{namehash}{KM1}
    \strng{fullhash}{KM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The power of deep neural networks has sparked renewed interest in
  reinforcement learning, with applications to games, robotics, and beyond.%
    }
    \verb{doi}
    \verb 10.1145/2949662
    \endverb
    \field{issn}{0001-0782}
    \field{number}{8}
    \field{pages}{12\bibrangedash 14}
    \field{title}{Reinforcement Renaissance}
    \verb{url}
    \verb https://doi-org.ezproxy.auckland.ac.nz/10.1145/2949662
    \endverb
    \field{volume}{59}
    \list{location}{1}{%
      {New York, NY, USA}%
    }
    \field{journaltitle}{Commun. ACM}
    \field{month}{07}
    \field{year}{2016}
  \endentry

  \entry{pmlr-v48-zaremba16}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=ZW}{%
         family={Zaremba},
         familyi={Z\bibinitperiod},
         given={Wojciech},
         giveni={W\bibinitperiod},
      }}%
      {{hash=MT}{%
         family={Mikolov},
         familyi={M\bibinitperiod},
         given={Tomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=JA}{%
         family={Joulin},
         familyi={J\bibinitperiod},
         given={Armand},
         giveni={A\bibinitperiod},
      }}%
      {{hash=FR}{%
         family={Fergus},
         familyi={F\bibinitperiod},
         given={Rob},
         giveni={R\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=BMF}{%
         family={Balcan},
         familyi={B\bibinitperiod},
         given={Maria\bibnamedelima Florina},
         giveni={M\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
      {{hash=WKQ}{%
         family={Weinberger},
         familyi={W\bibinitperiod},
         given={Kilian\bibnamedelima Q.},
         giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \keyw{NN, Reinforcement Learning, OpenAI Gym, Q-Learning}
    \strng{namehash}{ZW+1}
    \strng{fullhash}{ZWMTJAFR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We present an approach for learning simple algorithms such as copying,
  multi-digit addition and single digit multiplication directly from examples.
  Our framework consists of a set of interfaces, accessed by a controller.
  Typical interfaces are 1-D tapes or 2-D grids that hold the input and output
  data. For the controller, we explore a range of neural network-based models
  which vary in their ability to abstract the underlying algorithm from
  training instances and generalize to test examples with many thousands of
  digits. The controller is trained using Q-learning with several enhancements
  and we show that the bottleneck is in the capabilities of the controller
  rather than in the search incurred by Q-learning.%
    }
    \field{booktitle}{Proceedings of The 33rd International Conference on
  Machine Learning}
    \field{pages}{421\bibrangedash 429}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{Learning Simple Algorithms from Examples}
    \verb{url}
    \verb http://proceedings.mlr.press/v48/zaremba16.html
    \endverb
    \field{volume}{48}
    \list{location}{1}{%
      {New York, New York, USA}%
    }
    \verb{file}
    \verb http://proceedings.mlr.press/v48/zaremba16.pdf
    \endverb
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{9102924}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=ZJ}{%
         family={{Zhao}},
         familyi={Z\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=ZW}{%
         family={{Zhou}},
         familyi={Z\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=ZT}{%
         family={{Zhao}},
         familyi={Z\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={{Zhou}},
         familyi={Z\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LH}{%
         family={{Li}},
         familyi={L\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{DNN, Baselines, OpenAI Gym, computer games;computer vision;
  artificial intelligence;neural nets;OpenAI Baselines;MuJoCo continuous
  control tasks;Atari games;adjacent state consistency loss;optimization
  solvers;DNN;deep reinforcement learning;state representation learning;vision
  games;RL loss;ASC loss;DRL algorithms;state feature learning;Representation
  learning; Reinforcement Learning}
    \strng{namehash}{ZJ+1}
    \strng{fullhash}{ZJZWZTZYLH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Recent years have witnessed the great success of deep reinforcement
  learning (DRL) on a variety of vision games. Although DNN has demonstrated
  strong power in representation learning, such capacity is under-explored in
  most DRL works whose focus is usually on optimization solvers. In fact, we
  discover that the state feature learning is the main obstacle for further
  improvement of DRL algorithms. To address this issue, we propose a new state
  representation learning scheme with our Adjacent State Consistency Loss (ASC
  Loss). The loss is defined based on the hypothesis that there are fewer
  changes between adjacent states than that of far apart ones, since scenes in
  videos generally evolve smoothly. In this paper, we exploit ASC loss as an
  assistant of RL loss in the training phase to boost the state feature
  learning. We conduct evaluation on Atari games and MuJoCo continuous control
  tasks, which demonstrates that our method is superior to OpenAI baselines.%
    }
    \field{booktitle}{2020 IEEE International Conference on Multimedia and Expo
  (ICME)}
    \verb{doi}
    \verb 10.1109/ICME46284.2020.9102924
    \endverb
    \field{issn}{1945-788X}
    \field{pages}{1\bibrangedash 6}
    \field{title}{State Representation Learning For Effective Deep
  Reinforcement Learning}
    \field{year}{2020}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{ijcai2019-0452}{inproceedings}{}
    \name{author}{11}{}{%
      {{hash=PSF}{%
         family={Petroski\bibnamedelima Such},
         familyi={P\bibinitperiod\bibinitdelim S\bibinitperiod},
         given={Felipe},
         giveni={F\bibinitperiod},
      }}%
      {{hash=MV}{%
         family={Madhavan},
         familyi={M\bibinitperiod},
         given={Vashisht},
         giveni={V\bibinitperiod},
      }}%
      {{hash=LR}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Rosanne},
         giveni={R\bibinitperiod},
      }}%
      {{hash=WR}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Rui},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CPS}{%
         family={Castro},
         familyi={C\bibinitperiod},
         given={Pablo\bibnamedelima Samuel},
         giveni={P\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Yulun},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZJ}{%
         family={Zhi},
         familyi={Z\bibinitperiod},
         given={Jiale},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Schubert},
         familyi={S\bibinitperiod},
         given={Ludwig},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BMG}{%
         family={Bellemare},
         familyi={B\bibinitperiod},
         given={Marc\bibnamedelima G.},
         giveni={M\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Clune},
         familyi={C\bibinitperiod},
         given={Jeff},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lehman},
         familyi={L\bibinitperiod},
         given={Joel},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {International Joint Conferences on Artificial Intelligence
  Organization}%
    }
    \keyw{DNN, Reinforcement Learning, Atari, OpenAI Gym, Baselines}
    \strng{namehash}{PSF+1}
    \strng{fullhash}{PSFMVLRWRCPSLYZJSLBMGCJLJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the Twenty-Eighth International Joint
  Conference on Artificial Intelligence, {IJCAI-19}}
    \verb{doi}
    \verb 10.24963/ijcai.2019/452
    \endverb
    \field{pages}{3260\bibrangedash 3267}
    \field{title}{An Atari Model Zoo for Analyzing, Visualizing, and Comparing
  Deep Reinforcement Learning Agents}
    \verb{url}
    \verb https://doi.org/10.24963/ijcai.2019/452
    \endverb
    \field{month}{07}
    \field{year}{2019}
  \endentry

  \entry{8461456}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=SC}{%
         family={Sakr},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Choi},
         familyi={C\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Z.},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=GK}{%
         family={Gopalakrishnan},
         familyi={G\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Shanbhag},
         familyi={S\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
    }
    \keyw{BNN; gradient methods;learning (artificial intelligence);neural
  nets;gradient-based training;deep binary activated neural networks;continuous
  binarization;deep learning;tremendous complexity;resource constrained
  platforms;training procedure;binary activation functions;minimal accuracy
  degradation;gradient-based learning;back-propagation algorithm;straight
  through estimator;floating-point baseline;STE;Training;Neural
  networks;Complexity theory;Machine learning;Stochastic processes;Perturbation
  methods;Approximation algorithms;deep learning;binary neural
  networks;activation functions}
    \strng{namehash}{SC+1}
    \strng{fullhash}{SCCJWZGKSN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    With the ever growing popularity of deep learning, the tremendous
  complexity of deep neural networks is becoming problematic when one considers
  inference on resource constrained platforms. Binary networks have emerged as
  a potential solution, however, they exhibit a fundamentallimi-tation in
  realizing gradient-based learning as their activations are
  non-differentiable. Current work has so far relied on approximating gradients
  in order to use the back-propagation algorithm via the straight through
  estimator (STE). Such approximations harm the quality of the training
  procedure causing a noticeable gap in accuracy between binary neural networks
  and their full precision baselines. We present a novel method to train binary
  activated neural networks using true gradient-based learning. Our idea is
  motivated by the similarities between clipping and binary activation
  functions. We show that our method has minimal accuracy degradation with
  respect to the full precision baseline. Finally, we test our method on three
  benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show
  that continuous binarization using true gradient-based learning achieves an
  accuracy within 1.5% of the floating-point baseline, as compared to accuracy
  drops as high as 6% when training the same binary activated network using the
  STE.%
    }
    \field{booktitle}{2018 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}
    \verb{doi}
    \verb 10.1109/ICASSP.2018.8461456
    \endverb
    \field{issn}{2379-190X}
    \field{pages}{2346\bibrangedash 2350}
    \field{title}{True Gradient-Based Training of Deep Binary Activated Neural
  Networks Via Continuous Binarization}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{978-1-4842-5126-3}{book}{}
    \name{author}{1}{}{%
      {{hash=BT}{%
         family={Beysolow},
         familyi={B\bibinitperiod},
         given={Taweh},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Apress, Berkeley, CA}%
    }
    \keyw{OpenAI Gym, Artificial Intelligence, Python, open source}
    \strng{namehash}{BT2}
    \strng{fullhash}{BT2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{XV, 168}
    \field{title}{Applied Reinforcement Learning with Python}
    \field{year}{2019}
  \endentry

  \entry{Palanisamy:2018:HIA:3285236}{book}{}
    \name{author}{1}{}{%
      {{hash=PP}{%
         family={Palanisamy},
         familyi={P\bibinitperiod},
         given={Praveen},
         giveni={P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Packt Publishing}%
    }
    \keyw{OpenAI Gym, Deep Learning, DeepQNetwork, Atari}
    \strng{namehash}{PP1}
    \strng{fullhash}{PP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Hands-On Intelligent Agents with OpenAI Gym: Your Guide to
  Developing AI Agents Using Deep Reinforcement Learning}
    \field{year}{2018}
  \endentry

  \entry{AppleFaceDetectionURL}{online}{}
    \name{author}{1}{}{%
      {{hash=TCVML}{%
         family={Team},
         familyi={T\bibinitperiod},
         given={Computer Vision Machine\bibnamedelima Learning},
         giveni={C\bibinitperiod\bibinitdelim V\bibinitperiod\bibinitdelim
  M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \keyw{URL, NPU, Neural engine, Apple, Vision Framework, Face recognition}
    \strng{namehash}{TCVML1}
    \strng{fullhash}{TCVML1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{An On-device Deep Neural Network for Face Detection}
    \verb{url}
    \verb https://machinelearning.apple.com/2017/11/16/face-detection.html
    \endverb
    \field{month}{11}
    \field{year}{2017}
    \warn{\item Invalid format of field 'urldate'}
  \endentry

  \entry{7460666}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=LND}{%
         family={{Lane}},
         familyi={L\bibinitperiod},
         given={N.\bibnamedelima D.},
         giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=BS}{%
         family={{Bhattacharya}},
         familyi={B\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=GP}{%
         family={{Georgiev}},
         familyi={G\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
      {{hash=FC}{%
         family={{Forlivesi}},
         familyi={F\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=KF}{%
         family={{Kawsar}},
         familyi={K\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{embedded systems;learning (artificial intelligence);mobile
  computing;neural nets;sensors;Qualcomm Snapdragon 400;Nvidia Tegra
  K1;resource constrained embedded platforms;convolutional neural networks;deep
  neural networks;software accelerator;mobile platforms;resource constrained
  wearable platforms;computational techniques;computational power;inference
  accuracies;sensor measurements;DeepX;embedded deep learning;Machine
  learning;Computational modeling;Mobile communication;Image recognition;Neural
  networks;Computer architecture;Prototypes}
    \strng{namehash}{LND+1}
    \strng{fullhash}{LNDBSGPFCKF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Deep learning has revolutionized the way sensor measurements are
  interpreted and application of deep learning has seen a great leap in
  inference accuracies in a number of fields. However, the significant
  requirement for memory and computational power has hindered the wide scale
  adoption of these novel computational techniques on resource constrained
  wearable and mobile platforms. In this demonstration we present DeepX, a
  software accelerator for efficiently running deep neural networks and
  convolutional neural networks on resource constrained embedded platforms,
  e.g., Nvidia Tegra K1 and Qualcomm Snapdragon 400.%
    }
    \field{booktitle}{2016 15th ACM/IEEE International Conference on
  Information Processing in Sensor Networks (IPSN)}
    \verb{doi}
    \verb 10.1109/IPSN.2016.7460666
    \endverb
    \field{pages}{1\bibrangedash 2}
    \field{title}{Demonstration Abstract: Accelerating Embedded Deep Learning
  Using DeepX}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{HauweiAscend}{misc}{}
    \name{author}{1}{}{%
      {{hash=H}{%
         family={Huawei},
         familyi={H\bibinitperiod},
      }}%
    }
    \keyw{URL, Huawei, NPU}
    \strng{namehash}{H1}
    \strng{fullhash}{H1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Huawei launches Ascend 910, the world's most powerful AI
  processor, and MindSpore, an all-scenario AI computing framework}
    \verb{url}
    \verb https://www.huawei.com/en/news/2019/8/huawei-ascend-910-most-powerful
    \verb -ai-processor
    \endverb
    \warn{\item Invalid format of field 'urldate'}
  \endentry

  \entry{5607329}{article}{}
    \name{author}{3}{}{%
      {{hash=GA}{%
         family={Gomperts},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=UA}{%
         family={Ukil},
         familyi={U\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=ZF}{%
         family={Zurfluh},
         familyi={Z\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{backpropagation;field programmable gate arrays;hardware description
  languages;multilayer perceptrons;FPGA;VLSI hardware description
  language;arithmetic operation;artificial neural network;backpropagation
  multilayer perceptron;fast prototyping;field programmable gate array;general
  purpose neural network;hardware-based MLP;learning capability;online
  application;space exploration;Backpropagation;NIR spectra
  calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware
  implementation;multilayer perceptron;neural network;spectroscopy}
    \strng{namehash}{GAUAZF1}
    \strng{fullhash}{GAUAZF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper presents the development and implementation of a generalized
  backpropagation multilayer perceptron (MLP) architecture described in VLSI
  hardware description language (VHDL). The development of hardware platforms
  has been complicated by the high hardware cost and quantity of the arithmetic
  operations required in online artificial neural networks (ANNs), i.e.,
  general purpose ANNs with learning capability. Besides, there remains a
  dearth of hardware platforms for design space exploration, fast prototyping,
  and testing of these networks. Our general purpose architecture seeks to fill
  that gap and at the same time serve as a tool to gain a better understanding
  of issues unique to ANNs implemented in hardware, particularly using field
  programmable gate array (FPGA). The challenge is thus to find an architecture
  that minimizes hardware costs, while maximizing performance, accuracy, and
  parameterization. This work describes a platform that offers a high degree of
  parameterization, while maintaining generalized network design with
  performance comparable to other hardware-based MLP implementations.
  Application of the hardware implementation of ANN with backpropagation
  learning algorithm for a realistic application is also presented.%
    }
    \verb{doi}
    \verb 10.1109/TII.2010.2085006
    \endverb
    \field{issn}{1551-3203}
    \field{number}{1}
    \field{pages}{78\bibrangedash 89}
    \field{title}{Development and Implementation of Parameterized FPGA-Based
  General Purpose Neural Networks for Online Applications}
    \field{volume}{7}
    \field{journaltitle}{IEEE Transactions on Industrial Informatics}
    \field{year}{2011}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8594633}{article}{}
    \name{author}{3}{}{%
      {{hash=SA}{%
         family={{Shawahna}},
         familyi={S\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SSM}{%
         family={{Sait}},
         familyi={S\bibinitperiod},
         given={S.\bibnamedelima M.},
         giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=EA}{%
         family={{El-Maleh}},
         familyi={E\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{FPGA-NN, Deep learning; FPGA; Neural networks; Hardware;
  Acceleration;Convolution; Adaptable architectures; CNN; deep learning;
  dynamic reconfiguration; energy-efficient architecture; hardware accelerator;
  machine learning; neural networks; optimization;parallel computer
  architecture;reconfigurable computing}
    \strng{namehash}{SASSMEA1}
    \strng{fullhash}{SASSMEA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Due to recent advances in digital technologies, and availability of
  credible data, an area of artificial intelligence, deep learning, has emerged
  and has demonstrated its ability and effectiveness in solving complex
  learning problems not possible before. In particular, convolutional neural
  networks (CNNs) have demonstrated their effectiveness in the image detection
  and recognition applications. However, they require intensive CPU operations
  and memory bandwidth that make general CPUs fail to achieve the desired
  performance levels. Consequently, hardware accelerators that use
  application-specific integrated circuits, field-programmable gate arrays
  (FPGAs), and graphic processing units have been employed to improve the
  throughput of CNNs. More precisely, FPGAs have been recently adopted for
  accelerating the implementation of deep learning networks due to their
  ability to maximize parallelism and their energy efficiency. In this paper,
  we review the recent existing techniques for accelerating deep learning
  networks on FPGAs. We highlight the key features employed by the various
  techniques for improving the acceleration performance. In addition, we
  provide recommendations for enhancing the utilization of FPGAs for CNNs
  acceleration. The techniques investigated in this paper represent the recent
  trends in the FPGA-based accelerators of deep learning networks. Thus, this
  paper is expected to direct the future advances on efficient hardware
  accelerators and to be useful for deep learning researchers.%
    }
    \verb{doi}
    \verb 10.1109/ACCESS.2018.2890150
    \endverb
    \field{pages}{7823\bibrangedash 7859}
    \field{title}{FPGA-Based Accelerators of Deep Learning Networks for
  Learning and Classification: A Review}
    \field{volume}{7}
    \field{journaltitle}{IEEE Access}
    \field{year}{2019}
  \endentry

  \entry{7561676}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SR}{%
         family={{Sang}},
         familyi={S\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=LQ}{%
         family={{Liu}},
         familyi={L\bibinitperiod},
         given={Q.},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=ZQ}{%
         family={{Zhang}},
         familyi={Z\bibinitperiod},
         given={Q.},
         giveni={Q\bibinitperiod},
      }}%
    }
    \keyw{FPGA-NN, cost reduction;field programmable gate arrays;learning
  (artificial intelligence);microwave devices;FPGA-based acceleration;neural
  network training;microwave device modeling;model training process;development
  cost reduction;Training;Field programmable gate arrays;Artificial neural
  networks;Software;Random access memory;Hardware;neural network;quasi-Newton
  method;FPGA;hardware acceleration}
    \strng{namehash}{SRLQZQ1}
    \strng{fullhash}{SRLQZQ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Neural networks (NNs) have been widely used in microwave device modeling.
  One of the greatest challenges is how to speed up the model training process
  and reduce the development cost. To address the issue, this paper exploits
  FPGAs to accelerate NN training. Experimental results demonstrate that the
  model training time can be reduced by up to 99.1%, compared to the
  traditional software implementation.%
    }
    \field{booktitle}{2016 IEEE MTT-S International Conference on Numerical
  Electromagnetic and Multiphysics Modeling and Optimization (NEMO)}
    \verb{doi}
    \verb 10.1109/NEMO.2016.7561676
    \endverb
    \field{pages}{1\bibrangedash 2}
    \field{title}{FPGA-based acceleration of neural network training}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8641739}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=DS}{%
         family={{Dey}},
         familyi={D\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=CD}{%
         family={{Chen}},
         familyi={C\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=LZ}{%
         family={{Li}},
         familyi={L\bibinitperiod},
         given={Z.},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={{Kundu}},
         familyi={K\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=HK}{%
         family={{Huang}},
         familyi={H\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=CKM}{%
         family={{Chugg}},
         familyi={C\bibinitperiod},
         given={K.\bibnamedelima M.},
         giveni={K\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=BPA}{%
         family={{Beerel}},
         familyi={B\bibinitperiod},
         given={P.\bibnamedelima A.},
         giveni={P\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
    }
    \keyw{FPGA-NN, Training, field programmable gate arrays;neural
  nets;reconfigurable architectures;inference;network connectivity;structured
  sparsity;computational requirements;edge-processing;parallelization;training
  time;datasets;complexity reduction;network hyperparameters;structures
  on-chip;Artix-7 FPGA;sparse neural network training;parallel
  architecture;reconfigurable architecture;sparse neural networks;on-chip
  training;pipelining;reconfigurability;parallel FPGA
  implementation;Junctions;Training;Hardware;Neurons;Field programmable gate
  arrays;Artificial neural networks;Computer architecture;Machine
  learning;Neural networks;Sparse neural networks;FPGA
  Training;Parallelism;Pipelining}
    \strng{namehash}{DS+1}
    \strng{fullhash}{DSCDLZKSHKCKMBPA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper describes the development of an FPGA implementation of a
  parallel and reconfigurable architecture for sparse neural networks, capable
  of on-chip training and inference. The network connectivity uses
  pre-determined, structured sparsity to significantly reduce complexity by
  lowering memory and computational requirements. The architecture uses a
  notion of edge-processing, leading to efficient pipelining and
  parallelization. Moreover, the device can be reconfigured to trade off
  resource utilization with training time to fit networks and datasets of
  varying sizes. The combined effects of complexity reduction and easy
  reconfigurability enable greater exploration of network hyperparameters and
  structures on-chip. As proof of concept, we show implementation results on an
  Artix-7 FPGA.%
    }
    \field{booktitle}{2018 International Conference on ReConFigurable Computing
  and FPGAs (ReConFig)}
    \verb{doi}
    \verb 10.1109/RECONFIG.2018.8641739
    \endverb
    \field{issn}{2640-0472}
    \field{pages}{1\bibrangedash 4}
    \field{title}{A Highly Parallel FPGA Implementation of Sparse Neural
  Network Training}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7154838}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=AS}{%
         family={{Ahish}},
         familyi={A\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=KYBN}{%
         family={{Kumar}},
         familyi={K\bibinitperiod},
         given={Y.\bibnamedelima B.\bibnamedelima N.},
         giveni={Y\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim
  N\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={{Sharma}},
         familyi={S\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=VMH}{%
         family={{Vasantha}},
         familyi={V\bibinitperiod},
         given={M.\bibnamedelima H.},
         giveni={M\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \keyw{NN; MAC; digital signal processing chips;logic design;multiplying
  circuits;high-performance multiply-accumulate computation unit design;digital
  signal processing;DSP;MAC unit;partial-product reduction
  block;PPRB;multiplier implementation;multibit adder blocks;delay
  reduction;power consumption reduction;area requirement
  reduction;Adders;Delays;Digital signal processing;Computer
  architecture;Computers;Consumer electronics;Carry-lookahead adder;brent-kung
  adder;wallace tree;booth multiplier;multiply-accumulate unit}
    \strng{namehash}{AS+1}
    \strng{fullhash}{ASKYBNSDVMH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In Digital Signal Processing (DSP), Multiply-Accumulate Computation (MAC)
  unit plays a very important role and lies in the critical path. Multiplier is
  one of the most important block in MAC unit. The overall performance of the
  MAC unit depends on the resources used by the multiplier. Therefore, this
  paper describes the design of a Partial Product Reduction Block (PPRB) that
  is used in the implementation of multiplier having better area, delay and
  power performances. PPRB reduces the partial products row wise by using
  different multi-bit adder blocks instead of conventional coloumn wise
  reduction. MAC unit consisting of the multiplier realized using the proposed
  partial product reduction technique has a delay reduction of 46%, power
  consumption is reduced by 39% and area requirement is reduced by 17% when
  compared to MAC unit realised using conventional multiplier architecture.%
    }
    \field{booktitle}{2015 IEEE International Advance Computing Conference
  (IACC)}
    \verb{doi}
    \verb 10.1109/IADCC.2015.7154838
    \endverb
    \field{pages}{915\bibrangedash 918}
    \field{title}{Design of high performance Multiply-Accumulate Computation
  unit}
    \field{year}{2015}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Omondi2006}{inbook}{}
    \name{author}{3}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Bajger},
         familyi={B\bibinitperiod},
         given={Mariusz},
         giveni={M\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, NN, Neurocomputers, Book Chapter}
    \strng{namehash}{OARRJCBM1}
    \strng{fullhash}{OARRJCBM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This introductory chapter reviews the basics of artificial-neural-network
  theory, discusses various aspects of the hardware implementation of neural
  networks (in both ASIC and FPGA technologies, with a focus on special
  features of artificial neural networks), and concludes with a brief note on
  performance-evaluation. Special points are the exploitation of the
  parallelism inherent in neural networks and the appropriate implementation of
  arithmetic functions, especially the sigmoid function. With respect to the
  sigmoid function, the chapter includes a significant contribution.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_1
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{1\bibrangedash 36}
    \field{title}{FPGA Neurocomputers}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_1
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{Omondi:2010:FIN:1941654}{book}{}
    \name{author}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Publishing Company, Incorporated}%
    }
    \keyw{FPGA-NN, ASIC, Book by Publications}
    \strng{namehash}{OARRJC1}
    \strng{fullhash}{OARRJC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{edition}{1st}
    \field{isbn}{1441939423, 9781441939425}
    \field{title}{FPGA Implementations of Neural Networks}
    \field{year}{2010}
  \endentry

  \entry{8892195}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=KS}{%
         family={{Kolala Venkataramanaiah}},
         familyi={K\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=MY}{%
         family={{Ma}},
         familyi={M\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YS}{%
         family={{Yin}},
         familyi={Y\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NE}{%
         family={{Nurvithadhi}},
         familyi={N\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=DA}{%
         family={{Dasu}},
         familyi={D\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CY}{%
         family={{Cao}},
         familyi={C\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={{Seo}},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{FPGA-NN, training, convolutional neural nets;DRAM chips;field
  programmable gate arrays;learning (artificial intelligence);program
  compilers;automatic compiler;FPGA accelerator;convolutional neural
  networks;embedded platforms;on-device learning;designing flexible training
  hardware;inference hardware;16-bit fixed-point precision;complete CNN
  training;forward pass;FP;backward pass;weight update;optimized RTL
  library;training-specific tasks;RTL compiler;FPGA-synthesizable
  RTL;user-defined constraints;BP phases;representative CNNs;Intel Stratix 10
  GX FPGA;hardware architecture;GOPS performance;Training;Field programmable
  gate arrays;Kernel;Hardware;Libraries;Task analysis;Random access
  memory;Convolution neural networks, neural network training,
  back-propagation, hardware accelerator, FPGA}
    \strng{namehash}{KS+1}
    \strng{fullhash}{KSMYYSNEDACYSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Training of convolutional neural networks (CNNs) on embedded platforms to
  support on-device learning is earning vital importance in recent days.
  Designing flexible training hardware is much more challenging than inference
  hardware, due to design complexity and large computation/memory requirement.
  In this work, we present an automatic compiler based FPGA accelerator with
  16-bit fixed-point precision for complete CNN training, including Forward
  Pass (FP), Backward Pass (BP) and Weight Update (WU). We implemented an
  optimized RTL library to perform training-specific tasks and developed an RTL
  compiler to automatically generate FPGA-synthesizable RTL based on
  user-defined constraints. We present a new cyclic weight storage/access
  scheme for on-chip BRAM and off-chip DRAM to efficiently implement
  non-transpose and transpose operations during FP and BP phases, respectively.
  Representative CNNs for CIFAR-10 dataset are implemented and trained on Intel
  Stratix 10 GX FPGA using proposed hardware architecture, demonstrating up to
  479 GOPS performance.%
    }
    \field{booktitle}{2019 29th International Conference on Field Programmable
  Logic and Applications (FPL)}
    \verb{doi}
    \verb 10.1109/FPL.2019.00034
    \endverb
    \field{issn}{1946-1488}
    \field{pages}{166\bibrangedash 172}
    \field{title}{Automatic Compiler Based FPGA Accelerator for CNN Training}
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Simonyan14c}{article}{}
    \name{author}{2}{}{%
      {{hash=SK}{%
         family={Simonyan},
         familyi={S\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZA}{%
         family={Zisserman},
         familyi={Z\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{CNN; image classification, VGG network}
    \strng{namehash}{SKZA1}
    \strng{fullhash}{SKZA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Very Deep Convolutional Networks for Large-Scale Image
  Recognition}
    \field{volume}{abs/1409.1556}
    \field{journaltitle}{ILSVRC - CoRR}
    \field{year}{2014}
  \endentry

  \entry{8766229}{article}{}
    \name{author}{1}{}{%
      {{hash=I}{%
         family={IEEE},
         familyi={I\bibinitperiod},
      }}%
    }
    \keyw{Other, IEEE Standards;Floating-point
  arithmetic;arithmetic;binary;computer;decimal;exponent;floating-point;format;IEEE
  754;interchange;NaN;number;rounding;significand;subnormal.}
    \strng{namehash}{I1}
    \strng{fullhash}{I1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This standard specifies interchange and arithmetic formats and methods for
  binary and decimal floating-point arithmetic in computer programming
  environments. This standard specifies exception conditions and their default
  handling. An implementation of a floating-point system conforming to this
  standard may be realized entirely in software, entirely in hardware, or in
  any combination of software and hardware. For operations specified in the
  normative part of this standard, numerical results and exceptions are
  uniquely determined by the values of the input data, sequence of operations,
  and destination formats, all under user control.%
    }
    \verb{doi}
    \verb 10.1109/IEEESTD.2019.8766229
    \endverb
    \field{pages}{1\bibrangedash 84}
    \field{title}{IEEE Standard for Floating-Point Arithmetic}
    \field{journaltitle}{IEEE Std 754-2019 (Revision of IEEE 754-2008)}
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{4790104}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KM}{%
         family={Kam},
         familyi={K\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN;State-space methods;Neural networks;Information analysis;Pattern
  analysis;Hopfield neural networks;Pattern recognition;Information
  retrieval;Convergence;Hamming distance;Content based retrieval}
    \strng{namehash}{KMCRGA1}
    \strng{fullhash}{KMCRGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Analysis of the state space for the fully-connected binary neural network
  ("the Hopfield model") remains an important objective in utilizing the
  network in pattern recognition and associative information retrieval. Most of
  the research pertaining to the network's state space so far concentrated on
  stable-state enumeration and often it was assumed that the patterns which are
  to be stored are random. We discuss the case of deterministic known codewords
  whose storage is required, and show that for this important case bounds on
  the retrieval probabilities and convergence rates can be achieved. The main
  tool which we employ is Birth-and-Death Markov chains, describing the Hamming
  distance of the network's state from the stored patterns. The results are
  applicable to both the asynchronous network and to the Boltzmann machine, and
  can be utilized to compare codeword sets in terms of efficiency of their
  retrieval, when the neural network is used as a content addressable memory.%
    }
    \field{booktitle}{1988 American Control Conference}
    \verb{doi}
    \verb 10.23919/ACC.1988.4790104
    \endverb
    \field{pages}{2276\bibrangedash 2281}
    \field{title}{On the State Space of the Binary Neural Network}
    \field{year}{1988}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5726804}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=BSA}{%
         family={Brodsky},
         familyi={B\bibinitperiod},
         given={S.\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=GCC}{%
         family={Guest},
         familyi={G\bibinitperiod},
         given={C.\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \keyw{BNN, content-addressable storage;learning systems;neural
  nets;arbitrary bit-level significance;binary backpropagation;bit connection
  weights;content addressable memory;continuous backpropagation network
  learning model;local computation;pseudoanalog extension}
    \strng{namehash}{BSAGCC1}
    \strng{fullhash}{BSAGCC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{1990 IJCNN International Joint Conference on Neural
  Networks}
    \verb{doi}
    \verb 10.1109/IJCNN.1990.137846
    \endverb
    \field{pages}{205\bibrangedash 210 vol.3}
    \field{title}{Binary backpropagation in content addressable memory}
    \field{year}{1990}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Brodsky:93}{article}{}
    \name{author}{3}{}{%
      {{hash=BSA}{%
         family={Brodsky},
         familyi={B\bibinitperiod},
         given={Stephen\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=MGC}{%
         family={Marsden},
         familyi={M\bibinitperiod},
         given={Gary\bibnamedelima C.},
         giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=GCC}{%
         family={Guest},
         familyi={G\bibinitperiod},
         given={Clark\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {OSA}%
    }
    \keyw{BNN, Cylindrical lenses; Light valves; Neural networks; Optical
  components; Optical neural systems; Parallel processing}
    \strng{namehash}{BSAMGCGCC1}
    \strng{fullhash}{BSAMGCGCC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The content-addressable network (CAN) is an efficient, intrinsically
  discrete training algorithm for binary-valued classification networks. The
  binary nature of the CAN network permits accelerated learning and
  significantly reduced hardware-implementation requirements. A multilayer
  optoelectronic CAN network employing matrix--vector multiplication was
  constructed. The network learned and correctly classified trained patterns,
  gaining a measure of fault tolerance by learning associative solutions to
  optical hardware imperfections. Operation of this system is possible owing to
  the reduced hardware accuracy requirements of the CAN learning algorithm.%
    }
    \verb{doi}
    \verb 10.1364/AO.32.001338
    \endverb
    \field{number}{8}
    \field{pages}{1338\bibrangedash 1345}
    \field{title}{Optical matrix--vector implementation of the
  content-addressable network}
    \verb{url}
    \verb http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338
    \endverb
    \field{volume}{32}
    \field{journaltitle}{Appl. Opt.}
    \field{year}{1993}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{1206405}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=BA}{%
         family={Bermak},
         familyi={B\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; pattern classification;VLSI;multiprecision neural chip}
    \strng{namehash}{BA1}
    \strng{fullhash}{BA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper describes a 3D VLSI Chip for binary neural network
  classification applications. The 3D circuit includes three layers of MCM
  integrating 4 chips each making it a total of 12 chips integrated in a volume
  of (2 /spl times/ 2 /spl times/ 0.7)cm/sup 3/. The architecture is scalable,
  and real-time binary neural network classifier systems could be built with
  one, two or all twelve chip solutions. Each basic chip includes an on-chip
  control unit for programming options of the neural network topology and
  precision. The system is modular and presents easy expansibility without
  requiring extra devices. Experimental test results showed that a full recall
  operation is obtained in less than 1.2/spl mu/s for any topology with 4-bit
  or 8-bit precision while it is obtained in less than 2.2/spl mu/s for any
  16-bit precision. As a consequence the 3D chip is a very powerful
  reconfigurable and a multiprecision neural chip exhibiting a significant
  speed of 1.25 GCPS.%
    }
    \field{booktitle}{Proceedings of the 2003 International Symposium on
  Circuits and Systems, 2003. ISCAS '03.}
    \verb{doi}
    \verb 10.1109/ISCAS.2003.1206405
    \endverb
    \field{pages}{V\bibrangedash V}
    \field{title}{A highly scalable 3D chip for binary neural network
  classification applications}
    \field{volume}{5}
    \field{year}{2003}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5159360}{article}{}
    \name{author}{5}{}{%
      {{hash=CF}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
      {{hash=CG}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=HQ}{%
         family={He},
         familyi={H\bibinitperiod},
         given={Q.},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=HG}{%
         family={He},
         familyi={H\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=XX}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
    }
    \keyw{BNN; Boolean functions;learning (artificial intelligence);multilayer
  perceptrons;binary neural network;linearly nonseparable Boolean
  functions;nonLSBF;DNA-like learning and decomposing algorithm;DNA-like
  LDA;DNA-like offset sequence;logic XOR operation;weight-threshold
  value;multilayer perceptron;function mapping;parity Boolean function;Neural
  networks;Boolean functions;Neurons;Multilayer perceptrons;Linear discriminant
  analysis;Logic;Backpropagation algorithms;Input variables;Mathematics;Binary
  neural network;DNA-like learning and decomposing algorithm (DNA-like
  LDA);linearly nonseparable Boolean function (non-LSBF);multilayer perceptron
  (MLP);parity Boolean function (PBF);Algorithms;Artificial
  Intelligence;DNA;Linear Models;Neural Networks (Computer)}
    \strng{namehash}{CF+1}
    \strng{fullhash}{CFCGHQHGXX1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Implementing linearly nonseparable Boolean functions (non-LSBF) has been an
  important and yet challenging task due to the extremely high complexity of
  this kind of functions and the exponentially increasing percentage of the
  number of non-LSBF in the entire set of Boolean functions as the number of
  input variables increases. In this paper, an algorithm named DNA-like
  learning and decomposing algorithm (DNA-like LDA) is proposed, which is
  capable of effectively implementing non-LSBF. The novel algorithm first
  trains the DNA-like offset sequence and decomposes non-LSBF into logic XOR
  operations of a sequence of LSBF, and then determines the weight-threshold
  values of the multilayer perceptron (MLP) that perform both the
  decompositions of LSBF and the function mapping the hidden neurons to the
  output neuron. The algorithm is validated by two typical examples about the
  problem of approximating the circular region and the well-known
  &lt;i&gt;n&lt;/i&gt; -bit parity Boolean function (PBF).%
    }
    \verb{doi}
    \verb 10.1109/TNN.2009.2023122
    \endverb
    \field{issn}{1045-9227}
    \field{number}{8}
    \field{pages}{1293\bibrangedash 1301}
    \field{title}{Universal Perceptron and DNA-Like Learning Algorithm for
  Binary Neural Networks: Non-LSBF Implementation}
    \field{volume}{20}
    \field{journaltitle}{IEEE Transactions on Neural Networks}
    \field{year}{2009}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{616215}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=FN}{%
         family={Funabiki},
         familyi={F\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
      {{hash=KJ}{%
         family={Kitamichi},
         familyi={K\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=NS}{%
         family={Nishikawa},
         familyi={N\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; neural nets;genetic algorithms;set theory;graph
  theory;minimisation;computational complexity;evolutionary neural network
  algorithm;ENN;max cut problems;undirected graph;NP-hard
  problem;partition;disjoint subsets;evolutionary initialization scheme;energy
  minimization criteria;binary neural network;randomly weighted complete
  graphs;unweighted random graphs;maximum neural network;mean field
  annealing;simulated annealing;greedy algorithm;Neural
  networks;Neurons;Simulated annealing;NP-hard problem;Equations;Multi-layer
  neural network;Computer networks;Minimization;Greedy algorithms;Approximation
  algorithms}
    \strng{namehash}{FNKJNS1}
    \strng{fullhash}{FNKJNS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    An "evolutionary neural network (ENN)" is presented for the max cut problem
  of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem
  is to find a partition of V into two disjoint subsets such that the cut size
  be maximized. The cut size is the sum of weights on edges in E whose
  endpoints belong to different subsets. The ENN combines the evolutionary
  initialization scheme of the neural state into the energy minimization
  criteria of the binary neural network. The performance of ENN is evaluated
  through simulations in randomly weighted complete graphs and unweighted
  random graphs with up to 1000 vertices. The results show that the
  evolutionary initialization scheme drastically improves the solution quality.
  ENN can always find better solutions than the maximum neural network, the
  mean field annealing, the simulated annealing, and the greedy algorithm.%
    }
    \field{booktitle}{Proceedings of International Conference on Neural
  Networks (ICNN'97)}
    \verb{doi}
    \verb 10.1109/ICNN.1997.616215
    \endverb
    \field{pages}{1260\bibrangedash 1265 vol.2}
    \field{title}{An evolutionary neural network algorithm for max cut
  problems}
    \field{volume}{2}
    \field{year}{1997}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.2307/169420}{article}{}
    \name{author}{1}{}{%
      {{hash=PM}{%
         family={Pincus},
         familyi={P\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {INFORMS}%
    }
    \keyw{Other, Simulated annealing, Monte Carlo, Probability}
    \strng{namehash}{PM1}
    \strng{fullhash}{PM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper considers the problem of minimizing a function F(x1,â‹¯,xn) over
  a closed, bounded region S in n-dimensional space under the assumption that
  there exists a unique minimizing point (z1,â‹¯,zn)âˆˆ S. In a previous paper
  I represented the coordinates of the minimizing point as the limit of a ratio
  of integrals. The same type of ratio appears, in a different context, in
  statistical mechanics where a Monte Carlo method has been developed, by
  Metropolis et al., for its numerical evaluation. The purpose of this paper is
  to point out the connection of Metropolis's method with the above type of
  minimization problem. The idea of the method is to associate with the
  minimization problem a Markov chain whose sample averages converge with
  probability one to (approximately) the minimizing point (z1,â‹¯,zn). The
  Markov chain should be easily realizable on a computer. An estimate of the
  error from sampling over a finite time period is given.%
    }
    \field{issn}{0030364X, 15265463}
    \field{number}{6}
    \field{pages}{1225\bibrangedash 1228}
    \field{title}{A Monte Carlo Method for the Approximate Solution of Certain
  Types of Constrained Optimization Problems}
    \verb{url}
    \verb http://www.jstor.org/stable/169420
    \endverb
    \field{volume}{18}
    \field{journaltitle}{Operations Research}
    \field{year}{1970}
  \endentry

  \entry{NIPS2016_6573}{incollection}{}
    \name{author}{5}{}{%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=EYR}{%
         family={El-Yaniv},
         familyi={E\bibinithyphendelim Y\bibinitperiod},
         given={Ran},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{BNN; Neural Network}
    \strng{namehash}{HI+1}
    \strng{fullhash}{HICMSDEYRBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 29}
    \field{pages}{4107\bibrangedash 4115}
    \field{title}{Binarized Neural Networks}
    \verb{url}
    \verb http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf
    \endverb
    \field{annotation}{%
    Editor: D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R.
  Garnett%
    }
    \field{year}{2016}
  \endentry

  \entry{8541786}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=LS}{%
         family={{Leroux}},
         familyi={L\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BS}{%
         family={{Bohez}},
         familyi={B\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=VT}{%
         family={{Verbelen}},
         familyi={V\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=VB}{%
         family={{Vankeirsbilck}},
         familyi={V\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
      {{hash=SP}{%
         family={{Simoens}},
         familyi={S\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
      {{hash=DB}{%
         family={{Dhoedt}},
         familyi={D\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {NIPS}%
    }
    \keyw{BNN; Computer Science; Neural and Evolutionary Computing; Computer
  Vision; Pattern Recognition}
    \strng{namehash}{LS+1}
    \strng{fullhash}{LSBSVTVBSPDB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{31st Conference on Neural Information Processing Systems}
    \verb{eprint}
    \verb 8541786
    \endverb
    \field{pages}{1\bibrangedash 4}
    \field{title}{{Transfer Learning with Binary Neural Networks}}
    \field{journaltitle}{NIPS 2017}
    \field{eprinttype}{arXiv}
    \field{month}{12}
    \field{year}{2017}
  \endentry

  \entry{NIPS2015_5647}{incollection}{}
    \name{author}{3}{}{%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=DJP}{%
         family={David},
         familyi={D\bibinitperiod},
         given={Jean-Pierre},
         giveni={J\bibinithyphendelim P\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=CC}{%
         family={Cortes},
         familyi={C\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LND}{%
         family={Lawrence},
         familyi={L\bibinitperiod},
         given={N.\bibnamedelima D.},
         giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=LDD}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={D.\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sugiyama},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{BNN; Machine Learning; Computer Vision; Pattern Recognition; Neural
  and Evolutionary Computing}
    \strng{namehash}{CMBYDJP1}
    \strng{fullhash}{CMBYDJP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 28}
    \field{pages}{3123\bibrangedash 3131}
    \field{title}{BinaryConnect: Training Deep Neural Networks with binary
  weights during propagations}
    \field{annotation}{%
  http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf%
    }
    \field{year}{2015}
  \endentry

  \entry{8226999}{article}{}
    \name{author}{11}{}{%
      {{hash=AK}{%
         family={Ando},
         familyi={A\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=UK}{%
         family={Ueyoshi},
         familyi={U\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=OK}{%
         family={Orimo},
         familyi={O\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={Yonekawa},
         familyi={Y\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Sato},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NH}{%
         family={Nakahara},
         familyi={N\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=TYS}{%
         family={Takamaeda-Yamazaki},
         familyi={T\bibinithyphendelim Y\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=IM}{%
         family={Ikebe},
         familyi={I\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AT}{%
         family={Asai},
         familyi={A\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KT}{%
         family={Kuroda},
         familyi={K\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Motomura},
         familyi={M\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{BNN;low-power electronics;neural nets;random-access
  storage;reconfigurable architectures;deep neural network
  accelerator;binary/ternary deep neural networks;In-memory neural network
  processing;binary/ternaty neural network;BRein memory;single-chip
  binary/ternary reconfigurable in-memory;reconfigurable accelerator
  architecture;external data access;power 0.6 W;frequency 400 MHz;Biological
  neural networks;Random access memory;Memory
  management;Neurons;System-on-chip;Parallel processing;Binary neural
  networks;in-memory processing;near-memory processing;neural
  networks;reconfigurable array;ternary neural networks}
    \strng{namehash}{AK+1}
    \strng{fullhash}{AKUKOKYHSSNHTYSIMATKTMM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    A versatile reconfigurable accelerator architecture for binary/ternary deep
  neural networks is presented. In-memory neural network processing without any
  external data accesses, sustained by the symmetry and simplicity of the
  computation of the binary/ternaty neural network, improves the energy
  efficiency dramatically. The prototype chip is fabricated, and it achieves
  1.4 TOPS (tera operations per second) peak performance with 0.6-W power
  consumption at 400-MHz clock. The application examination is also conducted.%
    }
    \verb{doi}
    \verb 10.1109/JSSC.2017.2778702
    \endverb
    \field{issn}{0018-9200}
    \field{number}{4}
    \field{pages}{983\bibrangedash 994}
    \field{title}{BRein Memory: A Single-Chip Binary/Ternary Reconfigurable
  in-Memory Deep Neural Network Accelerator Achieving 1.4 TOPS at 0.6 W}
    \field{volume}{53}
    \field{journaltitle}{IEEE Journal of Solid-State Circuits}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Sun:2018:FPR:3201607.3201741}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=SX}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Xiaoyu},
         giveni={X\bibinitperiod},
      }}%
      {{hash=PX}{%
         family={Peng},
         familyi={P\bibinitperiod},
         given={Xiaochen},
         giveni={X\bibinitperiod},
      }}%
      {{hash=CPY}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Pai-Yu},
         giveni={P\bibinithyphendelim Y\bibinitperiod},
      }}%
      {{hash=LR}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Rui},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SJs}{%
         family={Seo},
         familyi={S\bibinitperiod},
         given={Jae-sun},
         giveni={J\bibinithyphendelim s\bibinitperiod},
      }}%
      {{hash=YS}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Shimeng},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IEEE Press}%
    }
    \keyw{BNN, P-BNN, CSM, MNIST}
    \strng{namehash}{SX+1}
    \strng{fullhash}{SXPXCPYLRSJsYS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 23rd Asia and South Pacific Design
  Automation Conference}
    \field{pages}{574\bibrangedash 579}
    \field{series}{ASPDAC '18}
    \field{title}{Fully Parallel RRAM Synaptic Array for Implementing Binary
  Neural Network with (+1, -1) Weights and (+1, 0) Neurons}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=3201607.3201741
    \endverb
    \list{location}{1}{%
      {Jeju, Republic of Korea}%
    }
    \field{year}{2018}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{8425178}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=HY}{%
         family={Hu},
         familyi={H\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZJ}{%
         family={Zhai},
         familyi={Z\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LD}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=GY}{%
         family={Gong},
         familyi={G\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhu},
         familyi={Z\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LW}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Su},
         familyi={S\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=JJ}{%
         family={Jin},
         familyi={J\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{BNN;computational complexity;field programmable gate arrays;graphics
  processing units;learning (artificial intelligence);multiprocessing
  systems;neural nets;optimisation;parallel processing;vector
  parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big
  bang;Deep Neural Networks;high computational complexity;Binary Neural
  Networks;BNNs;arithmetic operations;bitwise operations;image-to-column
  method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow
  features;efficient binary convolution;VGG network;counterpart full-precision
  DNNs;GPU;Convolution;Neural networks;Layout;Parallel
  processing;Acceleration;Graphics processing units;Machine learning;Network
  Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi}
    \strng{namehash}{HY+1}
    \strng{fullhash}{HYZJLDGYZYLWSLJJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Deep learning has revolutionized computer vision and other fields since its
  big bang in 2012. However, it is challenging to deploy Deep Neural Networks
  (DNNs) into real-world applications due to their high computational
  complexity. Binary Neural Networks (BNNs) dramatically reduce computational
  complexity by replacing most arithmetic operations with bitwise operations.
  Existing implementations of BNNs have been focusing on GPU or FPGA, and using
  the conventional image-to-column method that doesn't perform well for binary
  convolution due to low arithmetic intensity and unfriendly pattern for
  bitwise operations. We propose BitFlow, a gemm-operator-network three-level
  optimization framework for fully exploiting the computing power of BNNs on
  CPU. BitFlow features a new class of algorithm named PressedConv for
  efficient binary convolution using locality-aware layout and vector
  parallelism. We evaluate BitFlow with the VGG network. On a single core of
  Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN
  implementations, and 11.5x speedup over counterpart full-precision DNNs. Over
  64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart
  full-precision DNNs on GPU (GTX 1080).%
    }
    \field{booktitle}{2018 IEEE International Parallel and Distributed
  Processing Symposium (IPDPS)}
    \verb{doi}
    \verb 10.1109/IPDPS.2018.00034
    \endverb
    \field{issn}{1530-2075}
    \field{pages}{244\bibrangedash 253}
    \field{title}{BitFlow: Exploiting Vector Parallelism for Binary Neural
  Networks on CPU}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8457633}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=GM}{%
         family={Ghasemzadeh},
         familyi={G\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Samragh},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=KF}{%
         family={Koushanfar},
         familyi={K\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{BNN; field programmable gate arrays;learning (artificial
  intelligence);matrix multiplication;neural nets;ReBNet;residual binarized
  neural network;large-scale deep learning models;power-hungry
  matrix-multiplication;light-weight XnorPopcount operations;fixed-point
  counterparts;FPGA;memory footprint;hardware accelerator;Hardware;Neural
  networks;Training;Field programmable gate arrays;Parallel processing;Cost
  function;Libraries;Deep neural networks;Reconfigurable computing;Domain
  customized computing;Binary neural network;Residual binarization}
    \strng{namehash}{GMSMKF1}
    \strng{fullhash}{GMSMKF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper proposes ReBNet, an end-to-end framework for training
  reconfigurable binary neural networks on software and developing efficient
  accelerators for execution on FPGA. Binary neural networks offer an
  intriguing opportunity for deploying large-scale deep learning models on
  resource-constrained devices. Binarization reduces the memory footprint and
  replaces the power-hungry matrix-multiplication with light-weight
  XnorPopcount operations. However, binary networks suffer from a degraded
  accuracy compared to their fixed-point counterparts. We show that the
  state-of-the-art methods for optimizing binary networks accuracy,
  significantly increase the implementation cost and complexity. To compensate
  for the degraded accuracy while adhering to the simplicity of binary
  networks, we devise the first reconfigurable scheme that can adjust the
  classification accuracy based on the application. Our proposition improves
  the classification accuracy by representing features with multiple levels of
  residual binarization. Unlike previous methods, our approach does not
  exacerbate the area cost of the hardware accelerator. Instead, it provides a
  tradeoff between throughput and accuracy while the area overhead of
  multi-level binarization is negligible.%
    }
    \field{booktitle}{2018 IEEE 26th Annual International Symposium on
  Field-Programmable Custom Computing Machines (FCCM)}
    \verb{doi}
    \verb 10.1109/FCCM.2018.00018
    \endverb
    \field{issn}{2576-2621}
    \field{pages}{57\bibrangedash 64}
    \field{title}{ReBNet: Residual Binarized Neural Network}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8429420}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rossi},
         familyi={R\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; computer vision;feedforward neural nets;Internet of
  Things;learning (artificial intelligence);low-power
  electronics;microprocessor chips;hardware accelerators;core efficiency;I/O
  bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable
  architecture;state-of-the-art BNN accelerators;resource-intensive FP16
  arithmetic;TOp/s/W system-level efficiency;binary-weight streaming
  approach;BWN;hyperdrive;weight quantization;binary-weight neural
  networks;memory footprint;aggressive
  quantization;mW-devices;memory-intensive;machine learning;computer
  vision;impressive results;deep neural networks;mW IoT end-nodes;systolically
  scalable binary-weight CNN inference engine;Frequency modulation;Computer
  architecture;Quantization (signal);Neural
  networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary
  Weights Neural Networks;IoT}
    \strng{namehash}{AR+1}
    \strng{fullhash}{ARCLRDBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Deep neural networks have achieved impressive results in computer vision
  and machine learning. Unfortunately, state-of-the-art networks are extremely
  compute-and memory-intensive which makes them unsuitable for mW-devices such
  as IoT end-nodes. Aggressive quantization of these networks dramatically
  reduces the computation and memory footprint. Binary-weight neural networks
  (BWNs) follow this trend, pushing weight quantization to the limit. Hardware
  accelerators for BWNs presented up to now have focused on core efficiency,
  disregarding I/O bandwidth and system-level efficiency that are crucial for
  deployment of accelerators in ultra-low power devices. We present Hyperdrive:
  a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel
  binary-weight streaming approach, and capable of handling high-resolution
  images by virtue of its systolic-scalable architecture. We achieve a 5.9
  TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than
  state-of-the-art BNN accelerators, even if our core uses resource-intensive
  FP16 arithmetic for increased robustness.%
    }
    \field{booktitle}{2018 IEEE Computer Society Annual Symposium on VLSI
  (ISVLSI)}
    \verb{doi}
    \verb 10.1109/ISVLSI.2018.00099
    \endverb
    \field{issn}{2159-3477}
    \field{pages}{509\bibrangedash 515}
    \field{title}{Hyperdrive: A Systolically Scalable Binary-Weight CNN
  Inference Engine for mW IoT End-Nodes}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{robbins1951}{article}{}
    \name{author}{2}{}{%
      {{hash=RH}{%
         family={Robbins},
         familyi={R\bibinitperiod},
         given={Herbert},
         giveni={H\bibinitperiod},
      }}%
      {{hash=MS}{%
         family={Monro},
         familyi={M\bibinitperiod},
         given={Sutton},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {The Institute of Mathematical Statistics}%
    }
    \keyw{Other, Stochastic Gradient Descent, SGD, Derivates}
    \strng{namehash}{RHMS1}
    \strng{fullhash}{RHMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1214/aoms/1177729586
    \endverb
    \field{number}{3}
    \field{pages}{400\bibrangedash 407}
    \field{title}{A Stochastic Approximation Method}
    \verb{url}
    \verb https://doi.org/10.1214/aoms/1177729586
    \endverb
    \field{volume}{22}
    \field{journaltitle}{Ann. Math. Statist.}
    \field{month}{09}
    \field{year}{1951}
  \endentry

  \entry{Stanley:2019aa}{article}{}
    \name{author}{4}{}{%
      {{hash=SKO}{%
         family={Stanley},
         familyi={S\bibinitperiod},
         given={Kenneth\bibnamedelima O.},
         giveni={K\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Clune},
         familyi={C\bibinitperiod},
         given={Jeff},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lehman},
         familyi={L\bibinitperiod},
         given={Joel},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={Miikkulainen},
         familyi={M\bibinitperiod},
         given={Risto},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, NeuroEvolution, Nature, Survey}
    \strng{namehash}{SKO+1}
    \strng{fullhash}{SKOCJLJMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Much of recent machine learning has focused on deep learning, in which
  neural network weights are trained through variants of stochastic gradient
  descent. An alternative approach comes from the field of neuroevolution,
  which harnesses evolutionary algorithms to optimize neural networks, inspired
  by the fact that natural brains themselves are the products of an
  evolutionary process. Neuroevolution enables important capabilities that are
  typically unavailable to gradient-based approaches, including learning neural
  network building blocks (for example activation functions), hyperparameters,
  architectures and even the algorithms for learning themselves. Neuroevolution
  also differs from deep learning (and deep reinforcement learning) by
  maintaining a population of solutions during search, enabling extreme
  exploration and massive parallelization. Finally, because neuroevolution
  research has (until recently) developed largely in isolation from
  gradient-based neural network research, it has developed many unique and
  effective techniques that should be effective in other machine learning areas
  too. This Review looks at several key aspects of modern neuroevolution,
  including large-scale computing, the benefits of novelty and diversity, the
  power of indirect encoding, and the field's contributions to meta-learning
  and architecture search. Our hope is to inspire renewed interest in the field
  as it meets the potential of the increasing computation available today, to
  highlight how many of its ideas can provide an exciting resource for
  inspiration and hybridization to the deep learning, deep reinforcement
  learning and machine learning communities, and to explain how neuroevolution
  could prove to be a critical tool in the long-term pursuit of artificial
  general intelligence.%
    }
    \verb{doi}
    \verb 10.1038/s42256-018-0006-z
    \endverb
    \field{isbn}{2522-5839}
    \field{number}{1}
    \field{pages}{24\bibrangedash 35}
    \field{title}{Designing neural networks through neuroevolution}
    \verb{url}
    \verb https://doi.org/10.1038/s42256-018-0006-z
    \endverb
    \field{volume}{1}
    \field{journaltitle}{Nature Machine Intelligence}
    \field{year}{2019}
  \endentry

  \entry{stanley:phd04}{thesis}{}
    \name{author}{1}{}{%
      {{hash=SKO}{%
         family={Stanley},
         familyi={S\bibinitperiod},
         given={Kenneth\bibnamedelima O.},
         giveni={K\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
    }
    \keyw{Books, NeuroEvolution, NEAT}
    \strng{namehash}{SKO1}
    \strng{fullhash}{SKO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Efficient Evolution of Neural Networks Through
  Complexification}
    \verb{url}
    \verb http://nn.cs.utexas.edu/?stanley:phd2004
    \endverb
    \list{institution}{1}{%
      {Department of Computer Sciences, The University of Texas at Austin}%
    }
    \field{type}{phdthesis}
    \field{year}{2004}
  \endentry

  \entry{10.1007/3-540-61108-8_27}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=BT}{%
         family={B{\"a}ck},
         familyi={B\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=AJM}{%
         family={Alliot},
         familyi={A\bibinitperiod},
         given={Jean-Marc},
         giveni={J\bibinithyphendelim M\bibinitperiod},
      }}%
      {{hash=LE}{%
         family={Lutton},
         familyi={L\bibinitperiod},
         given={Evelyne},
         giveni={E\bibinitperiod},
      }}%
      {{hash=RE}{%
         family={Ronald},
         familyi={R\bibinitperiod},
         given={Edmund},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Schoenauer},
         familyi={S\bibinitperiod},
         given={Marc},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Snyers},
         familyi={S\bibinitperiod},
         given={Dominique},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Evolutionary, evolution strategies, Genetic Algorithm}
    \strng{namehash}{BT1}
    \strng{fullhash}{BT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this paper, evolution strategies (ESs) --- a class of evolutionary
  algorithms using normally distributed mutations, recombination, deterministic
  selection of the $\mu$>1 best offspring individuals, and the principle of
  self-adaptation for the collective on-line learning of strategy parameters
  --- are described by demonstrating their differences to genetic algorithms.
  By comparison of the algorithms, it is argued that the application of
  canonical genetic algorithms for continuous parameter optimization problems
  implies some difficulties caused by the encoding of continuous object
  variables by binary strings and the constant mutation rate used in genetic
  algorithms. Because they utilize a problem-adequate representation and a
  suitable self-adaptive step size control guaranteeing linear convergence for
  strictly convex problems, evolution strategies are argued to be more adequate
  for continuous problems.%
    }
    \field{booktitle}{Artificial Evolution}
    \field{isbn}{978-3-540-49948-0}
    \field{pages}{1\bibrangedash 20}
    \field{title}{Evolution strategies: An alternative evolutionary algorithm}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{1996}
  \endentry

  \entry{DeepNeuroEvol}{misc}{}
    \name{author}{1}{}{%
      {{hash=U}{%
         family={Uber},
         familyi={U\bibinitperiod},
      }}%
    }
    \keyw{Neuroevolution; Atari, Videogames; evolution strategies; uber}
    \strng{namehash}{U1}
    \strng{fullhash}{U1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Accelerating Deep Neuroevolution: Train Atari in Hours on a
  Single Personal Computer}
    \verb{url}
    \verb https://eng.uber.com/accelerated-neuroevolution/
    \endverb
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1007/978-3-319-02621-3_15}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=TAJ}{%
         family={Turner},
         familyi={T\bibinitperiod},
         given={Andrew\bibnamedelima James},
         giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=MJF}{%
         family={Miller},
         familyi={M\bibinitperiod},
         given={Julian\bibnamedelima Francis},
         giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=BM}{%
         family={Bramer},
         familyi={B\bibinitperiod},
         given={Max},
         giveni={M\bibinitperiod},
      }}%
      {{hash=PM}{%
         family={Petridis},
         familyi={P\bibinitperiod},
         given={Miltos},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer International Publishing}%
    }
    \keyw{Book, Evolutionary, CGP, TWEANN,}
    \strng{namehash}{TAJMJF1}
    \strng{fullhash}{TAJMJF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    NeuroEvolution (NE) is the application of evolutionary algorithms to
  Artificial Neural Networks (ANN). This paper reports on an investigation into
  the relative importance of weight evolution and topology evolution when
  training ANN using NE. This investigation used the NE technique Cartesian
  Genetic Programming of Artificial Neural Networks (CGPANN). The results
  presented show that the choice of topology has a dramatic impact on the
  effectiveness of NE when only evolving weights; an issue not faced when
  manipulating both weights and topology. This paper also presents the
  surprising result that topology evolution alone is far more effective when
  training ANN than weight evolution alone. This is a significant result as
  many methods which train ANN manipulate only weights.%
    }
    \field{booktitle}{Research and Development in Intelligent Systems XXX}
    \field{isbn}{978-3-319-02621-3}
    \field{pages}{213\bibrangedash 226}
    \field{title}{The Importance of Topology Evolution in NeuroEvolution: A
  Case Study Using Cartesian Genetic Programming of Artificial Neural Networks}
    \list{location}{1}{%
      {Cham}%
    }
    \field{year}{2013}
  \endentry

  \entry{DBLP:conf/esann/CliffHH93}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=CD}{%
         family={Cliff},
         familyi={C\bibinitperiod},
         given={Dave},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HI}{%
         family={Harvey},
         familyi={H\bibinitperiod},
         given={Inman},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Husbands},
         familyi={H\bibinitperiod},
         given={Phil},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, Neuroevolution, SAGA, TWEANN}
    \strng{namehash}{CDHIHP1}
    \strng{fullhash}{CDHIHP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{{ESANN} 1993, 1st European Symposium on Artificial Neural
  Networks, Brussels, Belgium, April 7-9, 1993, Proceedings}
    \field{title}{Incremental evolution of neural network architectures for
  adaptive behavior}
    \verb{url}
    \verb https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1993-506-S.pd
    \verb f
    \endverb
    \field{year}{1993}
  \endentry

  \entry{10.1016/j.neucom.2013.04.005}{article}{}
    \name{author}{4}{}{%
      {{hash=MKM}{%
         family={Mahsal\bibnamedelima Khan},
         familyi={M\bibinitperiod\bibinitdelim K\bibinitperiod},
         given={Maryam},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MAA}{%
         family={Masood\bibnamedelima Ahmad},
         familyi={M\bibinitperiod\bibinitdelim A\bibinitperiod},
         given={Arbab},
         giveni={A\bibinitperiod},
      }}%
      {{hash=MKG}{%
         family={Muhammad\bibnamedelima Khan},
         familyi={M\bibinitperiod\bibinitdelim K\bibinitperiod},
         given={Gul},
         giveni={G\bibinitperiod},
      }}%
      {{hash=MJF}{%
         family={Miller},
         familyi={M\bibinitperiod},
         given={Julian\bibnamedelima F.},
         giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Elsevier Science Publishers B. V.}%
    }
    \keyw{Evolutionary, CGP, Breast cancer, Artificial neural network,
  Neuroevolution, Recurrent networks, Pole balancing}
    \strng{namehash}{MKM+1}
    \strng{fullhash}{MKMMAAMKGMJF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1016/j.neucom.2013.04.005
    \endverb
    \field{issn}{0925-2312}
    \field{pages}{274\bibrangedash 289}
    \field{title}{Fast Learning Neural Networks Using Cartesian Genetic
  Programming}
    \verb{url}
    \verb https://doi.org/10.1016/j.neucom.2013.04.005
    \endverb
    \field{volume}{121}
    \list{location}{1}{%
      {NLD}%
    }
    \field{journaltitle}{Neurocomput.}
    \field{month}{12}
    \field{year}{2013}
  \endentry

  \entry{1004508}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=SKO}{%
         family={{Stanley}},
         familyi={S\bibinitperiod},
         given={K.\bibnamedelima O.},
         giveni={K\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={{Miikkulainen}},
         familyi={M\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; AI; neural nets;genetic algorithms;neural network
  topologies;neuroevolution;evolving artificial neural networks;genetic
  algorithms;reinforcement learning;hidden state information;neuroevolution of
  augmenting topologies;fixed-topology methods;ablation studies;Neural
  networks;Network topology;Evolution (biology);Artificial neural
  networks;Learning;Genetic algorithms;Benchmark
  testing;Protection;Technological innovation;System testing}
    \strng{namehash}{SKOMR1}
    \strng{fullhash}{SKOMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Neuroevolution, i.e. evolving artificial neural networks with genetic
  algorithms, has been highly effective in reinforcement learning tasks,
  particularly those with hidden state information. An important question in
  neuroevolution is how to gain an advantage from evolving neural network
  topologies along with weights. We present a method, NeuroEvolution of
  Augmenting Topologies (NEAT) that outperforms the best fixed-topology methods
  on a challenging benchmark reinforcement learning task. We claim that the
  increased efficiency is due to (1) employing a principled method of crossover
  of different topologies, (2) protecting structural innovation using
  speciation, and (3) incrementally growing from minimal structure. We test
  this claim through a series of ablation studies that demonstrate that each
  component is necessary to the system as a whole and to each other. What
  results is significantly faster learning. NEAT is also an important
  contribution to GAs because it shows how it is possible for evolution to both
  optimize and complexify solutions simultaneously, making it possible to
  evolve increasingly complex solutions over time, thereby strengthening the
  analogy with biological evolution.%
    }
    \field{booktitle}{Proceedings of the 2002 Congress on Evolutionary
  Computation. CEC'02 (Cat. No.02TH8600)}
    \verb{doi}
    \verb 10.1109/CEC.2002.1004508
    \endverb
    \field{issn}{null}
    \field{pages}{1757\bibrangedash 1762 vol.2}
    \field{title}{Efficient evolution of neural network topologies}
    \field{volume}{2}
    \field{year}{2002}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Rempis2012}{thesis}{}
    \name{author}{1}{}{%
      {{hash=RCW}{%
         family={Rempis},
         familyi={R\bibinitperiod},
         given={Christian\bibnamedelima Wilhelm},
         giveni={C\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, TWEANN, ICONE, Neuroevolution}
    \strng{namehash}{RCW1}
    \strng{fullhash}{RCW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Evolving Complex Neuro-Controllers with Interactively
  Constrained Neuro-Evolution}
    \list{institution}{1}{%
      {Universit{\"a}t Osnabr{\"u}ck}%
    }
    \field{type}{phdthesis}
    \field{year}{2012}
  \endentry

  \entry{NIPS2006_3048}{incollection}{}
    \name{author}{4}{}{%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Lamblin},
         familyi={L\bibinitperiod},
         given={Pascal},
         giveni={P\bibinitperiod},
      }}%
      {{hash=PD}{%
         family={Popovici},
         familyi={P\bibinitperiod},
         given={Dan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=LH}{%
         family={Larochelle},
         familyi={L\bibinitperiod},
         given={Hugo},
         giveni={H\bibinitperiod},
      }}%
    }
    \name{editor}{3}{}{%
      {{hash=SB}{%
         family={Sch\"{o}lkopf},
         familyi={S\bibinitperiod},
         given={B.},
         giveni={B\bibinitperiod},
      }}%
      {{hash=PJC}{%
         family={Platt},
         familyi={P\bibinitperiod},
         given={J.\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hoffman},
         familyi={H\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{NN, Topology}
    \strng{namehash}{BY+1}
    \strng{fullhash}{BYLPPDLH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 19}
    \field{pages}{153\bibrangedash 160}
    \field{title}{Greedy Layer-Wise Training of Deep Networks}
    \verb{url}
    \verb http://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-n
    \verb etworks.pdf
    \endverb
    \field{year}{2007}
  \endentry

  \entry{Turner:2014aa}{article}{}
    \name{author}{2}{}{%
      {{hash=TAJ}{%
         family={Turner},
         familyi={T\bibinitperiod},
         given={Andrew\bibnamedelima James},
         giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=MJF}{%
         family={Miller},
         familyi={M\bibinitperiod},
         given={Julian\bibnamedelima Francis},
         giveni={J\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, Neuroevolution, ANN training}
    \strng{namehash}{TAJMJF1}
    \strng{fullhash}{TAJMJF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    NeuroEvolution is the application of Evolutionary Algorithms to the
  training of Artificial Neural Networks. Currently the vast majority of
  NeuroEvolutionary methods create homogeneous networks of user defined
  transfer functions. This is despite NeuroEvolution being capable of creating
  heterogeneous networks where each neuron's transfer function is not chosen by
  the user, but selected or optimised during evolution. This paper demonstrates
  how NeuroEvolution can be used to select or optimise each neuron's transfer
  function and empirically shows that doing so significantly aids training.
  This result is important as the majority of NeuroEvolutionary methods are
  capable of creating heterogeneous networks using the methods described.%
    }
    \verb{doi}
    \verb 10.1007/s12065-014-0115-5
    \endverb
    \field{isbn}{1864-5917}
    \field{number}{3}
    \field{pages}{135\bibrangedash 154}
    \field{title}{NeuroEvolution: Evolving Heterogeneous Artificial Neural
  Networks}
    \verb{url}
    \verb https://doi.org/10.1007/s12065-014-0115-5
    \endverb
    \field{volume}{7}
    \field{journaltitle}{Evolutionary Intelligence}
    \field{year}{2014}
  \endentry

  \entry{Pujol:1998aa}{article}{}
    \name{author}{2}{}{%
      {{hash=PJCF}{%
         family={Pujol},
         familyi={P\bibinitperiod},
         given={Jo{\~a}o Carlos\bibnamedelima Figueira},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod\bibinitdelim
  F\bibinitperiod},
      }}%
      {{hash=PR}{%
         family={Poli},
         familyi={P\bibinitperiod},
         given={Riccardo},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, weight optimization, XOR}
    \strng{namehash}{PJCFPR1}
    \strng{fullhash}{PJCFPR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Evolutionary computation is a class of global search techniques based on
  the learning process of a population of potential solutions to a given
  problem, that has been successfully applied to a variety of problems. In this
  paper a new approach to the construction of neural networks based on
  evolutionary computation is presented. A linear chromosome combined to a
  graph representation of the network are used by genetic operators, which
  allow the evolution of the architecture and the weights simultaneously
  without the need of local weight optimization. This paper describes the
  approach, the operators and reports results of the application of this
  technique to several binary classification problems.%
    }
    \verb{doi}
    \verb 10.1023/A:1008272615525
    \endverb
    \field{isbn}{1573-7497}
    \field{number}{1}
    \field{pages}{73\bibrangedash 84}
    \field{title}{Evolving the Topology and the Weights of Neural Networks
  Using a Dual Representation}
    \verb{url}
    \verb https://doi.org/10.1023/A:1008272615525
    \endverb
    \field{volume}{8}
    \field{journaltitle}{Applied Intelligence}
    \field{year}{1998}
  \endentry

  \entry{7460958}{article}{}
    \name{author}{2}{}{%
      {{hash=VD}{%
         family={Vargas},
         familyi={V\bibinitperiod},
         given={Danilo},
         giveni={D\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Murata},
         familyi={M\bibinitperiod},
         given={Junichi},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; computational complexity;evolutionary
  computation;learning (artificial intelligence);neural nets;search
  problems;spectrum-diverse neuroevolution;unified neural models;learning
  algorithms;solution representation;representation complexity;search
  space;efficient searching algorithm;neural networks features;diversity
  preserving method;chromosome spectrum;NeuroEvolution;augmenting
  topologies;unified neuron representation;chromosome size;Neurons;Biological
  neural networks;Topology;Network topology;Biological
  cells;Encoding;Technological innovation;General artificial
  intelligence;neuroevolution;neuroEvolution of Augmenting Topology
  (NEAT);reinforcement learning;spectrum diversity;topology and weight evolving
  artificial neural network (TWEANN);unified neuron model; SUNA}
    \strng{namehash}{VDMJ1}
    \strng{fullhash}{VDMJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Learning algorithms are being increasingly adopted in various applications.
  However, further expansion will require methods that work more automatically.
  To enable this level of automation, a more powerful solution representation
  is needed. However, by increasing the representation complexity, a second
  problem arises. The search space becomes huge, and therefore, an associated
  scalable and efficient searching algorithm is also required. To solve both
  the problems, first a powerful representation is proposed that unifies most
  of the neural networks features from the literature into one representation.
  Second, a new diversity preserving method called spectrum diversity is
  created based on the new concept of chromosome spectrum that creates a
  spectrum out of the characteristics and frequency of alleles in a chromosome.
  The combination of spectrum diversity with a unified neuron representation
  enables the algorithm to either surpass or equal NeuroEvolution of Augmenting
  Topologies on all of the five classes of problems tested. Ablation tests
  justify the good results, showing the importance of added new features in the
  unified neuron representation. Part of the success is attributed to the
  novelty-focused evolution and good scalability with a chromosome size
  provided by spectrum diversity. Thus, this paper sheds light on a new
  representation and diversity preserving mechanism that should impact
  algorithms and applications to come.%
    }
    \verb{doi}
    \verb 10.1109/TNNLS.2016.2551748
    \endverb
    \field{issn}{2162-237X}
    \field{number}{8}
    \field{pages}{1759\bibrangedash 1773}
    \field{title}{Spectrum-Diverse Neuroevolution With Unified Neural Models}
    \field{volume}{28}
    \field{journaltitle}{IEEE Transactions on Neural Networks and Learning
  Systems}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Conti:2018:IEE:3327345.3327410}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=CE}{%
         family={Conti},
         familyi={C\bibinitperiod},
         given={Edoardo},
         giveni={E\bibinitperiod},
      }}%
      {{hash=MV}{%
         family={Madhavan},
         familyi={M\bibinitperiod},
         given={Vashisht},
         giveni={V\bibinitperiod},
      }}%
      {{hash=SFP}{%
         family={Such},
         familyi={S\bibinitperiod},
         given={Felipe\bibnamedelima Petroski},
         giveni={F\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lehman},
         familyi={L\bibinitperiod},
         given={Joel},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SKO}{%
         family={Stanley},
         familyi={S\bibinitperiod},
         given={Kenneth\bibnamedelima O.},
         giveni={K\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Clune},
         familyi={C\bibinitperiod},
         given={Jeff},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates Inc.}%
    }
    \keyw{Evolutionary, evolution strategies, Uber, Mujoco, Atari}
    \strng{namehash}{CE+1}
    \strng{fullhash}{CEMVSFPLJSKOCJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 32Nd International Conference on
  Neural Information Processing Systems}
    \field{pages}{5032\bibrangedash 5043}
    \field{series}{NIPS'18}
    \field{title}{Improving Exploration in Evolution Strategies for Deep
  Reinforcement Learning via a Population of Novelty-seeking Agents}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=3327345.3327410
    \endverb
    \list{location}{1}{%
      {Montreal, Canada}%
    }
    \field{year}{2018}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{10.1007/3-540-36178-2_18}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KA}{%
         family={Klimov},
         familyi={K\bibinitperiod},
         given={Alexander},
         giveni={A\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mityagin},
         familyi={M\bibinitperiod},
         given={Anton},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SA}{%
         family={Shamir},
         familyi={S\bibinitperiod},
         given={Adi},
         giveni={A\bibinitperiod},
      }}%
    }
    \name{editor}{1}{}{%
      {{hash=ZY}{%
         family={Zheng},
         familyi={Z\bibinitperiod},
         given={Yuliang},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Crypto, Neurocryptography, Public key}
    \strng{namehash}{KAMASA1}
    \strng{fullhash}{KAMASA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this paper we analyse the security of a new key exchange protocol
  proposed in [3], which is based on mutually learning neural networks. This is
  a new potential source for public key cryptographic schemes which are not
  based on number theoretic functions, and have small time and memory
  complexities. In the first part of the paper we analyse the scheme, explain
  why the two parties converge to a common key, and why an attacker using a
  similar neural network is unlikely to converge to the same key. However, in
  the second part of the paper we show that this key exchange protocol can be
  broken in three different ways, and thus it is completely insecure.%
    }
    \field{booktitle}{Advances in Cryptology --- ASIACRYPT 2002}
    \field{isbn}{978-3-540-36178-7}
    \field{pages}{288\bibrangedash 298}
    \field{title}{Analysis of Neural Cryptography}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{2002}
  \endentry

  \entry{8590945}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=ZY}{%
         family={{Zhu}},
         familyi={Z\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=VDV}{%
         family={{Vargas}},
         familyi={V\bibinitperiod},
         given={D.\bibnamedelima V.},
         giveni={D\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={{Sakurai}},
         familyi={S\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; SUNA; cryptography;learning (artificial
  intelligence);neural nets;topology evolving neural networks;neural network
  architecture;neural cryptography scheme;neural symmetric
  cryptosystem;mathematical theory;spectrum-diverse unified neuroevolution
  architecture;automatic encryption;automatic decryption;adversarial
  training;Neurons;Encryption;Biological neural networks;Network
  topology;Topology;Neural cryptography, Symmetric cryptosystem,
  Spectrum-diverse unified neuroevolution architecture, Topology evolving
  neural networks}
    \strng{namehash}{ZYVDVSK1}
    \strng{fullhash}{ZYVDVSK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Modern cryptographic schemes is developed based on the mathematical theory.
  Recently works show a new direction about cryptography based on the neural
  networks. Instead of learning a specific algorithm, a cryptographic scheme is
  generated automatically. While one kind of neural network is used to achieve
  the scheme, the idea of the neural cryptography can be realized by other
  neural network architecture is unknown. In this paper, we make use of this
  property to create neural cryptography scheme on a new topology evolving
  neural network architecture called Spectrum-diverse unified neuroevolution
  architecture. First, experiments are conducted to verify that
  Spectrum-diverse unified neuroevolution architecture is able to achieve
  automatic encryption and decryption. Subsequently, we do experiments to
  achieve the neural symmetric cryptosystem by using adversarial training.%
    }
    \field{booktitle}{2018 Sixth International Symposium on Computing and
  Networking Workshops (CANDARW)}
    \verb{doi}
    \verb 10.1109/CANDARW.2018.00091
    \endverb
    \field{pages}{472\bibrangedash 478}
    \field{title}{Neural Cryptography Based on the Topology Evolving Neural
  Networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{ijcai2017-316}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LT}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Tao},
         giveni={T\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Guo},
         familyi={G\bibinitperiod},
         given={Tian},
         giveni={T\bibinitperiod},
      }}%
      {{hash=AK}{%
         family={Aberer},
         familyi={A\bibinitperiod},
         given={Karl},
         giveni={K\bibinitperiod},
      }}%
    }
    \keyw{NN; Neural Network, LSTM, Convolutional Neural Network, Hybrid}
    \strng{namehash}{LTGTAK1}
    \strng{fullhash}{LTGTAK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the Twenty-Sixth International Joint
  Conference on Artificial Intelligence, {IJCAI-17}}
    \verb{doi}
    \verb 10.24963/ijcai.2017/316
    \endverb
    \field{pages}{2273\bibrangedash 2279}
    \field{title}{Hybrid Neural Networks for Learning the Trend in Time Series}
    \verb{url}
    \verb https://doi.org/10.24963/ijcai.2017/316
    \endverb
    \field{year}{2017}
  \endentry

  \entry{5678632}{article}{}
    \name{author}{2}{}{%
      {{hash=OCO}{%
         family={{Oluigbo}},
         familyi={O\bibinitperiod},
         given={C.\bibnamedelima O.},
         giveni={C\bibinitperiod\bibinitdelim O\bibinitperiod},
      }}%
      {{hash=RAR}{%
         family={{Rezai}},
         familyi={R\bibinitperiod},
         given={A.\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \keyw{Other; SUNA; biochemistry;bioelectric
  phenomena;brain;cognition;electrochemistry;macromolecules;medical
  disorders;molecular biophysics;neuromuscular
  stimulation;prosthetics;neurological disorders;neuromodulation;emotional
  disability;aging population;chronic physical disability;patient
  therapy;implantable devices;reversible adjustable application;electrical
  agents;chemical agents;biological agents;central nervous system;peripheral
  nervous system;clinical applications;electrical stimulation;cognitive
  disability;Neuromuscular;Aging;Medical
  services;Neuromodulation;Gerontology;Neuromodulation;neurological
  disorders;Adult;Biomedical Engineering;Deep Brain Stimulation;Electric
  Stimulation Therapy;Electric Stimulation Therapy;Electrodes,
  Implanted;Female;Humans;Infusion Pumps, Implantable;Male;Nerve Net;Nervous
  System Diseases;Neurosciences;Vagus Nerve Stimulation}
    \strng{namehash}{OCORAR1}
    \strng{fullhash}{OCORAR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Neurological disorders are becoming increasingly common in developed
  countries as a result of the aging population. In spite of medications, these
  disorders can result in progressive loss of function as well as chronic
  physical, cognitive, and emotional disability that ultimately places enormous
  emotional and economic on the patient, caretakers, and the society in
  general. Neuromodulation is emerging as a therapeutic option in these
  patients. Neuromodulation is a field, which involves implantable devices that
  allow for the reversible adjustable application of electrical, chemical, or
  biological agents to the central or peripheral nervous system with the
  objective of altering its functioning with the objective of achieving a
  therapeutic or clinically beneficial effect. It is a rapidly evolving field
  that brings together many different specialties in the fields of medicine,
  materials science, computer science and technology, biomedical, and neural
  engineering as well as the surgical or interventional specialties. It has
  multiple current and emerging indications, and an enormous potential for
  growth. The main challenges before it are in the need for effective
  collaboration between engineers, basic scientists, and clinicians to develop
  innovations that address specific problems resulting in new devices and
  clinical applications.%
    }
    \verb{doi}
    \verb 10.1109/TBME.2010.2102758
    \endverb
    \field{issn}{0018-9294}
    \field{number}{7}
    \field{pages}{1907\bibrangedash 1917}
    \field{title}{Addressing Neurological Disorders With Neuromodulation}
    \field{volume}{58}
    \field{journaltitle}{IEEE Transactions on Biomedical Engineering}
    \field{year}{2011}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{6804688}{article}{}
    \name{author}{5}{}{%
      {{hash=RM}{%
         family={{Rahimi Azghadi}},
         familyi={R\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=IN}{%
         family={{Iannella}},
         familyi={I\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
      {{hash=ASF}{%
         family={{Al-Sarawi}},
         familyi={A\bibinitperiod},
         given={S.\bibnamedelima F.},
         giveni={S\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
      {{hash=IG}{%
         family={{Indiveri}},
         familyi={I\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=AD}{%
         family={{Abbott}},
         familyi={A\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{analogue integrated circuits;learning (artificial
  intelligence);neural chips;VLSI;spike based synaptic plasticity;artificial
  spiking neural networks;mathematical prescriptions;learning rules;abstract
  computational neuroscience models;analog very large scale integration
  circuit;VLSI;neural chips;Transistors;Logic
  gates;Silicon;Neuromorphics;Neurons;Neuroscience;Learning
  systems;Plastics;Analog/digital synapse;Bienenstock--Cooper--Munro
  (BCM);calcium-based plasticity;learning;local correlation plasticity
  (LCP);neuromorphic engineering;rate-based plasticity;spike-timing-dependent
  plasticity (STDP);spike-based plasticity;spiking neural networks;synaptic
  plasticity;triplet STDP;very large-scale integration (VLSI);voltage-based
  STDP;Analog/digital synapse;BienenstockÂ¿CooperÂ¿Munro (BCM);calcium-based
  plasticity;learning;local correlation plasticity (LCP);neuromorphic
  engineering;rate-based plasticity;spike-timing-dependent plasticity
  (STDP);spike-based plasticity;spiking neural networks;synaptic
  plasticity;triplet STDP;very large-scale integration (VLSI);voltage-based
  STDP}
    \strng{namehash}{RM+1}
    \strng{fullhash}{RMINASFIGAD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The ability to carry out signal processing, classification, recognition,
  and computation in artificial spiking neural networks (SNNs) is mediated by
  their synapses. In particular, through activity-dependent alteration of their
  efficacies, synapses play a fundamental role in learning. The mathematical
  prescriptions under which synapses modify their weights are termed synaptic
  plasticity rules. These learning rules can be based on abstract computational
  neuroscience models or on detailed biophysical ones. As these rules are being
  proposed and developed by experimental and computational neuroscientists,
  engineers strive to design and implement them in silicon and en masse in
  order to employ them in complex real-world applications. In this paper, we
  describe analog very large-scale integration (VLSI) circuit implementations
  of multiple synaptic plasticity rules, ranging from phenomenological ones
  (e.g., based on spike timing, mean firing rates, or both) to biophysically
  realistic ones (e.g., calcium-dependent models). We discuss the application
  domains, weaknesses, and strengths of various representative approaches
  proposed in the literature, and provide insight into the challenges that
  engineers face when designing and implementing synaptic plasticity rules in
  VLSI technology for utilizing them in real-world applications.%
    }
    \verb{doi}
    \verb 10.1109/JPROC.2014.2314454
    \endverb
    \field{issn}{0018-9219}
    \field{number}{5}
    \field{pages}{717\bibrangedash 737}
    \field{title}{Spike-Based Synaptic Plasticity in Silicon: Design,
  Implementation, Application, and Challenges}
    \field{volume}{102}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{year}{2014}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{6215974}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=BD}{%
         family={{Bhargavaram}},
         familyi={B\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=PMGK}{%
         family={{Pillai}},
         familyi={P\bibinitperiod},
         given={M.\bibnamedelima G.\bibnamedelima K.},
         giveni={M\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim
  K\bibinitperiod},
      }}%
    }
    \keyw{Other, SUNA, circuit switching;CMOS logic circuits;flip-flops;logic
  design;low-power electronics;NAND circuits;fow power dual edge triggered
  flip-flop;digital circuit;power consumption;synchronous circuit;pulse
  generator;power dissipation reduction;dual edge static pulsed flip-flop;dual
  edge trigger sense amplifier flip-flop;dual edge trigger NAND keeper
  flip-flop keeper technique;keeper transistor width;conditional
  switching;circuit design;CMOS process;power 347 muW;voltage 1.8 V;size 0.18
  mum;Switches;Flip-flops;Clocks;MOS devices;Topology;Dual pulse
  generator;sense amplifier flip-flop;Static pulsed flip-flop;NAND keeper
  flip-flop;Pulsed flip-flop}
    \strng{namehash}{BDPMGK1}
    \strng{fullhash}{BDPMGK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Flip-flops are critical timing elements in digital circuits which have a
  large impact on circuit speed and power consumption. The performance of the
  Flip-Flop is an important element to determine the performance of the whole
  synchronous circuit. The pulse generator can be shared among many flip-flops
  to reduce the power dissipation. Firstly, in the Dual edge static pulsed
  flip-flop suffers from high leakage current leads to more power consumption.
  Secondly, Dual edge trigger sense amplifier flip-flop having unnecessary
  transitions which causes power consumption. Thirdly, Dual edge trigger NAND
  keeper flip-flop keeper technique is used to pull up the voltage to VDD
  having full swing and this keeper transistor width is high and which consumes
  more power. The power consumption of the Dual edge nand keeper flip-flop is
  347uW. Lastly, Dual edge trigger pulsed flip-flop is introduced by employing
  a technique called conditional switching for further power reduction. The
  circuits are designed in a 0.18-um standard CMOS process with a 1.8V power
  supply voltage.%
    }
    \field{booktitle}{IEEE-International Conference On Advances In Engineering,
  Science And Management (ICAESM -2012)}
    \field{pages}{63\bibrangedash 67}
    \field{title}{Low power dual edge triggered flip-flop}
    \field{year}{2012}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8398183}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LS}{%
         family={{Liu}},
         familyi={L\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LG}{%
         family={{Liao}},
         familyi={L\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=DY}{%
         family={{Ding}},
         familyi={D\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{NN-Fin; SUNA; nonlinear dynamical systems;pricing;recurrent neural
  nets;stock markets;time series;homogeneous time process;time recurrent neural
  network;time series;LSTM neural network algorithm;LSTM recurrent neural
  networks;stock price volatility;nonlinear dynamic system;stock transaction
  prediction modeling;stock trading volume;stock self correlation;term memory
  long short;feature value extraction;stock data analysis;Predictive
  models;Logic gates;Computational modeling;Indexes;Neural networks;Time series
  analysis;Feature extraction;machine learning;neural network;stock transaction
  prediction;LSTM}
    \strng{namehash}{LSLGDY1}
    \strng{fullhash}{LSLGDY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Stock price volatility is a highly complex nonlinear dynamic system. The
  stock's trading volume affects the stock's self correlation, self correlation
  and inertial effect, and the adjustment of the stock is not to advance with a
  homogeneous time process, which has its own independent time to promote the
  process. LSTM (Term Memory Long-Short) is a kind of time recurrent neural
  network, which is suitable for processing and predicting the important events
  of interval and long delay in time series. Based on temporal characteristics
  of stock and LSTM neural network algorithm, this paper uses the LSTM
  recurrent neural networks to filter, extract feature value and analyze the
  stock data, and set up the the prediction model of the corresponding stock
  transaction.%
    }
    \field{booktitle}{2018 13th IEEE Conference on Industrial Electronics and
  Applications (ICIEA)}
    \verb{doi}
    \verb 10.1109/ICIEA.2018.8398183
    \endverb
    \field{issn}{2158-2297}
    \field{pages}{2787\bibrangedash 2790}
    \field{title}{Stock transaction prediction modeling and analysis based on
  LSTM}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Han:2016:EEI:3001136.3001163}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=HS}{%
         family={Han},
         familyi={H\bibinitperiod},
         given={Song},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LX}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Xingyu},
         giveni={X\bibinitperiod},
      }}%
      {{hash=MH}{%
         family={Mao},
         familyi={M\bibinitperiod},
         given={Huizi},
         giveni={H\bibinitperiod},
      }}%
      {{hash=PJ}{%
         family={Pu},
         familyi={P\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=PA}{%
         family={Pedram},
         familyi={P\bibinitperiod},
         given={Ardavan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HMA}{%
         family={Horowitz},
         familyi={H\bibinitperiod},
         given={Mark\bibnamedelima A.},
         giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=DWJ}{%
         family={Dally},
         familyi={D\bibinitperiod},
         given={William\bibnamedelima J.},
         giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IEEE Press}%
    }
    \keyw{FPGA-NN, ASIC, algorithm-hardware co-design, deep learning, hardware
  acceleration, model compression}
    \strng{namehash}{HS+1}
    \strng{fullhash}{HSLXMHPJPAHMADWJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 43rd International Symposium on
  Computer Architecture}
    \verb{doi}
    \verb 10.1109/ISCA.2016.30
    \endverb
    \field{isbn}{978-1-4673-8947-1}
    \field{pages}{243\bibrangedash 254}
    \field{series}{ISCA '16}
    \field{title}{EIE: Efficient Inference Engine on Compressed Deep Neural
  Network}
    \verb{url}
    \verb https://doi.org/10.1109/ISCA.2016.30
    \endverb
    \list{location}{1}{%
      {Seoul, Republic of Korea}%
    }
    \field{year}{2016}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{Kassahun07commongenetic}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=KY}{%
         family={Kassahun},
         familyi={K\bibinitperiod},
         given={Yohannes},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=SG}{%
         family={Sommer},
         familyi={S\bibinitperiod},
         given={Gerald},
         giveni={G\bibinitperiod},
      }}%
      {{hash=EM}{%
         family={Edgington},
         familyi={E\bibinitperiod},
         given={Mark},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MJH}{%
         family={Metzen},
         familyi={M\bibinitperiod},
         given={Jan\bibnamedelima Hendrik},
         giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=KF}{%
         family={Kirchner},
         familyi={K\bibinitperiod},
         given={Frank},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM Press}%
    }
    \keyw{Evolutionary, Genetic Encoding, Genotype Phenotype Mapping}
    \strng{namehash}{KY+1}
    \strng{fullhash}{KYSGEMMJHKF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{In Proceedings of the Genetic and Evolutionary
  Computation Conference (GECCO 2007}
    \field{pages}{1029\bibrangedash 1036}
    \field{title}{Common genetic encoding for both direct and indirect
  encodings of networks}
    \field{year}{2007}
  \endentry

  \entry{6772729}{article}{}
    \name{author}{1}{}{%
      {{hash=HRW}{%
         family={{Hamming}},
         familyi={H\bibinitperiod},
         given={R.\bibnamedelima W.},
         giveni={R\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \keyw{Other, Hamming Distance, Error detection}
    \strng{namehash}{HRW1}
    \strng{fullhash}{HRW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The author was led to the study given in this paper from a consideration of
  large scale computing machines in which a large number of operations must be
  performed without a single error in the end result. This problem of ``doing
  things right'' on a large scale is not essentially new; in a telephone
  central office, for example, a very large number of operations are performed
  while the errors leading to wrong numbers are kept well under control, though
  they have not been completely eliminated. This has been achieved, in part,
  through the use of self-checking circuits. The occasional failure that
  escapes routine checking is still detected by the customer and will, if it
  persists, result in customer complaint, while if it is transient it will
  produce only occasional wrong numbers. At the same time the rest of the
  central office functions satisfactorily. In a digital computer, on the other
  hand, a single failure usually means the complete failure, in the sense that
  if it is detected no more computing can be done until the failure is located
  and corrected, while if it escapes detection then it invalidates all
  subsequent operations of the machine. Put in other words, in a telephone
  central office there are a number of parallel paths which are more or less
  independent of each other; in a digital machine there is usually a single
  long path which passes through the same piece of equipment many, many times
  before the answer is obtained.%
    }
    \verb{doi}
    \verb 10.1002/j.1538-7305.1950.tb00463.x
    \endverb
    \field{issn}{0005-8580}
    \field{number}{2}
    \field{pages}{147\bibrangedash 160}
    \field{title}{Error detecting and error correcting codes}
    \field{volume}{29}
    \field{journaltitle}{The Bell System Technical Journal}
    \field{year}{1950}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{pmlr-v80-kleinberg18a}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KB}{%
         family={Kleinberg},
         familyi={K\bibinitperiod},
         given={Bobby},
         giveni={B\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Yuanzhi},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YY}{%
         family={Yuan},
         familyi={Y\bibinitperiod},
         given={Yang},
         giveni={Y\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=DJ}{%
         family={Dy},
         familyi={D\bibinitperiod},
         given={Jennifer},
         giveni={J\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Krause},
         familyi={K\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \keyw{Other, SDG, Stochastic Gradient Descent, Local Minima}
    \strng{namehash}{KBLYYY1}
    \strng{fullhash}{KBLYYY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Stochastic gradient descent (SGD) is widely used in machine learning.
  Although being commonly viewed as a fast but not accurate version of gradient
  descent (GD), it always finds better solutions than GD for modern neural
  networks. In order to understand this phenomenon, we take an alternative view
  that SGD is working on the convolved (thus smoothed) version of the loss
  function. We show that, even if the function $f$ has many bad local minima or
  saddle points, as long as for every point $x$, the weighted average of the
  gradients of its neighborhoods is one point convex with respect to the
  desired solution $x^*$, SGD will get close to, and then stay around $x^*$
  with constant probability. Our result identifies a set of functions that SGD
  provably works, which is much larger than the set of convex functions.
  Empirically, we observe that the loss surface of neural networks enjoys nice
  one point convexity properties locally, therefore our theorem helps explain
  why SGD works so well for neural networks.%
    }
    \field{pages}{2698\bibrangedash 2707}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{An Alternative View: When Does {SGD} Escape Local Minima?}
    \verb{url}
    \verb http://proceedings.mlr.press/v80/kleinberg18a.html
    \endverb
    \field{volume}{80}
    \list{location}{1}{%
      {Stockholmsm{\"a}ssan, Stockholm Sweden}%
    }
    \verb{file}
    \verb http://proceedings.mlr.press/v80/kleinberg18a/kleinberg18a.pdf
    \endverb
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{gRPCPage}{online}{}
    \name{author}{1}{}{%
      {{hash=FL}{%
         family={Foundation},
         familyi={F\bibinitperiod},
         given={Linux},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{URL}
    \strng{namehash}{FL1}
    \strng{fullhash}{FL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{gRPC}
    \verb{url}
    \verb https://grpc.io
    \endverb
    \field{year}{2020}
  \endentry

  \entry{4250190}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=CL}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=AD}{%
         family={Alahakoon},
         familyi={A\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; backpropagation;neural nets;pattern
  classification;search problems;topology;neuroevolution of augmenting
  topologies;augmenting topology;data classification learning;neural
  network;learning-NEAT training scheme;backpropagation;search problem;Network
  topology;Artificial neural networks;Neural networks;Supervised
  learning;Biological cells;Information technology;Evolutionary
  computation;Unsupervised learning;Technological innovation}
    \strng{namehash}{CLAD1}
    \strng{fullhash}{CLAD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Appropriate topology and connection weight are two very important
  properties a neural network must have in order to successfully perform data
  classification. In this paper, we propose a hybrid training scheme
  Learning-NEAT (L-NEAT) for data classification problem. L-NEAT simplifies
  evolution by dividing the complete problem domain into sub tasks and learn
  the sub tasks by incorporating back propagation rule into the NeuroEvolution
  of Augmenting Topologies (NEAT) algorithm. The new algorithm combines the
  strength of searching for topology and weights from NEAT and back propagation
  respectively while overcoming problems associated with direct use of NEAT. We
  claim that L-NEAT can produce neural network for classification problem
  effectively and efficiently. Empirical evaluation shows that L-NEAT evolves
  classifying neural network with good generalization ability. Its accuracy
  outperforms original NEAT.%
    }
    \field{booktitle}{2006 International Conference on Information and
  Automation}
    \verb{doi}
    \verb 10.1109/ICINFA.2006.374100
    \endverb
    \field{issn}{2151-1802}
    \field{pages}{367\bibrangedash 371}
    \field{title}{NeuroEvolution of Augmenting Topologies with Learning for
  Data Classification}
    \field{year}{2006}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{needle-python}{misc}{}
    \keyw{SourceCode, Python, DQN, TRPO}
    \field{labeltitlesource}{title}
    \field{title}{Needle Deep RL Examples}
    \verb{url}
    \verb https://github.com/roosephu/needle
    \endverb
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{BiSUNAGithub}{online}{}
    \name{author}{1}{}{%
      {{hash=VR}{%
         family={Valencia},
         familyi={V\bibinitperiod},
         given={Raul},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{BiSUNA,}
    \strng{namehash}{VR2}
    \strng{fullhash}{VR2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Binary Spectrum-diverse Unified Neuroevolution Architecture}
    \verb{url}
    \verb https://github.com/rval735/BiSUNA
    \endverb
    \field{year}{2019}
    \warn{\item Invalid format of field 'month' \item Invalid format of field
  'urldate'}
  \endentry

  \entry{Moore-1991-13223}{thesis}{}
    \name{author}{1}{}{%
      {{hash=MA}{%
         family={Moore},
         familyi={M\bibinitperiod},
         given={Andrew},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{NN, SUNA, Reinforcement Learning, Mountain Car}
    \strng{namehash}{MA1}
    \strng{fullhash}{MA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Efficient Memory-based Learning for Robot Control}
    \list{location}{1}{%
      {Pittsburgh, PA}%
    }
    \list{institution}{1}{%
      {Carnegie Mellon University}%
    }
    \field{type}{phdthesis}
    \field{year}{1991}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Strens:2000:BFR:645529.658114}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=SMJA}{%
         family={Strens},
         familyi={S\bibinitperiod},
         given={Malcolm J.\bibnamedelima A.},
         giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  A\bibinitperiod},
      }}%
    }
    \keyw{NN, Reinforcement Learning, OpenAI Gym}
    \strng{namehash}{SMJA1}
    \strng{fullhash}{SMJA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the Seventeenth International Conference
  on Machine Learning}
    \field{isbn}{1-55860-707-2}
    \field{pages}{943\bibrangedash 950}
    \field{series}{ICML '00}
    \field{title}{A Bayesian Framework for Reinforcement Learning}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=645529.658114
    \endverb
    \field{annotation}{%
    Publisher Morgan Kaufmann Publishers Inc. Address San Francisco, CA, USA%
    }
    \field{year}{2000}
  \endentry

  \entry{nchainv0OpenAI}{online}{}
    \name{author}{1}{}{%
      {{hash=b}{%
         family={blole},
         familyi={b\bibinitperiod},
      }}%
    }
    \keyw{WebPage, OpenAI Gym, SUNA}
    \strng{namehash}{b1}
    \strng{fullhash}{b1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Algorithm on NChain-v0}
    \verb{url}
    \verb https://gym.openai.com/evaluations/eval_8KB72MuwRqqdbRG4UMs1w/
    \endverb
    \field{year}{2016}
  \endentry

  \entry{rouletteV0OpenAI}{online}{}
    \name{author}{1}{}{%
      {{hash=t}{%
         family={tanemaki},
         familyi={t\bibinitperiod},
      }}%
    }
    \keyw{WebPage, OpenAI Gym, Reinforcement Learning, BiSUNA}
    \strng{namehash}{t1}
    \strng{fullhash}{t1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Algorithm on Roulette-v0}
    \verb{url}
    \verb https://gym.openai.com/evaluations/eval_DFWdtrdSCikuZWwf8HN8A/
    \endverb
    \field{year}{2016}
  \endentry

  \entry{duplicatedOpenAIGym}{online}{}
    \name{author}{1}{}{%
      {{hash=c}{%
         family={colinmorris},
         familyi={c\bibinitperiod},
      }}%
    }
    \keyw{WebPage, OpenAI Gym, Reinforcement Learning, BiSUNA}
    \strng{namehash}{c1}
    \strng{fullhash}{c1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Algorithm on DuplicatedInput-v0}
    \verb{url}
    \verb https://gym.openai.com/evaluations/eval_5PlrBv8wRNGI2J1lp3otUA/
    \endverb
    \field{year}{2016}
  \endentry

  \entry{oaiBaseline-Github}{misc}{}
    \name{author}{1}{}{%
      {{hash=O}{%
         family={OpenAI},
         familyi={O\bibinitperiod},
      }}%
    }
    \keyw{OpenAI Gym, Baselines, Github}
    \strng{namehash}{O1}
    \strng{fullhash}{O1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Baselines}
    \verb{url}
    \verb https://github.com/openai/baselines
    \endverb
  \endentry

  \entry{BiSUNAOCLGithub}{online}{}
    \name{author}{1}{}{%
      {{hash=VR}{%
         family={Valencia},
         familyi={V\bibinitperiod},
         given={Raul},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{BiSUNA, OCL}
    \strng{namehash}{VR2}
    \strng{fullhash}{VR2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Binary Spectrum-diverse Unified Neuroevolution Architecture}
    \verb{url}
    \verb github.com/rval735/bisunaocl
    \endverb
    \field{year}{2019}
    \warn{\item Invalid format of field 'month' \item Invalid format of field
  'urldate'}
  \endentry

  \entry{OpenVINOStarterKit}{misc}{}
    \name{author}{1}{}{%
      {{hash=T}{%
         family={Terasic},
         familyi={T\bibinitperiod},
      }}%
    }
    \keyw{Terasic, OpenVINO, FPGA}
    \strng{namehash}{T1}
    \strng{fullhash}{T1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Starter Platform for OpenVINO{\texttrademark} Toolkit}
    \verb{url}
    \verb https://www.terasic.com.tw/cgi-bin/page/archive.pl?Language=English&C
    \verb ategoryNo=167&No=1159
    \endverb
    \field{urlyear}{2013}
  \endentry

  \entry{Moussa2006}{inbook}{}
    \name{author}{3}{}{%
      {{hash=MM}{%
         family={Moussa},
         familyi={M\bibinitperiod},
         given={Medhat},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AS}{%
         family={Areibi},
         familyi={A\bibinitperiod},
         given={Shawki},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NK}{%
         family={Nichols},
         familyi={N\bibinitperiod},
         given={Kristian},
         giveni={K\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, Book Chapter}
    \strng{namehash}{MMASNK1}
    \strng{fullhash}{MMASNK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Artificial Neural Networks (ANNs) are inherently parallel architectures
  which represent a natural fit for custom implementation on FPGAs. One
  important implementation issue is to determine the numerical precision format
  that allows an optimum tradeoff between precision and implementation areas.
  Standard single or double precision floating-point representations minimize
  quantization errors while requiring significant hardware resources. Less
  precise fixed-point representation may require less hardware resources but
  add quantization errors that may prevent learning from taking place,
  especially in regression problems. This chapter examines this issue and
  reports on a recent experiment where we implemented a Multi-layer perceptron
  (MLP) on an FPGA using both fixed and floating point precision. Results show
  that the fixed-point MLP implementation was over 12x greater in speed, over
  13x smaller in area, and achieves far greater processing density compared to
  the floating-point FPGA-based MLP.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_2
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{37\bibrangedash 61}
    \field{title}{On the Arithmetic Precision for Implementing Back-Propagation
  Networks on FPGA: A Case Study}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_2
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{Zhang:2015:OFA:2684746.2689060}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=ZC}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Chen},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Peng},
         giveni={P\bibinitperiod},
      }}%
      {{hash=SG}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Guangyu},
         giveni={G\bibinitperiod},
      }}%
      {{hash=GY}{%
         family={Guan},
         familyi={G\bibinitperiod},
         given={Yijin},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=XB}{%
         family={Xiao},
         familyi={X\bibinitperiod},
         given={Bingjun},
         giveni={B\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Cong},
         familyi={C\bibinitperiod},
         given={Jason},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{Acceleration, convolutional neural network, fpga, roofline model}
    \strng{namehash}{ZC+2}
    \strng{fullhash}{ZCLPSGGYXBCJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 2015 ACM/SIGDA International Symposium
  on Field-Programmable Gate Arrays}
    \verb{doi}
    \verb 10.1145/2684746.2689060
    \endverb
    \field{isbn}{978-1-4503-3315-3}
    \field{pages}{161\bibrangedash 170}
    \field{series}{FPGA '15}
    \field{title}{Optimizing FPGA-based Accelerator Design for Deep
  Convolutional Neural Networks}
    \verb{url}
    \verb http://doi.acm.org/10.1145/2684746.2689060
    \endverb
    \list{location}{1}{%
      {Monterey, California, USA}%
    }
    \field{year}{2015}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{Zhao:2017:ABC:3020078.3021741}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=ZR}{%
         family={Zhao},
         familyi={Z\bibinitperiod},
         given={Ritchie},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SW}{%
         family={Song},
         familyi={S\bibinitperiod},
         given={Weinan},
         giveni={W\bibinitperiod},
      }}%
      {{hash=ZW}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Wentao},
         giveni={W\bibinitperiod},
      }}%
      {{hash=XT}{%
         family={Xing},
         familyi={X\bibinitperiod},
         given={Tianwei},
         giveni={T\bibinitperiod},
      }}%
      {{hash=LJH}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Jeng-Hau},
         giveni={J\bibinithyphendelim H\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Srivastava},
         familyi={S\bibinitperiod},
         given={Mani},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Gupta},
         familyi={G\bibinitperiod},
         given={Rajesh},
         giveni={R\bibinitperiod},
      }}%
      {{hash=ZZ}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Zhiru},
         giveni={Z\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{FPGAs, binarized, binarized convolutional networks, deep learning,
  high-level synthesis, reconfigurable computing}
    \strng{namehash}{ZR+1}
    \strng{fullhash}{ZRSWZWXTLJHSMGRZZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 2017 ACM/SIGDA International Symposium
  on Field-Programmable Gate Arrays}
    \verb{doi}
    \verb 10.1145/3020078.3021741
    \endverb
    \field{isbn}{978-1-4503-4354-1}
    \field{pages}{15\bibrangedash 24}
    \field{series}{FPGA '17}
    \field{title}{Accelerating Binarized Convolutional Neural Networks with
  Software-Programmable FPGAs}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3020078.3021741
    \endverb
    \field{annotation}{%
    Location Monterey, California, USA Address New York, NY, USA%
    }
    \field{year}{2017}
  \endentry

  \entry{LIANG20181072}{article}{}
    \name{author}{5}{}{%
      {{hash=LS}{%
         family={Liang},
         familyi={L\bibinitperiod},
         given={Shuang},
         giveni={S\bibinitperiod},
      }}%
      {{hash=YS}{%
         family={Yin},
         familyi={Y\bibinitperiod},
         given={Shouyi},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LL}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Leibo},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LW}{%
         family={Luk},
         familyi={L\bibinitperiod},
         given={Wayne},
         giveni={W\bibinitperiod},
      }}%
      {{hash=WS}{%
         family={Wei},
         familyi={W\bibinitperiod},
         given={Shaojun},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{BNN, Binarized neural network, Hardware accelerator, FPGA}
    \strng{namehash}{LS+2}
    \strng{fullhash}{LSYSLLLWWS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb https://doi.org/10.1016/j.neucom.2017.09.046
    \endverb
    \field{issn}{0925-2312}
    \field{pages}{1072 \bibrangedash  1086}
    \field{title}{FP-BNN: Binarized neural network on FPGA}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0925231217315655
    \endverb
    \field{volume}{275}
    \field{journaltitle}{Neurocomputing}
    \field{year}{2018}
  \endentry

  \entry{Gomez-Pulido2011}{article}{}
    \name{author}{5}{}{%
      {{hash=GPJA}{%
         family={Gomez-Pulido},
         familyi={G\bibinithyphendelim P\bibinitperiod},
         given={Juan\bibnamedelima A.},
         giveni={J\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=VRMA}{%
         family={Vega-Rodriguez},
         familyi={V\bibinithyphendelim R\bibinitperiod},
         given={Miguel\bibnamedelima A.},
         giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=SPJM}{%
         family={Sanchez-Perez},
         familyi={S\bibinithyphendelim P\bibinitperiod},
         given={Juan\bibnamedelima M.},
         giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=PMS}{%
         family={Priem-Mendes},
         familyi={P\bibinithyphendelim M\bibinitperiod},
         given={Silvio},
         giveni={S\bibinitperiod},
      }}%
      {{hash=CV}{%
         family={Carreira},
         familyi={C\bibinitperiod},
         given={Vitor},
         giveni={V\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, FPGA, GPU, floating-point}
    \strng{namehash}{GPJA+1}
    \strng{fullhash}{GPJAVRMASPJMPMSCV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Many large combinatorial optimization problems tackled with evolutionary
  algorithms often require very high computational times, usually due to the
  fitness evaluation. This fact forces programmers to use clusters of
  computers, a computational solution very useful for running applications of
  intensive calculus but having a high acquisition price and operation cost,
  mainly due to the Central Processing Unit (CPU) power consumption and
  refrigeration devices. A low-cost and high-performance alternative comes from
  reconfigurable computing, a hardware technology based on Field Programmable
  Gate Array devices (FPGAs). The main objective of the work presented in this
  paper is to compare implementations on FPGAs and CPUs of different fitness
  functions in evolutionary algorithms in order to study the performance of the
  floating-point arithmetic in FPGAs and CPUs that is often present in the
  optimization problems tackled by these algorithms. We have taken advantage of
  the parallelism at chip-level of FPGAs pursuing the acceleration of the
  fitness functions (and consequently, of the evolutionary algorithms) and
  showing the parallel scalability to reach low cost, low power and high
  performance computational solutions based on FPGA. Finally, the recent
  popularity of GPUs as computational units has moved us to introduce these
  devices in our performance comparisons. We analyze performance in terms of
  computation times and economic cost.%
    }
    \verb{doi}
    \verb 10.1007/s10710-011-9137-2
    \endverb
    \field{issn}{1573-7632}
    \field{number}{4}
    \field{pages}{403\bibrangedash 427}
    \field{title}{Accelerating floating-point fitness functions in evolutionary
  algorithms: a FPGA-CPU-GPU performance comparison}
    \verb{url}
    \verb https://doi.org/10.1007/s10710-011-9137-2
    \endverb
    \field{volume}{12}
    \field{journaltitle}{Genetic Programming and Evolvable Machines}
    \field{year}{2011}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Nurvitadhi:2017:FBG:3020078.3021740}{inproceedings}{}
    \name{author}{11}{}{%
      {{hash=NE}{%
         family={Nurvitadhi},
         familyi={N\bibinitperiod},
         given={Eriko},
         giveni={E\bibinitperiod},
      }}%
      {{hash=VG}{%
         family={Venkatesh},
         familyi={V\bibinitperiod},
         given={Ganesh},
         giveni={G\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sim},
         familyi={S\bibinitperiod},
         given={Jaewoong},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MD}{%
         family={Marr},
         familyi={M\bibinitperiod},
         given={Debbie},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HR}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Randy},
         giveni={R\bibinitperiod},
      }}%
      {{hash=OGHJ}{%
         family={Ong Gee\bibnamedelima Hock},
         familyi={O\bibinitperiod\bibinitdelim G\bibinitperiod\bibinitdelim
  H\bibinitperiod},
         given={Jason},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LYT}{%
         family={Liew},
         familyi={L\bibinitperiod},
         given={Yeong\bibnamedelima Tat},
         giveni={Y\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Srivatsan},
         familyi={S\bibinitperiod},
         given={Krishnan},
         giveni={K\bibinitperiod},
      }}%
      {{hash=MD}{%
         family={Moss},
         familyi={M\bibinitperiod},
         given={Duncan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Subhaschandra},
         familyi={S\bibinitperiod},
         given={Suchit},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BG}{%
         family={Boudoukh},
         familyi={B\bibinitperiod},
         given={Guy},
         giveni={G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{FPGA-NN; FPGA, GPU, accelerator, deep learning, intel stratix 10}
    \strng{namehash}{NE+2}
    \strng{fullhash}{NEVGSJMDHROGHJLYTSKMDSSBG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Proceedings of the 2017 ACM/SIGDA International Symposium
  on Field-Programmable Gate Arrays}
    \verb{doi}
    \verb 10.1145/3020078.3021740
    \endverb
    \field{isbn}{978-1-4503-4354-1}
    \field{pages}{5\bibrangedash 14}
    \field{series}{FPGA '17}
    \field{title}{Can FPGAs Beat GPUs in Accelerating Next-Generation Deep
  Neural Networks?}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3020078.3021740
    \endverb
    \list{location}{1}{%
      {Monterey, California, USA}%
    }
    \field{year}{2017}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{Zhao:2016aa}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=ZW}{%
         family={Zhao},
         familyi={Z\bibinitperiod},
         given={Wenlai},
         giveni={W\bibinitperiod},
      }}%
      {{hash=FH}{%
         family={Fu},
         familyi={F\bibinitperiod},
         given={Haohuan},
         giveni={H\bibinitperiod},
      }}%
      {{hash=LW}{%
         family={Luk},
         familyi={L\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=YT}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Teng},
         giveni={T\bibinitperiod},
      }}%
      {{hash=WS}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Shaojun},
         giveni={S\bibinitperiod},
      }}%
      {{hash=FB}{%
         family={Feng},
         familyi={F\bibinitperiod},
         given={Bo},
         giveni={B\bibinitperiod},
      }}%
      {{hash=MY}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={Yuchun},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YG}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Guangwen},
         giveni={G\bibinitperiod},
      }}%
    }
    \keyw{field programmable gate arrays; floating point arithmetic; neural
  nets; 32bit floating-point arithmetic; FPGA-based framework; bandwidth
  resources; convolutional neural networks; hardware resources; streaming
  datapath; Bandwidth; Computational modeling; Convolution; Field programmable
  gate arrays; Neural networks; Runtime; Training}
    \strng{namehash}{ZW+2}
    \strng{fullhash}{ZWFHLWYTWSFBMYYG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{2016 IEEE 27th International Conference on
  Application-specific Systems, Architectures and Processors (ASAP)}
    \verb{doi}
    \verb 10.1109/ASAP.2016.7760779
    \endverb
    \field{pages}{107\bibrangedash 114}
    \field{title}{F-CNN: An FPGA-based framework for training Convolutional
  Neural Networks}
    \field{journaltitle}{2016 IEEE 27th International Conference on
  Application-specific Systems, Architectures and Processors (ASAP)}
    \field{year}{2016}
  \endentry

  \entry{2017arXiv170303864S}{article}{}
    \name{author}{5}{}{%
      {{hash=ST}{%
         family={{Salimans}},
         familyi={S\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HJ}{%
         family={{Ho}},
         familyi={H\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=CX}{%
         family={{Chen}},
         familyi={C\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={{Sidor}},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SI}{%
         family={{Sutskever}},
         familyi={S\bibinitperiod},
         given={I.},
         giveni={I\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; Statistics - Machine Learning, Computer Science -
  Artificial Intelligence, Computer Science - Machine Learning, Computer
  Science - Neural and Evolutionary Computing; OpenAI}
    \strng{namehash}{ST+1}
    \strng{fullhash}{STHJCXSSSI1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1703.03864
    \endverb
    \field{title}{{Evolution Strategies as a Scalable Alternative to
  Reinforcement Learning}}
    \field{journaltitle}{ArXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{stat.ML}
    \field{month}{03}
    \field{year}{2017}
  \endentry

  \entry{10.1007/978-3-642-24013-3_11}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LI}{%
         family={L{\H{o}}rentz},
         familyi={L\bibinitperiod},
         given={Istv{\'a}n},
         giveni={I\bibinitperiod},
      }}%
      {{hash=AR}{%
         family={Andonie},
         familyi={A\bibinitperiod},
         given={R{\u{a}}zvan},
         giveni={R\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Mali{\c{T}}a},
         familyi={M\bibinitperiod},
         given={Mihaela},
         giveni={M\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=BFMT}{%
         family={Brazier},
         familyi={B\bibinitperiod},
         given={F.\bibnamedelima M.\bibnamedelima T.},
         giveni={F\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim
  T\bibinitperiod},
      }}%
      {{hash=NK}{%
         family={Nieuwenhuis},
         familyi={N\bibinitperiod},
         given={Kees},
         giveni={K\bibinitperiod},
      }}%
      {{hash=PG}{%
         family={Pavlin},
         familyi={P\bibinitperiod},
         given={Gregor},
         giveni={G\bibinitperiod},
      }}%
      {{hash=WM}{%
         family={Warnier},
         familyi={W\bibinitperiod},
         given={Martijn},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Badica},
         familyi={B\bibinitperiod},
         given={Costin},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Books, Evolutionary, Intelligent Distributed Computing V, OpenCL,
  Evolutionary algorithms, Genetic Algorithm}
    \strng{namehash}{LIARMM1}
    \strng{fullhash}{LIARMM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    We discuss the parallel implementation of Genetic Algorithms and Evolution
  Strategy on General-Purpose Graphical Units, using the OpenCL framework.
  Multiple evolutionary operators are tested (tournament, roulette wheel
  selection, uniform and Gaussian mutation, crossover, recombination), as well
  as different approaches for parallelism, for small and large problem sizes.
  We use the Island Model of Parallel GA, with random migration. Performance is
  measured using two graphic cards: NVidia GeForce GTX 560Ti and AMD Radeon
  6950. Tests are performed in a distributed grid, using the Java Parallel
  Processing Framework.%
    }
    \field{booktitle}{Intelligent Distributed Computing V}
    \field{isbn}{978-3-642-24013-3}
    \field{pages}{103\bibrangedash 113}
    \field{title}{An Implementation of Evolutionary Computation Operators in
  OpenCL}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{annotation}{%
    Inside book "Intelligent Distributed Computing V"%
    }
    \field{year}{2012}
  \endentry

  \entry{Grozea2010}{inbook}{}
    \name{author}{3}{}{%
      {{hash=GC}{%
         family={Grozea},
         familyi={G\bibinitperiod},
         given={Cristian},
         giveni={C\bibinitperiod},
      }}%
      {{hash=BZ}{%
         family={Bankovic},
         familyi={B\bibinitperiod},
         given={Zorana},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Laskov},
         familyi={L\bibinitperiod},
         given={Pavel},
         giveni={P\bibinitperiod},
      }}%
    }
    \name{editor}{3}{}{%
      {{hash=KR}{%
         family={Keller},
         familyi={K\bibinitperiod},
         given={Rainer},
         giveni={R\bibinitperiod},
      }}%
      {{hash=KD}{%
         family={Kramer},
         familyi={K\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=WJP}{%
         family={Weiss},
         familyi={W\bibinitperiod},
         given={Jan-Philipp},
         giveni={J\bibinithyphendelim P\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Books, FPGA, GPU, CPU, Sorting Algorithm}
    \strng{namehash}{GCBZLP1}
    \strng{fullhash}{GCBZLP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Currently there are several interesting alternatives for low-cost
  high-performance computing. We report here our experiences with an N-gram
  extraction and sorting problem, originated in the design of a real-time
  network intrusion detection system. We have considered FPGAs, multi-core CPUs
  in symmetric multi-CPU machines and GPUs and have created implementations for
  each of these platforms. After carefully comparing the advantages and
  disadvantages of each we have decided to go forward with the implementation
  written for multi-core CPUs. Arguments for and against each platform are
  presented -- corresponding to our hands-on experience -- that we intend to be
  useful in helping with the selection of the hardware acceleration solutions
  for new projects.%
    }
    \field{booktitle}{Facing the Multicore-Challenge: Aspects of New Paradigms
  and Technologies in Parallel Computing}
    \verb{doi}
    \verb 10.1007/978-3-642-16233-6_12
    \endverb
    \field{isbn}{978-3-642-16233-6}
    \field{pages}{105\bibrangedash 117}
    \field{title}{FPGA vs. Multi-core CPUs vs. GPUs: Hands-On Experience with a
  Sorting Application}
    \verb{url}
    \verb https://doi.org/10.1007/978-3-642-16233-6_12
    \endverb
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{2010}
  \endentry

  \entry{6643571}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=ZH}{%
         family={{Zhou}},
         familyi={Z\bibinitperiod},
         given={Hongxia},
         giveni={H\bibinitperiod},
      }}%
      {{hash=C}{%
         family={{Chiu-Wing Sham}},
         familyi={C\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={{Yao}},
         familyi={Y\bibinitperiod},
         given={Hailong},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{FPGAGroup;analogue integrated circuits;integrated circuit
  design;integrated circuit layout;mixed analogue-digital integrated
  circuits;VLSI;dummy nodes;constraints edges;expansion process;congestion
  probability model;subsequent channel routing;net congestion
  probability;routability driven placement;very-large-scale
  integration;VLSI;placement design;mixed signal circuits;analog
  circuits;congestion oriented approach;Routing;Probabilistic
  logic;Compaction;Arrays;Educational institutions;Very large scale
  integration;Pins}
    \strng{namehash}{ZHCYH1}
    \strng{fullhash}{ZHCYH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Fifth Asia Symposium on Quality Electronic Design (ASQED
  2013)}
    \verb{doi}
    \verb 10.1109/ASQED.2013.6643571
    \endverb
    \field{pages}{97\bibrangedash 102}
    \field{title}{{Congestion-oriented approach in placement for analog and
  mixed-signal circuits}}
    \field{year}{2013}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Shackleford2001}{article}{}
    \name{author}{7}{}{%
      {{hash=SB}{%
         family={Shackleford},
         familyi={S\bibinitperiod},
         given={Barry},
         giveni={B\bibinitperiod},
      }}%
      {{hash=SG}{%
         family={Snider},
         familyi={S\bibinitperiod},
         given={Greg},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CRJ}{%
         family={Carter},
         familyi={C\bibinitperiod},
         given={Richard\bibnamedelima J.},
         giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=OE}{%
         family={Okushi},
         familyi={O\bibinitperiod},
         given={Etsuko},
         giveni={E\bibinitperiod},
      }}%
      {{hash=YM}{%
         family={Yasuda},
         familyi={Y\bibinitperiod},
         given={Mitsuhiro},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Seo},
         familyi={S\bibinitperiod},
         given={Katsuhiko},
         giveni={K\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={Yasuura},
         familyi={Y\bibinitperiod},
         given={Hiroto},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{FPGA, GA, Pipeline, HPC}
    \strng{namehash}{SB+1}
    \strng{fullhash}{SBSGCRJOEYMSKYH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Accelerating a genetic algorithm (GA) by implementing it in a
  reconfigurable field programmable gate array (FPGA) is described. The
  implemented GA features: random parent selection, which conserves selection
  circuitry; a steady-state memory model, which conserves chip area; survival
  of fitter child chromosomes over their less-fit parent chromosomes, which
  promotes evolution. A net child chromosome generation rate of one per clock
  cycle is obtained by pipelining the parent selection, crossover, mutation,
  and fitness evaluation functions. Complex fitness functions can be further
  pipelined to maintain a high-speed clock cycle. Fitness functions with a
  pipeline initiation interval of greater than one can be plurally implemented
  to maintain a net evaluated-chromosome throughput of one per clock cycle. Two
  prototypes are described: The first prototype (c. 1996 technology) is a
  multiple-FPGA chip implementation, running at a 1 MHz clock rate, that solves
  a 94-row {\texttimes} 520-column set covering problem 2,200{\texttimes}
  faster than a 100 MHz workstation running the same algorithm in C. The second
  prototype (Xilinx XVC300) is a single-FPGA chip implementation, running at a
  66 MHZ clock rate, that solves a 36-residue protein folding problem in a 2-d
  lattice 320{\texttimes} faster than a 366 MHz Pentium II. The current largest
  FPGA (Xilinx XCV3200E) has circuitry available for the implementation of 30
  fitness function units which would yield an acceleration of 9,600{\texttimes}
  for the 36-residue protein folding problem.%
    }
    \verb{doi}
    \verb 10.1023/A:1010018632078
    \endverb
    \field{issn}{1573-7632}
    \field{number}{1}
    \field{pages}{33\bibrangedash 60}
    \field{title}{A High-Performance, Pipelined, FPGA-Based Genetic Algorithm
  Machine}
    \verb{url}
    \verb https://doi.org/10.1023/A:1010018632078
    \endverb
    \field{volume}{2}
    \field{journaltitle}{Genetic Programming and Evolvable Machines}
    \field{year}{2001}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{initializationIntervalXilinx}{misc}{}
    \name{author}{1}{}{%
      {{hash=X}{%
         family={Xilinx},
         familyi={X\bibinitperiod},
      }}%
    }
    \keyw{Webpage, Xilinx, FPGA,}
    \strng{namehash}{X1}
    \strng{fullhash}{X1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Loop Pipelining and Loop Unrolling}
    \verb{url}
    \verb https://www.xilinx.com/support/documentation/sw_manuals/xilinx2015_2/
    \verb sdsoc_doc/topics/calling-coding-guidelines/concept_pipelining_loop_un
    \verb rolling.html
    \endverb
    \field{year}{2020}
  \endentry

  \entry{1136}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=HSU}{%
         family={Hussain},
         familyi={H\bibinitperiod},
         given={Siam\bibnamedelima U.},
         giveni={S\bibinitperiod\bibinitdelim U\bibinitperiod},
      }}%
      {{hash=KF}{%
         family={Koushanfar},
         familyi={K\bibinitperiod},
         given={Farinaz},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{Crypto, garbled-circuit, Secure computation, FPGA}
    \strng{namehash}{HSUKF1}
    \strng{fullhash}{HSUKF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    <p>We present FASE, an FPGA accelerator for Secure Function Evaluation
  (SFE) by employing the well-known cryptographic protocol named Yao\&rsquo;s
  Garbled Circuit (GC). SFE allows two parties to jointly compute a function on
  their private data and learn the output without revealing their inputs to
  each other. FASE is designed to allow cloud servers to provide secure
  services to a large number of clients in parallel while preserving the
  privacy of the data from both sides. Current SFE accelerators either target
  specific applications, and therefore are not amenable to generic use, or have
  low throughput due to inefficient management of resources. In this work, we
  present a pipelined architecture along with an efficient scheduling scheme to
  ensure optimal usage of the available resources. The scheme is built around a
  simulator of the hardware design that schedules the workload and assigns the
  most suitable task to the encryption cores at each cycle. This, coupled with
  optimal management of the read and write cycles of the Block RAM on FPGA,
  results in a minimum 2 orders of magnitude improvement in terms of throughput
  per core for the reported benchmarks compared to the most recent generic GC
  accelerator. Moreover, our encryption core requires 17\% less resource
  compared to the most recent secure GC realization.\&nbsp;</p>%
    }
    \field{booktitle}{Field-Programmable Custom Computing Machines (FCCM)}
    \field{title}{FASE: FPGA Acceleration of Secure Function Evaluation}
    \list{location}{1}{%
      {San Diego}%
    }
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8425687}{article}{}
    \name{author}{2}{}{%
      {{hash=CHF}{%
         family={{Chou}},
         familyi={C\bibinitperiod},
         given={Hong-Fu},
         giveni={H\bibinithyphendelim F\bibinitperiod},
      }}%
      {{hash=C}{%
         family={{Chiu-Wing Sham}},
         familyi={C\bibinitperiod},
      }}%
    }
    \keyw{FPGAGroup;channel coding;error statistics;evolutionary
  computation;iterative decoding;parity check codes;runlength
  codes;optimization approach;RLL-constrained LDPC coded recording
  system;deliberate flipping;run-length-limited constraint;high error coding
  rate;correcting capability;RLL bit error;iterative decoding;hard error
  bits;differential evolution approach;unequal error protection LDPC
  code;optimal LDPC code distribution;density evolution;RLL flipped
  system;Decoding;Probability density function;Iterative
  decoding;Optimization;Error correction codes;Equalizers;Parity check
  codes;iterative decoding;partial response channels;error correction codes}
    \strng{namehash}{CHFC1}
    \strng{fullhash}{CHFC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{doi}
    \verb 10.1109/LCOMM.2018.2863363
    \endverb
    \field{issn}{1089-7798}
    \field{number}{10}
    \field{pages}{1976\bibrangedash 1979}
    \field{title}{{An Optimization Approach for an RLL-Constrained LDPC Coded
  Recording System Using Deliberate Flipping}}
    \field{volume}{22}
    \field{journaltitle}{IEEE Communications Letters}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{PETRICA2018251}{article}{}
    \name{author}{1}{}{%
      {{hash=PL}{%
         family={Petrica},
         familyi={P\bibinitperiod},
         given={Lucian},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{Crypto, NIST, STS, PRNG, FPGA, Cellular automaton, Random number
  generator}
    \strng{namehash}{PL1}
    \strng{fullhash}{PL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Pseudo-random number generators (PRNGs) are important to applications
  ranging from cryptography to Monte-Carlo methods. Consequently, many PRNG
  architectures have been proposed, including some optimized for FPGA, e.g the
  LUT-SR family of PRNGs which utilize embedded FPGA shift registers, and
  self-programmable cellular automaton (SPCA) PRNGs. However, LUT-SR and other
  PRNGs do not utilize key features of modern Xilinx FPGAs: embedded carry
  chains and splittable Look-Up Tables (LUTs), i.e., 6-input LUTs which can
  operate as two 5-input LUTs which share inputs. In this paper we explore the
  SPCA structure and derive a set of parameter constraints which allow a SPCA
  PRNG to produce 2 random bits per LUT in every clock cycle on modern Xilinx
  FPGAs. We determine this to be the maximum logic density achievable for SPCA,
  and propose an architectural improvement of SPCA to enable further density
  increase by making use of FPGA embedded carry chains as a method to compute
  an additional random bit per LUT in each clock cycle. The resulting
  Split-LUT-Carry SPCA (SLC-SPCA) PRNG achieves 6x improvement in logic density
  compared to LUT-SR, and a 1.5x density increase compared to SPCA. We evaluate
  the randomness of SLC-SPCA utilizing the NIST Statistical Test Suite, and we
  provide a power and energy comparison of LUT-SR and SLC-SPCA on a Xilinx Zynq
  7020 FPGA device. Our results indicate that SLC-SPCA generates 3x more bits
  per clock at approximately the same power dissipation as LUT-SR, and
  consequently 3x less energy to generate 1 gigabit of random data. SLC-SPCA is
  also 1.5x more energy-efficient than a SPCA PRNG.%
    }
    \verb{doi}
    \verb https://doi.org/10.1016/j.jpdc.2017.05.022
    \endverb
    \field{issn}{0743-7315}
    \field{pages}{251 \bibrangedash  259}
    \field{title}{FPGA optimized cellular automaton random number generator}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0743731517301892
    \endverb
    \field{volume}{111}
    \field{journaltitle}{Journal of Parallel and Distributed Computing}
    \field{year}{2018}
  \endentry

  \entry{Xilinx-Dev-Accel}{misc}{}
    \name{author}{1}{}{%
      {{hash=AR}{%
         family={Armstrong},
         familyi={A\bibinitperiod},
         given={Rob},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{URL, Xilinx, Tutorial, Developer, FPGA}
    \strng{namehash}{AR1}
    \strng{fullhash}{AR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Get Moving with Alveo}
    \verb{url}
    \verb https://developer.xilinx.com/en/articles/acceleration-basics.html
    \endverb
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{bisunaU50Git}{misc}{}
    \name{author}{1}{}{%
      {{hash=VR}{%
         family={{Valencia}},
         familyi={V\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{WebPage, BiSUNA}
    \strng{namehash}{VR1}
    \strng{fullhash}{VR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{BiSUNA - Alveo U50}
    \verb{url}
    \verb https://github.com/rval735/bisunaU50
    \endverb
    \field{year}{2020}
  \endentry

  \entry{alveoU50}{misc}{}
    \name{author}{1}{}{%
      {{hash=X}{%
         family={Xilinx},
         familyi={X\bibinitperiod},
      }}%
    }
    \keyw{WebPage, Xilinx, FPGA, Alveo U50}
    \strng{namehash}{X1}
    \strng{fullhash}{X1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Alveo U50}
    \verb{url}
    \verb https://www.xilinx.com/products/boards-and-kits/alveo/u50.html
    \endverb
    \field{year}{2020}
  \endentry

  \entry{nimbixAlveo}{misc}{}
    \name{author}{1}{}{%
      {{hash=N}{%
         family={Nimbix},
         familyi={N\bibinitperiod},
      }}%
    }
    \keyw{Webpage, Cloud, Numbix, FPGA}
    \strng{namehash}{N1}
    \strng{fullhash}{N1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Alveo FPGA}
    \verb{url}
    \verb https://www.nimbix.net/alveo
    \endverb
    \field{year}{2020}
  \endentry

  \entry{awsF1}{online}{}
    \name{author}{1}{}{%
      {{hash=SAW}{%
         family={Services},
         familyi={S\bibinitperiod},
         given={Amazon\bibnamedelima Web},
         giveni={A\bibinitperiod\bibinitdelim W\bibinitperiod},
      }}%
    }
    \keyw{Webpage, AWS F1, FPGA, Xilinx}
    \strng{namehash}{SAW1}
    \strng{fullhash}{SAW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{AWS F1 instance}
  \endentry

  \entry{OpenCL-In-Action}{book}{}
    \name{author}{1}{}{%
      {{hash=SM}{%
         family={Scarpino},
         familyi={S\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Manning}%
    }
    \keyw{OpenCL, Book}
    \strng{namehash}{SM1}
    \strng{fullhash}{SM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{OpenCL In Action}
    \verb{url}
    \verb https://www.manning.com/books/opencl-in-action
    \endverb
    \field{year}{2011}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2004_2744}{incollection}{}
    \name{author}{5}{}{%
      {{hash=KE}{%
         family={Klein},
         familyi={K\bibinitperiod},
         given={Einat},
         giveni={E\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={Mislovaty},
         familyi={M\bibinitperiod},
         given={Rachel},
         giveni={R\bibinitperiod},
      }}%
      {{hash=KI}{%
         family={Kanter},
         familyi={K\bibinitperiod},
         given={Ido},
         giveni={I\bibinitperiod},
      }}%
      {{hash=RA}{%
         family={Ruttor},
         familyi={R\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KW}{%
         family={Kinzel},
         familyi={K\bibinitperiod},
         given={Wolfgang},
         giveni={W\bibinitperiod},
      }}%
    }
    \name{editor}{3}{}{%
      {{hash=SLK}{%
         family={Saul},
         familyi={S\bibinitperiod},
         given={L.\bibnamedelima K.},
         giveni={L\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
      {{hash=WY}{%
         family={Weiss},
         familyi={W\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {MIT Press}%
    }
    \keyw{Cryptography, Neural Network, Mutual Learning}
    \strng{namehash}{KE+1}
    \strng{fullhash}{KEMRKIRAKW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 17}
    \field{pages}{689\bibrangedash 696}
    \field{title}{Synchronization of neural networks by mutual learning and its
  application to cryptography}
    \verb{url}
    \verb http://papers.nips.cc/paper/2744-synchronization-of-neural-networks-b
    \verb y-mutual-learning-and-its-application-to-cryptography.pdf
    \endverb
    \field{year}{2005}
  \endentry

  \entry{1133}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=RMS}{%
         family={Riazi},
         familyi={R\bibinitperiod},
         given={M.\bibnamedelima Sadegh},
         giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Samragh},
         familyi={S\bibinitperiod},
         given={Mohammad},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CH}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Hao},
         giveni={H\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Laine},
         familyi={L\bibinitperiod},
         given={Kim},
         giveni={K\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Lauter},
         familyi={L\bibinitperiod},
         given={Kristin},
         giveni={K\bibinitperiod},
      }}%
      {{hash=KF}{%
         family={Koushanfar},
         familyi={K\bibinitperiod},
         given={Farinaz},
         giveni={F\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {USENIX}%
    }
    \list{publisher}{1}{%
      {USENIX}%
    }
    \keyw{Crypto, Homomorphic encryption, BNN, Cryptography and Security}
    \strng{namehash}{RMS+1}
    \strng{fullhash}{RMSSMCHLKLKKF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{USENIX Security}
    \field{title}{XONN: XNOR-based Oblivious Deep Neural Network Inference}
    \verb{url}
    \verb https://arxiv.org/pdf/1902.07342.pdf
    \endverb
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2019_9194}{incollection}{}
    \name{author}{2}{}{%
      {{hash=LQ}{%
         family={Lou},
         familyi={L\bibinitperiod},
         given={Qian},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=JL}{%
         family={Jiang},
         familyi={J\bibinitperiod},
         given={Lei},
         giveni={L\bibinitperiod},
      }}%
    }
    \name{editor}{6}{}{%
      {{hash=WH}{%
         family={Wallach},
         familyi={W\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=LH}{%
         family={Larochelle},
         familyi={L\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Beygelzimer},
         familyi={B\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=ABF}{%
         family={Alch\'{e}-Buc},
         familyi={A\bibinithyphendelim B\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
      {{hash=FE}{%
         family={Fox},
         familyi={F\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{Crypto, FHE, DNN, encrypted data}
    \strng{namehash}{LQJL1}
    \strng{fullhash}{LQJL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 32}
    \field{pages}{10035\bibrangedash 10043}
    \field{title}{SHE: A Fast and Accurate Deep Neural Network for Encrypted
  Data}
    \verb{url}
    \verb http://papers.nips.cc/paper/9194-she-a-fast-and-accurate-deep-neural-
    \verb network-for-encrypted-data.pdf
    \endverb
    \field{year}{2019}
  \endentry

  \entry{861518}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=T}{%
         family={{Tai-Wen Yue}},
         familyi={T\bibinitperiod},
      }}%
      {{hash=S}{%
         family={{Suchen Chiang}},
         familyi={S\bibinitperiod},
      }}%
    }
    \keyw{Crypto; cryptography;neural nets;image processing;quantum
  computing;neural network approach;visual cryptography;cryptographic field;key
  management;message
  concealment;authorization;authentication;identification;entertainment;encrypting;NN;gray
  level images;binary images;access scheme;Neural
  networks;Cryptography;Books;Computer science;Application software;Engineering
  management;Authorization;Authentication;Image recognition;Target recognition}
    \strng{namehash}{TS1}
    \strng{fullhash}{TS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Visual cryptography finds many applications in the cryptographic field such
  as key management, message concealment, authorization, authentication,
  identification, and entertainment. The authors propose a novel approach for
  visual cryptography using neural networks (NNs). To perform encrypting, the
  input to the NN is a set of gray level images, and the output is a set of
  binary images (shares) that fulfils the desirable access scheme. This
  approach is considerably different from the traditional one, and can be
  applied to cope with very complex access schemes.%
    }
    \field{booktitle}{Proceedings of the IEEE-INNS-ENNS International Joint
  Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges
  and Perspectives for the New Millennium}
    \verb{doi}
    \verb 10.1109/IJCNN.2000.861518
    \endverb
    \field{issn}{1098-7576}
    \field{pages}{494\bibrangedash 499 vol.5}
    \field{title}{A neural network approach for visual cryptography}
    \field{volume}{5}
    \field{year}{2000}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{YU2006333}{article}{}
    \name{author}{2}{}{%
      {{hash=YW}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Wenwu},
         giveni={W\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Cao},
         familyi={C\bibinitperiod},
         given={Jinde},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{Crypto, Synchronization, Time varying delay, Neural network, Chaos,
  Encryption, Chaotic cryptosystem}
    \strng{namehash}{YWCJ1}
    \strng{fullhash}{YWCJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    In this Letter, a novel approach of encryption based on chaotic Hopfield
  neural networks with time varying delay is proposed. We use the chaotic
  neural network to generate binary sequences which will be used for masking
  plaintext. The plaintext is masked by switching of chaotic neural network
  maps and permutation of generated binary sequences. Simulation results were
  given to show the feasibility and effectiveness in the proposed scheme of
  this Letter. As a result, chaotic cryptography becomes more practical in the
  secure transmission of large multi-media files over public data communication
  network.%
    }
    \verb{doi}
    \verb https://doi.org/10.1016/j.physleta.2006.03.069
    \endverb
    \field{issn}{0375-9601}
    \field{number}{4}
    \field{pages}{333 \bibrangedash  338}
    \field{title}{Cryptography based on delayed chaotic neural networks}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S037596010600510X
    \endverb
    \field{volume}{356}
    \field{journaltitle}{Physics Letters A}
    \field{year}{2006}
  \endentry

  \entry{AAAI1817150}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=SJ}{%
         family={Song},
         familyi={S\bibinitperiod},
         given={Jingkuan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={He},
         familyi={H\bibinitperiod},
         given={Tao},
         giveni={T\bibinitperiod},
      }}%
      {{hash=GL}{%
         family={Gao},
         familyi={G\bibinitperiod},
         given={Lianli},
         giveni={L\bibinitperiod},
      }}%
      {{hash=XX}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={Xing},
         giveni={X\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Hanjalic},
         familyi={H\bibinitperiod},
         given={Alan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SHT}{%
         family={Shen},
         familyi={S\bibinitperiod},
         given={Heng\bibnamedelima Tao},
         giveni={H\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {AAAI}%
    }
    \keyw{DCGAN; hashing; GAN; image retrieval}
    \strng{namehash}{SJ+2}
    \strng{fullhash}{SJHTGLXXHASHT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The most striking successes in image retrieval using deep hashing have
  mostly involved discriminative models, which require labels. In this paper,
  we use binary generative adversarial networks (BGAN) to embed images to
  binary codes in an unsupervised way. By restricting the input noise variable
  of generative adversarial networks (GAN) to be binary and conditioned on the
  features of each input image, BGAN can simultaneously learn a binary
  representation per image, and generate an image plausibly similar to the
  original one. In the proposed framework, we address two main problems: 1) how
  to directly generate binary codes without relaxation? 2) how to equip the
  binary representation with the ability of accurate image retrieval? We
  resolve these problems by proposing new sign-activation strategy and a loss
  function steering the learning process, which consists of new models for
  adversarial loss, a content loss, and a neighborhood structure loss.
  Experimental results on standard datasets (CIFAR-10, NUSWIDE, and Flickr)
  demonstrate that our BGAN significantly outperforms existing hashing methods
  by up to 107% in terms of mAP (See Table 2).%
    }
    \field{booktitle}{AAAI Publications, Thirty-Second AAAI Conference on
  Artificial Intelligence}
    \field{title}{Binary Generative Adversarial Networks for Image Retrieval}
    \verb{url}
    \verb https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17150
    \endverb
    \field{year}{2018}
  \endentry

  \entry{5669194}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HS}{%
         family={{Hsiao}},
         familyi={H\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=WC}{%
         family={{Wen}},
         familyi={W\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=TM}{%
         family={{Tsai}},
         familyi={T\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={{Chen}},
         familyi={C\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{Crypto, CMOS logic circuits;cryptography;logic
  design;transistor-transistor logic;automatic generation;high performance
  multiple-input XOR/XNOR circuits;advanced encryption standard;pass transistor
  logic;static CMOS design;Data structures;IP networks;Lead;Boolean
  functions;Layout;CMOS integrated circuits;Encryption}
    \strng{namehash}{HS+2}
    \strng{fullhash}{HSWCTMCM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    Exclusive-OR (XOR) gate is one of the critical components in many
  applications such as cryptography. In this paper, we present an efficient
  multi-input XOR circuit design based on pass-transistor logic (PTL). A
  synthesis algorithm is developed to efficiently generate the PTL-based
  multi-input XOR circuits. Both pre-layout and post-layout simulation results
  show that our proposed multi-input XOR design outperforms static CMOS design.
  The multi-input XOR circuits are also used to design the transformations in
  the Advanced Encryption Standard (AES).%
    }
    \field{booktitle}{2010 International Symposium on Next Generation
  Electronics}
    \verb{doi}
    \verb 10.1109/ISNE.2010.5669194
    \endverb
    \field{issn}{2378-8607}
    \field{pages}{77\bibrangedash 80}
    \field{title}{Automatic generation of high-performance multiple-input
  XOR/XNOR circuits and its application in Advanced Encryption Standard (AES)}
    \field{year}{2010}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2015_5872}{incollection}{}
    \name{author}{6}{}{%
      {{hash=FM}{%
         family={Feurer},
         familyi={F\bibinitperiod},
         given={Matthias},
         giveni={M\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Klein},
         familyi={K\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
      {{hash=EK}{%
         family={Eggensperger},
         familyi={E\bibinitperiod},
         given={Katharina},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Springenberg},
         familyi={S\bibinitperiod},
         given={Jost},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Blum},
         familyi={B\bibinitperiod},
         given={Manuel},
         giveni={M\bibinitperiod},
      }}%
      {{hash=HF}{%
         family={Hutter},
         familyi={H\bibinitperiod},
         given={Frank},
         giveni={F\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=CC}{%
         family={Cortes},
         familyi={C\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LND}{%
         family={Lawrence},
         familyi={L\bibinitperiod},
         given={N.\bibnamedelima D.},
         giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=LDD}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={D.\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sugiyama},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{DNN, AutoML, NAS}
    \strng{namehash}{FM+1}
    \strng{fullhash}{FMKAEKSJBMHF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 28}
    \field{pages}{2962\bibrangedash 2970}
    \field{title}{Efficient and Robust Automated Machine Learning}
    \verb{url}
    \verb http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machi
    \verb ne-learning.pdf
    \endverb
    \field{year}{2015}
  \endentry

  \entry{NIPS2014_5423}{incollection}{}
    \name{author}{8}{}{%
      {{hash=GI}{%
         family={Goodfellow},
         familyi={G\bibinitperiod},
         given={Ian},
         giveni={I\bibinitperiod},
      }}%
      {{hash=PAJ}{%
         family={Pouget-Abadie},
         familyi={P\bibinithyphendelim A\bibinitperiod},
         given={Jean},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Mirza},
         familyi={M\bibinitperiod},
         given={Mehdi},
         giveni={M\bibinitperiod},
      }}%
      {{hash=XB}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={Bing},
         giveni={B\bibinitperiod},
      }}%
      {{hash=WFD}{%
         family={Warde-Farley},
         familyi={W\bibinithyphendelim F\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=OS}{%
         family={Ozair},
         familyi={O\bibinitperiod},
         given={Sherjil},
         giveni={S\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Courville},
         familyi={C\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=GZ}{%
         family={Ghahramani},
         familyi={G\bibinitperiod},
         given={Z.},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=WM}{%
         family={Welling},
         familyi={W\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CC}{%
         family={Cortes},
         familyi={C\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LND}{%
         family={Lawrence},
         familyi={L\bibinitperiod},
         given={N.\bibnamedelima D.},
         giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=WKQ}{%
         family={Weinberger},
         familyi={W\bibinitperiod},
         given={K.\bibnamedelima Q.},
         giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{DCGAN, GAN, CNN, adversarial training}
    \strng{namehash}{GI+1}
    \strng{fullhash}{GIPAJMMXBWFDOSCABY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Advances in Neural Information Processing Systems 27}
    \field{pages}{2672\bibrangedash 2680}
    \field{title}{Generative Adversarial Nets}
    \verb{url}
    \verb http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
    \endverb
    \field{year}{2014}
  \endentry

  \entry{0fb77cd6fdab4aa19e35fc2c45de5f30}{thesis}{}
    \name{author}{1}{}{%
      {{hash=AH}{%
         family={Alkhzaimi},
         familyi={A\bibinitperiod},
         given={{Hoda A.}},
         giveni={H\bibinitperiod},
      }}%
    }
    \list{language}{1}{%
      {English}%
    }
    \list{publisher}{1}{%
      {Technical University of Denmark}%
    }
    \keyw{Crypto, Cryptoanalisys, Block Cipher}
    \strng{namehash}{AH1}
    \strng{fullhash}{AH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The focus of this dissertation is to present cryptanalytic results on
  selected block ciphers. Block ciphers are the mathematical structure that
  will take a plaintext message and convert it into a ciphertext one block at a
  time using a secret key. They play an essential role in many cryptographic
  architectures and frameworks. For a long time they were known as the main
  building block that will provide confidentiality in an information system.
  They would also be able to represent a full spectrum of cryptographic
  services as many block ciphers can be used to construct stream ciphers, hash
  functions, pseudorandom number generators, and authenticated encryption
  designs. For this reason a multitude of initiatives over the years has been
  established to provide a secure and sound designs for block ciphers as in the
  calls for Data Encryption Standard (DES) and Advanced Encryption Standard
  (AES), lightweight ciphers initiatives, and the Competition for Authenticated
  Encryption: Security, Applicability, and Robustness (CAESAR).In this thesis,
  we first present cryptanalytic results on different ciphers. We propose
  attack named the Invariant Subspace Attack. It is utilized to break the full
  block cipher PRINTcipher for a significant fraction of its keys. This new
  attack also gives us new insights into other, more well-established attacks.
  In addition, we also show that for weak keys, strongly biased linear
  approximations exists for any number of rounds. Furthermore, we provide
  variety of attacks on the family of lightweight block cipher SIMON that was
  published by the U.S National Security Agency (NSA). The ciphers are
  developed with optimization towards both hardware and software in mind. While
  the specification paper discusses design requirements and performance of the
  presented lightweight ciphers thoroughly, no security assessment is given. We
  present a series of observations on the presented construction that, in some
  cases, yield attacks, while in other cases may provide basis of further
  analysis by the cryptographic community. Specifically, The attacks obtained
  are using classical- as well as truncated differentials. In addition to that,
  we also investigate the security of SIMON against different linear
  cryptanalysis methods, i.e., classic linear,and linear hull attacks. we
  present a connection between linear characteristic and differential
  characteristic, multiple linear and differential and linear hull and
  differential, and employ it to adapt the current known results on
  differential cryptanalysis of SIMON to linear cryptanalysis results. Finally,
  we investigate links between different methods of cryptanalysis and how they
  can be utilized for block cipher cryptanalysis. We consider the known results
  on the links among integral, impossible differential and zero-correlation
  linear hulls in order to prove that constructing a zero-correlation linear
  hull always implies the existence of an integral distinguisher. Moreover, we
  show that constructing zero-correlation linear hull on a Feistel structure
  with SP-type round functions, where P is a binary matrix, is equivalent to
  constructing impossible differential on the same structure except that P is
  substituted by the transposed matrix PT . We present an integral
  distinguishers of 5-round Feistel structure with bijective round functions
  and 3-round Feistel structure with round functions not necessarily being
  bijective. In addition to an integral distinguishers of Camellia so far,
  i.e., 7-round integral distinguishers of Camellia with FL/FLâˆ’1 layer and
  8-round integral distinguishers of Camellia without FL/FLâˆ’1 layer.%
    }
    \field{number}{360}
    \field{series}{DTU Compute PHD-2015}
    \field{title}{Cryptanalysis of Selected Block Ciphers}
    \field{type}{phdthesis}
    \field{year}{2016}
  \endentry

  \entry{katz2008introduction}{book}{}
    \name{author}{2}{}{%
      {{hash=KJ}{%
         family={Katz},
         familyi={K\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Lindell},
         familyi={L\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Chapman \& Hall/CRC}%
    }
    \keyw{Books, Cryptographic, CPA,}
    \strng{namehash}{KJLY1}
    \strng{fullhash}{KJLY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{isbn}{9781584885511}
    \field{series}{Chapman and Hall/CRC Cryptography and Network Security
  Series}
    \field{title}{Introduction to modern cryptography}
    \verb{url}
    \verb https://books.google.ca/books?id=WIc\_AQAAIAAJ
    \endverb
    \field{year}{2008}
  \endentry

  \entry{CommTheoryShannon}{article}{}
    \name{author}{1}{}{%
      {{hash=SC}{%
         family={Shannon},
         familyi={S\bibinitperiod},
         given={Claude},
         giveni={C\bibinitperiod},
      }}%
    }
    \keyw{Cryptography, Cryptanalysis}
    \strng{namehash}{SC1}
    \strng{fullhash}{SC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{number}{4}
    \field{pages}{656\bibrangedash 715}
    \field{title}{Communication Theory and Secrecy Systems}
    \field{volume}{28}
    \field{journaltitle}{Bell system Technical Journal}
    \field{year}{1949}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1371/journal.pone.0203434}{article}{}
    \name{author}{1}{}{%
      {{hash=KA}{%
         family={Kaso},
         familyi={K\bibinitperiod},
         given={Artan},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Public Library of Science}%
    }
    \keyw{Other, NCC, fast Fourier transforms}
    \strng{namehash}{KA1}
    \strng{fullhash}{KA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    The normalized cross-correlation (NCC), usually its 2D version, is
  routinely encountered in template matching algorithms, such as in facial
  recognition, motion-tracking, registration in medical imaging, etc. Its rapid
  computation becomes critical in time sensitive applications. Here I develop a
  scheme for the computation of NCC by fast Fourier transform that can
  favorably compare for speed efficiency with other existing techniques and may
  outperform some of them given an appropriate search scenario.%
    }
    \verb{doi}
    \verb 10.1371/journal.pone.0203434
    \endverb
    \field{number}{9}
    \field{pages}{1\bibrangedash 16}
    \field{title}{Computation of the normalized cross-correlation by fast
  Fourier transform}
    \verb{url}
    \verb https://doi.org/10.1371/journal.pone.0203434
    \endverb
    \field{volume}{13}
    \field{journaltitle}{PLOS ONE}
    \field{month}{09}
    \field{year}{2018}
  \endentry

  \entry{AES-NIST}{article}{}
    \name{author}{1}{}{%
      {{hash=MJDJRNJFLEBERJFDJEBB}{%
         family={Morris J.\bibnamedelima Dworkin},
         familyi={M\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  D\bibinitperiod},
         suffix={Elaine B.\bibnamedelima Barker},
         suffixi={E\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim
  B\bibinitperiod},
         given={James R. Nechvatal James Foti Lawrence E. Bassham E. Roback
  James F. Dray\bibnamedelima Jr.},
         giveni={J\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim
  N\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  F\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim
  E\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim
  E\bibinitperiod\bibinitdelim R\bibinitperiod\bibinitdelim
  J\bibinitperiod\bibinitdelim F\bibinitperiod\bibinitdelim
  D\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \keyw{Crypto, Cryptography, AES, NIST}
    \strng{namehash}{MJDEBBJRNJFLEBERJFDJ1}
    \strng{fullhash}{MJDEBBJRNJFLEBERJFDJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{pages}{47}
    \field{title}{Advanced Encryption Standard (AES)}
    \field{volume}{197}
    \field{journaltitle}{Federal Inf. Process. Stds. (NIST FIPS)}
    \field{year}{2001}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2016arXiv161006918A}{article}{}
    \name{author}{2}{}{%
      {{hash=AM}{%
         family={{Abadi}},
         familyi={A\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=ADG}{%
         family={{Andersen}},
         familyi={A\bibinitperiod},
         given={D.\bibnamedelima G.},
         giveni={D\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \keyw{DCGAN; Computer Science - Cryptography and Security, Computer Science
  - Machine Learning, Cryptonet}
    \strng{namehash}{AMADG1}
    \strng{fullhash}{AMADG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \verb{eprint}
    \verb 1610.06918
    \endverb
    \field{title}{{Learning to Protect Communications with Adversarial Neural
  Cryptography}}
    \field{journaltitle}{ArXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CR}
    \field{month}{10}
    \field{year}{2016}
  \endentry

  \entry{SP800-22Rev1a}{proceedings}{}
    \name{author}{1}{}{%
      {{hash=LBJSJNMSEBSLMLMVDBNHJDAR}{%
         family={Lawrence\bibnamedelima Bassham},
         familyi={L\bibinitperiod\bibinitdelim B\bibinitperiod},
         suffix={Andrew\bibnamedelima Rukhin},
         suffixi={A\bibinitperiod\bibinitdelim R\bibinitperiod},
         given={Juan Soto James Nechvatal Miles Smid Elaine Barker Stefan Leigh
  Mark Levenson Mark Vangel David Banks N. Heckert James\bibnamedelima Dray},
         giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod\bibinitdelim
  J\bibinitperiod\bibinitdelim N\bibinitperiod\bibinitdelim
  M\bibinitperiod\bibinitdelim S\bibinitperiod\bibinitdelim
  E\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim
  S\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim
  M\bibinitperiod\bibinitdelim L\bibinitperiod\bibinitdelim
  M\bibinitperiod\bibinitdelim V\bibinitperiod\bibinitdelim
  D\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim
  N\bibinitperiod\bibinitdelim H\bibinitperiod\bibinitdelim
  J\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
    }
    \name{editor}{1}{}{%
      {{hash=N}{%
         family={NIST},
         familyi={N\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {NIST}%
    }
    \list{publisher}{1}{%
      {NIST}%
    }
    \keyw{Crypto, hypothesis test; P-value; random number generator;
  statistical tests}
    \strng{namehash}{LBARJSJNMSEBSLMLMVDBNHJD1}
    \strng{fullhash}{LBARJSJNMSEBSLMLMVDBNHJD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{abstract}{%
    This paper discusses some aspects of selecting and testing random and
  pseudorandom number generators. The outputs of such generators may be used in
  many cryptographic applications, such as the generation of key material.
  Generators suitable for use in cryptographic applications may need to meet
  stronger requirements than for other applications. In particular, their
  outputs must be unpredictable in the absence of knowledge of the inputs. Some
  criteria for characterizing and selecting appropriate generators are
  discussed in this document. The subject of statistical testing and its
  relation to cryptanalysis is also discussed, and some recommended statistical
  tests are provided. These tests may be useful as a first step in determining
  whether or not a generator is suitable for a particular cryptographic
  application. However, no set of statistical tests can absolutely certify a
  generator as appropriate for usage in a particular application, i.e.,
  statistical testing cannot serve as a substitute for cryptanalysis. The
  design and cryptanalysis of generators is outside the scope of this paper.
  https://csrc.nist.gov/publications/detail/sp/800-22/rev-1a/final%
    }
    \field{number}{800}
    \field{title}{A Statistical Test Suite for Random and Pseudorandom Number
  Generators for Cryptographic Applications}
    \field{volume}{22}
    \field{annotation}{%
    https://csrc.nist.gov/publications/detail/sp/800-22/rev-1a/final%
    }
    \field{year}{2010}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{FastNIST-STS-Muni}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=SVMKASMZR}{%
         family={S{\'y}s},
         familyi={S\bibinitperiod},
         suffix={M.; Z.\bibnamedelima {\v R}{\'\i}ha},
         suffixi={M\bibinitperiod\bibinitdelim Z\bibinitperiod\bibinitdelim {\v
  R}\bibinitperiod},
         given={V.\bibnamedelima Maty{\'a}{\v s} K.M{\'a}rton A.\bibnamedelima
  Suciu},
         giveni={V\bibinitperiod\bibinitdelim M\bibinitperiod\bibinitdelim
  K\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \name{editor}{1}{}{%
      {{hash=otRAPH}{%
         prefix={of\bibnamedelima the},
         prefixi={o\bibinitperiod\bibinitdelim t\bibinitperiod},
         family={Romanian\bibnamedelima Academy},
         familyi={R\bibinitperiod\bibinitdelim A\bibinitperiod},
         given={Publishing\bibnamedelima House},
         giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \keyw{Crypto, NIST, Muni, Fast-STS, STS}
    \strng{namehash}{SMZRVMKAS1}
    \strng{fullhash}{SMZRVMKAS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{booktitle}{Romanian Journal of Information Science and Technology}
    \field{pages}{18\bibrangedash 32}
    \field{title}{On the Interpretation of Results from the NIST Statistical
  Test Suite}
    \field{volume}{18}
    \field{year}{2015}
  \endentry

  \entry{Fast-STS}{online}{}
    \name{author}{1}{}{%
      {{hash=ZRMS}{%
         family={Zdenek\bibnamedelima {\v R}{\'\i}ha},
         familyi={Z\bibinitperiod\bibinitdelim {\v R}\bibinitperiod},
         given={Marek\bibnamedelima S{\'y}s},
         giveni={M\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
    }
    \keyw{URL, STS, NIST, Randomness}
    \strng{namehash}{ZRMS1}
    \strng{fullhash}{ZRMS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Faster randomness testing}
    \verb{url}
    \verb https://randomness-tests.fi.muni.cz/
    \endverb
    \field{year}{2014}
  \endentry

  \entry{Schneier-Crypto}{misc}{}
    \name{author}{1}{}{%
      {{hash=SB}{%
         family={Schneier},
         familyi={S\bibinitperiod},
         given={Bruce},
         giveni={B\bibinitperiod},
      }}%
    }
    \keyw{URL, Cryptoanalisys, Cryptography}
    \strng{namehash}{SB1}
    \strng{fullhash}{SB1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{title}{Memo to the Amateur Cipher Designer}
    \verb{url}
    \verb https://www.schneier.com/crypto-gram/archives/1998/1015.html#cipherde
    \verb sign
    \endverb
    \field{year}{1998}
  \endentry
\enddatalist
\endinput
