Hello Everyone,

The name of this project is "FPGA deployment of neuroevolved Binary Neural Networks", it is an entry competition to the Adaptive Computer Developer contest sponsored by Xilinx.

----
Training of Deep Neural Network (DNN) has taken multiple approaches to make them faster, more accurate or power efficient; one technique used to simplify models is the utilization of quantized values for weights, activations, inputs or outputs.

This project introduces a brand new approach to train Binary Neural Network using neuroevolution to perform the space search, with the addition of FPGA acceleration as a viable alternative to CPU/GPU training duopoly.

----

It is well know that, in order to reach full performance and efficiency, it is necessary to create application-specific integrated circuits. However, they are expensive to design, test and manufacture. On the other side, central processing units and general-purpose graphic processing units generalize to any task, providing programmable flexibility, at the expense of efficiency and performance.

With the latest development tools offered by FPGAs, a middle ground can be achieved, where efficiency can be designed into programmable hardware capable of accelerating tasks while consuming less power.

----

Binary Neural Network offer a wide range of characteristics: bitset connections, reduced memory footprint and use of bitwise operations and model compression. On the other hand, BNN lack behind its continuous counterparts in accuracy when tested on Image Classification tests.

Most research around BNN has focused on the translation of backpropagation, an algorithm designed for floating-point values, to engage with discrete sequences; such transformation exacerbates instabilities at training time.

-----

Despite the Binary Neural Network hardware friendly characteristics, it do not enjoy of a large research base to be trained onto FPGA, where most of the development efforts focus on inference only. Even less research is available regarding neuroevolved metaheuristics deployed on reconfigurable hardware.

-----

To gain all the BNN features, extend its capabilities to Reinforcement Learning (RL) and avoid drawbacks from a quantized backpropagation, this project takes a groundbreaking approach:

- Weights and activations use bitsets.
- Only logic operations (AND, XOR, OR, XNOR) are applied, nullifying the need for Arithmetic Logic Unit (ALU).
- Neuroevolution is employed to drive the space search, replacing a two-stage backpropagation.
- It generates a dynamic network topology to adapt and optimize for the problem at hand.
- Training and inference can be performed by reconfigurable devices, like the Alveo U50.

The name of this algorithm is Binary Spectrum-diverse Unified Neural Architecture (BiSUNA).

BiSUNA takes advantage of diverse weight sizes along an evolving topology that adapts to Reinforcement Learning problems, with the addition of having binary sets instead of floating-point connections and neurons.

-----

This project capitalizes on the fast HBM2 memory, massive LUT resources and simple software environment provided by the Xilinx Alveo U50.

Limited research has taken place to take advantage of FPGA flexibility at the time of training deep neural networks, a place where GPGPU dominates the market. On the other hand, multiple examples have demonstrated FPGA's advantage when performing inference of DNN models.

By iteratively querying multiple agents in parallel using dedicated CUs, it is possible to achieve more performance at scale. The BiSUNA framework employs neuroevolution with only binary operations/values in the creation of non-sequential neural networks in charge of solving RL problems.

-----

Thanks to Binary Neural Network's computing requirements simplicity, it natively fits the FPGA's resources, acquiescing to a pipelined single task design that effectively adapts to the problem at hand and remains energy efficient.

BiSUNA uses a fixed number of "agents" to explore any RL environment where every entity is a possible solution to the problem. Therefore, every agent must perform three basic operations: Process Input, Process Primers/Control, Process Remaining/Output, all of them refer to neurons in the mesh.

Using the pipelined design pattern offered the right balance between resource utilization and development simplicity. At every clock cycle, an agent moves its execution from one stage to the next, given that the Initiation Interval of most loops had be close to 1.

-----

In order to test BiSUNA's execution, the MountainCar environment repeated multiple times. Enhancements statistical significance is achieved at each stage by repeating five times the same binary and configuration.

-----

The results as displayed show a configuration with 8 compute units, on the top represent the Binary execution, while the bottom are continuous neurons. It is possible to see that the use of discrete bitsets helps to reach the solution to the MountainCar problem with less variance, while at the same time using less time.

-----

It is crucial to employ as much hardware as available in the FPGA to reduce the time used to execute the environment. For example, using only 1 compute units, BiSUNA takes around 600 seconds, whereas 8 compute units performs the same task in less than 200 seconds.

-----

As predicted before, transforming continuous to binary neurons reduces LUT utilization from 36K to 16K, and DSP from 110 to only 4. Those savings allows the hardware to instantiate more compute units, which can be used to parallelize the algorithm execution.

-----
-----

The video demonstration in the background shows the U50 board connected to a PCI-Express port, accelerating the process of calculation of binary and continuous neurons.

-----
-----

In conclusion, the FPGA community benefits directly from this work, given that it adds a new tool to train BNN using evolutionary principles, a innovative alternative that sidesteps completely gradient descent.

As a result of BiSUNA's proficiency to conditionally compile between discrete/floating-point systems, this work can be generalized to more environments, setting a precedence of BNN application to RL.

Experiments showed how binary neurons consume fewer FPGA resources in comparison with their continuous counterpart.

Today's gains over a CPU execution lies around 16%, future versions will optimize its architecture to exploit all the Alveo U50 board features to make it faster; another improvement will be to extend its hardware support to other devices (ex. Ultra96-V2).

-----

Simplifying hardware requirements to train BNN contributes towards the creation of more efficient networks and circuits, facilitating FPGA adaptive compute acceleration inference and training.

The near future will convey more intelligent computation to any device, expanding their capabilities, increasing technology expertise to the next level, the Adaptive Internet of Things.

In less than 10 years from today, IoT devices will triplicate their computational performance while reducing their energy consumption. BiSUNA is well suited to exploit those advantages by bringing intelligence: train in the cloud, then deploy to the edge.
