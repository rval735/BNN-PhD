% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.0 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nty/global//global/global}
  \entry{EvolStraOAI}{misc}{}
    \name{author}{1}{}{%
      {{hash=AO}{%
         family={AI},
         familyi={A\bibinitperiod},
         given={Open},
         giveni={O\bibinitperiod},
      }}%
    }
    \keyw{evolution strategies, OpenAI, Atari, Videogames, Reinforcement
  learning}
    \strng{namehash}{AO1}
    \strng{fullhash}{AO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{title}{Evolution Strategies as a Scalable Alternative to
  Reinforcement Learning}
    \verb{url}
    \verb https://blog.openai.com/evolution-strategies/
    \endverb
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8226999}{article}{}
    \name{author}{11}{}{%
      {{hash=AK}{%
         family={Ando},
         familyi={A\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=UK}{%
         family={Ueyoshi},
         familyi={U\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=OK}{%
         family={Orimo},
         familyi={O\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={Yonekawa},
         familyi={Y\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Sato},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NH}{%
         family={Nakahara},
         familyi={N\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=TYS}{%
         family={Takamaeda-Yamazaki},
         familyi={T\bibinithyphendelim Y\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=IM}{%
         family={Ikebe},
         familyi={I\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AT}{%
         family={Asai},
         familyi={A\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KT}{%
         family={Kuroda},
         familyi={K\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Motomura},
         familyi={M\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{BNN;low-power electronics;neural nets;random-access
  storage;reconfigurable architectures;deep neural network
  accelerator;binary/ternary deep neural networks;In-memory neural network
  processing;binary/ternaty neural network;BRein memory;single-chip
  binary/ternary reconfigurable in-memory;reconfigurable accelerator
  architecture;external data access;power 0.6 W;frequency 400 MHz;Biological
  neural networks;Random access memory;Memory
  management;Neurons;System-on-chip;Parallel processing;Binary neural
  networks;in-memory processing;near-memory processing;neural
  networks;reconfigurable array;ternary neural networks}
    \strng{namehash}{AK+1}
    \strng{fullhash}{AKUKOKYHSSNHTYSIMATKTMM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    A versatile reconfigurable accelerator architecture for binary/ternary deep
  neural networks is presented. In-memory neural network processing without any
  external data accesses, sustained by the symmetry and simplicity of the
  computation of the binary/ternaty neural network, improves the energy
  efficiency dramatically. The prototype chip is fabricated, and it achieves
  1.4 TOPS (tera operations per second) peak performance with 0.6-W power
  consumption at 400-MHz clock. The application examination is also conducted.%
    }
    \verb{doi}
    \verb 10.1109/JSSC.2017.2778702
    \endverb
    \field{issn}{0018-9200}
    \field{number}{4}
    \field{pages}{983\bibrangedash 994}
    \field{title}{BRein Memory: A Single-Chip Binary/Ternary Reconfigurable
  in-Memory Deep Neural Network Accelerator Achieving 1.4 TOPS at 0.6 W}
    \field{volume}{53}
    \field{journaltitle}{IEEE Journal of Solid-State Circuits}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8429420}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rossi},
         familyi={R\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; computer vision;feedforward neural nets;Internet of
  Things;learning (artificial intelligence);low-power
  electronics;microprocessor chips;hardware accelerators;core efficiency;I/O
  bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable
  architecture;state-of-the-art BNN accelerators;resource-intensive FP16
  arithmetic;TOp/s/W system-level efficiency;binary-weight streaming
  approach;BWN;hyperdrive;weight quantization;binary-weight neural
  networks;memory footprint;aggressive
  quantization;mW-devices;memory-intensive;machine learning;computer
  vision;impressive results;deep neural networks;mW IoT end-nodes;systolically
  scalable binary-weight CNN inference engine;Frequency modulation;Computer
  architecture;Quantization (signal);Neural
  networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary
  Weights Neural Networks;IoT}
    \strng{namehash}{ARCLRDBL1}
    \strng{fullhash}{ARCLRDBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Deep neural networks have achieved impressive results in computer vision
  and machine learning. Unfortunately, state-of-the-art networks are extremely
  compute-and memory-intensive which makes them unsuitable for mW-devices such
  as IoT end-nodes. Aggressive quantization of these networks dramatically
  reduces the computation and memory footprint. Binary-weight neural networks
  (BWNs) follow this trend, pushing weight quantization to the limit. Hardware
  accelerators for BWNs presented up to now have focused on core efficiency,
  disregarding I/O bandwidth and system-level efficiency that are crucial for
  deployment of accelerators in ultra-low power devices. We present Hyperdrive:
  a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel
  binary-weight streaming approach, and capable of handling high-resolution
  images by virtue of its systolic-scalable architecture. We achieve a 5.9
  TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than
  state-of-the-art BNN accelerators, even if our core uses resource-intensive
  FP16 arithmetic for increased robustness.%
    }
    \field{booktitle}{2018 IEEE Computer Society Annual Symposium on VLSI
  (ISVLSI)}
    \verb{doi}
    \verb 10.1109/ISVLSI.2018.00099
    \endverb
    \field{issn}{2159-3477}
    \field{pages}{509\bibrangedash 515}
    \field{title}{Hyperdrive: A Systolically Scalable Binary-Weight CNN
  Inference Engine for mW IoT End-Nodes}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7878541}{article}{}
    \name{author}{4}{}{%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rossi},
         familyi={R\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; CMOS logic circuits;computer
  vision;convolution;coprocessors;embedded systems;image
  classification;integrated circuit design;low-power electronics;neural
  nets;optimisation;power aware computing;system-on-chip;I/O storage;I/O
  bandwidth;algorithmic advancements;binary weights;competitive classification
  accuracy;hard limitations;deeply embedded applications;mobile embedded
  applications;power envelope;energy consumption;system-on-chip integration;CNN
  accelerators;GP-GPUs;power-hungry parallel processors;computational
  effort;human accuracy;image classification;computer vision;convolutional
  neural networks;ultralow power binary-weight CNN acceleration;power
  dissipation;binary-weight CNNs;accelerator;optimization
  opportunities;ASIC;binary weights;convolutional neural networks
  (CNNs);hardware accelerator;Internet of Things (IoT)}
    \strng{namehash}{ARCLRDBL1}
    \strng{fullhash}{ARCLRDBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Convolutional neural networks (CNNs) have revolutionized the world of
  computer vision over the last few years, pushing image classification beyond
  human accuracy. The computational effort of today's CNNs requires
  power-hungry parallel processors or GP-GPUs. Recent developments in CNN
  accelerators for system-on-chip integration have reduced energy consumption
  significantly. Unfortunately, even these highly optimized devices are above
  the power envelope imposed by mobile and deeply embedded applications and
  face hard limitations caused by CNN weight I/O and storage. This prevents the
  adoption of CNNs in future ultralow power Internet of Things end-nodes for
  near-sensor analytics. Recent algorithmic and theoretical advancements enable
  competitive classification accuracy even when limiting CNNs to binary (+1/-1)
  weights during training. These new findings bring major optimization
  opportunities in the arithmetic core by removing the need for expensive
  multiplications, as well as reducing I/O bandwidth and storage. In this
  paper, we present an accelerator optimized for binary-weight CNNs that
  achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate
  equivalent (MGE) or 1.9 mm<sup>2</sup>and with a power dissipation of 895 μW
  in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms
  the state-of-the-art in terms of energy and area efficiency achieving 61.2
  TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.%
    }
    \verb{doi}
    \verb 10.1109/TCAD.2017.2682138
    \endverb
    \field{issn}{0278-0070}
    \field{number}{1}
    \field{pages}{48\bibrangedash 60}
    \field{title}{YodaNN: An Architecture for Ultralow Power Binary-Weight CNN
  Acceleration}
    \field{volume}{37}
    \field{journaltitle}{IEEE Transactions on Computer-Aided Design of
  Integrated Circuits and Systems}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{10.1007/3-540-61108-8_27}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=BT}{%
         family={B{\"a}ck},
         familyi={B\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=AJM}{%
         family={Alliot},
         familyi={A\bibinitperiod},
         given={Jean-Marc},
         giveni={J\bibinithyphendelim M\bibinitperiod},
      }}%
      {{hash=LE}{%
         family={Lutton},
         familyi={L\bibinitperiod},
         given={Evelyne},
         giveni={E\bibinitperiod},
      }}%
      {{hash=RE}{%
         family={Ronald},
         familyi={R\bibinitperiod},
         given={Edmund},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Schoenauer},
         familyi={S\bibinitperiod},
         given={Marc},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Snyers},
         familyi={S\bibinitperiod},
         given={Dominique},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Evolutionary, evolution strategies, Genetic Algorithm}
    \strng{namehash}{BT1}
    \strng{fullhash}{BT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    In this paper, evolution strategies (ESs) --- a class of evolutionary
  algorithms using normally distributed mutations, recombination, deterministic
  selection of the $\mu$>1 best offspring individuals, and the principle of
  self-adaptation for the collective on-line learning of strategy parameters
  --- are described by demonstrating their differences to genetic algorithms.
  By comparison of the algorithms, it is argued that the application of
  canonical genetic algorithms for continuous parameter optimization problems
  implies some difficulties caused by the encoding of continuous object
  variables by binary strings and the constant mutation rate used in genetic
  algorithms. Because they utilize a problem-adequate representation and a
  suitable self-adaptive step size control guaranteeing linear convergence for
  strictly convex problems, evolution strategies are argued to be more adequate
  for continuous problems.%
    }
    \field{booktitle}{Artificial Evolution}
    \field{isbn}{978-3-540-49948-0}
    \field{pages}{1\bibrangedash 20}
    \field{title}{Evolution strategies: An alternative evolutionary algorithm}
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{1996}
  \endentry

  \entry{8373076}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=BAA}{%
         family={Bahou},
         familyi={B\bibinitperiod},
         given={A.\bibnamedelima A.},
         giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=KG}{%
         family={Karunaratne},
         familyi={K\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; embedded systems;low-power electronics;neural nets;power aware
  computing;binary convolutional neural networks;off-chip memory;low-power
  embedded systems;extreme quantization;flexible accelerator;aggressive
  data;nontrivial network topologies;feature map volumes;energy
  efficiency;hardware accelerator;binary CNN;weight binarization;collapsing
  energy-intensive sum-of-products;XNOR-and-popcount
  operations;Hardware;Convolutional neural
  networks;System-on-chip;Computational modeling;Computer architecture;Program
  processors}
    \strng{namehash}{BAAKGARCLBL1}
    \strng{fullhash}{BAAKGARCLBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Deploying state-of-the-art CNNs requires power-hungry processors and
  off-chip memory. This precludes the implementation of CNNs in low-power
  embedded systems. Recent research shows CNNs sustain extreme quantization,
  binarizing their weights and intermediate feature maps, thereby saving 8-32x
  memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount
  operations. We present XNORBIN, a flexible accelerator for binary CNNs with
  computation tightly coupled to memory for aggressive data reuse supporting
  even non-trivial network topologies with large feature map volumes.
  Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of
  95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.%
    }
    \field{booktitle}{2018 IEEE Symposium in Low-Power and High-Speed Chips
  (COOL CHIPS)}
    \verb{doi}
    \verb 10.1109/CoolChips.2018.8373076
    \endverb
    \field{issn}{2473-4683}
    \field{pages}{1\bibrangedash 3}
    \field{title}{XNORBIN: A 95 TOp/s/W hardware accelerator for binary
  convolutional neural networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{1206405}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=BA}{%
         family={Bermak},
         familyi={B\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; pattern classification;VLSI;multiprecision neural chip}
    \strng{namehash}{BA1}
    \strng{fullhash}{BA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    This paper describes a 3D VLSI Chip for binary neural network
  classification applications. The 3D circuit includes three layers of MCM
  integrating 4 chips each making it a total of 12 chips integrated in a volume
  of (2 /spl times/ 2 /spl times/ 0.7)cm/sup 3/. The architecture is scalable,
  and real-time binary neural network classifier systems could be built with
  one, two or all twelve chip solutions. Each basic chip includes an on-chip
  control unit for programming options of the neural network topology and
  precision. The system is modular and presents easy expansibility without
  requiring extra devices. Experimental test results showed that a full recall
  operation is obtained in less than 1.2/spl mu/s for any topology with 4-bit
  or 8-bit precision while it is obtained in less than 2.2/spl mu/s for any
  16-bit precision. As a consequence the 3D chip is a very powerful
  reconfigurable and a multiprecision neural chip exhibiting a significant
  speed of 1.25 GCPS.%
    }
    \field{booktitle}{Proceedings of the 2003 International Symposium on
  Circuits and Systems, 2003. ISCAS '03.}
    \verb{doi}
    \verb 10.1109/ISCAS.2003.1206405
    \endverb
    \field{pages}{V\bibrangedash V}
    \field{title}{A highly scalable 3D chip for binary neural network
  classification applications}
    \field{volume}{5}
    \field{year}{2003}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5726804}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=BSA}{%
         family={Brodsky},
         familyi={B\bibinitperiod},
         given={S.\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=GCC}{%
         family={Guest},
         familyi={G\bibinitperiod},
         given={C.\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \keyw{BNN, content-addressable storage;learning systems;neural
  nets;arbitrary bit-level significance;binary backpropagation;bit connection
  weights;content addressable memory;continuous backpropagation network
  learning model;local computation;pseudoanalog extension}
    \strng{namehash}{BSAGCC1}
    \strng{fullhash}{BSAGCC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{booktitle}{1990 IJCNN International Joint Conference on Neural
  Networks}
    \verb{doi}
    \verb 10.1109/IJCNN.1990.137846
    \endverb
    \field{pages}{205\bibrangedash 210 vol.3}
    \field{title}{Binary backpropagation in content addressable memory}
    \field{year}{1990}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Brodsky:93}{article}{}
    \name{author}{3}{}{%
      {{hash=BSA}{%
         family={Brodsky},
         familyi={B\bibinitperiod},
         given={Stephen\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=MGC}{%
         family={Marsden},
         familyi={M\bibinitperiod},
         given={Gary\bibnamedelima C.},
         giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=GCC}{%
         family={Guest},
         familyi={G\bibinitperiod},
         given={Clark\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {OSA}%
    }
    \keyw{BNN, Cylindrical lenses; Light valves; Neural networks; Optical
  components; Optical neural systems; Parallel processing}
    \strng{namehash}{BSAMGCGCC1}
    \strng{fullhash}{BSAMGCGCC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    The content-addressable network (CAN) is an efficient, intrinsically
  discrete training algorithm for binary-valued classification networks. The
  binary nature of the CAN network permits accelerated learning and
  significantly reduced hardware-implementation requirements. A multilayer
  optoelectronic CAN network employing matrix--vector multiplication was
  constructed. The network learned and correctly classified trained patterns,
  gaining a measure of fault tolerance by learning associative solutions to
  optical hardware imperfections. Operation of this system is possible owing to
  the reduced hardware accuracy requirements of the CAN learning algorithm.%
    }
    \verb{doi}
    \verb 10.1364/AO.32.001338
    \endverb
    \field{number}{8}
    \field{pages}{1338\bibrangedash 1345}
    \field{title}{Optical matrix--vector implementation of the
  content-addressable network}
    \verb{url}
    \verb http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338
    \endverb
    \field{volume}{32}
    \field{journaltitle}{Appl. Opt.}
    \field{year}{1993}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5159360}{article}{}
    \name{author}{5}{}{%
      {{hash=CF}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
      {{hash=CG}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=HQ}{%
         family={He},
         familyi={H\bibinitperiod},
         given={Q.},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=HG}{%
         family={He},
         familyi={H\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=XX}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
    }
    \keyw{BNN; Boolean functions;learning (artificial intelligence);multilayer
  perceptrons;binary neural network;linearly nonseparable Boolean
  functions;nonLSBF;DNA-like learning and decomposing algorithm;DNA-like
  LDA;DNA-like offset sequence;logic XOR operation;weight-threshold
  value;multilayer perceptron;function mapping;parity Boolean function;Neural
  networks;Boolean functions;Neurons;Multilayer perceptrons;Linear discriminant
  analysis;Logic;Backpropagation algorithms;Input variables;Mathematics;Binary
  neural network;DNA-like learning and decomposing algorithm (DNA-like
  LDA);linearly nonseparable Boolean function (non-LSBF);multilayer perceptron
  (MLP);parity Boolean function (PBF);Algorithms;Artificial
  Intelligence;DNA;Linear Models;Neural Networks (Computer)}
    \strng{namehash}{CFCGHQHGXX1}
    \strng{fullhash}{CFCGHQHGXX1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Implementing linearly nonseparable Boolean functions (non-LSBF) has been an
  important and yet challenging task due to the extremely high complexity of
  this kind of functions and the exponentially increasing percentage of the
  number of non-LSBF in the entire set of Boolean functions as the number of
  input variables increases. In this paper, an algorithm named DNA-like
  learning and decomposing algorithm (DNA-like LDA) is proposed, which is
  capable of effectively implementing non-LSBF. The novel algorithm first
  trains the DNA-like offset sequence and decomposes non-LSBF into logic XOR
  operations of a sequence of LSBF, and then determines the weight-threshold
  values of the multilayer perceptron (MLP) that perform both the
  decompositions of LSBF and the function mapping the hidden neurons to the
  output neuron. The algorithm is validated by two typical examples about the
  problem of approximating the circular region and the well-known
  &lt;i&gt;n&lt;/i&gt; -bit parity Boolean function (PBF).%
    }
    \verb{doi}
    \verb 10.1109/TNN.2009.2023122
    \endverb
    \field{issn}{1045-9227}
    \field{number}{8}
    \field{pages}{1293\bibrangedash 1301}
    \field{title}{Universal Perceptron and DNA-Like Learning Algorithm for
  Binary Neural Networks: Non-LSBF Implementation}
    \field{volume}{20}
    \field{journaltitle}{IEEE Transactions on Neural Networks}
    \field{year}{2009}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{4250190}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=CL}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=AD}{%
         family={Alahakoon},
         familyi={A\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; backpropagation;neural nets;pattern
  classification;search problems;topology;neuroevolution of augmenting
  topologies;augmenting topology;data classification learning;neural
  network;learning-NEAT training scheme;backpropagation;search problem;Network
  topology;Artificial neural networks;Neural networks;Supervised
  learning;Biological cells;Information technology;Evolutionary
  computation;Unsupervised learning;Technological innovation}
    \strng{namehash}{CLAD1}
    \strng{fullhash}{CLAD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Appropriate topology and connection weight are two very important
  properties a neural network must have in order to successfully perform data
  classification. In this paper, we propose a hybrid training scheme
  Learning-NEAT (L-NEAT) for data classification problem. L-NEAT simplifies
  evolution by dividing the complete problem domain into sub tasks and learn
  the sub tasks by incorporating back propagation rule into the NeuroEvolution
  of Augmenting Topologies (NEAT) algorithm. The new algorithm combines the
  strength of searching for topology and weights from NEAT and back propagation
  respectively while overcoming problems associated with direct use of NEAT. We
  claim that L-NEAT can produce neural network for classification problem
  effectively and efficiently. Empirical evaluation shows that L-NEAT evolves
  classifying neural network with good generalization ability. Its accuracy
  outperforms original NEAT.%
    }
    \field{booktitle}{2006 International Conference on Information and
  Automation}
    \verb{doi}
    \verb 10.1109/ICINFA.2006.374100
    \endverb
    \field{issn}{2151-1802}
    \field{pages}{367\bibrangedash 371}
    \field{title}{NeuroEvolution of Augmenting Topologies with Learning for
  Data Classification}
    \field{year}{2006}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Chen2018UnderstandingTL}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=hCY}{%
         prefix={hsin},
         prefixi={h\bibinitperiod},
         family={Chen},
         familyi={C\bibinitperiod},
         given={Yu},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YTJ}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Tien-Ju},
         giveni={T\bibinithyphendelim J\bibinitperiod},
      }}%
      {{hash=EJS}{%
         family={Emer},
         familyi={E\bibinitperiod},
         given={Joel\bibnamedelima S.},
         giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=SV}{%
         family={Sze},
         familyi={S\bibinitperiod},
         given={Vivienne},
         giveni={V\bibinitperiod},
      }}%
    }
    \keyw{NN, DNN, Hardware, FPGA, ASIC}
    \strng{namehash}{CYhYTJEJSSV1}
    \strng{fullhash}{CYhYTJEJSSV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{booktitle}{SysML Conference}
    \field{title}{Understanding the Limitations of Existing Energy-Efficient
  Design Approaches for Deep Neural Networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{714090}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=CCH}{%
         family={Chu},
         familyi={C\bibinitperiod},
         given={C.\bibnamedelima H.},
         giveni={C\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=KJH}{%
         family={Kim},
         familyi={K\bibinitperiod},
         given={J.\bibnamedelima H.},
         giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \keyw{BNN; feedforward neural nets;pattern classification;learning
  (artificial intelligence);medical diagnostic computing;geometrical
  learning;binary neural networks;pattern classification;expand-and-truncate
  learning algorithm;connecting weights;three-layered feedforward
  network;binary-to-binary mappings;breast cancer;Pattern classification;Neural
  networks;Neurons;Machine learning algorithms;Testing;Breast
  cancer;Databases;Hamming distance;Computer networks;Joining processes}
    \strng{namehash}{CCHKJH1}
    \strng{fullhash}{CCHKJH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    This paper considers the use of binary neural networks for pattern
  classification. An expand-and-truncate learning (ETL) algorithm is used to
  determine the required number of neurons as well as the connecting weights in
  a three-layered feedforward network for classifying input patterns. The ETL
  algorithm is guaranteed to find a network for any binary-to-binary mappings.
  The ETL algorithm's performance in pattern classification is tested using a
  breast cancer database that have been used for benchmarking performance other
  machine learning methods.%
    }
    \field{booktitle}{Proceedings of 1993 International Conference on Neural
  Networks (IJCNN-93-Nagoya, Japan)}
    \verb{doi}
    \verb 10.1109/IJCNN.1993.714090
    \endverb
    \field{pages}{1039\bibrangedash 1042 vol.1}
    \field{title}{Pattern classification by geometrical learning of binary
  neural networks}
    \field{volume}{1}
    \field{year}{1993}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2015_5647}{incollection}{}
    \name{author}{3}{}{%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=DJP}{%
         family={David},
         familyi={D\bibinitperiod},
         given={Jean-Pierre},
         giveni={J\bibinithyphendelim P\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=CC}{%
         family={Cortes},
         familyi={C\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LND}{%
         family={Lawrence},
         familyi={L\bibinitperiod},
         given={N.\bibnamedelima D.},
         giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=LDD}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={D.\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sugiyama},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{BNN; Machine Learning; Computer Vision; Pattern Recognition; Neural
  and Evolutionary Computing}
    \strng{namehash}{CMBYDJP1}
    \strng{fullhash}{CMBYDJP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{booktitle}{Advances in Neural Information Processing Systems 28}
    \field{pages}{3123\bibrangedash 3131}
    \field{title}{BinaryConnect: Training Deep Neural Networks with binary
  weights during propagations}
    \field{annotation}{%
  http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf%
    }
    \field{year}{2015}
  \endentry

  \entry{s18051306}{article}{}
    \name{author}{5}{}{%
      {{hash=CM}{%
         family={Coutinho},
         familyi={C\bibinitperiod},
         given={Murilo},
         giveni={M\bibinitperiod},
      }}%
      {{hash=dOAR}{%
         prefix={de},
         prefixi={d\bibinitperiod},
         family={Oliveira\bibnamedelima Albuquerque},
         familyi={O\bibinitperiod\bibinitdelim A\bibinitperiod},
         given={Robson},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Borges},
         familyi={B\bibinitperiod},
         given={F{\'a}bio},
         giveni={F\bibinitperiod},
      }}%
      {{hash=GVLJ}{%
         family={Garc{\'\i}a\bibnamedelima Villalba},
         familyi={G\bibinitperiod\bibinitdelim V\bibinitperiod},
         given={Luis\bibnamedelima Javier},
         giveni={L\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=KTH}{%
         family={Kim},
         familyi={K\bibinitperiod},
         given={Tai-Hoon},
         giveni={T\bibinithyphendelim H\bibinitperiod},
      }}%
    }
    \keyw{DCGAN, Cryptography, Adversarial Neural Cryptography, Cryptonet}
    \strng{namehash}{CMOARdBFGVLJKTH1}
    \strng{fullhash}{CMOARdBFGVLJKTH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Researches in Artificial Intelligence (AI) have achieved many important
  breakthroughs, especially in recent years. In some cases, AI learns alone
  from scratch and performs human tasks faster and better than humans. With the
  recent advances in AI, it is natural to wonder whether Artificial Neural
  Networks will be used to successfully create or break cryptographic
  algorithms. Bibliographic review shows the main approach to this problem have
  been addressed throughout complex Neural Networks, but without understanding
  or proving the security of the generated model. This paper presents an
  analysis of the security of cryptographic algorithms generated by a new
  technique called Adversarial Neural Cryptography (ANC). Using the proposed
  network, we show limitations and directions to improve the current approach
  of ANC. Training the proposed Artificial Neural Network with the improved
  model of ANC, we show that artificially intelligent agents can learn the
  unbreakable One-Time Pad (OTP) algorithm, without human knowledge, to
  communicate securely through an insecure communication channel. This paper
  shows in which conditions an AI agent can learn a secure encryption scheme.
  However, it also shows that, without a stronger adversary, it is more likely
  to obtain an insecure one.%
    }
    \verb{doi}
    \verb 10.3390/s18051306
    \endverb
    \field{issn}{1424-8220}
    \field{number}{5}
    \field{title}{Learning Perfectly Secure Cryptography to Protect
  Communications with Adversarial Neural Cryptography}
    \verb{url}
    \verb http://www.mdpi.com/1424-8220/18/5/1306
    \endverb
    \field{volume}{18}
    \field{journaltitle}{Sensors}
    \field{year}{2018}
  \endentry

  \entry{DENG201849}{article}{}
    \name{author}{5}{}{%
      {{hash=DL}{%
         family={Deng},
         familyi={D\bibinitperiod},
         given={Lei},
         giveni={L\bibinitperiod},
      }}%
      {{hash=JP}{%
         family={Jiao},
         familyi={J\bibinitperiod},
         given={Peng},
         giveni={P\bibinitperiod},
      }}%
      {{hash=PJ}{%
         family={Pei},
         familyi={P\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Zhenzhi},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=LG}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Guoqi},
         giveni={G\bibinitperiod},
      }}%
    }
    \keyw{BNN, GXNOR-Net, Discrete state transition, Ternary neural networks,
  Sparse binary networks}
    \strng{namehash}{DLJPPJWZLG1}
    \strng{fullhash}{DLJPPJWZLG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \verb{doi}
    \verb https://doi.org/10.1016/j.neunet.2018.01.010
    \endverb
    \field{issn}{0893-6080}
    \field{pages}{49 \bibrangedash  58}
    \field{title}{GXNOR-Net: Training deep neural networks with ternary weights
  and activations without full-precision memory under a unified discretization
  framework}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0893608018300108
    \endverb
    \field{volume}{100}
    \field{journaltitle}{Neural Networks}
    \field{year}{2018}
  \endentry

  \entry{doi:10.1177/1555343417695197}{article}{}
    \name{author}{1}{}{%
      {{hash=EMR}{%
         family={Endsley},
         familyi={E\bibinitperiod},
         given={Mica\bibnamedelima R.},
         giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \keyw{NN, autopilot, tesla}
    \strng{namehash}{EMR1}
    \strng{fullhash}{EMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \field{abstract}{%
    Autonomous and semiautonomous vehicles are currently being developed by
  over14 companies. These vehicles may improve driving safety and convenience,
  or they may create new challenges for drivers, particularly with regard to
  situation awareness (SA) and autonomy interaction. I conducted a naturalistic
  driving study on the autonomy features in the Tesla Model S, recording my
  experiences over a 6-month period, including assessments of SA and problems
  with the autonomy. This preliminary analysis provides insights into the
  challenges that drivers may face in dealing with new autonomous automobiles
  in realistic driving conditions, and it extends previous research on
  human-autonomy interaction to the driving domain. Issues were found with
  driver training, mental model development, mode confusion, unexpected mode
  interactions, SA, and susceptibility to distraction. New insights into
  challenges with semiautonomous driving systems include increased variability
  in SA, the replacement of continuous control with serial discrete control,
  and the need for more complex decisions. Issues that deserve consideration in
  future research and a set of guidelines for driver interfaces of autonomous
  systems are presented and used to create recommendations for improving driver
  SA when interacting with autonomous vehicles.%
    }
    \verb{doi}
    \verb 10.1177/1555343417695197
    \endverb
    \verb{eprint}
    \verb https://doi.org/10.1177/1555343417695197
    \endverb
    \field{number}{3}
    \field{pages}{225\bibrangedash 238}
    \field{title}{Autonomous Driving Systems: A Preliminary Naturalistic Study
  of the Tesla Model S}
    \verb{url}
    \verb https://doi.org/10.1177/1555343417695197
    \endverb
    \field{volume}{11}
    \field{journaltitle}{Journal of Cognitive Engineering and Decision Making}
    \field{year}{2017}
  \endentry

  \entry{Faraone_2018_CVPR}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=FJ}{%
         family={Faraone},
         familyi={F\bibinitperiod},
         given={Julian},
         giveni={J\bibinitperiod},
      }}%
      {{hash=FN}{%
         family={Fraser},
         familyi={F\bibinitperiod},
         given={Nicholas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Blott},
         familyi={B\bibinitperiod},
         given={Michaela},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LPH}{%
         family={Leong},
         familyi={L\bibinitperiod},
         given={Philip\bibnamedelima H.W.},
         giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \keyw{BNN; Quantization, FPGA, LUT}
    \strng{namehash}{FJFNBMLPH1}
    \strng{fullhash}{FJFNBMLPH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{booktitle}{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}
    \field{title}{SYQ: Learning Symmetric Quantization for Efficient Deep
  Neural Networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{6591252}{article}{}
    \name{author}{3}{}{%
      {{hash=FLJ}{%
         family={Fogel},
         familyi={F\bibinitperiod},
         given={L.\bibnamedelima J.},
         giveni={L\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=OAJ}{%
         family={Owens},
         familyi={O\bibinitperiod},
         given={A.\bibnamedelima J.},
         giveni={A\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=WMJ}{%
         family={Walsh},
         familyi={W\bibinitperiod},
         given={M.\bibnamedelima J.},
         giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; Noise;Automata;Decision making;Markov
  processes;Prediction algorithms;Shape;Springs}
    \strng{namehash}{FLJOAJWMJ1}
    \strng{fullhash}{FLJOAJWMJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Artificial intelligence can be approached through the fast-time evolution
  of finite-state machines. Random mutation of an arbitrary machine yields an
  ``offspring.'' Both machines are driven by the available history and
  evaluated in terms of the given goal, and the machine having the higher score
  is selected to serve as the new parent. Such fast-time mutation and selection
  is continued with real-time decisions being based on the logic of the
  surviving machine. Saving the best few machines increases the security
  against gross nonstationarity of the environment. The efficiency of the
  evolutionary program is improved by introducing a cost-for-complexity
  weighting on each machine. An ability to predict one's environment is
  prerequisite to purposeful behavior. With this in mind, IBM7094 experiments
  were conducted to examine evolutionary prediction. As expected, cyclic
  signals in various degrees of noise were soon characterized by the
  predictor-machines. The transition probabilities within the sequence of
  predictions of low-order Markov processes were in close correspondence with
  those of the environment. The evolutionary program was also required to
  predict the (4-symbol) output sequence of an arbitrary machine that was
  driven by random binary noise. After 160 predictions the percent correct
  reached 51.5. When the evolutionary program was also given, the input binary
  variable this score reached 80 percent, showing a rapid approach toward the
  100 percent asymptote. In contrast, providing an uncorrelated binary variable
  degraded the performance to 40.5 percent by requiring an attempt to extract
  nonexistent information. A formal technique was devised which translates a
  predictor machine into a set of hypotheses concerning the logic of the
  environment.%
    }
    \verb{doi}
    \verb 10.1109/THFE.1965.6591252
    \endverb
    \field{issn}{0096-249X}
    \field{number}{1}
    \field{pages}{13\bibrangedash 23}
    \field{title}{Intelligent decision-making through a simulation of
  evolution}
    \field{volume}{HFE-6}
    \field{journaltitle}{IEEE Transactions on Human Factors in Electronics}
    \field{year}{1965}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{616215}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=FN}{%
         family={Funabiki},
         familyi={F\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
      {{hash=KJ}{%
         family={Kitamichi},
         familyi={K\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=NS}{%
         family={Nishikawa},
         familyi={N\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; neural nets;genetic algorithms;set theory;graph
  theory;minimisation;computational complexity;evolutionary neural network
  algorithm;ENN;max cut problems;undirected graph;NP-hard
  problem;partition;disjoint subsets;evolutionary initialization scheme;energy
  minimization criteria;binary neural network;randomly weighted complete
  graphs;unweighted random graphs;maximum neural network;mean field
  annealing;simulated annealing;greedy algorithm;Neural
  networks;Neurons;Simulated annealing;NP-hard problem;Equations;Multi-layer
  neural network;Computer networks;Minimization;Greedy algorithms;Approximation
  algorithms}
    \strng{namehash}{FNKJNS1}
    \strng{fullhash}{FNKJNS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    An "evolutionary neural network (ENN)" is presented for the max cut problem
  of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem
  is to find a partition of V into two disjoint subsets such that the cut size
  be maximized. The cut size is the sum of weights on edges in E whose
  endpoints belong to different subsets. The ENN combines the evolutionary
  initialization scheme of the neural state into the energy minimization
  criteria of the binary neural network. The performance of ENN is evaluated
  through simulations in randomly weighted complete graphs and unweighted
  random graphs with up to 1000 vertices. The results show that the
  evolutionary initialization scheme drastically improves the solution quality.
  ENN can always find better solutions than the maximum neural network, the
  mean field annealing, the simulated annealing, and the greedy algorithm.%
    }
    \field{booktitle}{Proceedings of International Conference on Neural
  Networks (ICNN'97)}
    \verb{doi}
    \verb 10.1109/ICNN.1997.616215
    \endverb
    \field{pages}{1260\bibrangedash 1265 vol.2}
    \field{title}{An evolutionary neural network algorithm for max cut
  problems}
    \field{volume}{2}
    \field{year}{1997}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{pmlr-v48-gilad-bachrach16}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=GBR}{%
         family={Gilad-Bachrach},
         familyi={G\bibinithyphendelim B\bibinitperiod},
         given={Ran},
         giveni={R\bibinitperiod},
      }}%
      {{hash=DN}{%
         family={Dowlin},
         familyi={D\bibinitperiod},
         given={Nathan},
         giveni={N\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Laine},
         familyi={L\bibinitperiod},
         given={Kim},
         giveni={K\bibinitperiod},
      }}%
      {{hash=LK}{%
         family={Lauter},
         familyi={L\bibinitperiod},
         given={Kristin},
         giveni={K\bibinitperiod},
      }}%
      {{hash=NM}{%
         family={Naehrig},
         familyi={N\bibinitperiod},
         given={Michael},
         giveni={M\bibinitperiod},
      }}%
      {{hash=WJ}{%
         family={Wernsing},
         familyi={W\bibinitperiod},
         given={John},
         giveni={J\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=BMF}{%
         family={Balcan},
         familyi={B\bibinitperiod},
         given={Maria\bibnamedelima Florina},
         giveni={M\bibinitperiod\bibinitdelim F\bibinitperiod},
      }}%
      {{hash=WKQ}{%
         family={Weinberger},
         familyi={W\bibinitperiod},
         given={Kilian\bibnamedelima Q.},
         giveni={K\bibinitperiod\bibinitdelim Q\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {PMLR}%
    }
    \keyw{NN; Cryptography, Neural Network, Homomorphic encryption, Cryptonet}
    \strng{namehash}{GBRDNLKLKNMWJ1}
    \strng{fullhash}{GBRDNLKLKNMWJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    Applying machine learning to a problem which involves medical, financial,
  or other types of sensitive data, not only requires accurate predictions but
  also careful attention to maintaining data privacy and security. Legal and
  ethical requirements may prevent the use of cloud-based machine learning
  solutions for such tasks. In this work, we will present a method to convert
  learned neural networks to CryptoNets, neural networks that can be applied to
  encrypted data. This allows a data owner to send their data in an encrypted
  form to a cloud service that hosts the network. The encryption ensures that
  the data remains confidential since the cloud does not have access to the
  keys needed to decrypt it. Nevertheless, we will show that the cloud service
  is capable of applying the neural network to the encrypted data to make
  encrypted predictions, and also return them in encrypted form. These
  encrypted predictions can be sent back to the owner of the secret key who can
  decrypt them. Therefore, the cloud service does not gain any information
  about the raw data nor about the prediction it made. We demonstrate
  CryptoNets on the MNIST optical character recognition tasks. CryptoNets
  achieve 99% accuracy and can make around 59000 predictions per hour on a
  single PC. Therefore, they allow high throughput, accurate, and private
  predictions.%
    }
    \field{booktitle}{Proceedings of The 33rd International Conference on
  Machine Learning}
    \field{pages}{201\bibrangedash 210}
    \field{series}{Proceedings of Machine Learning Research}
    \field{title}{CryptoNets: Applying Neural Networks to Encrypted Data with
  High Throughput and Accuracy}
    \verb{url}
    \verb http://proceedings.mlr.press/v48/gilad-bachrach16.html
    \endverb
    \field{volume}{48}
    \list{location}{1}{%
      {New York, New York, USA}%
    }
    \verb{file}
    \verb http://proceedings.mlr.press/v48/gilad-bachrach16.pdf
    \endverb
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5607329}{article}{}
    \name{author}{3}{}{%
      {{hash=GA}{%
         family={Gomperts},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=UA}{%
         family={Ukil},
         familyi={U\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=ZF}{%
         family={Zurfluh},
         familyi={Z\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{backpropagation;field programmable gate arrays;hardware description
  languages;multilayer perceptrons;FPGA;VLSI hardware description
  language;arithmetic operation;artificial neural network;backpropagation
  multilayer perceptron;fast prototyping;field programmable gate array;general
  purpose neural network;hardware-based MLP;learning capability;online
  application;space exploration;Backpropagation;NIR spectra
  calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware
  implementation;multilayer perceptron;neural network;spectroscopy}
    \strng{namehash}{GAUAZF1}
    \strng{fullhash}{GAUAZF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    This paper presents the development and implementation of a generalized
  backpropagation multilayer perceptron (MLP) architecture described in VLSI
  hardware description language (VHDL). The development of hardware platforms
  has been complicated by the high hardware cost and quantity of the arithmetic
  operations required in online artificial neural networks (ANNs), i.e.,
  general purpose ANNs with learning capability. Besides, there remains a
  dearth of hardware platforms for design space exploration, fast prototyping,
  and testing of these networks. Our general purpose architecture seeks to fill
  that gap and at the same time serve as a tool to gain a better understanding
  of issues unique to ANNs implemented in hardware, particularly using field
  programmable gate array (FPGA). The challenge is thus to find an architecture
  that minimizes hardware costs, while maximizing performance, accuracy, and
  parameterization. This work describes a platform that offers a high degree of
  parameterization, while maintaining generalized network design with
  performance comparable to other hardware-based MLP implementations.
  Application of the hardware implementation of ANN with backpropagation
  learning algorithm for a realistic application is also presented.%
    }
    \verb{doi}
    \verb 10.1109/TII.2010.2085006
    \endverb
    \field{issn}{1551-3203}
    \field{number}{1}
    \field{pages}{78\bibrangedash 89}
    \field{title}{Development and Implementation of Parameterized FPGA-Based
  General Purpose Neural Networks for Online Applications}
    \field{volume}{7}
    \field{journaltitle}{IEEE Transactions on Industrial Informatics}
    \field{year}{2011}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Guo2018FBNAAF}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=GP}{%
         family={Guo},
         familyi={G\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
      {{hash=MH}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Ruizhi},
         giveni={R\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Pin},
         giveni={P\bibinitperiod},
      }}%
      {{hash=XS}{%
         family={Xie},
         familyi={X\bibinitperiod},
         given={Shaolin},
         giveni={S\bibinitperiod},
      }}%
      {{hash=WD}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Donglin},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {Trinity College, Dublin}%
    }
    \keyw{BNN, Neural Network, FPGA, Accelerator}
    \strng{namehash}{GPMHCRLPXSWD1}
    \strng{fullhash}{GPMHCRLPXSWD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{booktitle}{FPL 2018}
    \field{title}{FBNA: A Fully Binarized Neural Network Accelerator}
    \field{volume}{1}
    \field{year}{2018}
  \endentry

  \entry{Han:2016:EEI:3001136.3001163}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=HS}{%
         family={Han},
         familyi={H\bibinitperiod},
         given={Song},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LX}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Xingyu},
         giveni={X\bibinitperiod},
      }}%
      {{hash=MH}{%
         family={Mao},
         familyi={M\bibinitperiod},
         given={Huizi},
         giveni={H\bibinitperiod},
      }}%
      {{hash=PJ}{%
         family={Pu},
         familyi={P\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=PA}{%
         family={Pedram},
         familyi={P\bibinitperiod},
         given={Ardavan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HMA}{%
         family={Horowitz},
         familyi={H\bibinitperiod},
         given={Mark\bibnamedelima A.},
         giveni={M\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=DWJ}{%
         family={Dally},
         familyi={D\bibinitperiod},
         given={William\bibnamedelima J.},
         giveni={W\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IEEE Press}%
    }
    \keyw{FPGA-NN, ASIC, algorithm-hardware co-design, deep learning, hardware
  acceleration, model compression}
    \strng{namehash}{HSLXMHPJPAHMADWJ1}
    \strng{fullhash}{HSLXMHPJPAHMADWJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Proceedings of the 43rd International Symposium on
  Computer Architecture}
    \verb{doi}
    \verb 10.1109/ISCA.2016.30
    \endverb
    \field{isbn}{978-1-4673-8947-1}
    \field{pages}{243\bibrangedash 254}
    \field{series}{ISCA '16}
    \field{title}{EIE: Efficient Inference Engine on Compressed Deep Neural
  Network}
    \verb{url}
    \verb https://doi.org/10.1109/ISCA.2016.30
    \endverb
    \list{location}{1}{%
      {Seoul, Republic of Korea}%
    }
    \field{year}{2016}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{7780459}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HK}{%
         family={He},
         familyi={H\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZX}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Ren},
         familyi={R\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{CNN, image classification; AI; neural nets;object detection, RESNET}
    \strng{namehash}{HKZXRSSJ1}
    \strng{fullhash}{HKZXRSSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Deeper neural networks are more difficult to train. We present a residual
  learning framework to ease the training of networks that are substantially
  deeper than those used previously. We explicitly reformulate the layers as
  learning residual functions with reference to the layer inputs, instead of
  learning unreferenced functions. We provide comprehensive empirical evidence
  showing that these residual networks are easier to optimize, and can gain
  accuracy from considerably increased depth. On the ImageNet dataset we
  evaluate residual nets with a depth of up to 152 layers - 8; deeper than VGG
  nets [40] but still having lower complexity. An ensemble of these residual
  nets achieves 3.57% error on the ImageNet test set. This result won the 1st
  place on the ILSVRC 2015 classification task. We also present analysis on
  CIFAR-10 with 100 and 1000 layers. The depth of representations is of central
  importance for many visual recognition tasks. Solely due to our extremely
  deep representations, we obtain a 28% relative improvement on the COCO object
  detection dataset. Deep residual nets are foundations of our submissions to
  ILSVRC; COCO 2015 competitions1, where we also won the 1st places on the
  tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO
  segmentation.%
    }
    \field{booktitle}{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}
    \verb{doi}
    \verb 10.1109/CVPR.2016.90
    \endverb
    \field{issn}{1063-6919}
    \field{pages}{770\bibrangedash 778}
    \field{title}{Deep Residual Learning for Image Recognition}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{He:2015:DDR:2919332.2919814}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HK}{%
         family={He},
         familyi={H\bibinitperiod},
         given={Kaiming},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZX}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Xiangyu},
         giveni={X\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Ren},
         familyi={R\bibinitperiod},
         given={Shaoqing},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Jian},
         giveni={J\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IEEE Computer Society}%
    }
    \keyw{NN; ReLu, ImageNet, Microsoft}
    \strng{namehash}{HKZXRSSJ2}
    \strng{fullhash}{HKZXRSSJ2}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Proceedings of the 2015 IEEE International Conference on
  Computer Vision (ICCV)}
    \verb{doi}
    \verb 10.1109/ICCV.2015.123
    \endverb
    \field{isbn}{978-1-4673-8391-2}
    \field{pages}{1026\bibrangedash 1034}
    \field{series}{ICCV '15}
    \field{title}{Delving Deep into Rectifiers: Surpassing Human-Level
  Performance on ImageNet Classification}
    \verb{url}
    \verb http://dx.doi.org/10.1109/ICCV.2015.123
    \endverb
    \list{location}{1}{%
      {Washington, DC, USA}%
    }
    \field{year}{2015}
  \endentry

  \entry{Hesamifard2017CryptoDLDN}{article}{}
    \name{author}{3}{}{%
      {{hash=HE}{%
         family={Hesamifard},
         familyi={H\bibinitperiod},
         given={Ehsan},
         giveni={E\bibinitperiod},
      }}%
      {{hash=TH}{%
         family={Takabi},
         familyi={T\bibinitperiod},
         given={Hassan},
         giveni={H\bibinitperiod},
      }}%
      {{hash=GM}{%
         family={Ghasemi},
         familyi={G\bibinitperiod},
         given={Mehdi},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{NN, Cryptography, Homomorphic encryption, MNIST}
    \strng{namehash}{HETHGM1}
    \strng{fullhash}{HETHGM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{title}{CryptoDL: Deep Neural Networks over Encrypted Data}
    \field{volume}{abs/1711.05189}
    \field{journaltitle}{CoRR}
    \field{year}{2017}
  \endentry

  \entry{8425178}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=HY}{%
         family={Hu},
         familyi={H\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZJ}{%
         family={Zhai},
         familyi={Z\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LD}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=GY}{%
         family={Gong},
         familyi={G\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhu},
         familyi={Z\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LW}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Su},
         familyi={S\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=JJ}{%
         family={Jin},
         familyi={J\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{BNN;computational complexity;field programmable gate arrays;graphics
  processing units;learning (artificial intelligence);multiprocessing
  systems;neural nets;optimisation;parallel processing;vector
  parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big
  bang;Deep Neural Networks;high computational complexity;Binary Neural
  Networks;BNNs;arithmetic operations;bitwise operations;image-to-column
  method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow
  features;efficient binary convolution;VGG network;counterpart full-precision
  DNNs;GPU;Convolution;Neural networks;Layout;Parallel
  processing;Acceleration;Graphics processing units;Machine learning;Network
  Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi}
    \strng{namehash}{HYZJLDGYZYLWSLJJ1}
    \strng{fullhash}{HYZJLDGYZYLWSLJJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Deep learning has revolutionized computer vision and other fields since its
  big bang in 2012. However, it is challenging to deploy Deep Neural Networks
  (DNNs) into real-world applications due to their high computational
  complexity. Binary Neural Networks (BNNs) dramatically reduce computational
  complexity by replacing most arithmetic operations with bitwise operations.
  Existing implementations of BNNs have been focusing on GPU or FPGA, and using
  the conventional image-to-column method that doesn't perform well for binary
  convolution due to low arithmetic intensity and unfriendly pattern for
  bitwise operations. We propose BitFlow, a gemm-operator-network three-level
  optimization framework for fully exploiting the computing power of BNNs on
  CPU. BitFlow features a new class of algorithm named PressedConv for
  efficient binary convolution using locality-aware layout and vector
  parallelism. We evaluate BitFlow with the VGG network. On a single core of
  Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN
  implementations, and 11.5x speedup over counterpart full-precision DNNs. Over
  64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart
  full-precision DNNs on GPU (GTX 1080).%
    }
    \field{booktitle}{2018 IEEE International Parallel and Distributed
  Processing Symposium (IPDPS)}
    \verb{doi}
    \verb 10.1109/IPDPS.2018.00034
    \endverb
    \field{issn}{1530-2075}
    \field{pages}{244\bibrangedash 253}
    \field{title}{BitFlow: Exploiting Vector Parallelism for Binary Neural
  Networks on CPU}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2016_6573}{incollection}{}
    \name{author}{5}{}{%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=EYR}{%
         family={El-Yaniv},
         familyi={E\bibinithyphendelim Y\bibinitperiod},
         given={Ran},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=LDD}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={D.\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sugiyama},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LUV}{%
         family={Luxburg},
         familyi={L\bibinitperiod},
         given={U.\bibnamedelima V.},
         giveni={U\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
      {{hash=GI}{%
         family={Guyon},
         familyi={G\bibinitperiod},
         given={I.},
         giveni={I\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{BNN; Neural Network}
    \strng{namehash}{HICMSDEYRBY1}
    \strng{fullhash}{HICMSDEYRBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Advances in Neural Information Processing Systems 29}
    \field{pages}{4107\bibrangedash 4115}
    \field{title}{Binarized Neural Networks}
    \verb{url}
    \verb http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf
    \endverb
    \field{year}{2016}
  \endentry

  \entry{Ioffe:2015:BNA:3045118.3045167}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=IS}{%
         family={Ioffe},
         familyi={I\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SC}{%
         family={Szegedy},
         familyi={S\bibinitperiod},
         given={Christian},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {JMLR.org}%
    }
    \keyw{NN, batch normalization operation, Google}
    \strng{namehash}{ISSC1}
    \strng{fullhash}{ISSC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{I}
    \field{sortinithash}{I}
    \field{booktitle}{Proceedings of the 32Nd International Conference on
  International Conference on Machine Learning - Volume 37}
    \field{pages}{448\bibrangedash 456}
    \field{series}{ICML'15}
    \field{title}{Batch Normalization: Accelerating Deep Network Training by
  Reducing Internal Covariate Shift}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=3045118.3045167
    \endverb
    \list{location}{1}{%
      {Lille, France}%
    }
    \field{year}{2015}
  \endentry

  \entry{8056820}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=JL}{%
         family={Jiao},
         familyi={J\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LC}{%
         family={Luo},
         familyi={L\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CW}{%
         family={Cao},
         familyi={C\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=ZX}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=WL}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; digital arithmetic;embedded systems;field programmable gate
  arrays;neural nets;low bit-width CNN accelerators;Zynq XC7Z020
  FPGA;DoReFa-Net neural nets;two-stage arithmetic unit;embedded
  systems;embedded systems;binarized activations;binarized weights;Binarized
  Neural Networks;complex computation;high classification accuracy;embedded
  FPGA;low bit-width convolutional neural networks;Kernel;Field programmable
  gate arrays;Table lookup;Neural networks;Computational modeling;Quantization
  (signal);Convolution}
    \strng{namehash}{JLLCCWZXWL1}
    \strng{fullhash}{JLLCCWZXWL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{J}
    \field{sortinithash}{J}
    \field{abstract}{%
    Convolutional Neural Networks (CNNs) can achieve high classification
  accuracy while they require complex computation. Binarized Neural Networks
  (BNNs) with binarized weights and activations can simplify computation but
  suffer from obvious accuracy loss. In this paper, low bit-width CNNs, BNNs
  and standard CNNs are compared to show that low bit-width CNNs is better
  suited for embedded systems. An architecture based on the two-stage
  arithmetic unit (TSAU) as the basic processing element is proposed to process
  each layer iteratively for low bit-width CNN accelerators. Then the
  DoReFa-Net which is trained with weights and activations represented in 1 bit
  and 2 bits respectively is implemented on Zynq XC7Z020 FPGA with a 410.2 GOPS
  performance. The accelerator can meet the real-time requirement of embedded
  applications with a 106 FPS throughput and a 73.1% top-5 accuracy on the
  ImageNet dataset. The accelerator outperforms existing FPGA-based CNN
  accelerators in the tradeoff among accuracy, energy and resource efficiency.%
    }
    \field{booktitle}{2017 27th International Conference on Field Programmable
  Logic and Applications (FPL)}
    \verb{doi}
    \verb 10.23919/FPL.2017.8056820
    \endverb
    \field{issn}{1946-1488}
    \field{pages}{1\bibrangedash 4}
    \field{title}{Accelerating low bit-width convolutional neural networks with
  embedded FPGA}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{4790104}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KM}{%
         family={Kam},
         familyi={K\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN;State-space methods;Neural networks;Information analysis;Pattern
  analysis;Hopfield neural networks;Pattern recognition;Information
  retrieval;Convergence;Hamming distance;Content based retrieval}
    \strng{namehash}{KMCRGA1}
    \strng{fullhash}{KMCRGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Analysis of the state space for the fully-connected binary neural network
  ("the Hopfield model") remains an important objective in utilizing the
  network in pattern recognition and associative information retrieval. Most of
  the research pertaining to the network's state space so far concentrated on
  stable-state enumeration and often it was assumed that the patterns which are
  to be stored are random. We discuss the case of deterministic known codewords
  whose storage is required, and show that for this important case bounds on
  the retrieval probabilities and convergence rates can be achieved. The main
  tool which we employ is Birth-and-Death Markov chains, describing the Hamming
  distance of the network's state from the stored patterns. The results are
  applicable to both the asynchronous network and to the Boltzmann machine, and
  can be utilized to compare codeword sets in terms of efficiency of their
  retrieval, when the neural network is used as a content addressable memory.%
    }
    \field{booktitle}{1988 American Control Conference}
    \verb{doi}
    \verb 10.23919/ACC.1988.4790104
    \endverb
    \field{pages}{2276\bibrangedash 2281}
    \field{title}{On the State Space of the Binary Neural Network}
    \field{year}{1988}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Krizhevsky2999257}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KA}{%
         family={Krizhevsky},
         familyi={K\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SI}{%
         family={Sutskever},
         familyi={S\bibinitperiod},
         given={Ilya},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HGE}{%
         family={Hinton},
         familyi={H\bibinitperiod},
         given={Geoffrey\bibnamedelima E.},
         giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates Inc.}%
    }
    \keyw{CNN, ImageNet, ILSVRC, Convolution, AlexNet}
    \strng{namehash}{KASIHGE1}
    \strng{fullhash}{KASIHGE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{booktitle}{Proceedings of the 25th International Conference on
  Neural Information Processing Systems - Volume 1}
    \field{pages}{1097\bibrangedash 1105}
    \field{series}{NIPS'12}
    \field{title}{ImageNet Classification with Deep Convolutional Neural
  Networks}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=2999134.2999257
    \endverb
    \list{location}{1}{%
      {Lake Tahoe, Nevada}%
    }
    \field{year}{2012}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{726791}{article}{}
    \name{author}{4}{}{%
      {{hash=LY}{%
         family={Lecun},
         familyi={L\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Haffner},
         familyi={H\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{NN; optical character recognition;multilayer
  perceptrons;backpropagation;convolution;gradient-based learning;document
  recognition;multilayer neural networks;back-propagation;gradient based
  learning technique;complex decision surface synthesis;high-dimensional
  patterns;handwritten character recognition;handwritten digit recognition
  task;2D shape variability;document recognition systems;field
  extraction;segmentation recognition;language modeling;graph transformer
  networks;GTN;multimodule systems;performance measure minimization;cheque
  reading;convolutional neural network character recognizers;Neural
  networks;Pattern recognition;Machine learning;Optical character recognition
  software;Character recognition;Feature extraction;Multi-layer neural
  network;Optical computing;Hidden Markov models;Principal component analysis}
    \strng{namehash}{LYBLBYHP1}
    \strng{fullhash}{LYBLBYHP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Multilayer neural networks trained with the back-propagation algorithm
  constitute the best example of a successful gradient based learning
  technique. Given an appropriate network architecture, gradient-based learning
  algorithms can be used to synthesize a complex decision surface that can
  classify high-dimensional patterns, such as handwritten characters, with
  minimal preprocessing. This paper reviews various methods applied to
  handwritten character recognition and compares them on a standard handwritten
  digit recognition task. Convolutional neural networks, which are specifically
  designed to deal with the variability of 2D shapes, are shown to outperform
  all other techniques. Real-life document recognition systems are composed of
  multiple modules including field extraction, segmentation recognition, and
  language modeling. A new learning paradigm, called graph transformer networks
  (GTN), allows such multimodule systems to be trained globally using
  gradient-based methods so as to minimize an overall performance measure. Two
  systems for online handwriting recognition are described. Experiments
  demonstrate the advantage of global training, and the flexibility of graph
  transformer networks. A graph transformer network for reading a bank cheque
  is also described. It uses convolutional neural network character recognizers
  combined with global training techniques to provide record accuracy on
  business and personal cheques. It is deployed commercially and reads several
  million cheques per day.%
    }
    \verb{doi}
    \verb 10.1109/5.726791
    \endverb
    \field{issn}{0018-9219}
    \field{number}{11}
    \field{pages}{2278\bibrangedash 2324}
    \field{title}{Gradient-based learning applied to document recognition}
    \field{volume}{86}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{year}{1998}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2017arXiv171111294L}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LX}{%
         family={{Lin}},
         familyi={L\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=ZC}{%
         family={{Zhao}},
         familyi={Z\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=PW}{%
         family={{Pan}},
         familyi={P\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
    }
    \keyw{BNN; Computer Science; Learning; Statistics; Machine Learning}
    \strng{namehash}{LXZCPW1}
    \strng{fullhash}{LXZCPW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{booktitle}{31st Conference on Neural Information Processing Systems}
    \verb{eprint}
    \verb 1711.11294
    \endverb
    \field{title}{{Towards Accurate Binary Convolutional Neural Network}}
    \field{journaltitle}{NIPS 2017}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{month}{11}
    \field{year}{2017}
  \endentry

  \entry{Linnainmaa1976}{article}{}
    \name{author}{1}{}{%
      {{hash=LS}{%
         family={Linnainmaa},
         familyi={L\bibinitperiod},
         given={Seppo},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{NN, Neural Networks, Backpropagation}
    \strng{namehash}{LS1}
    \strng{fullhash}{LS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    The article describes analytic and algorithmic methods for determining the
  coefficients of the Taylor expansion of an accumulated rounding error with
  respect to the local rounding errors, and hence determining the influence of
  the local errors on the accumulated error. Second and higher order
  coefficients are also discussed, and some possible methods of reducing the
  extensive storage requirements are analyzed.%
    }
    \verb{doi}
    \verb 10.1007/BF01931367
    \endverb
    \field{issn}{1572-9125}
    \field{number}{2}
    \field{pages}{146\bibrangedash 160}
    \field{title}{Taylor expansion of the accumulated rounding error}
    \verb{url}
    \verb https://doi.org/10.1007/BF01931367
    \endverb
    \field{volume}{16}
    \field{journaltitle}{BIT Numerical Mathematics}
    \field{year}{1976}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Liu_2018_ECCV}{inproceedings}{}
    \name{author}{10}{}{%
      {{hash=LC}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Chenxi},
         giveni={C\bibinitperiod},
      }}%
      {{hash=ZB}{%
         family={Zoph},
         familyi={Z\bibinitperiod},
         given={Barret},
         giveni={B\bibinitperiod},
      }}%
      {{hash=NM}{%
         family={Neumann},
         familyi={N\bibinitperiod},
         given={Maxim},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Shlens},
         familyi={S\bibinitperiod},
         given={Jonathon},
         giveni={J\bibinitperiod},
      }}%
      {{hash=HW}{%
         family={Hua},
         familyi={H\bibinitperiod},
         given={Wei},
         giveni={W\bibinitperiod},
      }}%
      {{hash=LLJ}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Li-Jia},
         giveni={L\bibinithyphendelim J\bibinitperiod},
      }}%
      {{hash=FFL}{%
         family={Fei-Fei},
         familyi={F\bibinithyphendelim F\bibinitperiod},
         given={Li},
         giveni={L\bibinitperiod},
      }}%
      {{hash=YA}{%
         family={Yuille},
         familyi={Y\bibinitperiod},
         given={Alan},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HJ}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Jonathan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MK}{%
         family={Murphy},
         familyi={M\bibinitperiod},
         given={Kevin},
         giveni={K\bibinitperiod},
      }}%
    }
    \keyw{CNN; search problem, Reinforcement learning,}
    \strng{namehash}{LC+1}
    \strng{fullhash}{LCZBNMSJHWLLJFFLYAHJMK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{booktitle}{The European Conference on Computer Vision (ECCV)}
    \field{title}{Progressive Neural Architecture Search}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{McCulloch1943}{article}{}
    \name{author}{2}{}{%
      {{hash=MWS}{%
         family={McCulloch},
         familyi={M\bibinitperiod},
         given={Warren\bibnamedelima S.},
         giveni={W\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=PW}{%
         family={Pitts},
         familyi={P\bibinitperiod},
         given={Walter},
         giveni={W\bibinitperiod},
      }}%
    }
    \keyw{NN, Neural network, mathematical definition}
    \strng{namehash}{MWSPW1}
    \strng{fullhash}{MWSPW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Because of the ``all-or-none'' character of nervous activity, neural events
  and the relations among them can be treated by means of propositional logic.
  It is found that the behavior of every net can be described in these terms,
  with the addition of more complicated logical means for nets containing
  circles; and that for any logical expression satisfying certain conditions,
  one can find a net behaving in the fashion it describes. It is shown that
  many particular choices among possible neurophysiological assumptions are
  equivalent, in the sense that for every net behaving under one assumption,
  there exists another net which behaves under the other and gives the same
  results, although perhaps not in the same time. Various applications of the
  calculus are discussed.%
    }
    \verb{doi}
    \verb 10.1007/BF02478259
    \endverb
    \field{issn}{1522-9602}
    \field{number}{4}
    \field{pages}{115\bibrangedash 133}
    \field{title}{A logical calculus of the ideas immanent in nervous activity}
    \verb{url}
    \verb https://doi.org/10.1007/BF02478259
    \endverb
    \field{volume}{5}
    \field{journaltitle}{The bulletin of mathematical biophysics}
    \field{year}{1943}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Moussa2006}{inbook}{}
    \name{author}{3}{}{%
      {{hash=MM}{%
         family={Moussa},
         familyi={M\bibinitperiod},
         given={Medhat},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AS}{%
         family={Areibi},
         familyi={A\bibinitperiod},
         given={Shawki},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NK}{%
         family={Nichols},
         familyi={N\bibinitperiod},
         given={Kristian},
         giveni={K\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, Book Chapter}
    \strng{namehash}{MMASNK1}
    \strng{fullhash}{MMASNK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Artificial Neural Networks (ANNs) are inherently parallel architectures
  which represent a natural fit for custom implementation on FPGAs. One
  important implementation issue is to determine the numerical precision format
  that allows an optimum tradeoff between precision and implementation areas.
  Standard single or double precision floating-point representations minimize
  quantization errors while requiring significant hardware resources. Less
  precise fixed-point representation may require less hardware resources but
  add quantization errors that may prevent learning from taking place,
  especially in regression problems. This chapter examines this issue and
  reports on a recent experiment where we implemented a Multi-layer perceptron
  (MLP) on an FPGA using both fixed and floating point precision. Results show
  that the fixed-point MLP implementation was over 12x greater in speed, over
  13x smaller in area, and achieves far greater processing density compared to
  the floating-point FPGA-based MLP.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_2
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{37\bibrangedash 61}
    \field{title}{On the Arithmetic Precision for Implementing Back-Propagation
  Networks on FPGA: A Case Study}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_2
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{nirkin2018_faceswap}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=NY}{%
         family={Nirkin},
         familyi={N\bibinitperiod},
         given={Yuval},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=MI}{%
         family={Masi},
         familyi={M\bibinitperiod},
         given={Iacopo},
         giveni={I\bibinitperiod},
      }}%
      {{hash=TAT}{%
         family={Tran},
         familyi={T\bibinitperiod},
         given={Anh\bibnamedelima Tuan},
         giveni={A\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hassner},
         familyi={H\bibinitperiod},
         given={Tal},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MG}{%
         family={Medioni},
         familyi={M\bibinitperiod},
         given={G\'{e}rard},
         giveni={G\bibinitperiod},
      }}%
    }
    \keyw{CNN, Faceswap, DeepFakes, CNN}
    \strng{namehash}{NYMITATHTMG1}
    \strng{fullhash}{NYMITATHTMG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{booktitle}{IEEE Conference on Automatic Face and Gesture
  Recognition}
    \field{title}{On Face Segmentation, Face Swapping, and Face Perception}
    \field{year}{2018}
  \endentry

  \entry{7929192}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=NE}{%
         family={Nurvitadhi},
         familyi={N\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Sheffield},
         familyi={S\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sim},
         familyi={S\bibinitperiod},
         given={Jaewoong},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mishra},
         familyi={M\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VG}{%
         family={Venkatesh},
         familyi={V\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=MD}{%
         family={Marr},
         familyi={M\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{BNN, application specific integrated circuits;field programmable gate
  arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria
  10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural
  networks;deep neural network;hardware acceleration;Biological neural
  networks;Field programmable gate arrays;Graphics processing
  units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep
  learning;FPGA;GPU;binarized neural networks;data analytics;hardware
  accelerator}
    \strng{namehash}{NESDSJMAVGMD1}
    \strng{fullhash}{NESDSJMAVGMD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{booktitle}{2016 International Conference on Field-Programmable
  Technology (FPT)}
    \verb{doi}
    \verb 10.1109/FPT.2016.7929192
    \endverb
    \field{pages}{77\bibrangedash 84}
    \field{title}{Accelerating Binarized Neural Networks: Comparison of FPGA,
  CPU, GPU, and ASIC}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Omondi:2010:FIN:1941654}{book}{}
    \name{author}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Publishing Company, Incorporated}%
    }
    \keyw{FPGA-NN, ASIC, Book by Publications}
    \strng{namehash}{OARRJC1}
    \strng{fullhash}{OARRJC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{O}
    \field{sortinithash}{O}
    \field{edition}{1st}
    \field{isbn}{1441939423, 9781441939425}
    \field{title}{FPGA Implementations of Neural Networks}
    \field{year}{2010}
  \endentry

  \entry{Omondi2006}{inbook}{}
    \name{author}{3}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Bajger},
         familyi={B\bibinitperiod},
         given={Mariusz},
         giveni={M\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, NN, Neurocomputers, Book Chapter}
    \strng{namehash}{OARRJCBM1}
    \strng{fullhash}{OARRJCBM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{O}
    \field{sortinithash}{O}
    \field{abstract}{%
    This introductory chapter reviews the basics of artificial-neural-network
  theory, discusses various aspects of the hardware implementation of neural
  networks (in both ASIC and FPGA technologies, with a focus on special
  features of artificial neural networks), and concludes with a brief note on
  performance-evaluation. Special points are the exploitation of the
  parallelism inherent in neural networks and the appropriate implementation of
  arithmetic functions, especially the sigmoid function. With respect to the
  sigmoid function, the chapter includes a significant contribution.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_1
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{1\bibrangedash 36}
    \field{title}{FPGA Neurocomputers}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_1
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{7033335}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=POP}{%
         family={Patel},
         familyi={P\bibinitperiod},
         given={O.\bibnamedelima P.},
         giveni={O\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=TA}{%
         family={Tiwari},
         familyi={T\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; generalisation (artificial intelligence);learning (artificial
  intelligence);neural nets;optimisation;pattern classification;quantum
  computing;quantum based binary neural network learning algorithm;network
  structure optimisation;neurons;classification accuracy;hidden layer;training
  accuracy;generalization accuracy;QANN;Neurons;Accuracy;Training;Biological
  neural networks;Testing;Diabetes;Binary neural network;Quantum
  processing;Qubits;Back propagation learning}
    \strng{namehash}{POPTA1}
    \strng{fullhash}{POPTA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{booktitle}{2014 International Conference on Information Technology}
    \verb{doi}
    \verb 10.1109/ICIT.2014.29
    \endverb
    \field{pages}{270\bibrangedash 274}
    \field{title}{Quantum Inspired Binary Neural Network Algorithm}
    \field{year}{2014}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Paul2006}{inbook}{}
    \name{author}{2}{}{%
      {{hash=PK}{%
         family={Paul},
         familyi={P\bibinitperiod},
         given={Kolin},
         giveni={K\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Rajopadhye},
         familyi={R\bibinitperiod},
         given={Sanjay},
         giveni={S\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, Book Chapter}
    \strng{namehash}{PKRS1}
    \strng{fullhash}{PKRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Back propagation is a well known technique used in the implementation of
  artificial neural networks. The algorithm can be described essentially as a
  sequence of matrix vector multiplications and outer product operations
  interspersed with the application of a point wise non linear function. The
  algorithm is compute intensive and lends itself to a high degree of
  parallelism. These features motivate a systolic design of hardware to
  implement the Back Propagation algorithm. We present in this chapter a new
  systolic architecture for the complete back propagation algorithm. For a
  neural network with N input neurons, P hidden layer neurons and M output
  neurons, the proposed architecture with P processors, has a running time of
  (2N + 2M + P + max(M,P)) for each training set vector. This is the first such
  implementation of the back propagation algorithm which completely
  parallelizes the entire computation of learning phase. The array has been
  implemented on an Annapolis FPGA based coprocessor and it achieves very
  favorable performance with range of 5 GOPS. The proposed new design targets
  Virtex boards.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_5
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{137\bibrangedash 165}
    \field{title}{Back-Propagation Algorithm Achieving 5 Gops on the Virtex-E}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_5
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{Raina:2009:LDU:1553374.1553486}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=RR}{%
         family={Raina},
         familyi={R\bibinitperiod},
         given={Rajat},
         giveni={R\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Madhavan},
         familyi={M\bibinitperiod},
         given={Anand},
         giveni={A\bibinitperiod},
      }}%
      {{hash=NAY}{%
         family={Ng},
         familyi={N\bibinitperiod},
         given={Andrew\bibnamedelima Y.},
         giveni={A\bibinitperiod\bibinitdelim Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{NN; GPU, Neural Network}
    \strng{namehash}{RRMANAY1}
    \strng{fullhash}{RRMANAY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{booktitle}{Proceedings of the 26th Annual International Conference
  on Machine Learning}
    \verb{doi}
    \verb 10.1145/1553374.1553486
    \endverb
    \field{isbn}{978-1-60558-516-1}
    \field{pages}{873\bibrangedash 880}
    \field{series}{ICML '09}
    \field{title}{Large-scale Deep Unsupervised Learning Using Graphics
  Processors}
    \verb{url}
    \verb http://doi.acm.org/10.1145/1553374.1553486
    \endverb
    \list{location}{1}{%
      {Montreal, Quebec, Canada}%
    }
    \field{year}{2009}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{xnor.ai-webpage}{online}{}
    \name{author}{1}{}{%
      {{hash=RAFM}{%
         family={Rastegari},
         familyi={R\bibinitperiod},
         given={Ali Farhadi;\bibnamedelima Mohammad},
         giveni={A\bibinitperiod\bibinitdelim F\bibinitperiod\bibinitdelim
  M\bibinitperiod},
      }}%
    }
    \keyw{BNN, XNOR, Yolo}
    \strng{namehash}{RAFM1}
    \strng{fullhash}{RAFM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{title}{xnor.ai Company}
    \verb{url}
    \verb https://www.xnor.ai/
    \endverb
    \field{year}{2016}
    \field{urlyear}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8461456}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=SC}{%
         family={Sakr},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Choi},
         familyi={C\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Z.},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=GK}{%
         family={Gopalakrishnan},
         familyi={G\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Shanbhag},
         familyi={S\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
    }
    \keyw{BNN; gradient methods;learning (artificial intelligence);neural
  nets;gradient-based training;deep binary activated neural networks;continuous
  binarization;deep learning;tremendous complexity;resource constrained
  platforms;training procedure;binary activation functions;minimal accuracy
  degradation;gradient-based learning;back-propagation algorithm;straight
  through estimator;floating-point baseline;STE;Training;Neural
  networks;Complexity theory;Machine learning;Stochastic processes;Perturbation
  methods;Approximation algorithms;deep learning;binary neural
  networks;activation functions}
    \strng{namehash}{SCCJWZGKSN1}
    \strng{fullhash}{SCCJWZGKSN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    With the ever growing popularity of deep learning, the tremendous
  complexity of deep neural networks is becoming problematic when one considers
  inference on resource constrained platforms. Binary networks have emerged as
  a potential solution, however, they exhibit a fundamentallimi-tation in
  realizing gradient-based learning as their activations are
  non-differentiable. Current work has so far relied on approximating gradients
  in order to use the back-propagation algorithm via the straight through
  estimator (STE). Such approximations harm the quality of the training
  procedure causing a noticeable gap in accuracy between binary neural networks
  and their full precision baselines. We present a novel method to train binary
  activated neural networks using true gradient-based learning. Our idea is
  motivated by the similarities between clipping and binary activation
  functions. We show that our method has minimal accuracy degradation with
  respect to the full precision baseline. Finally, we test our method on three
  benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show
  that continuous binarization using true gradient-based learning achieves an
  accuracy within 1.5% of the floating-point baseline, as compared to accuracy
  drops as high as 6% when training the same binary activated network using the
  STE.%
    }
    \field{booktitle}{2018 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}
    \verb{doi}
    \verb 10.1109/ICASSP.2018.8461456
    \endverb
    \field{issn}{2379-190X}
    \field{pages}{2346\bibrangedash 2350}
    \field{title}{True Gradient-Based Training of Deep Binary Activated Neural
  Networks Via Continuous Binarization}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{SCHMIDHUBER201585}{article}{}
    \name{author}{1}{}{%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={J{\"u}rgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{DNN; Deep learning, Supervised learning, Unsupervised learning,
  Reinforcement learning, Evolutionary computation}
    \strng{namehash}{SJ1}
    \strng{fullhash}{SJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    In recent years, deep artificial neural networks (including recurrent ones)
  have won numerous contests in pattern recognition and machine learning. This
  historical survey compactly summarizes relevant work, much of it from the
  previous millennium. Shallow and Deep Learners are distinguished by the depth
  of their credit assignment paths, which are chains of possibly learnable,
  causal links between actions and effects. I review deep supervised learning
  (also recapitulating the history of backpropagation), unsupervised learning,
  reinforcement learning & evolutionary computation, and indirect search for
  short programs encoding deep and large networks.%
    }
    \verb{doi}
    \verb https://doi.org/10.1016/j.neunet.2014.09.003
    \endverb
    \field{issn}{0893-6080}
    \field{pages}{85 \bibrangedash  117}
    \field{title}{Deep learning in neural networks: An overview}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0893608014002135
    \endverb
    \field{volume}{61}
    \field{journaltitle}{Neural Networks}
    \field{year}{2015}
  \endentry

  \entry{Silver:2017aa}{article}{}
    \name{author}{17}{}{%
      {{hash=SD}{%
         family={Silver},
         familyi={S\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schrittwieser},
         familyi={S\bibinitperiod},
         given={Julian},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Simonyan},
         familyi={S\bibinitperiod},
         given={Karen},
         giveni={K\bibinitperiod},
      }}%
      {{hash=AI}{%
         family={Antonoglou},
         familyi={A\bibinitperiod},
         given={Ioannis},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Aja},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={Arthur},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hubert},
         familyi={H\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Baker},
         familyi={B\bibinitperiod},
         given={Lucas},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LM}{%
         family={Lai},
         familyi={L\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Bolton},
         familyi={B\bibinitperiod},
         given={Adrian},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CY}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Yutian},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LT}{%
         family={Lillicrap},
         familyi={L\bibinitperiod},
         given={Timothy},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HF}{%
         family={Hui},
         familyi={H\bibinitperiod},
         given={Fan},
         giveni={F\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Sifre},
         familyi={S\bibinitperiod},
         given={Laurent},
         giveni={L\bibinitperiod},
      }}%
      {{hash=vdDG}{%
         prefix={van\bibnamedelima den},
         prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod},
         family={Driessche},
         familyi={D\bibinitperiod},
         given={George},
         giveni={G\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Graepel},
         familyi={G\bibinitperiod},
         given={Thore},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HD}{%
         family={Hassabis},
         familyi={H\bibinitperiod},
         given={Demis},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Macmillan Publishers Limited, part of Springer Nature. All rights
  reserved. SN -}%
    }
    \keyw{NN, Go, Google, AlphaGo}
    \strng{namehash}{SD+1}
    \strng{fullhash}{SDSJSKAIHAGAHTBLLMBACYLTHFSLDGvdGTHD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{pages}{354 EP \bibrangedash }
    \field{title}{Mastering the game of Go without human knowledge}
    \verb{url}
    \verb https://doi.org/10.1038/nature24270
    \endverb
    \field{volume}{550}
    \field{journaltitle}{Nature}
    \field{month}{10}
    \field{year}{2017}
    \warn{\item Invalid format of field 'date' \item Invalid format of field
  'date'}
  \endentry

  \entry{Simonyan14c}{article}{}
    \name{author}{2}{}{%
      {{hash=SK}{%
         family={Simonyan},
         familyi={S\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZA}{%
         family={Zisserman},
         familyi={Z\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{CNN; image classification, VGG network}
    \strng{namehash}{SKZA1}
    \strng{fullhash}{SKZA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{title}{Very Deep Convolutional Networks for Large-Scale Image
  Recognition}
    \field{volume}{abs/1409.1556}
    \field{journaltitle}{ILSVRC - CoRR}
    \field{year}{2014}
  \endentry

  \entry{Soudry2014ExpectationBP}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={Meir},
         familyi={M\bibinitperiod},
         given={Ron},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{DNN; Backpropagation; Discrete weight space; Continuos weight space}
    \strng{namehash}{SDHIMR1}
    \strng{fullhash}{SDHIMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Multilayer Neural Networks (MNNs) are commonly trained using gradient
  descent-based methods, such as BackPropagation (BP). Inference in
  probabilistic graphical models is often done using variational Bayes methods,
  such as Expectation Propagation (EP). We show how an EP based approach can
  also be used to train deterministic MNNs. Specifically, we approximate the
  posterior of the weights given the data using a ``mean-field'' factorized
  distribution, in an online setting. Using online EP and the central limit
  theorem we find an analytical approximation to the Bayes update of this
  posterior, as well as the resulting Bayes estimates of the weights and
  outputs. Despite a different origin, the resulting algorithm, Expectation
  BackPropagation (EBP), is very similar to BP in form and efficiency. However,
  it has several additional advantages: (1) Training is parameter-free, given
  initial conditions (prior) and the MNN architecture. This is useful for
  large-scale problems, where parameter tuning is a major challenge. (2) The
  weights can be restricted to have discrete values. This is especially useful
  for implementing trained MNNs in precision limited hardware chips, thus
  improving their speed and energy efficiency by several orders of magnitude.
  We test the EBP algorithm numerically in eight binary text classification
  tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal
  constant learning rate (2) previously reported state of the art.
  Interestingly, EBP-trained MNNs with binary weights usually perform better
  than MNNs with continuous (real) weights - if we average the MNN output using
  the inferred posterior.%
    }
    \field{booktitle}{NIPS}
    \field{title}{Expectation Backpropagation: Parameter-Free Training of
  Multilayer Neural Networks with Continuous or Discrete Weights}
    \field{annotation}{%
  https://www.semanticscholar.org/paper/Expectation-Backpropagation%3A-Parameter-Free-of-with-Soudry-Hubara/0b88ed755c4dcd0ac3d25842a5bbbe4ebcefeeec
  https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf%
    }
    \field{year}{2014}
  \endentry

  \entry{5524599}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SM}{%
         family={Stoica},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CGA}{%
         family={Calangiu},
         familyi={C\bibinitperiod},
         given={G.\bibnamedelima A.},
         giveni={G\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=SF}{%
         family={Sisak},
         familyi={S\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{NN; graph theory;learning (artificial intelligence);neural nets;robot
  programming;neural network training;training steps;artificial neural
  networks;robot programming;unidirectional multi-layer neural
  network;graph;Time measurement;Neural networks;Robot kinematics;Service
  robots;Sliding mode control;Control systems;Biological system
  modeling;Humans;Biological neural networks;Artificial neural networks}
    \strng{namehash}{SMCGASF1}
    \strng{fullhash}{SMCGASF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Artificial neural networks play an important role in robot programming by
  demonstration. In this paper we present a method for artificial neural
  network training. The main idea of this method is to train the artificial
  neural network with all of the data, before the current training step, and at
  a certain step the network is already trained a huge number of times. Some
  features of the quality of neural network training, using this method, were
  presented in. Because the method uses all of the data before the current
  training step, in this paper, we are concerned about training time and
  computing time comportment of the neural network. A software application for
  obtaining training time based on the number of training steps was designed.
  This software application implements the training method on an unidirectional
  multi-layer neural network and prints into a graph the training time and
  computing time. The results obtained using the software application and
  important conclusions towards the training and computing time comportment are
  also presented.%
    }
    \field{booktitle}{19th International Workshop on Robotics in
  Alpe-Adria-Danube Region (RAAD 2010)}
    \verb{doi}
    \verb 10.1109/RAAD.2010.5524599
    \endverb
    \field{pages}{109\bibrangedash 113}
    \field{title}{Measuring the time needed for training a neural network based
  on the number of training steps}
    \field{year}{2010}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Sun:2018:FPR:3201607.3201741}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=SX}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Xiaoyu},
         giveni={X\bibinitperiod},
      }}%
      {{hash=PX}{%
         family={Peng},
         familyi={P\bibinitperiod},
         given={Xiaochen},
         giveni={X\bibinitperiod},
      }}%
      {{hash=CPY}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Pai-Yu},
         giveni={P\bibinithyphendelim Y\bibinitperiod},
      }}%
      {{hash=LR}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Rui},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SJs}{%
         family={Seo},
         familyi={S\bibinitperiod},
         given={Jae-sun},
         giveni={J\bibinithyphendelim s\bibinitperiod},
      }}%
      {{hash=YS}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Shimeng},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IEEE Press}%
    }
    \keyw{BNN, P-BNN, CSM, MNIST}
    \strng{namehash}{SXPXCPYLRSJsYS1}
    \strng{fullhash}{SXPXCPYLRSJsYS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{booktitle}{Proceedings of the 23rd Asia and South Pacific Design
  Automation Conference}
    \field{pages}{574\bibrangedash 579}
    \field{series}{ASPDAC '18}
    \field{title}{Fully Parallel RRAM Synaptic Array for Implementing Binary
  Neural Network with (+1, -1) Weights and (+1, 0) Neurons}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=3201607.3201741
    \endverb
    \list{location}{1}{%
      {Jeju, Republic of Korea}%
    }
    \field{year}{2018}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{2017arXiv170309039S}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=SV}{%
         family={{Sze}},
         familyi={S\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=CYH}{%
         family={{Chen}},
         familyi={C\bibinitperiod},
         given={Y.-H.},
         giveni={Y\bibinithyphendelim H\bibinitperiod},
      }}%
      {{hash=YTJ}{%
         family={{Yang}},
         familyi={Y\bibinitperiod},
         given={T.-J.},
         giveni={T\bibinithyphendelim J\bibinitperiod},
      }}%
      {{hash=EJ}{%
         family={{Emer}},
         familyi={E\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{NN, Computer Science; Computer Vision; Pattern Recognition}
    \strng{namehash}{SVCYHYTJEJ1}
    \strng{fullhash}{SVCYHYTJEJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{booktitle}{Proceedings of the IEEE}
    \verb{eprint}
    \verb 1703.09039
    \endverb
    \field{number}{12}
    \field{title}{Efficient Processing of Deep Neural Networks: A Tutorial and
  Survey}
    \field{volume}{105}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{12}
    \field{year}{2017}
  \endentry

  \entry{2016arXiv160207261S}{proceedings}{}
    \name{author}{4}{}{%
      {{hash=SC}{%
         family={{Szegedy}},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=IS}{%
         family={{Ioffe}},
         familyi={I\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=VV}{%
         family={{Vanhoucke}},
         familyi={V\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={{Alemi}},
         familyi={A\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {AAAI}%
    }
    \list{publisher}{1}{%
      {AAAI}%
    }
    \keyw{CNN; Computer Vision; Pattern Recognition; Inception}
    \strng{namehash}{SCISVVAA1}
    \strng{fullhash}{SCISVVAA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \verb{eprint}
    \verb 1602.07261
    \endverb
    \field{number}{31}
    \field{title}{{Inception-v4, Inception-ResNet and the Impact of Residual
  Connections on Learning}}
    \field{volume}{31}
    \field{journaltitle}{Prceedings of the 31st AAAI Conference on Artificial
  Intelligence}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{02}
    \field{year}{2017}
  \endentry

  \entry{AppleFaceDetectionURL}{online}{}
    \name{author}{1}{}{%
      {{hash=TCVML}{%
         family={Team},
         familyi={T\bibinitperiod},
         given={Computer Vision Machine\bibnamedelima Learning},
         giveni={C\bibinitperiod\bibinitdelim V\bibinitperiod\bibinitdelim
  M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \keyw{Neural Network, Vision Framework, Face recognition}
    \strng{namehash}{TCVML1}
    \strng{fullhash}{TCVML1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{title}{An On-device Deep Neural Network for Face Detection}
    \verb{url}
    \verb https://machinelearning.apple.com/2017/11/16/face-detection.html
    \endverb
    \field{month}{11}
    \field{year}{2017}
    \warn{\item Invalid format of field 'urldate'}
  \endentry

  \entry{Tettamanzi2001}{inbook}{}
    \name{author}{2}{}{%
      {{hash=TA}{%
         family={Tettamanzi},
         familyi={T\bibinitperiod},
         given={Andrea},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TM}{%
         family={Tomassini},
         familyi={T\bibinitperiod},
         given={Marco},
         giveni={M\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Berlin Heidelberg}%
    }
    \keyw{Evolutionary, Neural Network, Books}
    \strng{namehash}{TATM1}
    \strng{fullhash}{TATM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{abstract}{%
    WE saw in Chapter 2 that artificial neural networks are
  biologically-inspired computational models that have the capability of
  somehow ``learning'' or ``self-organizing'' to accomplish a given task. They
  are particularly efficient when the nature of the task is ill-defined and the
  input/output mapping largely unknown. However, many aspects may affect the
  performance of an ANN on a given problem. Among them, the most important is
  the structure of the neuron connections i.e., the topology of the net, the
  connection weights, the details of the learning rules and of the neural
  activation function, and the data sets to be used for learning. There are
  guidelines for picking or finding reasonable values for all of these network
  parameters but most are rules of thumb with little theoretical background and
  without any relationship with each other.%
    }
    \field{booktitle}{Soft Computing: Integrating Evolutionary, Neural, and
  Fuzzy Systems}
    \verb{doi}
    \verb 10.1007/978-3-662-04335-6_4
    \endverb
    \field{isbn}{978-3-662-04335-6}
    \field{pages}{123\bibrangedash 159}
    \field{title}{Evolutionary Design of Artificial Neural Networks}
    \verb{url}
    \verb https://doi.org/10.1007/978-3-662-04335-6_4
    \endverb
    \list{location}{1}{%
      {Berlin, Heidelberg}%
    }
    \field{year}{2001}
  \endentry

  \entry{DeepNeuroEvol}{misc}{}
    \name{author}{1}{}{%
      {{hash=U}{%
         family={Uber},
         familyi={U\bibinitperiod},
      }}%
    }
    \keyw{Neuroevolution; Atari, Videogames; evolution strategies; uber}
    \strng{namehash}{U1}
    \strng{fullhash}{U1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{U}
    \field{sortinithash}{U}
    \field{title}{Accelerating Deep Neuroevolution: Train Atari in Hours on a
  Single Personal Computer}
    \verb{url}
    \verb https://eng.uber.com/accelerated-neuroevolution/
    \endverb
    \field{year}{2019}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Umuroglu:2017:FFF:3020078.3021744}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=UY}{%
         family={Umuroglu},
         familyi={U\bibinitperiod},
         given={Yaman},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=FNJ}{%
         family={Fraser},
         familyi={F\bibinitperiod},
         given={Nicholas\bibnamedelima J.},
         giveni={N\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
      {{hash=GG}{%
         family={Gambardella},
         familyi={G\bibinitperiod},
         given={Giulio},
         giveni={G\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Blott},
         familyi={B\bibinitperiod},
         given={Michaela},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Leong},
         familyi={L\bibinitperiod},
         given={Philip},
         giveni={P\bibinitperiod},
      }}%
      {{hash=JM}{%
         family={Jahre},
         familyi={J\bibinitperiod},
         given={Magnus},
         giveni={M\bibinitperiod},
      }}%
      {{hash=VK}{%
         family={Vissers},
         familyi={V\bibinitperiod},
         given={Kees},
         giveni={K\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{BNN; FPGA, binarized neural network, binary neural network, hardware
  acceleration, neural networks, reconfigurable logic}
    \strng{namehash}{UYFNJGGBMLPJMVK1}
    \strng{fullhash}{UYFNJGGBMLPJMVK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{U}
    \field{sortinithash}{U}
    \field{booktitle}{Proceedings of the 2017 ACM/SIGDA International Symposium
  on Field-Programmable Gate Arrays}
    \verb{doi}
    \verb 10.1145/3020078.3021744
    \endverb
    \field{isbn}{978-1-4503-4354-1}
    \field{pages}{65\bibrangedash 74}
    \field{series}{FPGA '17}
    \field{title}{FINN: A Framework for Fast, Scalable Binarized Neural Network
  Inference}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3020078.3021744
    \endverb
    \list{location}{1}{%
      {Monterey, California, USA}%
    }
    \field{year}{2017}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{titanRTXSpecs}{online}{}
    \name{author}{1}{}{%
      {{hash=UTP}{%
         family={Up},
         familyi={U\bibinitperiod},
         given={Tech\bibnamedelima Power},
         giveni={T\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
    }
    \keyw{NVidia, GPU, URL}
    \strng{namehash}{UTP1}
    \strng{fullhash}{UTP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{U}
    \field{sortinithash}{U}
    \field{title}{NVidia Titan RTX}
    \verb{url}
    \verb https://www.techpowerup.com/gpu-specs/titan-rtx.c3311
    \endverb
  \endentry

  \entry{Williams1992}{article}{}
    \name{author}{1}{}{%
      {{hash=WRJ}{%
         family={Williams},
         familyi={W\bibinitperiod},
         given={Ronald\bibnamedelima J.},
         giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary, Gradient Methods, Reinforcement Learning, REINFORCE}
    \strng{namehash}{WRJ1}
    \strng{fullhash}{WRJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{abstract}{%
    This article presents a general class of associative reinforcement learning
  algorithms for connectionist networks containing stochastic units. These
  algorithms, called REINFORCE algorithms, are shown to make weight adjustments
  in a direction that lies along the gradient of expected reinforcement in both
  immediate-reinforcement tasks and certain limited forms of
  delayed-reinforcement tasks, and they do this without explicitly computing
  gradient estimates or even storing information from which such estimates
  could be computed. Specific examples of such algorithms are presented, some
  of which bear a close relationship to certain existing algorithms while
  others are novel but potentially interesting in their own right. Also given
  are results that show how such algorithms can be naturally integrated with
  backpropagation. We close with a brief discussion of a number of additional
  issues surrounding the use of such algorithms, including what is known about
  their limiting behaviors as well as further considerations that might be used
  to help develop similar but potentially more powerful reinforcement learning
  algorithms.%
    }
    \verb{doi}
    \verb 10.1007/BF00992696
    \endverb
    \field{issn}{1573-0565}
    \field{number}{3}
    \field{pages}{229\bibrangedash 256}
    \field{title}{Simple statistical gradient-following algorithms for
  connectionist reinforcement learning}
    \verb{url}
    \verb https://doi.org/10.1007/BF00992696
    \endverb
    \field{volume}{8}
    \field{journaltitle}{Machine Learning}
    \field{year}{1992}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Yamada1992}{article}{}
    \name{author}{3}{}{%
      {{hash=YM}{%
         family={Yamada},
         familyi={Y\bibinitperiod},
         given={Manabu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=NT}{%
         family={Nakagawa},
         familyi={N\bibinitperiod},
         given={Tohru},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KH}{%
         family={Kitagawa},
         familyi={K\bibinitperiod},
         given={Hajime},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{BNN; Parallel processing; Sorting Algorithm}
    \strng{namehash}{YMNTKH1}
    \strng{fullhash}{YMNTKH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{abstract}{%
    This paper presents an ultra-high-speed sorter based upon a simplified
  parallel sorting algorithm using a binary neural network which consists both
  of binary neurons and of AND-OR synaptic connections to solve sorting
  problems at two and only two clock cycles. Our simplified algorithm is based
  on the super parallel sorting algorithm proposed by Takefuji and Lee.
  Nevertheless, our algorithm does not need any adders, while Takefuji's
  algorithm needs n{\texttimes}(n−1) analog adders of which each has multiple
  input ports. For an example of the simplified parallel sorter, a hardware
  design and its implementation will be introduced in this paper, which
  performs a sorting operation at two clock cycles. Both results of a logic
  circuit simulation and of an algorithm simulation show the justice of our
  hardware implementation even if in the practical size of the problem.%
    }
    \verb{doi}
    \verb 10.1007/BF00228719
    \endverb
    \field{issn}{1573-1979}
    \field{number}{4}
    \field{pages}{389\bibrangedash 393}
    \field{title}{A super parallel sorter using a binary neural network with
  AND-OR synaptic connections}
    \verb{url}
    \verb https://doi.org/10.1007/BF00228719
    \endverb
    \field{volume}{2}
    \field{journaltitle}{Analog Integrated Circuits and Signal Processing}
    \field{year}{1992}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{bmxnet}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=YH}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Haojin},
         giveni={H\bibinitperiod},
      }}%
      {{hash=FM}{%
         family={Fritzsche},
         familyi={F\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Bartz},
         familyi={B\bibinitperiod},
         given={Christian},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MC}{%
         family={Meinel},
         familyi={M\bibinitperiod},
         given={Christoph},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{BNN; binary neural networks, computer vision, machine learning, open
  source}
    \strng{namehash}{YHFMBCMC1}
    \strng{fullhash}{YHFMBCMC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{booktitle}{Proceedings of the 2017 ACM on Multimedia Conference}
    \verb{doi}
    \verb 10.1145/3123266.3129393
    \endverb
    \field{isbn}{978-1-4503-4906-2}
    \field{pages}{1209\bibrangedash 1212}
    \field{series}{MM '17}
    \field{title}{BMXNet: An Open-Source Binary Neural Network Implementation
  Based on MXNet}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3123266.3129393
    \endverb
    \list{location}{1}{%
      {Mountain View, California, USA}%
    }
    \field{annotation}{%
    https://arxiv.org/abs/1705.09864 https://github.com/hpi-xnor/BMXNet%
    }
    \field{year}{2017}
    \warn{\item Can't use 'location' + 'address'}
  \endentry
\enddatalist
\endinput
