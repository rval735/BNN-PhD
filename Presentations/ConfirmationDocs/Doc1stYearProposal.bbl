% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.8 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\sortlist[entry]{nty/global/}
  \entry{8226999}{article}{}
    \name{author}{11}{}{%
      {{hash=AK}{%
         family={Ando},
         familyi={A\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=UK}{%
         family={Ueyoshi},
         familyi={U\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=OK}{%
         family={Orimo},
         familyi={O\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={Yonekawa},
         familyi={Y\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Sato},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NH}{%
         family={Nakahara},
         familyi={N\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=TYS}{%
         family={Takamaeda-Yamazaki},
         familyi={T\bibinitperiod-Y\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=IM}{%
         family={Ikebe},
         familyi={I\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AT}{%
         family={Asai},
         familyi={A\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KT}{%
         family={Kuroda},
         familyi={K\bibinitperiod},
         given={T.},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MM}{%
         family={Motomura},
         familyi={M\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
    }
    \keyw{BNN;low-power electronics;neural nets;random-access
  storage;reconfigurable architectures;deep neural network
  accelerator;binary/ternary deep neural networks;In-memory neural network
  processing;binary/ternaty neural network;BRein memory;single-chip
  binary/ternary reconfigurable in-memory;reconfigurable accelerator
  architecture;external data access;power 0.6 W;frequency 400 MHz;Biological
  neural networks;Random access memory;Memory
  management;Neurons;System-on-chip;Parallel processing;Binary neural
  networks;in-memory processing;near-memory processing;neural
  networks;reconfigurable array;ternary neural networks}
    \strng{namehash}{AK+1}
    \strng{fullhash}{AKUKOKYHSSNHTYSIMATKTMM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    A versatile reconfigurable accelerator architecture for binary/ternary deep
  neural networks is presented. In-memory neural network processing without any
  external data accesses, sustained by the symmetry and simplicity of the
  computation of the binary/ternaty neural network, improves the energy
  efficiency dramatically. The prototype chip is fabricated, and it achieves
  1.4 TOPS (tera operations per second) peak performance with 0.6-W power
  consumption at 400-MHz clock. The application examination is also conducted.%
    }
    \verb{doi}
    \verb 10.1109/JSSC.2017.2778702
    \endverb
    \field{issn}{0018-9200}
    \field{number}{4}
    \field{pages}{983\bibrangedash 994}
    \field{title}{BRein Memory: A Single-Chip Binary/Ternary Reconfigurable
  in-Memory Deep Neural Network Accelerator Achieving 1.4 TOPS at 0.6 W}
    \field{volume}{53}
    \field{journaltitle}{IEEE Journal of Solid-State Circuits}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8429420}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rossi},
         familyi={R\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; computer vision;feedforward neural nets;Internet of
  Things;learning (artificial intelligence);low-power
  electronics;microprocessor chips;hardware accelerators;core efficiency;I/O
  bandwidth;ultra-low power devices;BWN accelerator;systolic-scalable
  architecture;state-of-the-art BNN accelerators;resource-intensive FP16
  arithmetic;TOp/s/W system-level efficiency;binary-weight streaming
  approach;BWN;hyperdrive;weight quantization;binary-weight neural
  networks;memory footprint;aggressive
  quantization;mW-devices;memory-intensive;machine learning;computer
  vision;impressive results;deep neural networks;mW IoT end-nodes;systolically
  scalable binary-weight CNN inference engine;Frequency modulation;Computer
  architecture;Quantization (signal);Neural
  networks;System-on-chip;Hardware;Bandwidth;Hardware Accelerator;Binary
  Weights Neural Networks;IoT}
    \strng{namehash}{AR+1}
    \strng{fullhash}{ARCLRDBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Deep neural networks have achieved impressive results in computer vision
  and machine learning. Unfortunately, state-of-the-art networks are extremely
  compute-and memory-intensive which makes them unsuitable for mW-devices such
  as IoT end-nodes. Aggressive quantization of these networks dramatically
  reduces the computation and memory footprint. Binary-weight neural networks
  (BWNs) follow this trend, pushing weight quantization to the limit. Hardware
  accelerators for BWNs presented up to now have focused on core efficiency,
  disregarding I/O bandwidth and system-level efficiency that are crucial for
  deployment of accelerators in ultra-low power devices. We present Hyperdrive:
  a BWN accelerator dramatically reducing the I/O bandwidth exploiting a novel
  binary-weight streaming approach, and capable of handling high-resolution
  images by virtue of its systolic-scalable architecture. We achieve a 5.9
  TOp/s/W system-level efficiency (i.e. including I/Os)-2.2x higher than
  state-of-the-art BNN accelerators, even if our core uses resource-intensive
  FP16 arithmetic for increased robustness.%
    }
    \field{booktitle}{2018 IEEE Computer Society Annual Symposium on VLSI
  (ISVLSI)}
    \verb{doi}
    \verb 10.1109/ISVLSI.2018.00099
    \endverb
    \field{issn}{2159-3477}
    \field{pages}{509\bibrangedash 515}
    \field{title}{Hyperdrive: A Systolically Scalable Binary-Weight CNN
  Inference Engine for mW IoT End-Nodes}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{7878541}{article}{}
    \name{author}{4}{}{%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Rossi},
         familyi={R\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; CMOS logic circuits;computer
  vision;convolution;coprocessors;embedded systems;image
  classification;integrated circuit design;low-power electronics;neural
  nets;optimisation;power aware computing;system-on-chip;I/O storage;I/O
  bandwidth;algorithmic advancements;binary weights;competitive classification
  accuracy;hard limitations;deeply embedded applications;mobile embedded
  applications;power envelope;energy consumption;system-on-chip integration;CNN
  accelerators;GP-GPUs;power-hungry parallel processors;computational
  effort;human accuracy;image classification;computer vision;convolutional
  neural networks;ultralow power binary-weight CNN acceleration;power
  dissipation;binary-weight CNNs;accelerator;optimization
  opportunities;ASIC;binary weights;convolutional neural networks
  (CNNs);hardware accelerator;Internet of Things (IoT)}
    \strng{namehash}{AR+1}
    \strng{fullhash}{ARCLRDBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{A}
    \field{sortinithash}{A}
    \field{abstract}{%
    Convolutional neural networks (CNNs) have revolutionized the world of
  computer vision over the last few years, pushing image classification beyond
  human accuracy. The computational effort of today's CNNs requires
  power-hungry parallel processors or GP-GPUs. Recent developments in CNN
  accelerators for system-on-chip integration have reduced energy consumption
  significantly. Unfortunately, even these highly optimized devices are above
  the power envelope imposed by mobile and deeply embedded applications and
  face hard limitations caused by CNN weight I/O and storage. This prevents the
  adoption of CNNs in future ultralow power Internet of Things end-nodes for
  near-sensor analytics. Recent algorithmic and theoretical advancements enable
  competitive classification accuracy even when limiting CNNs to binary (+1/-1)
  weights during training. These new findings bring major optimization
  opportunities in the arithmetic core by removing the need for expensive
  multiplications, as well as reducing I/O bandwidth and storage. In this
  paper, we present an accelerator optimized for binary-weight CNNs that
  achieves 1.5 TOp/s at 1.2 V on a core area of only 1.33 million gate
  equivalent (MGE) or 1.9 mm<sup>2</sup>and with a power dissipation of 895 Î¼W
  in UMC 65-nm technology at 0.6 V. Our accelerator significantly outperforms
  the state-of-the-art in terms of energy and area efficiency achieving 61.2
  TOp/s/W@0.6 V and 1.1 TOp/s/MGE@1.2 V, respectively.%
    }
    \verb{doi}
    \verb 10.1109/TCAD.2017.2682138
    \endverb
    \field{issn}{0278-0070}
    \field{number}{1}
    \field{pages}{48\bibrangedash 60}
    \field{title}{YodaNN: An Architecture for Ultralow Power Binary-Weight CNN
  Acceleration}
    \field{volume}{37}
    \field{journaltitle}{IEEE Transactions on Computer-Aided Design of
  Integrated Circuits and Systems}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8373076}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=BAA}{%
         family={Bahou},
         familyi={B\bibinitperiod},
         given={A.\bibnamedelima A.},
         giveni={A\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=KG}{%
         family={Karunaratne},
         familyi={K\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=AR}{%
         family={Andri},
         familyi={A\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=CL}{%
         family={Cavigelli},
         familyi={C\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Benini},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; embedded systems;low-power electronics;neural nets;power aware
  computing;binary convolutional neural networks;off-chip memory;low-power
  embedded systems;extreme quantization;flexible accelerator;aggressive
  data;nontrivial network topologies;feature map volumes;energy
  efficiency;hardware accelerator;binary CNN;weight binarization;collapsing
  energy-intensive sum-of-products;XNOR-and-popcount
  operations;Hardware;Convolutional neural
  networks;System-on-chip;Computational modeling;Computer architecture;Program
  processors}
    \strng{namehash}{BAA+1}
    \strng{fullhash}{BAAKGARCLBL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Deploying state-of-the-art CNNs requires power-hungry processors and
  off-chip memory. This precludes the implementation of CNNs in low-power
  embedded systems. Recent research shows CNNs sustain extreme quantization,
  binarizing their weights and intermediate feature maps, thereby saving 8-32x
  memory and collapsing energy-intensive sum-of-products into XNOR-and-popcount
  operations. We present XNORBIN, a flexible accelerator for binary CNNs with
  computation tightly coupled to memory for aggressive data reuse supporting
  even non-trivial network topologies with large feature map volumes.
  Implemented in UMC 65nm technology XNORBIN achieves an energy efficiency of
  95 TOp/s/W and an area efficiency of 2.0TOp/s/MGE at 0.8 V.%
    }
    \field{booktitle}{2018 IEEE Symposium in Low-Power and High-Speed Chips
  (COOL CHIPS)}
    \verb{doi}
    \verb 10.1109/CoolChips.2018.8373076
    \endverb
    \field{issn}{2473-4683}
    \field{pages}{1\bibrangedash 3}
    \field{title}{XNORBIN: A 95 TOp/s/W hardware accelerator for binary
  convolutional neural networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{1206405}{inproceedings}{}
    \name{author}{1}{}{%
      {{hash=BA}{%
         family={Bermak},
         familyi={B\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; pattern classification;VLSI;multiprecision neural chip}
    \strng{namehash}{BA1}
    \strng{fullhash}{BA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    This paper describes a 3D VLSI Chip for binary neural network
  classification applications. The 3D circuit includes three layers of MCM
  integrating 4 chips each making it a total of 12 chips integrated in a volume
  of (2 /spl times/ 2 /spl times/ 0.7)cm/sup 3/. The architecture is scalable,
  and real-time binary neural network classifier systems could be built with
  one, two or all twelve chip solutions. Each basic chip includes an on-chip
  control unit for programming options of the neural network topology and
  precision. The system is modular and presents easy expansibility without
  requiring extra devices. Experimental test results showed that a full recall
  operation is obtained in less than 1.2/spl mu/s for any topology with 4-bit
  or 8-bit precision while it is obtained in less than 2.2/spl mu/s for any
  16-bit precision. As a consequence the 3D chip is a very powerful
  reconfigurable and a multiprecision neural chip exhibiting a significant
  speed of 1.25 GCPS.%
    }
    \field{booktitle}{Proceedings of the 2003 International Symposium on
  Circuits and Systems, 2003. ISCAS '03.}
    \verb{doi}
    \verb 10.1109/ISCAS.2003.1206405
    \endverb
    \field{pages}{V\bibrangedash V}
    \field{title}{A highly scalable 3D chip for binary neural network
  classification applications}
    \field{volume}{5}
    \field{year}{2003}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2018arXiv180910463B}{article}{}
    \name{author}{4}{}{%
      {{hash=BJ}{%
         family={{Bethge}},
         familyi={B\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=YH}{%
         family={{Yang}},
         familyi={Y\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={{Bartz}},
         familyi={B\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MC}{%
         family={{Meinel}},
         familyi={M\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
    }
    \keyw{BNN;Computer Science; Machine Learning, Computer Science - Computer
  Vision and Pattern Recognition, Statistics - Machine Learning}
    \strng{namehash}{BJ+1}
    \strng{fullhash}{BJYHBCMC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \verb{eprint}
    \verb 1809.10463
    \endverb
    \field{title}{{Learning to Train a Binary Neural Network}}
    \field{journaltitle}{ArXiv e-prints}
    \field{eprinttype}{arXiv}
    \field{month}{09}
    \field{year}{2018}
  \endentry

  \entry{5726804}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=BSA}{%
         family={Brodsky},
         familyi={B\bibinitperiod},
         given={S.\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=GCC}{%
         family={Guest},
         familyi={G\bibinitperiod},
         given={C.\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \keyw{BNN, content-addressable storage;learning systems;neural
  nets;arbitrary bit-level significance;binary backpropagation;bit connection
  weights;content addressable memory;continuous backpropagation network
  learning model;local computation;pseudoanalog extension}
    \strng{namehash}{BSAGCC1}
    \strng{fullhash}{BSAGCC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{booktitle}{1990 IJCNN International Joint Conference on Neural
  Networks}
    \verb{doi}
    \verb 10.1109/IJCNN.1990.137846
    \endverb
    \field{pages}{205\bibrangedash 210 vol.3}
    \field{title}{Binary backpropagation in content addressable memory}
    \field{year}{1990}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Brodsky:93}{article}{}
    \name{author}{3}{}{%
      {{hash=BSA}{%
         family={Brodsky},
         familyi={B\bibinitperiod},
         given={Stephen\bibnamedelima A.},
         giveni={S\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=MGC}{%
         family={Marsden},
         familyi={M\bibinitperiod},
         given={Gary\bibnamedelima C.},
         giveni={G\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=GCC}{%
         family={Guest},
         familyi={G\bibinitperiod},
         given={Clark\bibnamedelima C.},
         giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {OSA}%
    }
    \keyw{BNN, Cylindrical lenses; Light valves; Neural networks; Optical
  components; Optical neural systems; Parallel processing}
    \strng{namehash}{BSAMGCGCC1}
    \strng{fullhash}{BSAMGCGCC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    The content-addressable network (CAN) is an efficient, intrinsically
  discrete training algorithm for binary-valued classification networks. The
  binary nature of the CAN network permits accelerated learning and
  significantly reduced hardware-implementation requirements. A multilayer
  optoelectronic CAN network employing matrix--vector multiplication was
  constructed. The network learned and correctly classified trained patterns,
  gaining a measure of fault tolerance by learning associative solutions to
  optical hardware imperfections. Operation of this system is possible owing to
  the reduced hardware accuracy requirements of the CAN learning algorithm.%
    }
    \verb{doi}
    \verb 10.1364/AO.32.001338
    \endverb
    \field{number}{8}
    \field{pages}{1338\bibrangedash 1345}
    \field{title}{Optical matrix--vector implementation of the
  content-addressable network}
    \verb{url}
    \verb http://ao.osa.org/abstract.cfm?URI=ao-32-8-1338
    \endverb
    \field{volume}{32}
    \field{journaltitle}{Appl. Opt.}
    \field{year}{1993}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2016arXiv160507678C}{article}{}
    \name{author}{3}{}{%
      {{hash=CA}{%
         family={{Canziani}},
         familyi={C\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=PA}{%
         family={{Paszke}},
         familyi={P\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CE}{%
         family={{Culurciello}},
         familyi={C\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
    }
    \keyw{Computer Science - Computer Vision and Pattern Recognition}
    \strng{namehash}{CAPACE1}
    \strng{fullhash}{CAPACE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \verb{eprint}
    \verb 1605.07678
    \endverb
    \field{title}{{An Analysis of Deep Neural Network Models for Practical
  Applications}}
    \field{journaltitle}{ArXiv e-prints}
    \field{annotation}{%
  https://medium.com/@culurciello/analysis-of-deep-neural-networks-dcf398e71aae
  https://towardsdatascience.com/neural-network-architectures-156e5bad51ba%
    }
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{05}
    \field{year}{2016}
  \endentry

  \entry{5159360}{article}{}
    \name{author}{5}{}{%
      {{hash=CF}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
      {{hash=CG}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=HQ}{%
         family={He},
         familyi={H\bibinitperiod},
         given={Q.},
         giveni={Q\bibinitperiod},
      }}%
      {{hash=HG}{%
         family={He},
         familyi={H\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=XX}{%
         family={Xu},
         familyi={X\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
    }
    \keyw{BNN; Boolean functions;learning (artificial intelligence);multilayer
  perceptrons;binary neural network;linearly nonseparable Boolean
  functions;nonLSBF;DNA-like learning and decomposing algorithm;DNA-like
  LDA;DNA-like offset sequence;logic XOR operation;weight-threshold
  value;multilayer perceptron;function mapping;parity Boolean function;Neural
  networks;Boolean functions;Neurons;Multilayer perceptrons;Linear discriminant
  analysis;Logic;Backpropagation algorithms;Input variables;Mathematics;Binary
  neural network;DNA-like learning and decomposing algorithm (DNA-like
  LDA);linearly nonseparable Boolean function (non-LSBF);multilayer perceptron
  (MLP);parity Boolean function (PBF);Algorithms;Artificial
  Intelligence;DNA;Linear Models;Neural Networks (Computer)}
    \strng{namehash}{CF+1}
    \strng{fullhash}{CFCGHQHGXX1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Implementing linearly nonseparable Boolean functions (non-LSBF) has been an
  important and yet challenging task due to the extremely high complexity of
  this kind of functions and the exponentially increasing percentage of the
  number of non-LSBF in the entire set of Boolean functions as the number of
  input variables increases. In this paper, an algorithm named DNA-like
  learning and decomposing algorithm (DNA-like LDA) is proposed, which is
  capable of effectively implementing non-LSBF. The novel algorithm first
  trains the DNA-like offset sequence and decomposes non-LSBF into logic XOR
  operations of a sequence of LSBF, and then determines the weight-threshold
  values of the multilayer perceptron (MLP) that perform both the
  decompositions of LSBF and the function mapping the hidden neurons to the
  output neuron. The algorithm is validated by two typical examples about the
  problem of approximating the circular region and the well-known
  &lt;i&gt;n&lt;/i&gt; -bit parity Boolean function (PBF).%
    }
    \verb{doi}
    \verb 10.1109/TNN.2009.2023122
    \endverb
    \field{issn}{1045-9227}
    \field{number}{8}
    \field{pages}{1293\bibrangedash 1301}
    \field{title}{Universal Perceptron and DNA-Like Learning Algorithm for
  Binary Neural Networks: Non-LSBF Implementation}
    \field{volume}{20}
    \field{journaltitle}{IEEE Transactions on Neural Networks}
    \field{year}{2009}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Chen2018UnderstandingTL}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=hCY}{%
         prefix={hsin},
         prefixi={h\bibinitperiod},
         family={Chen},
         familyi={C\bibinitperiod},
         given={Yu},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=YTJ}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Tien-Ju},
         giveni={T\bibinitperiod-J\bibinitperiod},
      }}%
      {{hash=EJS}{%
         family={Emer},
         familyi={E\bibinitperiod},
         given={Joel\bibnamedelima S.},
         giveni={J\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=SV}{%
         family={Sze},
         familyi={S\bibinitperiod},
         given={Vivienne},
         giveni={V\bibinitperiod},
      }}%
    }
    \keyw{NN, DNN, Hardware, FPGA, ASIC}
    \strng{namehash}{CYh+1}
    \strng{fullhash}{CYhYTJEJSSV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{booktitle}{SysML Conference}
    \field{title}{Understanding the Limitations of Existing Energy-Efficient
  Design Approaches for Deep Neural Networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{714090}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=CCH}{%
         family={Chu},
         familyi={C\bibinitperiod},
         given={C.\bibnamedelima H.},
         giveni={C\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
      {{hash=KJH}{%
         family={Kim},
         familyi={K\bibinitperiod},
         given={J.\bibnamedelima H.},
         giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \keyw{BNN; feedforward neural nets;pattern classification;learning
  (artificial intelligence);medical diagnostic computing;geometrical
  learning;binary neural networks;pattern classification;expand-and-truncate
  learning algorithm;connecting weights;three-layered feedforward
  network;binary-to-binary mappings;breast cancer database;Pattern
  classification;Neural networks;Neurons;Machine learning
  algorithms;Testing;Breast cancer;Databases;Hamming distance;Computer
  networks;Joining processes}
    \strng{namehash}{CCHKJH1}
    \strng{fullhash}{CCHKJH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    This paper considers the use of binary neural networks for pattern
  classification. An expand-and-truncate learning (ETL) algorithm is used to
  determine the required number of neurons as well as the connecting weights in
  a three-layered feedforward network for classifying input patterns. The ETL
  algorithm is guaranteed to find a network for any binary-to-binary mappings.
  The ETL algorithm's performance in pattern classification is tested using a
  breast cancer database that have been used for benchmarking performance other
  machine learning methods.%
    }
    \field{booktitle}{Proceedings of 1993 International Conference on Neural
  Networks (IJCNN-93-Nagoya, Japan)}
    \verb{doi}
    \verb 10.1109/IJCNN.1993.714090
    \endverb
    \field{pages}{1039\bibrangedash 1042 vol.1}
    \field{title}{Pattern classification by geometrical learning of binary
  neural networks}
    \field{volume}{1}
    \field{year}{1993}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2015_5647}{incollection}{}
    \name{author}{3}{}{%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=DJP}{%
         family={David},
         familyi={D\bibinitperiod},
         given={Jean-Pierre},
         giveni={J\bibinitperiod-P\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=CC}{%
         family={Cortes},
         familyi={C\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=LND}{%
         family={Lawrence},
         familyi={L\bibinitperiod},
         given={N.\bibnamedelima D.},
         giveni={N\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=LDD}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={D.\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sugiyama},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{BNN; Machine Learning; Computer Vision; Pattern Recognition; Neural
  and Evolutionary Computing}
    \strng{namehash}{CMBYDJP1}
    \strng{fullhash}{CMBYDJP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{booktitle}{Advances in Neural Information Processing Systems 28}
    \field{pages}{3123\bibrangedash 3131}
    \field{title}{BinaryConnect: Training Deep Neural Networks with binary
  weights during propagations}
    \verb{url}
    \verb http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-n
    \verb etworks-with-binary-weights-during-propagations.pdf
    \endverb
    \field{year}{2015}
  \endentry

  \entry{DENG201849}{article}{}
    \name{author}{5}{}{%
      {{hash=DL}{%
         family={Deng},
         familyi={D\bibinitperiod},
         given={Lei},
         giveni={L\bibinitperiod},
      }}%
      {{hash=JP}{%
         family={Jiao},
         familyi={J\bibinitperiod},
         given={Peng},
         giveni={P\bibinitperiod},
      }}%
      {{hash=PJ}{%
         family={Pei},
         familyi={P\bibinitperiod},
         given={Jing},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Zhenzhi},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=LG}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Guoqi},
         giveni={G\bibinitperiod},
      }}%
    }
    \keyw{BNN, GXNOR-Net, Discrete state transition, Ternary neural networks,
  Sparse binary networks}
    \strng{namehash}{DL+1}
    \strng{fullhash}{DLJPPJWZLG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \verb{doi}
    \verb https://doi.org/10.1016/j.neunet.2018.01.010
    \endverb
    \field{issn}{0893-6080}
    \field{pages}{49 \bibrangedash  58}
    \field{title}{GXNOR-Net: Training deep neural networks with ternary weights
  and activations without full-precision memory under a unified discretization
  framework}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0893608018300108
    \endverb
    \field{volume}{100}
    \field{journaltitle}{Neural Networks}
    \field{year}{2018}
  \endentry

  \entry{doi:10.1177/1555343417695197}{article}{}
    \name{author}{1}{}{%
      {{hash=EMR}{%
         family={Endsley},
         familyi={E\bibinitperiod},
         given={Mica\bibnamedelima R.},
         giveni={M\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
    }
    \keyw{NN, autopilot, tesla}
    \strng{namehash}{EMR1}
    \strng{fullhash}{EMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{E}
    \field{sortinithash}{E}
    \field{abstract}{%
    Autonomous and semiautonomous vehicles are currently being developed by
  over14 companies. These vehicles may improve driving safety and convenience,
  or they may create new challenges for drivers, particularly with regard to
  situation awareness (SA) and autonomy interaction. I conducted a naturalistic
  driving study on the autonomy features in the Tesla Model S, recording my
  experiences over a 6-month period, including assessments of SA and problems
  with the autonomy. This preliminary analysis provides insights into the
  challenges that drivers may face in dealing with new autonomous automobiles
  in realistic driving conditions, and it extends previous research on
  human-autonomy interaction to the driving domain. Issues were found with
  driver training, mental model development, mode confusion, unexpected mode
  interactions, SA, and susceptibility to distraction. New insights into
  challenges with semiautonomous driving systems include increased variability
  in SA, the replacement of continuous control with serial discrete control,
  and the need for more complex decisions. Issues that deserve consideration in
  future research and a set of guidelines for driver interfaces of autonomous
  systems are presented and used to create recommendations for improving driver
  SA when interacting with autonomous vehicles.%
    }
    \verb{doi}
    \verb 10.1177/1555343417695197
    \endverb
    \verb{eprint}
    \verb https://doi.org/10.1177/1555343417695197
    \endverb
    \field{number}{3}
    \field{pages}{225\bibrangedash 238}
    \field{title}{Autonomous Driving Systems: A Preliminary Naturalistic Study
  of the Tesla Model S}
    \verb{url}
    \verb https://doi.org/10.1177/1555343417695197
    \endverb
    \field{volume}{11}
    \field{journaltitle}{Journal of Cognitive Engineering and Decision Making}
    \field{year}{2017}
  \endentry

  \entry{Faraone_2018_CVPR}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=FJ}{%
         family={Faraone},
         familyi={F\bibinitperiod},
         given={Julian},
         giveni={J\bibinitperiod},
      }}%
      {{hash=FN}{%
         family={Fraser},
         familyi={F\bibinitperiod},
         given={Nicholas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Blott},
         familyi={B\bibinitperiod},
         given={Michaela},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LPH}{%
         family={Leong},
         familyi={L\bibinitperiod},
         given={Philip\bibnamedelima H.W.},
         giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod},
      }}%
    }
    \keyw{BNN; Quantization, FPGA, LUT}
    \strng{namehash}{FJ+1}
    \strng{fullhash}{FJFNBMLPH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{booktitle}{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}
    \field{title}{SYQ: Learning Symmetric Quantization for Efficient Deep
  Neural Networks}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{616215}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=FN}{%
         family={Funabiki},
         familyi={F\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
      {{hash=KJ}{%
         family={Kitamichi},
         familyi={K\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=NS}{%
         family={Nishikawa},
         familyi={N\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{Evolutionary; neural nets;genetic algorithms;set theory;graph
  theory;minimisation;computational complexity;evolutionary neural network
  algorithm;ENN;max cut problems;undirected graph;NP-hard
  problem;partition;disjoint subsets;evolutionary initialization scheme;energy
  minimization criteria;binary neural network;randomly weighted complete
  graphs;unweighted random graphs;maximum neural network;mean field
  annealing;simulated annealing;greedy algorithm;Neural
  networks;Neurons;Simulated annealing;NP-hard problem;Equations;Multi-layer
  neural network;Computer networks;Minimization;Greedy algorithms;Approximation
  algorithms}
    \strng{namehash}{FNKJNS1}
    \strng{fullhash}{FNKJNS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    An "evolutionary neural network (ENN)" is presented for the max cut problem
  of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem
  is to find a partition of V into two disjoint subsets such that the cut size
  be maximized. The cut size is the sum of weights on edges in E whose
  endpoints belong to different subsets. The ENN combines the evolutionary
  initialization scheme of the neural state into the energy minimization
  criteria of the binary neural network. The performance of ENN is evaluated
  through simulations in randomly weighted complete graphs and unweighted
  random graphs with up to 1000 vertices. The results show that the
  evolutionary initialization scheme drastically improves the solution quality.
  ENN can always find better solutions than the maximum neural network, the
  mean field annealing, the simulated annealing, and the greedy algorithm.%
    }
    \field{booktitle}{Proceedings of International Conference on Neural
  Networks (ICNN'97)}
    \verb{doi}
    \verb 10.1109/ICNN.1997.616215
    \endverb
    \field{pages}{1260\bibrangedash 1265 vol.2}
    \field{title}{An evolutionary neural network algorithm for max cut
  problems}
    \field{volume}{2}
    \field{year}{1997}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{5607329}{article}{}
    \name{author}{3}{}{%
      {{hash=GA}{%
         family={Gomperts},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=UA}{%
         family={Ukil},
         familyi={U\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=ZF}{%
         family={Zurfluh},
         familyi={Z\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{backpropagation;field programmable gate arrays;hardware description
  languages;multilayer perceptrons;FPGA;VLSI hardware description
  language;arithmetic operation;artificial neural network;backpropagation
  multilayer perceptron;fast prototyping;field programmable gate array;general
  purpose neural network;hardware-based MLP;learning capability;online
  application;space exploration;Backpropagation;NIR spectra
  calibration;VHDL;Xilinx FPGA;field programmable gate array (FPGA);hardware
  implementation;multilayer perceptron;neural network;spectroscopy}
    \strng{namehash}{GAUAZF1}
    \strng{fullhash}{GAUAZF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{abstract}{%
    This paper presents the development and implementation of a generalized
  backpropagation multilayer perceptron (MLP) architecture described in VLSI
  hardware description language (VHDL). The development of hardware platforms
  has been complicated by the high hardware cost and quantity of the arithmetic
  operations required in online artificial neural networks (ANNs), i.e.,
  general purpose ANNs with learning capability. Besides, there remains a
  dearth of hardware platforms for design space exploration, fast prototyping,
  and testing of these networks. Our general purpose architecture seeks to fill
  that gap and at the same time serve as a tool to gain a better understanding
  of issues unique to ANNs implemented in hardware, particularly using field
  programmable gate array (FPGA). The challenge is thus to find an architecture
  that minimizes hardware costs, while maximizing performance, accuracy, and
  parameterization. This work describes a platform that offers a high degree of
  parameterization, while maintaining generalized network design with
  performance comparable to other hardware-based MLP implementations.
  Application of the hardware implementation of ANN with backpropagation
  learning algorithm for a realistic application is also presented.%
    }
    \verb{doi}
    \verb 10.1109/TII.2010.2085006
    \endverb
    \field{issn}{1551-3203}
    \field{number}{1}
    \field{pages}{78\bibrangedash 89}
    \field{title}{Development and Implementation of Parameterized FPGA-Based
  General Purpose Neural Networks for Online Applications}
    \field{volume}{7}
    \field{journaltitle}{IEEE Transactions on Industrial Informatics}
    \field{year}{2011}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Guo2018FBNAAF}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=GP}{%
         family={Guo},
         familyi={G\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
      {{hash=MH}{%
         family={Ma},
         familyi={M\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Ruizhi},
         giveni={R\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Pin},
         giveni={P\bibinitperiod},
      }}%
      {{hash=XS}{%
         family={Xie},
         familyi={X\bibinitperiod},
         given={Shaolin},
         giveni={S\bibinitperiod},
      }}%
      {{hash=WD}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Donglin},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {Trinity College, Dublin}%
    }
    \keyw{BNN, Neural Network, FPGA, Accelerator}
    \strng{namehash}{GP+1}
    \strng{fullhash}{GPMHCRLPXSWD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{booktitle}{FPL 2018}
    \field{title}{FBNA: A Fully Binarized Neural Network Accelerator}
    \field{volume}{1}
    \field{year}{2018}
  \endentry

  \entry{7780459}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=HK}{%
         family={He},
         familyi={H\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZX}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Ren},
         familyi={R\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{CNN, image classification; AI; neural nets;object detection, RESNET}
    \strng{namehash}{HK+1}
    \strng{fullhash}{HKZXRSSJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Deeper neural networks are more difficult to train. We present a residual
  learning framework to ease the training of networks that are substantially
  deeper than those used previously. We explicitly reformulate the layers as
  learning residual functions with reference to the layer inputs, instead of
  learning unreferenced functions. We provide comprehensive empirical evidence
  showing that these residual networks are easier to optimize, and can gain
  accuracy from considerably increased depth. On the ImageNet dataset we
  evaluate residual nets with a depth of up to 152 layers - 8; deeper than VGG
  nets [40] but still having lower complexity. An ensemble of these residual
  nets achieves 3.57% error on the ImageNet test set. This result won the 1st
  place on the ILSVRC 2015 classification task. We also present analysis on
  CIFAR-10 with 100 and 1000 layers. The depth of representations is of central
  importance for many visual recognition tasks. Solely due to our extremely
  deep representations, we obtain a 28% relative improvement on the COCO object
  detection dataset. Deep residual nets are foundations of our submissions to
  ILSVRC; COCO 2015 competitions1, where we also won the 1st places on the
  tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO
  segmentation.%
    }
    \field{booktitle}{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}
    \verb{doi}
    \verb 10.1109/CVPR.2016.90
    \endverb
    \field{issn}{1063-6919}
    \field{pages}{770\bibrangedash 778}
    \field{title}{Deep Residual Learning for Image Recognition}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Hertz:1991:ITN:104000}{book}{}
    \name{author}{3}{}{%
      {{hash=HJ}{%
         family={Hertz},
         familyi={H\bibinitperiod},
         given={John},
         giveni={J\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Krogh},
         familyi={K\bibinitperiod},
         given={Anders},
         giveni={A\bibinitperiod},
      }}%
      {{hash=PRG}{%
         family={Palmer},
         familyi={P\bibinitperiod},
         given={Richard\bibnamedelima G.},
         giveni={R\bibinitperiod\bibinitdelim G\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Addison-Wesley Longman Publishing Co., Inc.}%
    }
    \keyw{NN; Book, Neural Computation}
    \strng{namehash}{HJKAPRG1}
    \strng{fullhash}{HJKAPRG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{isbn}{0-201-50395-6}
    \field{title}{Introduction to the Theory of Neural Computation}
    \list{location}{1}{%
      {Boston, MA, USA}%
    }
    \field{year}{1991}
  \endentry

  \entry{8425178}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=HY}{%
         family={Hu},
         familyi={H\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZJ}{%
         family={Zhai},
         familyi={Z\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=LD}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=GY}{%
         family={Gong},
         familyi={G\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=ZY}{%
         family={Zhu},
         familyi={Z\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LW}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Su},
         familyi={S\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=JJ}{%
         family={Jin},
         familyi={J\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{BNN;computational complexity;field programmable gate arrays;graphics
  processing units;learning (artificial intelligence);multiprocessing
  systems;neural nets;optimisation;parallel processing;vector
  parallelism;binary Neural Networks;CPU;Deep learning;computer vision;big
  bang;Deep Neural Networks;high computational complexity;Binary Neural
  Networks;BNNs;arithmetic operations;bitwise operations;image-to-column
  method;low arithmetic intensity;gemm-operator-network;computing power;BitFlow
  features;efficient binary convolution;VGG network;counterpart full-precision
  DNNs;GPU;Convolution;Neural networks;Layout;Parallel
  processing;Acceleration;Graphics processing units;Machine learning;Network
  Compression;Binary Neural Network (BNN);Vector Parallelism;Intel Xeon Phi}
    \strng{namehash}{HY+1}
    \strng{fullhash}{HYZJLDGYZYLWSLJJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Deep learning has revolutionized computer vision and other fields since its
  big bang in 2012. However, it is challenging to deploy Deep Neural Networks
  (DNNs) into real-world applications due to their high computational
  complexity. Binary Neural Networks (BNNs) dramatically reduce computational
  complexity by replacing most arithmetic operations with bitwise operations.
  Existing implementations of BNNs have been focusing on GPU or FPGA, and using
  the conventional image-to-column method that doesn't perform well for binary
  convolution due to low arithmetic intensity and unfriendly pattern for
  bitwise operations. We propose BitFlow, a gemm-operator-network three-level
  optimization framework for fully exploiting the computing power of BNNs on
  CPU. BitFlow features a new class of algorithm named PressedConv for
  efficient binary convolution using locality-aware layout and vector
  parallelism. We evaluate BitFlow with the VGG network. On a single core of
  Intel Xeon Phi, BitFlow obtains 1.8x speedup over unoptimized BNN
  implementations, and 11.5x speedup over counterpart full-precision DNNs. Over
  64 cores, BitFlow enables BNNs to run 1.1x faster than counterpart
  full-precision DNNs on GPU (GTX 1080).%
    }
    \field{booktitle}{2018 IEEE International Parallel and Distributed
  Processing Symposium (IPDPS)}
    \verb{doi}
    \verb 10.1109/IPDPS.2018.00034
    \endverb
    \field{issn}{1530-2075}
    \field{pages}{244\bibrangedash 253}
    \field{title}{BitFlow: Exploiting Vector Parallelism for Binary Neural
  Networks on CPU}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{NIPS2016_6573}{incollection}{}
    \name{author}{5}{}{%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=CM}{%
         family={Courbariaux},
         familyi={C\bibinitperiod},
         given={Matthieu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=EYR}{%
         family={El-Yaniv},
         familyi={E\bibinitperiod-Y\bibinitperiod},
         given={Ran},
         giveni={R\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \name{editor}{5}{}{%
      {{hash=LDD}{%
         family={Lee},
         familyi={L\bibinitperiod},
         given={D.\bibnamedelima D.},
         giveni={D\bibinitperiod\bibinitdelim D\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Sugiyama},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=LUV}{%
         family={Luxburg},
         familyi={L\bibinitperiod},
         given={U.\bibnamedelima V.},
         giveni={U\bibinitperiod\bibinitdelim V\bibinitperiod},
      }}%
      {{hash=GI}{%
         family={Guyon},
         familyi={G\bibinitperiod},
         given={I.},
         giveni={I\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Garnett},
         familyi={G\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates, Inc.}%
    }
    \keyw{BNN; Neural Network}
    \strng{namehash}{HI+1}
    \strng{fullhash}{HICMSDEYRBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{booktitle}{Advances in Neural Information Processing Systems 29}
    \field{pages}{4107\bibrangedash 4115}
    \field{title}{Binarized Neural Networks}
    \verb{url}
    \verb http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf
    \endverb
    \field{year}{2016}
  \endentry

  \entry{4790104}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KM}{%
         family={Kam},
         familyi={K\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Cheng},
         familyi={C\bibinitperiod},
         given={R.},
         giveni={R\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN;State-space methods;Neural networks;Information analysis;Pattern
  analysis;Hopfield neural networks;Pattern recognition;Information
  retrieval;Convergence;Hamming distance;Content based retrieval}
    \strng{namehash}{KMCRGA1}
    \strng{fullhash}{KMCRGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{abstract}{%
    Analysis of the state space for the fully-connected binary neural network
  ("the Hopfield model") remains an important objective in utilizing the
  network in pattern recognition and associative information retrieval. Most of
  the research pertaining to the network's state space so far concentrated on
  stable-state enumeration and often it was assumed that the patterns which are
  to be stored are random. We discuss the case of deterministic known codewords
  whose storage is required, and show that for this important case bounds on
  the retrieval probabilities and convergence rates can be achieved. The main
  tool which we employ is Birth-and-Death Markov chains, describing the Hamming
  distance of the network's state from the stored patterns. The results are
  applicable to both the asynchronous network and to the Boltzmann machine, and
  can be utilized to compare codeword sets in terms of efficiency of their
  retrieval, when the neural network is used as a content addressable memory.%
    }
    \field{booktitle}{1988 American Control Conference}
    \verb{doi}
    \verb 10.23919/ACC.1988.4790104
    \endverb
    \field{pages}{2276\bibrangedash 2281}
    \field{title}{On the State Space of the Binary Neural Network}
    \field{year}{1988}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Krizhevsky2999257}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=KA}{%
         family={Krizhevsky},
         familyi={K\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=SI}{%
         family={Sutskever},
         familyi={S\bibinitperiod},
         given={Ilya},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HGE}{%
         family={Hinton},
         familyi={H\bibinitperiod},
         given={Geoffrey\bibnamedelima E.},
         giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Curran Associates Inc.}%
    }
    \keyw{CNN, ImageNet, ILSVRC, Convolution, AlexNet}
    \strng{namehash}{KASIHGE1}
    \strng{fullhash}{KASIHGE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{K}
    \field{sortinithash}{K}
    \field{booktitle}{Proceedings of the 25th International Conference on
  Neural Information Processing Systems - Volume 1}
    \field{pages}{1097\bibrangedash 1105}
    \field{series}{NIPS'12}
    \field{title}{ImageNet Classification with Deep Convolutional Neural
  Networks}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=2999134.2999257
    \endverb
    \list{location}{1}{%
      {Lake Tahoe, Nevada}%
    }
    \field{year}{2012}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{726791}{article}{}
    \name{author}{4}{}{%
      {{hash=LY}{%
         family={Lecun},
         familyi={L\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Bottou},
         familyi={B\bibinitperiod},
         given={L.},
         giveni={L\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Haffner},
         familyi={H\bibinitperiod},
         given={P.},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{NN; optical character recognition;multilayer
  perceptrons;backpropagation;convolution;gradient-based learning;document
  recognition;multilayer neural networks;back-propagation;gradient based
  learning technique;complex decision surface synthesis;high-dimensional
  patterns;handwritten character recognition;handwritten digit recognition
  task;2D shape variability;document recognition systems;field
  extraction;segmentation recognition;language modeling;graph transformer
  networks;GTN;multimodule systems;performance measure minimization;cheque
  reading;convolutional neural network character recognizers;Neural
  networks;Pattern recognition;Machine learning;Optical character recognition
  software;Character recognition;Feature extraction;Multi-layer neural
  network;Optical computing;Hidden Markov models;Principal component analysis}
    \strng{namehash}{LY+1}
    \strng{fullhash}{LYBLBYHP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    Multilayer neural networks trained with the back-propagation algorithm
  constitute the best example of a successful gradient based learning
  technique. Given an appropriate network architecture, gradient-based learning
  algorithms can be used to synthesize a complex decision surface that can
  classify high-dimensional patterns, such as handwritten characters, with
  minimal preprocessing. This paper reviews various methods applied to
  handwritten character recognition and compares them on a standard handwritten
  digit recognition task. Convolutional neural networks, which are specifically
  designed to deal with the variability of 2D shapes, are shown to outperform
  all other techniques. Real-life document recognition systems are composed of
  multiple modules including field extraction, segmentation recognition, and
  language modeling. A new learning paradigm, called graph transformer networks
  (GTN), allows such multimodule systems to be trained globally using
  gradient-based methods so as to minimize an overall performance measure. Two
  systems for online handwriting recognition are described. Experiments
  demonstrate the advantage of global training, and the flexibility of graph
  transformer networks. A graph transformer network for reading a bank cheque
  is also described. It uses convolutional neural network character recognizers
  combined with global training techniques to provide record accuracy on
  business and personal cheques. It is deployed commercially and reads several
  million cheques per day.%
    }
    \verb{doi}
    \verb 10.1109/5.726791
    \endverb
    \field{issn}{0018-9219}
    \field{number}{11}
    \field{pages}{2278\bibrangedash 2324}
    \field{title}{Gradient-based learning applied to document recognition}
    \field{volume}{86}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{year}{1998}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2017arXiv171111294L}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=LX}{%
         family={{Lin}},
         familyi={L\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
      {{hash=ZC}{%
         family={{Zhao}},
         familyi={Z\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=PW}{%
         family={{Pan}},
         familyi={P\bibinitperiod},
         given={W.},
         giveni={W\bibinitperiod},
      }}%
    }
    \keyw{BNN; Computer Science; Learning; Statistics; Machine Learning}
    \strng{namehash}{LXZCPW1}
    \strng{fullhash}{LXZCPW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{booktitle}{31st Conference on Neural Information Processing Systems}
    \verb{eprint}
    \verb 1711.11294
    \endverb
    \field{title}{{Towards Accurate Binary Convolutional Neural Network}}
    \field{journaltitle}{NIPS 2017}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{month}{11}
    \field{year}{2017}
  \endentry

  \entry{Linnainmaa1976}{article}{}
    \name{author}{1}{}{%
      {{hash=LS}{%
         family={Linnainmaa},
         familyi={L\bibinitperiod},
         given={Seppo},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{NN, Neural Networks, Backpropagation}
    \strng{namehash}{LS1}
    \strng{fullhash}{LS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{L}
    \field{sortinithash}{L}
    \field{abstract}{%
    The article describes analytic and algorithmic methods for determining the
  coefficients of the Taylor expansion of an accumulated rounding error with
  respect to the local rounding errors, and hence determining the influence of
  the local errors on the accumulated error. Second and higher order
  coefficients are also discussed, and some possible methods of reducing the
  extensive storage requirements are analyzed.%
    }
    \verb{doi}
    \verb 10.1007/BF01931367
    \endverb
    \field{issn}{1572-9125}
    \field{number}{2}
    \field{pages}{146\bibrangedash 160}
    \field{title}{Taylor expansion of the accumulated rounding error}
    \verb{url}
    \verb https://doi.org/10.1007/BF01931367
    \endverb
    \field{volume}{16}
    \field{journaltitle}{BIT Numerical Mathematics}
    \field{year}{1976}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{McCulloch1943}{article}{}
    \name{author}{2}{}{%
      {{hash=MWS}{%
         family={McCulloch},
         familyi={M\bibinitperiod},
         given={Warren\bibnamedelima S.},
         giveni={W\bibinitperiod\bibinitdelim S\bibinitperiod},
      }}%
      {{hash=PW}{%
         family={Pitts},
         familyi={P\bibinitperiod},
         given={Walter},
         giveni={W\bibinitperiod},
      }}%
    }
    \keyw{NN, Neural network, mathematical definition}
    \strng{namehash}{MWSPW1}
    \strng{fullhash}{MWSPW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Because of the ``all-or-none'' character of nervous activity, neural events
  and the relations among them can be treated by means of propositional logic.
  It is found that the behavior of every net can be described in these terms,
  with the addition of more complicated logical means for nets containing
  circles; and that for any logical expression satisfying certain conditions,
  one can find a net behaving in the fashion it describes. It is shown that
  many particular choices among possible neurophysiological assumptions are
  equivalent, in the sense that for every net behaving under one assumption,
  there exists another net which behaves under the other and gives the same
  results, although perhaps not in the same time. Various applications of the
  calculus are discussed.%
    }
    \verb{doi}
    \verb 10.1007/BF02478259
    \endverb
    \field{issn}{1522-9602}
    \field{number}{4}
    \field{pages}{115\bibrangedash 133}
    \field{title}{A logical calculus of the ideas immanent in nervous activity}
    \verb{url}
    \verb https://doi.org/10.1007/BF02478259
    \endverb
    \field{volume}{5}
    \field{journaltitle}{The bulletin of mathematical biophysics}
    \field{year}{1943}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{8056823}{inproceedings}{}
    \name{author}{7}{}{%
      {{hash=MDJM}{%
         family={Moss},
         familyi={M\bibinitperiod},
         given={D.\bibnamedelima J.\bibnamedelima M.},
         giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim
  M\bibinitperiod},
      }}%
      {{hash=NE}{%
         family={Nurvitadhi},
         familyi={N\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sim},
         familyi={S\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mishra},
         familyi={M\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=MD}{%
         family={Marr},
         familyi={M\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Subhaschandra},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LPHW}{%
         family={Leong},
         familyi={L\bibinitperiod},
         given={P.\bibnamedelima H.\bibnamedelima W.},
         giveni={P\bibinitperiod\bibinitdelim H\bibinitperiod\bibinitdelim
  W\bibinitperiod},
      }}%
    }
    \keyw{BNN, CNN, FPGA, computational complexity;field programmable gate
  arrays;graphics processing units;image recognition;neural nets;object
  detection;high performance binary neural networks;convolutional neural
  networks;CNN;Xeon CPU;Intel Xeon+FPGA platform;FPGA BNN accelerators;Nvidia
  Titan X Pascal GPU;specialised FPGA architecture;Xeon+FPGA system;high
  performance BNN accelerator;lower computational complexity;complex
  topologies;object detection applications;scene segmentation;image
  recognition;Field programmable gate arrays;Graphics processing
  units;Topology;Computer architecture;Performance evaluation;Network
  topology;IP networks}
    \strng{namehash}{MDJM+1}
    \strng{fullhash}{MDJMNESJMAMDSSLPHW1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Convolutional neural networks (CNNs) are deployed in a wide range of image
  recognition, scene segmentation and object detection applications. Achieving
  state of the art accuracy in CNNs often results in large models and complex
  topologies that require significant compute resources to complete in a timely
  manner. Binarised neural networks (BNNs) have been proposed as an optimised
  variant of CNNs, which constrain the weights and activations to +1 or -1 and
  thus offer compact models and lower computational complexity per operation.
  This paper presents a high performance BNN accelerator on the
  Intel{\textregistered}Xeon+FPGA{\texttrademark} platform. The proposed
  accelerator is designed to take advantage of the Xeon+FPGA system in a way
  that a specialised FPGA architecture can be targeted for the most compute
  intensive parts of the BNN whilst other parts of the topology can be handled
  by the Xeon{\texttrademark} CPU. The implementation is evaluated by comparing
  the raw compute performance and energy efficiency for key layers in standard
  CNN topologies against an Nvidia Titan X Pascal GPU and other published FPGA
  BNN accelerators. The results show that our single-package integrated
  Arria{\texttrademark} 10 FPGA accelerator coupled with a high-end Xeon CPU
  can offer comparable performance and better energy efficiency than a high-end
  discrete Titan X GPU card. In addition, our solution delivers the best
  performance compared to previous BNN FPGA implementations.%
    }
    \field{booktitle}{2017 27th International Conference on Field Programmable
  Logic and Applications (FPL)}
    \verb{doi}
    \verb 10.23919/FPL.2017.8056823
    \endverb
    \field{issn}{1946-1488}
    \field{pages}{1\bibrangedash 4}
    \field{title}{High performance binary neural networks on the Xeon-FPGA
  platform}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Moussa2006}{inbook}{}
    \name{author}{3}{}{%
      {{hash=MM}{%
         family={Moussa},
         familyi={M\bibinitperiod},
         given={Medhat},
         giveni={M\bibinitperiod},
      }}%
      {{hash=AS}{%
         family={Areibi},
         familyi={A\bibinitperiod},
         given={Shawki},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NK}{%
         family={Nichols},
         familyi={N\bibinitperiod},
         given={Kristian},
         giveni={K\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, Book Chapter}
    \strng{namehash}{MMASNK1}
    \strng{fullhash}{MMASNK1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    Artificial Neural Networks (ANNs) are inherently parallel architectures
  which represent a natural fit for custom implementation on FPGAs. One
  important implementation issue is to determine the numerical precision format
  that allows an optimum tradeoff between precision and implementation areas.
  Standard single or double precision floating-point representations minimize
  quantization errors while requiring significant hardware resources. Less
  precise fixed-point representation may require less hardware resources but
  add quantization errors that may prevent learning from taking place,
  especially in regression problems. This chapter examines this issue and
  reports on a recent experiment where we implemented a Multi-layer perceptron
  (MLP) on an FPGA using both fixed and floating point precision. Results show
  that the fixed-point MLP implementation was over 12x greater in speed, over
  13x smaller in area, and achieves far greater processing density compared to
  the floating-point FPGA-based MLP.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_2
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{37\bibrangedash 61}
    \field{title}{On the Arithmetic Precision for Implementing Back-Propagation
  Networks on FPGA: A Case Study}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_2
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{nirkin2018_faceswap}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=NY}{%
         family={Nirkin},
         familyi={N\bibinitperiod},
         given={Yuval},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=MI}{%
         family={Masi},
         familyi={M\bibinitperiod},
         given={Iacopo},
         giveni={I\bibinitperiod},
      }}%
      {{hash=TAT}{%
         family={Tran},
         familyi={T\bibinitperiod},
         given={Anh\bibnamedelima Tuan},
         giveni={A\bibinitperiod\bibinitdelim T\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hassner},
         familyi={H\bibinitperiod},
         given={Tal},
         giveni={T\bibinitperiod},
      }}%
      {{hash=MG}{%
         family={Medioni},
         familyi={M\bibinitperiod},
         given={G\'{e}rard},
         giveni={G\bibinitperiod},
      }}%
    }
    \keyw{CNN, Faceswap, DeepFakes, CNN}
    \strng{namehash}{NY+1}
    \strng{fullhash}{NYMITATHTMG1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{booktitle}{IEEE Conference on Automatic Face and Gesture
  Recognition}
    \field{title}{On Face Segmentation, Face Swapping, and Face Perception}
    \field{year}{2018}
  \endentry

  \entry{7929192}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=NE}{%
         family={Nurvitadhi},
         familyi={N\bibinitperiod},
         given={E.},
         giveni={E\bibinitperiod},
      }}%
      {{hash=SD}{%
         family={Sheffield},
         familyi={S\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Sim},
         familyi={S\bibinitperiod},
         given={Jaewoong},
         giveni={J\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Mishra},
         familyi={M\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
      {{hash=VG}{%
         family={Venkatesh},
         familyi={V\bibinitperiod},
         given={G.},
         giveni={G\bibinitperiod},
      }}%
      {{hash=MD}{%
         family={Marr},
         familyi={M\bibinitperiod},
         given={D.},
         giveni={D\bibinitperiod},
      }}%
    }
    \keyw{BNN, application specific integrated circuits;field programmable gate
  arrays;graphics processing units;microprocessor chips;neural nets;ASIC;Aria
  10 FPGA;BNN hardware accelerator design;CPU;DNN;GPU;binarized neural
  networks;deep neural network;hardware acceleration;Biological neural
  networks;Field programmable gate arrays;Graphics processing
  units;Hardware;Neurons;Random access memory;System-on-chip;ASIC;CPU;Deep
  learning;FPGA;GPU;binarized neural networks;data analytics;hardware
  accelerator}
    \strng{namehash}{NE+1}
    \strng{fullhash}{NESDSJMAVGMD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{N}
    \field{sortinithash}{N}
    \field{booktitle}{2016 International Conference on Field-Programmable
  Technology (FPT)}
    \verb{doi}
    \verb 10.1109/FPT.2016.7929192
    \endverb
    \field{pages}{77\bibrangedash 84}
    \field{title}{Accelerating Binarized Neural Networks: Comparison of FPGA,
  CPU, GPU, and ASIC}
    \field{year}{2016}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Omondi:2010:FIN:1941654}{book}{}
    \name{author}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer Publishing Company, Incorporated}%
    }
    \keyw{FPGA-NN, ASIC, Book by Publications}
    \strng{namehash}{OARRJC1}
    \strng{fullhash}{OARRJC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{O}
    \field{sortinithash}{O}
    \field{edition}{1st}
    \field{isbn}{1441939423, 9781441939425}
    \field{title}{FPGA Implementations of Neural Networks}
    \field{year}{2010}
  \endentry

  \entry{Omondi2006}{inbook}{}
    \name{author}{3}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
      {{hash=BM}{%
         family={Bajger},
         familyi={B\bibinitperiod},
         given={Mariusz},
         giveni={M\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, NN, Neurocomputers, Book Chapter}
    \strng{namehash}{OARRJCBM1}
    \strng{fullhash}{OARRJCBM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{O}
    \field{sortinithash}{O}
    \field{abstract}{%
    This introductory chapter reviews the basics of artificial-neural-network
  theory, discusses various aspects of the hardware implementation of neural
  networks (in both ASIC and FPGA technologies, with a focus on special
  features of artificial neural networks), and concludes with a brief note on
  performance-evaluation. Special points are the exploitation of the
  parallelism inherent in neural networks and the appropriate implementation of
  arithmetic functions, especially the sigmoid function. With respect to the
  sigmoid function, the chapter includes a significant contribution.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_1
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{1\bibrangedash 36}
    \field{title}{FPGA Neurocomputers}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_1
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{7033335}{inproceedings}{}
    \name{author}{2}{}{%
      {{hash=POP}{%
         family={Patel},
         familyi={P\bibinitperiod},
         given={O.\bibnamedelima P.},
         giveni={O\bibinitperiod\bibinitdelim P\bibinitperiod},
      }}%
      {{hash=TA}{%
         family={Tiwari},
         familyi={T\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; generalisation (artificial intelligence);learning (artificial
  intelligence);neural nets;optimisation;pattern classification;quantum
  computing;quantum based binary neural network learning algorithm;network
  structure optimisation;neurons;classification accuracy;hidden layer;training
  accuracy;generalization accuracy;QANN;Neurons;Accuracy;Training;Biological
  neural networks;Testing;Diabetes;Binary neural network;Quantum
  processing;Qubits;Back propagation learning}
    \strng{namehash}{POPTA1}
    \strng{fullhash}{POPTA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{booktitle}{2014 International Conference on Information Technology}
    \verb{doi}
    \verb 10.1109/ICIT.2014.29
    \endverb
    \field{pages}{270\bibrangedash 274}
    \field{title}{Quantum Inspired Binary Neural Network Algorithm}
    \field{year}{2014}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Paul2006}{inbook}{}
    \name{author}{2}{}{%
      {{hash=PK}{%
         family={Paul},
         familyi={P\bibinitperiod},
         given={Kolin},
         giveni={K\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Rajopadhye},
         familyi={R\bibinitperiod},
         given={Sanjay},
         giveni={S\bibinitperiod},
      }}%
    }
    \name{editor}{2}{}{%
      {{hash=OAR}{%
         family={Omondi},
         familyi={O\bibinitperiod},
         given={Amos\bibnamedelima R.},
         giveni={A\bibinitperiod\bibinitdelim R\bibinitperiod},
      }}%
      {{hash=RJC}{%
         family={Rajapakse},
         familyi={R\bibinitperiod},
         given={Jagath\bibnamedelima C.},
         giveni={J\bibinitperiod\bibinitdelim C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Springer US}%
    }
    \keyw{FPGA-NN, Book Chapter}
    \strng{namehash}{PKRS1}
    \strng{fullhash}{PKRS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{P}
    \field{sortinithash}{P}
    \field{abstract}{%
    Back propagation is a well known technique used in the implementation of
  artificial neural networks. The algorithm can be described essentially as a
  sequence of matrix vector multiplications and outer product operations
  interspersed with the application of a point wise non linear function. The
  algorithm is compute intensive and lends itself to a high degree of
  parallelism. These features motivate a systolic design of hardware to
  implement the Back Propagation algorithm. We present in this chapter a new
  systolic architecture for the complete back propagation algorithm. For a
  neural network with N input neurons, P hidden layer neurons and M output
  neurons, the proposed architecture with P processors, has a running time of
  (2N + 2M + P + max(M,P)) for each training set vector. This is the first such
  implementation of the back propagation algorithm which completely
  parallelizes the entire computation of learning phase. The array has been
  implemented on an Annapolis FPGA based coprocessor and it achieves very
  favorable performance with range of 5 GOPS. The proposed new design targets
  Virtex boards.%
    }
    \field{booktitle}{FPGA Implementations of Neural Networks}
    \verb{doi}
    \verb 10.1007/0-387-28487-7_5
    \endverb
    \field{isbn}{978-0-387-28487-3}
    \field{pages}{137\bibrangedash 165}
    \field{title}{Back-Propagation Algorithm Achieving 5 Gops on the Virtex-E}
    \verb{url}
    \verb https://doi.org/10.1007/0-387-28487-7_5
    \endverb
    \list{location}{1}{%
      {Boston, MA}%
    }
    \field{year}{2006}
  \endentry

  \entry{Raina:2009:LDU:1553374.1553486}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=RR}{%
         family={Raina},
         familyi={R\bibinitperiod},
         given={Rajat},
         giveni={R\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Madhavan},
         familyi={M\bibinitperiod},
         given={Anand},
         giveni={A\bibinitperiod},
      }}%
      {{hash=NAY}{%
         family={Ng},
         familyi={N\bibinitperiod},
         given={Andrew\bibnamedelima Y.},
         giveni={A\bibinitperiod\bibinitdelim Y\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{NN; GPU, Neural Network}
    \strng{namehash}{RRMANAY1}
    \strng{fullhash}{RRMANAY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{booktitle}{Proceedings of the 26th Annual International Conference
  on Machine Learning}
    \verb{doi}
    \verb 10.1145/1553374.1553486
    \endverb
    \field{isbn}{978-1-60558-516-1}
    \field{pages}{873\bibrangedash 880}
    \field{series}{ICML '09}
    \field{title}{Large-scale Deep Unsupervised Learning Using Graphics
  Processors}
    \verb{url}
    \verb http://doi.acm.org/10.1145/1553374.1553486
    \endverb
    \list{location}{1}{%
      {Montreal, Quebec, Canada}%
    }
    \field{year}{2009}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{xnor.ai-webpage}{online}{}
    \name{author}{1}{}{%
      {{hash=RAFM}{%
         family={Rastegari},
         familyi={R\bibinitperiod},
         given={Ali Farhadi;\bibnamedelima Mohammad},
         giveni={A\bibinitperiod\bibinitdelim F\bibinitperiod\bibinitdelim
  M\bibinitperiod},
      }}%
    }
    \keyw{BNN, XNOR, Yolo}
    \strng{namehash}{RAFM1}
    \strng{fullhash}{RAFM1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \field{title}{xnor.ai Company}
    \verb{url}
    \verb https://www.xnor.ai/
    \endverb
    \field{year}{2016}
    \field{urlyear}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{2016arXiv160305279R}{article}{}
    \name{author}{4}{}{%
      {{hash=RM}{%
         family={{Rastegari}},
         familyi={R\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=OV}{%
         family={{Ordonez}},
         familyi={O\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=RJ}{%
         family={{Redmon}},
         familyi={R\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=FA}{%
         family={{Farhadi}},
         familyi={F\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{BNN; Computer Vision; Pattern Recognition}
    \strng{namehash}{RM+1}
    \strng{fullhash}{RMOVRJFA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{R}
    \field{sortinithash}{R}
    \verb{eprint}
    \verb 1603.05279
    \endverb
    \field{title}{{XNOR-Net: ImageNet Classification Using Binary Convolutional
  Neural Networks}}
    \field{journaltitle}{European Conference on Computer Vision}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{year}{2016}
  \endentry

  \entry{8461456}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=SC}{%
         family={Sakr},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CJ}{%
         family={Choi},
         familyi={C\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
      {{hash=WZ}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Z.},
         giveni={Z\bibinitperiod},
      }}%
      {{hash=GK}{%
         family={Gopalakrishnan},
         familyi={G\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=SN}{%
         family={Shanbhag},
         familyi={S\bibinitperiod},
         given={N.},
         giveni={N\bibinitperiod},
      }}%
    }
    \keyw{BNN; gradient methods;learning (artificial intelligence);neural
  nets;gradient-based training;deep binary activated neural networks;continuous
  binarization;deep learning;tremendous complexity;resource constrained
  platforms;training procedure;binary activation functions;minimal accuracy
  degradation;gradient-based learning;back-propagation algorithm;straight
  through estimator;floating-point baseline;STE;Training;Neural
  networks;Complexity theory;Machine learning;Stochastic processes;Perturbation
  methods;Approximation algorithms;deep learning;binary neural
  networks;activation functions}
    \strng{namehash}{SC+1}
    \strng{fullhash}{SCCJWZGKSN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    With the ever growing popularity of deep learning, the tremendous
  complexity of deep neural networks is becoming problematic when one considers
  inference on resource constrained platforms. Binary networks have emerged as
  a potential solution, however, they exhibit a fundamentallimi-tation in
  realizing gradient-based learning as their activations are
  non-differentiable. Current work has so far relied on approximating gradients
  in order to use the back-propagation algorithm via the straight through
  estimator (STE). Such approximations harm the quality of the training
  procedure causing a noticeable gap in accuracy between binary neural networks
  and their full precision baselines. We present a novel method to train binary
  activated neural networks using true gradient-based learning. Our idea is
  motivated by the similarities between clipping and binary activation
  functions. We show that our method has minimal accuracy degradation with
  respect to the full precision baseline. Finally, we test our method on three
  benchmarking datasets: MNIST, CIFAR-10, and SVHN. For each benchmark, we show
  that continuous binarization using true gradient-based learning achieves an
  accuracy within 1.5% of the floating-point baseline, as compared to accuracy
  drops as high as 6% when training the same binary activated network using the
  STE.%
    }
    \field{booktitle}{2018 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP)}
    \verb{doi}
    \verb 10.1109/ICASSP.2018.8461456
    \endverb
    \field{issn}{2379-190X}
    \field{pages}{2346\bibrangedash 2350}
    \field{title}{True Gradient-Based Training of Deep Binary Activated Neural
  Networks Via Continuous Binarization}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{SCHMIDHUBER201585}{article}{}
    \name{author}{1}{}{%
      {{hash=SJ}{%
         family={Schmidhuber},
         familyi={S\bibinitperiod},
         given={J{\"u}rgen},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{DNN; Deep learning, Supervised learning, Unsupervised learning,
  Reinforcement learning, Evolutionary computation}
    \strng{namehash}{SJ1}
    \strng{fullhash}{SJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    In recent years, deep artificial neural networks (including recurrent ones)
  have won numerous contests in pattern recognition and machine learning. This
  historical survey compactly summarizes relevant work, much of it from the
  previous millennium. Shallow and Deep Learners are distinguished by the depth
  of their credit assignment paths, which are chains of possibly learnable,
  causal links between actions and effects. I review deep supervised learning
  (also recapitulating the history of backpropagation), unsupervised learning,
  reinforcement learning & evolutionary computation, and indirect search for
  short programs encoding deep and large networks.%
    }
    \verb{doi}
    \verb https://doi.org/10.1016/j.neunet.2014.09.003
    \endverb
    \field{issn}{0893-6080}
    \field{pages}{85 \bibrangedash  117}
    \field{title}{Deep learning in neural networks: An overview}
    \verb{url}
    \verb http://www.sciencedirect.com/science/article/pii/S0893608014002135
    \endverb
    \field{volume}{61}
    \field{journaltitle}{Neural Networks}
    \field{year}{2015}
  \endentry

  \entry{shayer2018learning}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SO}{%
         family={Shayer},
         familyi={S\bibinitperiod},
         given={Oran},
         giveni={O\bibinitperiod},
      }}%
      {{hash=LD}{%
         family={Levi},
         familyi={L\bibinitperiod},
         given={Dan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=FE}{%
         family={Fetaya},
         familyi={F\bibinitperiod},
         given={Ethan},
         giveni={E\bibinitperiod},
      }}%
    }
    \keyw{BNN, Discrete, MNIST, CIFAR, ImageNet}
    \strng{namehash}{SOLDFE1}
    \strng{fullhash}{SOLDFE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{booktitle}{International Conference on Learning Representations}
    \field{title}{Learning Discrete Weights Using the Local Reparameterization
  Trick}
    \verb{url}
    \verb https://openreview.net/forum?id=BySRH6CpW
    \endverb
    \field{year}{2018}
  \endentry

  \entry{Silver:2017aa}{article}{}
    \name{author}{17}{}{%
      {{hash=SD}{%
         family={Silver},
         familyi={S\bibinitperiod},
         given={David},
         giveni={D\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Schrittwieser},
         familyi={S\bibinitperiod},
         given={Julian},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SK}{%
         family={Simonyan},
         familyi={S\bibinitperiod},
         given={Karen},
         giveni={K\bibinitperiod},
      }}%
      {{hash=AI}{%
         family={Antonoglou},
         familyi={A\bibinitperiod},
         given={Ioannis},
         giveni={I\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={Aja},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Guez},
         familyi={G\bibinitperiod},
         given={Arthur},
         giveni={A\bibinitperiod},
      }}%
      {{hash=HT}{%
         family={Hubert},
         familyi={H\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
      {{hash=BL}{%
         family={Baker},
         familyi={B\bibinitperiod},
         given={Lucas},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LM}{%
         family={Lai},
         familyi={L\bibinitperiod},
         given={Matthew},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Bolton},
         familyi={B\bibinitperiod},
         given={Adrian},
         giveni={A\bibinitperiod},
      }}%
      {{hash=CY}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Yutian},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=LT}{%
         family={Lillicrap},
         familyi={L\bibinitperiod},
         given={Timothy},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HF}{%
         family={Hui},
         familyi={H\bibinitperiod},
         given={Fan},
         giveni={F\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Sifre},
         familyi={S\bibinitperiod},
         given={Laurent},
         giveni={L\bibinitperiod},
      }}%
      {{hash=vdDG}{%
         prefix={van\bibnamedelima den},
         prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod},
         family={Driessche},
         familyi={D\bibinitperiod},
         given={George},
         giveni={G\bibinitperiod},
      }}%
      {{hash=GT}{%
         family={Graepel},
         familyi={G\bibinitperiod},
         given={Thore},
         giveni={T\bibinitperiod},
      }}%
      {{hash=HD}{%
         family={Hassabis},
         familyi={H\bibinitperiod},
         given={Demis},
         giveni={D\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Macmillan Publishers Limited, part of Springer Nature. All rights
  reserved. SN -}%
    }
    \keyw{NN, Go, Google, AlphaGo}
    \strng{namehash}{SD+1}
    \strng{fullhash}{SDSJSKAIHAGAHTBLLMBACYLTHFSLDGvdGTHD1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{pages}{354 EP \bibrangedash }
    \field{title}{Mastering the game of Go without human knowledge}
    \verb{url}
    \verb https://doi.org/10.1038/nature24270
    \endverb
    \field{volume}{550}
    \field{journaltitle}{Nature}
    \field{month}{10}
    \field{year}{2017}
    \warn{\item Invalid format of field 'date' \item Invalid format of field
  'date'}
  \endentry

  \entry{Simonyan14c}{article}{}
    \name{author}{2}{}{%
      {{hash=SK}{%
         family={Simonyan},
         familyi={S\bibinitperiod},
         given={K.},
         giveni={K\bibinitperiod},
      }}%
      {{hash=ZA}{%
         family={Zisserman},
         familyi={Z\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{CNN; image classification, VGG network}
    \strng{namehash}{SKZA1}
    \strng{fullhash}{SKZA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{title}{Very Deep Convolutional Networks for Large-Scale Image
  Recognition}
    \field{volume}{abs/1409.1556}
    \field{journaltitle}{ILSVRC - CoRR}
    \field{year}{2014}
  \endentry

  \entry{Soudry2014ExpectationBP}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SD}{%
         family={Soudry},
         familyi={S\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=HI}{%
         family={Hubara},
         familyi={H\bibinitperiod},
         given={Itay},
         giveni={I\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={Meir},
         familyi={M\bibinitperiod},
         given={Ron},
         giveni={R\bibinitperiod},
      }}%
    }
    \keyw{DNN; Backpropagation; Discrete weight space; Continuos weight space}
    \strng{namehash}{SDHIMR1}
    \strng{fullhash}{SDHIMR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Multilayer Neural Networks (MNNs) are commonly trained using gradient
  descent-based methods, such as BackPropagation (BP). Inference in
  probabilistic graphical models is often done using variational Bayes methods,
  such as Expectation Propagation (EP). We show how an EP based approach can
  also be used to train deterministic MNNs. Specifically, we approximate the
  posterior of the weights given the data using a ``mean-field'' factorized
  distribution, in an online setting. Using online EP and the central limit
  theorem we find an analytical approximation to the Bayes update of this
  posterior, as well as the resulting Bayes estimates of the weights and
  outputs. Despite a different origin, the resulting algorithm, Expectation
  BackPropagation (EBP), is very similar to BP in form and efficiency. However,
  it has several additional advantages: (1) Training is parameter-free, given
  initial conditions (prior) and the MNN architecture. This is useful for
  large-scale problems, where parameter tuning is a major challenge. (2) The
  weights can be restricted to have discrete values. This is especially useful
  for implementing trained MNNs in precision limited hardware chips, thus
  improving their speed and energy efficiency by several orders of magnitude.
  We test the EBP algorithm numerically in eight binary text classification
  tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal
  constant learning rate (2) previously reported state of the art.
  Interestingly, EBP-trained MNNs with binary weights usually perform better
  than MNNs with continuous (real) weights - if we average the MNN output using
  the inferred posterior.%
    }
    \field{booktitle}{NIPS}
    \field{title}{Expectation Backpropagation: Parameter-Free Training of
  Multilayer Neural Networks with Continuous or Discrete Weights}
    \field{annotation}{%
  https://www.semanticscholar.org/paper/Expectation-Backpropagation%3A-Parameter-Free-of-with-Soudry-Hubara/0b88ed755c4dcd0ac3d25842a5bbbe4ebcefeeec
  https://papers.nips.cc/paper/5269-expectation-backpropagation-parameter-free-training-of-multilayer-neural-networks-with-continuous-or-discrete-weights.pdf%
    }
    \field{year}{2014}
  \endentry

  \entry{5524599}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=SM}{%
         family={Stoica},
         familyi={S\bibinitperiod},
         given={M.},
         giveni={M\bibinitperiod},
      }}%
      {{hash=CGA}{%
         family={Calangiu},
         familyi={C\bibinitperiod},
         given={G.\bibnamedelima A.},
         giveni={G\bibinitperiod\bibinitdelim A\bibinitperiod},
      }}%
      {{hash=SF}{%
         family={Sisak},
         familyi={S\bibinitperiod},
         given={F.},
         giveni={F\bibinitperiod},
      }}%
    }
    \keyw{NN; graph theory;learning (artificial intelligence);neural nets;robot
  programming;neural network training;training steps;artificial neural
  networks;robot programming;unidirectional multi-layer neural
  network;graph;Time measurement;Neural networks;Robot kinematics;Service
  robots;Sliding mode control;Control systems;Biological system
  modeling;Humans;Biological neural networks;Artificial neural networks}
    \strng{namehash}{SMCGASF1}
    \strng{fullhash}{SMCGASF1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{abstract}{%
    Artificial neural networks play an important role in robot programming by
  demonstration. In this paper we present a method for artificial neural
  network training. The main idea of this method is to train the artificial
  neural network with all of the data, before the current training step, and at
  a certain step the network is already trained a huge number of times. Some
  features of the quality of neural network training, using this method, were
  presented in. Because the method uses all of the data before the current
  training step, in this paper, we are concerned about training time and
  computing time comportment of the neural network. A software application for
  obtaining training time based on the number of training steps was designed.
  This software application implements the training method on an unidirectional
  multi-layer neural network and prints into a graph the training time and
  computing time. The results obtained using the software application and
  important conclusions towards the training and computing time comportment are
  also presented.%
    }
    \field{booktitle}{19th International Workshop on Robotics in
  Alpe-Adria-Danube Region (RAAD 2010)}
    \verb{doi}
    \verb 10.1109/RAAD.2010.5524599
    \endverb
    \field{pages}{109\bibrangedash 113}
    \field{title}{Measuring the time needed for training a neural network based
  on the number of training steps}
    \field{year}{2010}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Sun:2018:FPR:3201607.3201741}{inproceedings}{}
    \name{author}{6}{}{%
      {{hash=SX}{%
         family={Sun},
         familyi={S\bibinitperiod},
         given={Xiaoyu},
         giveni={X\bibinitperiod},
      }}%
      {{hash=PX}{%
         family={Peng},
         familyi={P\bibinitperiod},
         given={Xiaochen},
         giveni={X\bibinitperiod},
      }}%
      {{hash=CPY}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Pai-Yu},
         giveni={P\bibinitperiod-Y\bibinitperiod},
      }}%
      {{hash=LR}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Rui},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SJs}{%
         family={Seo},
         familyi={S\bibinitperiod},
         given={Jae-sun},
         giveni={J\bibinitperiod-s\bibinitperiod},
      }}%
      {{hash=YS}{%
         family={Yu},
         familyi={Y\bibinitperiod},
         given={Shimeng},
         giveni={S\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {IEEE Press}%
    }
    \keyw{BNN, P-BNN, CSM, MNIST}
    \strng{namehash}{SX+1}
    \strng{fullhash}{SXPXCPYLRSJsYS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{booktitle}{Proceedings of the 23rd Asia and South Pacific Design
  Automation Conference}
    \field{pages}{574\bibrangedash 579}
    \field{series}{ASPDAC '18}
    \field{title}{Fully Parallel RRAM Synaptic Array for Implementing Binary
  Neural Network with (+1, -1) Weights and (+1, 0) Neurons}
    \verb{url}
    \verb http://dl.acm.org/citation.cfm?id=3201607.3201741
    \endverb
    \list{location}{1}{%
      {Jeju, Republic of Korea}%
    }
    \field{year}{2018}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{2017arXiv170309039S}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=SV}{%
         family={{Sze}},
         familyi={S\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=CYH}{%
         family={{Chen}},
         familyi={C\bibinitperiod},
         given={Y.-H.},
         giveni={Y\bibinitperiod-H\bibinitperiod},
      }}%
      {{hash=YTJ}{%
         family={{Yang}},
         familyi={Y\bibinitperiod},
         given={T.-J.},
         giveni={T\bibinitperiod-J\bibinitperiod},
      }}%
      {{hash=EJ}{%
         family={{Emer}},
         familyi={E\bibinitperiod},
         given={J.},
         giveni={J\bibinitperiod},
      }}%
    }
    \keyw{NN, Computer Science; Computer Vision; Pattern Recognition}
    \strng{namehash}{SV+1}
    \strng{fullhash}{SVCYHYTJEJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \field{booktitle}{Proceedings of the IEEE}
    \verb{eprint}
    \verb 1703.09039
    \endverb
    \field{number}{12}
    \field{title}{Efficient Processing of Deep Neural Networks: A Tutorial and
  Survey}
    \field{volume}{105}
    \field{journaltitle}{Proceedings of the IEEE}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{12}
    \field{year}{2017}
  \endentry

  \entry{2016arXiv160207261S}{proceedings}{}
    \name{author}{4}{}{%
      {{hash=SC}{%
         family={{Szegedy}},
         familyi={S\bibinitperiod},
         given={C.},
         giveni={C\bibinitperiod},
      }}%
      {{hash=IS}{%
         family={{Ioffe}},
         familyi={I\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=VV}{%
         family={{Vanhoucke}},
         familyi={V\bibinitperiod},
         given={V.},
         giveni={V\bibinitperiod},
      }}%
      {{hash=AA}{%
         family={{Alemi}},
         familyi={A\bibinitperiod},
         given={A.},
         giveni={A\bibinitperiod},
      }}%
    }
    \list{organization}{1}{%
      {AAAI}%
    }
    \list{publisher}{1}{%
      {AAAI}%
    }
    \keyw{CNN; Computer Vision; Pattern Recognition; Inception}
    \strng{namehash}{SC+2}
    \strng{fullhash}{SCISVVAA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{S}
    \field{sortinithash}{S}
    \verb{eprint}
    \verb 1602.07261
    \endverb
    \field{number}{31}
    \field{title}{{Inception-v4, Inception-ResNet and the Impact of Residual
  Connections on Learning}}
    \field{volume}{31}
    \field{journaltitle}{Prceedings of the 31st AAAI Conference on Artificial
  Intelligence}
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{02}
    \field{year}{2017}
  \endentry

  \entry{AppleFaceDetectionURL}{online}{}
    \name{author}{1}{}{%
      {{hash=TCVML}{%
         family={Team},
         familyi={T\bibinitperiod},
         given={Computer Vision Machine\bibnamedelima Learning},
         giveni={C\bibinitperiod\bibinitdelim V\bibinitperiod
  M\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \keyw{Neural Network, Vision Framework, Face recognition}
    \strng{namehash}{TCVML1}
    \strng{fullhash}{TCVML1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{T}
    \field{sortinithash}{T}
    \field{title}{An On-device Deep Neural Network for Face Detection}
    \verb{url}
    \verb https://machinelearning.apple.com/2017/11/16/face-detection.html
    \endverb
    \field{month}{11}
    \field{year}{2017}
    \warn{\item Invalid format of field 'urldate'}
  \endentry

  \entry{wu2018training}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=WS}{%
         family={Wu},
         familyi={W\bibinitperiod},
         given={Shuang},
         giveni={S\bibinitperiod},
      }}%
      {{hash=LG}{%
         family={Li},
         familyi={L\bibinitperiod},
         given={Guoqi},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CF}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Feng},
         giveni={F\bibinitperiod},
      }}%
      {{hash=SL}{%
         family={Shi},
         familyi={S\bibinitperiod},
         given={Luping},
         giveni={L\bibinitperiod},
      }}%
    }
    \keyw{BNN; Binary, MNIST, CIFAR, ImageNet}
    \strng{namehash}{WS+1}
    \strng{fullhash}{WSLGCFSL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{booktitle}{International Conference on Learning Representations}
    \field{title}{Training and Inference with Integers in Deep Neural Networks}
    \verb{url}
    \verb https://openreview.net/forum?id=HJGXzmspb
    \endverb
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Yamada1992}{article}{}
    \name{author}{3}{}{%
      {{hash=YM}{%
         family={Yamada},
         familyi={Y\bibinitperiod},
         given={Manabu},
         giveni={M\bibinitperiod},
      }}%
      {{hash=NT}{%
         family={Nakagawa},
         familyi={N\bibinitperiod},
         given={Tohru},
         giveni={T\bibinitperiod},
      }}%
      {{hash=KH}{%
         family={Kitagawa},
         familyi={K\bibinitperiod},
         given={Hajime},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{BNN; Parallel processing; Sorting Algorithm}
    \strng{namehash}{YMNTKH1}
    \strng{fullhash}{YMNTKH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{abstract}{%
    This paper presents an ultra-high-speed sorter based upon a simplified
  parallel sorting algorithm using a binary neural network which consists both
  of binary neurons and of AND-OR synaptic connections to solve sorting
  problems at two and only two clock cycles. Our simplified algorithm is based
  on the super parallel sorting algorithm proposed by Takefuji and Lee.
  Nevertheless, our algorithm does not need any adders, while Takefuji's
  algorithm needs n{\texttimes}(nâ1) analog adders of which each has multiple
  input ports. For an example of the simplified parallel sorter, a hardware
  design and its implementation will be introduced in this paper, which
  performs a sorting operation at two clock cycles. Both results of a logic
  circuit simulation and of an algorithm simulation show the justice of our
  hardware implementation even if in the practical size of the problem.%
    }
    \verb{doi}
    \verb 10.1007/BF00228719
    \endverb
    \field{issn}{1573-1979}
    \field{number}{4}
    \field{pages}{389\bibrangedash 393}
    \field{title}{A super parallel sorter using a binary neural network with
  AND-OR synaptic connections}
    \verb{url}
    \verb https://doi.org/10.1007/BF00228719
    \endverb
    \field{volume}{2}
    \field{journaltitle}{Analog Integrated Circuits and Signal Processing}
    \field{year}{1992}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{bmxnet}{inproceedings}{}
    \name{author}{4}{}{%
      {{hash=YH}{%
         family={Yang},
         familyi={Y\bibinitperiod},
         given={Haojin},
         giveni={H\bibinitperiod},
      }}%
      {{hash=FM}{%
         family={Fritzsche},
         familyi={F\bibinitperiod},
         given={Martin},
         giveni={M\bibinitperiod},
      }}%
      {{hash=BC}{%
         family={Bartz},
         familyi={B\bibinitperiod},
         given={Christian},
         giveni={C\bibinitperiod},
      }}%
      {{hash=MC}{%
         family={Meinel},
         familyi={M\bibinitperiod},
         given={Christoph},
         giveni={C\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{BNN; binary neural networks, computer vision, machine learning, open
  source}
    \strng{namehash}{YH+1}
    \strng{fullhash}{YHFMBCMC1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{booktitle}{Proceedings of the 2017 ACM on Multimedia Conference}
    \verb{doi}
    \verb 10.1145/3123266.3129393
    \endverb
    \field{isbn}{978-1-4503-4906-2}
    \field{pages}{1209\bibrangedash 1212}
    \field{series}{MM '17}
    \field{title}{BMXNet: An Open-Source Binary Neural Network Implementation
  Based on MXNet}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3123266.3129393
    \endverb
    \list{location}{1}{%
      {Mountain View, California, USA}%
    }
    \field{annotation}{%
    https://arxiv.org/abs/1705.09864 https://github.com/hpi-xnor/BMXNet%
    }
    \field{year}{2017}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{8416941}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=YH}{%
         family={Yonekawa},
         familyi={Y\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
      {{hash=SS}{%
         family={Sato},
         familyi={S\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=NH}{%
         family={Nakahara},
         familyi={N\bibinitperiod},
         given={H.},
         giveni={H\bibinitperiod},
      }}%
    }
    \keyw{BNN; Neurons;Training;Two dimensional displays;Embedded
  systems;Character recognition;Computational modeling;Convolutional neural
  networks}
    \strng{namehash}{YHSSNH1}
    \strng{fullhash}{YHSSNH1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{booktitle}{2018 IEEE 48th International Symposium on Multiple-Valued
  Logic (ISMVL)}
    \verb{doi}
    \verb 10.1109/ISMVL.2018.00038
    \endverb
    \field{issn}{2378-2226}
    \field{pages}{174\bibrangedash 179}
    \field{title}{A Ternary Weight Binary Input Convolutional Neural Network:
  Realization on the Embedded Processor}
    \verb{url}
    \verb doi.ieeecomputersociety.org/10.1109/ISMVL.2018.00038
    \endverb
    \field{volume}{00}
    \field{year}{2018}
    \warn{\item Invalid format of field 'month'}
  \endentry

  \entry{Zhao:2017:ABC:3020078.3021741}{inproceedings}{}
    \name{author}{8}{}{%
      {{hash=ZR}{%
         family={Zhao},
         familyi={Z\bibinitperiod},
         given={Ritchie},
         giveni={R\bibinitperiod},
      }}%
      {{hash=SW}{%
         family={Song},
         familyi={S\bibinitperiod},
         given={Weinan},
         giveni={W\bibinitperiod},
      }}%
      {{hash=ZW}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Wentao},
         giveni={W\bibinitperiod},
      }}%
      {{hash=XT}{%
         family={Xing},
         familyi={X\bibinitperiod},
         given={Tianwei},
         giveni={T\bibinitperiod},
      }}%
      {{hash=LJH}{%
         family={Lin},
         familyi={L\bibinitperiod},
         given={Jeng-Hau},
         giveni={J\bibinitperiod-H\bibinitperiod},
      }}%
      {{hash=SM}{%
         family={Srivastava},
         familyi={S\bibinitperiod},
         given={Mani},
         giveni={M\bibinitperiod},
      }}%
      {{hash=GR}{%
         family={Gupta},
         familyi={G\bibinitperiod},
         given={Rajesh},
         giveni={R\bibinitperiod},
      }}%
      {{hash=ZZ}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Zhiru},
         giveni={Z\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {ACM}%
    }
    \keyw{FPGAs, binarized, binarized convolutional networks, deep learning,
  high-level synthesis, reconfigurable computing}
    \strng{namehash}{ZR+1}
    \strng{fullhash}{ZRSWZWXTLJHSMGRZZ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{booktitle}{Proceedings of the 2017 ACM/SIGDA International Symposium
  on Field-Programmable Gate Arrays}
    \verb{doi}
    \verb 10.1145/3020078.3021741
    \endverb
    \field{isbn}{978-1-4503-4354-1}
    \field{pages}{15\bibrangedash 24}
    \field{series}{FPGA '17}
    \field{title}{Accelerating Binarized Convolutional Neural Networks with
  Software-Programmable FPGAs}
    \verb{url}
    \verb http://doi.acm.org/10.1145/3020078.3021741
    \endverb
    \list{location}{1}{%
      {Monterey, California, USA}%
    }
    \field{year}{2017}
    \warn{\item Can't use 'location' + 'address'}
  \endentry

  \entry{8052915}{inproceedings}{}
    \name{author}{3}{}{%
      {{hash=ZY}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Y.},
         giveni={Y\bibinitperiod},
      }}%
      {{hash=RS}{%
         family={Redkar},
         familyi={R\bibinitperiod},
         given={S.},
         giveni={S\bibinitperiod},
      }}%
      {{hash=HX}{%
         family={Huang},
         familyi={H\bibinitperiod},
         given={X.},
         giveni={X\bibinitperiod},
      }}%
    }
    \keyw{BNN;field programmable gate arrays;fixed point arithmetic;image
  classification;learning (artificial intelligence);neural nets;object
  recognition;quantisation (signal);1-bit XNOR operation;CIFAR-10
  benchmark;FPGA platform;batch normalization operation;computational
  complexity reduction;convolutional neural network;deep learning binary neural
  network;fixed point multiplication operation;local receptive fields;low-power
  embedded applications;memory requirement reduction;on-chip memories;spatial
  correlation;Biological neural networks;Convolution;Field programmable gate
  arrays;Hardware;Memory management;Training}
    \strng{namehash}{ZYRSHX1}
    \strng{fullhash}{ZYRSHX1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    As a popular deep learning technique, convolutional neural network has been
  widely used in many tasks such as image classification and object
  recognition. Convolutional neural network exploits spatial correlations in
  the images by performing convolution operations in local receptive fields.
  Convolutional neural networks are preferred over fully connected neural
  networks because they have fewer weights and are easier to train. Many
  research works have been conducted to reduce the computational complexity and
  memory requirements of convolutional neural network, to make it applicable to
  the low-power embedded applications with limited memories. This paper
  presents the architecture design of convolutional neural network with binary
  weights and activations, also known as binary neural network, on an FPGA
  platform. Weights and input activations are binarized with only two values,
  +1 and -1. This reduces all the fixed point multiplication operations in
  convolutional layers and fully connected layers to 1-bit XNOR operations. The
  proposed design uses only on-chip memories. Furthermore, an efficient
  implementation of batch normalization operation is introduced. When
  evaluating the CIFAR-10 benchmark, the proposed FPGA design can achieve a
  processing rate of 332,158 images per second with with accuracy of 86.06%
  using 1-bit quantized weights and activations.%
    }
    \field{booktitle}{2017 IEEE 60th International Midwest Symposium on
  Circuits and Systems (MWSCAS)}
    \verb{doi}
    \verb 10.1109/MWSCAS.2017.8052915
    \endverb
    \field{pages}{281\bibrangedash 284}
    \field{title}{Deep learning binary neural network on an FPGA}
    \field{year}{2017}
    \warn{\item Invalid format of field 'month'}
  \endentry
\endsortlist
\endinput
