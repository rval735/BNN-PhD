@INPROCEEDINGS{7965079,  author={Y. Qiao and K. Hashimoto and A. Eriguchi and H. Wang and D. Wang and Y. Tsuruoka and K. Taura}, booktitle={2017 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},  title={Cache Friendly Parallelization of Neural Encoder-Decoder Models Without Padding on Multi-core Architecture},  year={2017}, volume={}, number={}, pages={437-440}, abstract={Scaling up Artificial Intelligence (AI) algorithms for massive datasets to improve their performance is becoming crucial. In Machine Translation (MT), one of most important research fields of AI, models based on Recurrent Neural Net- works (RNN) show state-of-the-art performance in recent years, and many researchers keep working on improving RNN-based models to achieve better accuracy in translation tasks. Most implementations of Neural Machine Translation (NMT) models employ a padding strategy when processing a mini-batch to make all sentences in a mini-batch have the same length. This enables an efficient utilization of caches and GPU/SIMD parallelism but leads to a waste of computation time. In this paper, we implement and parallelize batch learning for a Sequence-to- Sequence (Seq2Seq) model, which is the most basic model of NMT, without using a padding strategy. More specifically, our approach forms vectors which represent the input words as well as the neural network's states at different time steps into matrices when it processes one sentence, and as a result, the approach makes a better use of cache and optimizes the process that adjusts weights and biases during the back-propagation phase. Our experimental evaluation shows that our implementation achieves better scalability on multi-core CPUs. We also discuss our approach's potential to be used in other implementations of RNN-based models.}, keywords={backpropagation;cache storage;graphics processing units;language translation;matrix algebra;multiprocessing systems;recurrent neural nets;vectors;cache friendly parallelization;neural encoder-decoder models;multicore architecture;scaling up artificial intelligence algorithms;neural machine translation models;NMT models;recurrent neural networks;RNN-based models;translation tasks;GPU-SIMD parallelism;batch learning parallelization;sequence-to-sequence model;Seq2Seq model;back-propagation phase;multicore CPUs;Computational modeling;Instruction sets;Scalability;Electronic mail;Recurrent neural networks;Training;Libraries;Neural Machine Translation;Cache Optimization;Parallel Programming}, doi={10.1109/IPDPSW.2017.165}, ISSN={}, month={May},}
