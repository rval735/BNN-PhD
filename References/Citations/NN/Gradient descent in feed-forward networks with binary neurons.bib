@INPROCEEDINGS{857854,
  author={M. {Costa} and D. {Palmisano} and E. {Pasero}},
  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium}, 
  title={Gradient descent in feed-forward networks with binary neurons}, 
  year={2000},
  volume={1},
  number={},
  pages={311-316 vol.1},
  abstract={In this paper we show how the familiar concept of gradient descent can be extended in presence of binary neurons. The procedure we devised formally operates on generic feedforward networks of logistic-like neurons whose activations are re-scaled by an arbitrarily large gauge. Whereas the gradient decays exponentially with increasing values of the gauge, the sign of each component becomes definitely equal to a constant value. Those values are actually computed by means of a "twin" network of binary neurons. This allows the application of any "Manhattan" training algorithm such as resilient propagation.},
  keywords={feedforward neural nets;gradient methods;learning (artificial intelligence);gradient descent;feedforward networks;binary neurons;logistic-like neurons;twin neural network;Manhattan training algorithm;resilient propagation;Intelligent networks;Feedforward systems;Neurons;Computer networks;Logistics;Turning;Cost function;Signal resolution;Computer architecture;Parallel processing},
  doi={10.1109/IJCNN.2000.857854},
  ISSN={1098-7576},
  month={July},}
