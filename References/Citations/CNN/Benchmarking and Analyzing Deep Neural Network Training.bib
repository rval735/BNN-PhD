@INPROCEEDINGS{8573476,
  author={H. {Zhu} and M. {Akrout} and B. {Zheng} and A. {Pelegris} and A. {Jayarajan} and A. {Phanishayee} and B. {Schroeder} and G. {Pekhimenko}},
  booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Benchmarking and Analyzing Deep Neural Network Training}, 
  year={2018},
  volume={},
  number={},
  pages={88-100},
  abstract={The recent popularity of deep neural networks (DNNs) has generated considerable research interest in performing DNN-related computation efficiently. However, the primary focus is usually very narrow and limited to (i) inference - i.e. how to efficiently execute already trained models and (ii) image classification networks as the primary benchmark for evaluation. Our primary goal in this work is to break this myopic view by (i) proposing a new benchmark suite for DNN training, called TBD1, which comprises a representative set of eight DNN models and covers six major machine learning applications: image classification, machine translation, speech recognition, object detection, adversarial networks, reinforcement learning, and (ii) performing an extensive performance analysis of these models on three major deep learning frameworks (TensorFlow, MXNet, CNTK) across different hardware configurations (single-GPU, multi-GPU, and multi-machine). We present a new toolchain for performance analysis for these models that combines the targeted usage of existing performance analysis tools, careful selection of performance metrics, and methodologies to analyze the results. We also build a new set of tools for memory profiling in three major frameworks. These tools can shed light on precisely how much memory is consumed by different data structures (weights, activations, gradients, workspace) in DNN training. Using our tools and methodologies, we make several important observations and recommendations on where future DNN training research and optimization should be focused.},
  keywords={data handling;data structures;learning (artificial intelligence);neural nets;deep neural network training;deep neural networks;DNN-related computation;trained models;image classification networks;primary benchmark;myopic view;benchmark suite;DNN training;representative set;DNN models;machine translation;speech recognition;object detection;adversarial networks;reinforcement learning;extensive performance analysis;deep learning frameworks;multiGPU;performance analysis tools;performance metrics;optimization;TBD;machine learning applications;hardware configurations;Training;Benchmark testing;Tools;Graphics processing units;Computational modeling;Speech recognition;Hardware},
  doi={10.1109/IISWC.2018.8573476},
  ISSN={},
  month={Sep.},}
