TY  - JOUR
AU  - Schrittwieser, Julian
AU  - Antonoglou, Ioannis
AU  - Hubert, Thomas
AU  - Simonyan, Karen
AU  - Sifre, Laurent
AU  - Schmitt, Simon
AU  - Guez, Arthur
AU  - Lockhart, Edward
AU  - Hassabis, Demis
AU  - Graepel, Thore
AU  - Lillicrap, Timothy
AU  - Silver, David
PY  - 2020
DA  - 2020/12/01
TI  - Mastering Atari, Go, chess and shogi by planning with a learned model
JO  - Nature
SP  - 604
EP  - 609
VL  - 588
IS  - 7839
AB  - Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3—the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4—the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi—canonical environments for high-performance planning—the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.
SN  - 1476-4687
UR  - https://doi.org/10.1038/s41586-020-03051-4
DO  - 10.1038/s41586-020-03051-4
ID  - Schrittwieser2020
ER  - 
