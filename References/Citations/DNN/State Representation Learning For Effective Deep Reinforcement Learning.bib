@INPROCEEDINGS{9102924,
  author={J. {Zhao} and W. {Zhou} and T. {Zhao} and Y. {Zhou} and H. {Li}},
  booktitle={2020 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={State Representation Learning For Effective Deep Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  abstract={Recent years have witnessed the great success of deep reinforcement learning (DRL) on a variety of vision games. Although DNN has demonstrated strong power in representation learning, such capacity is under-explored in most DRL works whose focus is usually on optimization solvers. In fact, we discover that the state feature learning is the main obstacle for further improvement of DRL algorithms. To address this issue, we propose a new state representation learning scheme with our Adjacent State Consistency Loss (ASC Loss). The loss is defined based on the hypothesis that there are fewer changes between adjacent states than that of far apart ones, since scenes in videos generally evolve smoothly. In this paper, we exploit ASC loss as an assistant of RL loss in the training phase to boost the state feature learning. We conduct evaluation on Atari games and MuJoCo continuous control tasks, which demonstrates that our method is superior to OpenAI baselines.},
  keywords={computer games;computer vision;learning (artificial intelligence);neural nets;OpenAI baselines;MuJoCo continuous control tasks;Atari games;adjacent state consistency loss;optimization solvers;DNN;deep reinforcement learning;state representation learning;vision games;RL loss;ASC loss;DRL algorithms;state feature learning;Representation learning;reinforcement learning},
  doi={10.1109/ICME46284.2020.9102924},
  ISSN={1945-788X},
  month={July},}
