@INPROCEEDINGS{8588788,  author={S. {Srivastava} and A. {Bhatia}}, booktitle={2018 IEEE International Conference on Big Knowledge (ICBK)},  title={On the Learning Capabilities of Recurrent Neural Networks: A Cryptographic Perspective},  year={2018}, volume={}, number={}, pages={162-167}, abstract={It has been proven that Recurrent Neural Networks (RNNs) are Turing Complete, i.e. for any given computable function there exists a finite RNN to compute it. Consequently, researchers have trained Recurrent Neural Networks to learn simple functions like sorting, addition, compression and more recently, even classical cryptographic ciphers such as the Enigma. In this paper, we try to identify the characteristics of functions that make them easy or difficult for the RNN to learn. We look at functions from a cryptographic point of view by studying the ways in which the output depends on the input. We use cryptographic parameters (confusion and diffusion) for determining the strength of a cipher and quantify this dependence to show that a strong correlation exists between the learning capability of an RNN and the function's cryptographic parameters.}, keywords={cryptography;learning (artificial intelligence);recurrent neural nets;learning capability;recurrent neural networks;cryptographic perspective;simple functions;classical cryptographic ciphers;RNN;computable function;Ciphers;Task analysis;Recurrent neural networks;Computer architecture;Training;Recurrent Neural Networks;Cryptographic Ciphers;Confusion Parameter;Diffusion Parameter}, doi={10.1109/ICBK.2018.00029}, ISSN={}, month={Nov},}
