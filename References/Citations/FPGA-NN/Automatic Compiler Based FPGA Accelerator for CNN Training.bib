@INPROCEEDINGS{8892195,
  author={S. {Kolala Venkataramanaiah} and Y. {Ma} and S. {Yin} and E. {Nurvithadhi} and A. {Dasu} and Y. {Cao} and J. {Seo}},
  booktitle={2019 29th International Conference on Field Programmable Logic and Applications (FPL)}, 
  title={Automatic Compiler Based FPGA Accelerator for CNN Training}, 
  year={2019},
  volume={},
  number={},
  pages={166-172},
  abstract={Training of convolutional neural networks (CNNs) on embedded platforms to support on-device learning is earning vital importance in recent days. Designing flexible training hardware is much more challenging than inference hardware, due to design complexity and large computation/memory requirement. In this work, we present an automatic compiler based FPGA accelerator with 16-bit fixed-point precision for complete CNN training, including Forward Pass (FP), Backward Pass (BP) and Weight Update (WU). We implemented an optimized RTL library to perform training-specific tasks and developed an RTL compiler to automatically generate FPGA-synthesizable RTL based on user-defined constraints. We present a new cyclic weight storage/access scheme for on-chip BRAM and off-chip DRAM to efficiently implement non-transpose and transpose operations during FP and BP phases, respectively. Representative CNNs for CIFAR-10 dataset are implemented and trained on Intel Stratix 10 GX FPGA using proposed hardware architecture, demonstrating up to 479 GOPS performance.},
  keywords={convolutional neural nets;DRAM chips;field programmable gate arrays;learning (artificial intelligence);program compilers;automatic compiler;FPGA accelerator;convolutional neural networks;embedded platforms;on-device learning;designing flexible training hardware;inference hardware;16-bit fixed-point precision;complete CNN training;forward pass;FP;backward pass;weight update;optimized RTL library;training-specific tasks;RTL compiler;FPGA-synthesizable RTL;user-defined constraints;BP phases;representative CNNs;Intel Stratix 10 GX FPGA;hardware architecture;GOPS performance;Training;Field programmable gate arrays;Kernel;Hardware;Libraries;Task analysis;Random access memory;Convolution neural networks, neural network training, back-propagation, hardware accelerator, FPGA},
  doi={10.1109/FPL.2019.00034},
  ISSN={1946-1488},
  month={Sep.},}
