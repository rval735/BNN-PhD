@INPROCEEDINGS{8641739,
  author={S. {Dey} and D. {Chen} and Z. {Li} and S. {Kundu} and K. {Huang} and K. M. {Chugg} and P. A. {Beerel}},
  booktitle={2018 International Conference on ReConFigurable Computing and FPGAs (ReConFig)}, 
  title={A Highly Parallel FPGA Implementation of Sparse Neural Network Training}, 
  year={2018},
  volume={},
  number={},
  pages={1-4},
  abstract={This paper describes the development of an FPGA implementation of a parallel and reconfigurable architecture for sparse neural networks, capable of on-chip training and inference. The network connectivity uses pre-determined, structured sparsity to significantly reduce complexity by lowering memory and computational requirements. The architecture uses a notion of edge-processing, leading to efficient pipelining and parallelization. Moreover, the device can be reconfigured to trade off resource utilization with training time to fit networks and datasets of varying sizes. The combined effects of complexity reduction and easy reconfigurability enable greater exploration of network hyperparameters and structures on-chip. As proof of concept, we show implementation results on an Artix-7 FPGA.},
  keywords={field programmable gate arrays;neural nets;reconfigurable architectures;inference;network connectivity;structured sparsity;computational requirements;edge-processing;parallelization;training time;datasets;complexity reduction;network hyperparameters;structures on-chip;Artix-7 FPGA;sparse neural network training;parallel architecture;reconfigurable architecture;sparse neural networks;on-chip training;pipelining;reconfigurability;parallel FPGA implementation;Junctions;Training;Hardware;Neurons;Field programmable gate arrays;Artificial neural networks;Computer architecture;Machine learning;Neural networks;Sparse neural networks;FPGA Training;Parallelism;Pipelining},
  doi={10.1109/RECONFIG.2018.8641739},
  ISSN={2640-0472},
  month={Dec},}
